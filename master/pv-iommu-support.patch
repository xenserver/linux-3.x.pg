diff --git a/arch/x86/include/asm/xen/hypercall.h b/arch/x86/include/asm/xen/hypercall.h
index ed335cb..08520a5 100644
--- a/arch/x86/include/asm/xen/hypercall.h
+++ b/arch/x86/include/asm/xen/hypercall.h
@@ -47,6 +47,7 @@
 #include <xen/interface/xen.h>
 #include <xen/interface/sched.h>
 #include <xen/interface/physdev.h>
+#include <xen/interface/pv-iommu.h>
 #include <xen/interface/platform.h>
 #include <xen/interface/xen-mca.h>
 #include <xen/interface/domctl.h>
@@ -350,6 +351,12 @@ HYPERVISOR_multicall(void *call_list, int nr_calls)
 }
 
 static inline int
+HYPERVISOR_iommu_op(void *uop, unsigned int count)
+{
+       return _hypercall2(int, iommu_op, uop, count);
+}
+
+static inline int
 HYPERVISOR_update_va_mapping(unsigned long va, pte_t new_val,
 			     unsigned long flags)
 {
diff --git a/arch/x86/xen/p2m.c b/arch/x86/xen/p2m.c
index 9932704..57c3066ce 100644
--- a/arch/x86/xen/p2m.c
+++ b/arch/x86/xen/p2m.c
@@ -178,6 +178,9 @@
 #include "multicalls.h"
 #include "xen-ops.h"
 
+extern int xen_iommu_map_page(unsigned long pfn, unsigned long mfn);
+extern int xen_iommu_unmap_page(unsigned long pfn);
+
 unsigned long xen_max_p2m_pfn __read_mostly;
 
 /* Placeholders for holes in the address space */
@@ -913,7 +916,6 @@ bool set_phys_to_machine(unsigned long pfn, unsigned long mfn)
 		if (!__set_phys_to_machine(pfn, mfn))
 			return false;
 	}
-
 	return true;
 }
 
diff --git a/arch/x86/xen/pci-swiotlb-xen.c b/arch/x86/xen/pci-swiotlb-xen.c
index 98eef08..6eca385 100644
--- a/arch/x86/xen/pci-swiotlb-xen.c
+++ b/arch/x86/xen/pci-swiotlb-xen.c
@@ -2,21 +2,54 @@
 
 #include <linux/dma-mapping.h>
 #include <linux/pci.h>
+#include <linux/kthread.h>
 #include <xen/swiotlb-xen.h>
 
 #include <asm/xen/hypervisor.h>
 #include <xen/xen.h>
 #include <asm/iommu_table.h>
+#include <asm/xen/hypercall.h>
+#include <xen/interface/memory.h>
 
 
 #include <asm/xen/swiotlb-xen.h>
+#include <asm/xen/page.h>
 #ifdef CONFIG_X86_64
 #include <asm/iommu.h>
 #include <asm/dma.h>
 #endif
 #include <linux/export.h>
 
+#define IOMMU_BATCH_SIZE 128
+
+extern unsigned long max_pfn;
+dma_addr_t pv_iommu_1_to_1_offset;
+EXPORT_SYMBOL(pv_iommu_1_to_1_offset);
+
+bool pv_iommu_1_to_1_setup_complete;
+EXPORT_SYMBOL(pv_iommu_1_to_1_setup_complete);
+
 int xen_swiotlb __read_mostly;
+static struct pv_iommu_op iommu_ops[IOMMU_BATCH_SIZE];
+static struct task_struct *xen_pv_iommu_setup_task;
+
+int xen_pv_iommu_map_sg_attrs(struct device *hwdev, struct scatterlist *sgl,
+			 int nelems, enum dma_data_direction dir,
+			 struct dma_attrs *attrs);
+
+dma_addr_t xen_pv_iommu_map_page(struct device *dev, struct page *page,
+				unsigned long offset, size_t size,
+				enum dma_data_direction dir,
+				struct dma_attrs *attrs);
+
+void *xen_pv_iommu_alloc_coherent(struct device *hwdev, size_t size,
+					dma_addr_t *dma_handle, gfp_t flags,
+					struct dma_attrs *attrs);
+
+void xen_pv_iommu_free_coherent(struct device *dev, size_t size,
+				      void *vaddr, dma_addr_t dma_addr,
+				      struct dma_attrs *attrs);
+
 
 static struct dma_map_ops xen_swiotlb_dma_ops = {
 	.mapping_error = xen_swiotlb_dma_mapping_error,
@@ -34,6 +67,131 @@ static struct dma_map_ops xen_swiotlb_dma_ops = {
 	.get_required_mask = xen_swiotlb_get_required_mask,
 };
 
+static struct dma_map_ops xen_pv_iommu_dma_ops = {
+	.mapping_error = swiotlb_dma_mapping_error,
+	.alloc = xen_pv_iommu_alloc_coherent,
+	.free = xen_pv_iommu_free_coherent,
+	.map_sg = xen_pv_iommu_map_sg_attrs,
+	.map_page = xen_pv_iommu_map_page,
+	.unmap_sg = swiotlb_unmap_sg_attrs,
+	.unmap_page = swiotlb_unmap_page,
+	.get_required_mask = xen_swiotlb_get_required_mask,
+	.sync_single_for_cpu = swiotlb_sync_single_for_cpu,
+	.sync_single_for_device = swiotlb_sync_single_for_device,
+	.sync_sg_for_cpu = swiotlb_sync_sg_for_cpu,
+	.sync_sg_for_device = swiotlb_sync_sg_for_device,
+};
+
+int xen_iommu_map_page(unsigned long bfn, unsigned long mfn)
+{
+	struct pv_iommu_op iommu_op;
+	int rc;
+
+	iommu_op.u.map_page.bfn = bfn;
+	iommu_op.u.map_page.gfn = mfn;
+	iommu_op.flags = IOMMU_OP_readable | IOMMU_OP_writeable | IOMMU_MAP_OP_no_ref_cnt;
+	iommu_op.subop_id = IOMMUOP_map_page;
+	rc = HYPERVISOR_iommu_op(&iommu_op, 1);
+	if (rc < 0) {
+		printk("Failed to setup IOMMU mapping for gpfn 0x%lx, mfn 0x%lx, err %d\n",
+				bfn, mfn, rc);
+		return rc;
+	}
+	return iommu_op.status;
+}
+EXPORT_SYMBOL_GPL(xen_iommu_map_page);
+
+int xen_iommu_unmap_page(unsigned long bfn)
+{
+	struct pv_iommu_op iommu_op;
+	int rc;
+
+	iommu_op.u.unmap_page.bfn = bfn;
+	iommu_op.flags = IOMMU_MAP_OP_no_ref_cnt;
+	iommu_op.subop_id = IOMMUOP_unmap_page;
+	rc = HYPERVISOR_iommu_op(&iommu_op, 1);
+	if (rc < 0) {
+		printk("Failed to remove IOMMU mapping for gpfn 0x%lx, err %d\n", bfn, rc);
+		return rc;
+	}
+	return iommu_op.status;
+}
+
+int xen_iommu_batch(struct pv_iommu_op *iommu_ops, int count)
+{
+	int rc;
+
+	rc = HYPERVISOR_iommu_op(iommu_ops, count);
+	if (rc < 0) {
+		printk("Failed to batch IOMMU map, err %d\n", rc);
+	}
+	return rc;
+}
+EXPORT_SYMBOL_GPL(xen_iommu_batch);
+
+static int pv_iommu_setup(void *data)
+{
+	int i, count = 0;
+	u64 max_host_mfn = 0;
+
+	if (pv_iommu_1_to_1_setup_complete)
+		return 0;
+
+	max_host_mfn = HYPERVISOR_memory_op(XENMEM_maximum_ram_page, NULL);
+
+	/* Remove Xen setup 1-1 mapping */
+	for (i=0; i < max_host_mfn; i++)
+	{
+		if (get_phys_to_machine(i) == INVALID_P2M_ENTRY)
+		{
+			iommu_ops[count].u.unmap_page.bfn = i;
+			iommu_ops[count].flags = IOMMU_MAP_OP_no_ref_cnt;
+			iommu_ops[count].subop_id = IOMMUOP_unmap_page;
+			count++;
+		}
+		if (count == IOMMU_BATCH_SIZE)
+		{
+			count = 0;
+			if (xen_iommu_batch(iommu_ops,
+						IOMMU_BATCH_SIZE))
+				panic("Xen PV-IOMMU: failed to remove legacy"
+						" mappings\n");
+			cond_resched();
+		}
+	}
+	if (count && xen_iommu_batch(iommu_ops, count))
+		panic("Xen PV-IOMMU: failed to remove legacy mappings\n");
+
+	count = 0;
+	/* Create offset IOMMU mapping of all of host RAM, start from
+	 * top of host RAM */
+	for (i=0; i < max_host_mfn; i++)
+	{
+		iommu_ops[count].u.map_page.bfn = max_host_mfn + i;
+		iommu_ops[count].u.map_page.gfn = i;
+		iommu_ops[count].flags = IOMMU_OP_readable | IOMMU_OP_writeable |
+					IOMMU_MAP_OP_no_ref_cnt |
+					IOMMU_MAP_OP_add_m2b;
+		iommu_ops[count].subop_id = IOMMUOP_map_page;
+		count++;
+		if (count == IOMMU_BATCH_SIZE)
+		{
+			count = 0;
+			if (xen_iommu_batch(iommu_ops,
+						IOMMU_BATCH_SIZE))
+				panic("Xen PV-IOMMU: failed to setup"
+					" 1-1 mapping\n");
+			cond_resched();
+		}
+	}
+	if (count && xen_iommu_batch(iommu_ops, count))
+		panic("Xen PV-IOMMU: failed to setup 1-1 mappings");
+
+	printk(KERN_INFO "XEN-PV-IOMMU - completed setting up 1-1 mapping\n");
+	pv_iommu_1_to_1_setup_complete = true;
+	return 0;
+}
+
 /*
  * pci_xen_swiotlb_detect - set xen_swiotlb to 1 if necessary
  *
@@ -42,10 +200,78 @@ static struct dma_map_ops xen_swiotlb_dma_ops = {
  */
 int __init pci_xen_swiotlb_detect(void)
 {
-
 	if (!xen_pv_domain())
 		return 0;
 
+	if (xen_initial_domain()){
+		int i, j, count = 0;
+		u64 max_host_mfn = 0;
+		struct pv_iommu_op iommu_op;
+		int rc;
+
+		iommu_op.flags = 0;
+		iommu_op.status = 0;
+		iommu_op.subop_id = IOMMUOP_query_caps;
+		rc = HYPERVISOR_iommu_op(&iommu_op, 1);
+
+		if (rc || !(iommu_op.flags & IOMMU_QUERY_map_cap))
+			goto no_pv_iommu;
+
+		max_host_mfn = HYPERVISOR_memory_op(XENMEM_maximum_ram_page, NULL);
+		printk("Max host RAM MFN is 0x%llx\n",max_host_mfn);
+		printk("max_pfn is 0x%lx\n",max_pfn);
+
+		/* Setup 1-1 mapping of GPFN to MFN */
+		for (i=0; i < max_host_mfn; i++)
+		{
+			if ((get_phys_to_machine(i) != INVALID_P2M_ENTRY)
+					&& (i != pfn_to_mfn(i)))
+			{
+				iommu_ops[count].u.unmap_page.bfn = i;
+				iommu_ops[count].flags = IOMMU_MAP_OP_no_ref_cnt;
+				iommu_ops[count].subop_id = IOMMUOP_unmap_page;
+				count++;
+				iommu_ops[count].u.map_page.bfn = i;
+				iommu_ops[count].u.map_page.gfn = pfn_to_mfn(i);
+				iommu_ops[count].flags = IOMMU_OP_readable |
+							IOMMU_OP_writeable |
+							IOMMU_MAP_OP_no_ref_cnt;
+				iommu_ops[count].subop_id = IOMMUOP_map_page;
+				count++;
+			}
+			if (count == IOMMU_BATCH_SIZE)
+			{
+				count = 0;
+				if (xen_iommu_batch(iommu_ops,
+							IOMMU_BATCH_SIZE))
+					goto remove_iommu_mappings;
+				for (j = 1; j < IOMMU_BATCH_SIZE; j +=2)
+				{
+					if ( iommu_ops[j].status )
+						printk("Iommu op %d went wrong,"
+								" subop id %d, bfn 0x%lx, gfn 0x%lx\n, err %d, flags 0x%x\n", j, iommu_ops[j].subop_id, 
+							       	iommu_ops[j].u.map_page.bfn, iommu_ops[j].u.map_page.gfn, iommu_ops[j].status, iommu_ops[j].flags );
+
+				}
+			}
+
+		}
+		if (count && xen_iommu_batch(iommu_ops, count))
+			goto remove_iommu_mappings;
+
+		/* Setup 1-1 host RAM offset location and hook the PV IOMMU DMA ops */
+		pv_iommu_1_to_1_offset = max_host_mfn << PAGE_SHIFT;
+		xen_swiotlb = 0;
+
+		printk("Using GPFN IOMMU mode, 1-to-1 offset is 0x%llx\n",
+				pv_iommu_1_to_1_offset);
+		return 1;
+
+remove_iommu_mappings:
+		BUG();
+	}
+
+no_pv_iommu:
 	/* If running as PV guest, either iommu=soft, or swiotlb=force will
 	 * activate this IOMMU. If running as PV privileged, activate it
 	 * irregardless.
@@ -70,6 +296,15 @@ int __init pci_xen_swiotlb_detect(void)
 	return xen_swiotlb;
 }
 
+void __init pci_xen_pv_iommu_late_init(void)
+{
+	if (pv_iommu_1_to_1_offset){
+		xen_pv_iommu_setup_task = kthread_create(pv_iommu_setup,
+				NULL,"xen-pv-iommu-setup");
+		wake_up_process(xen_pv_iommu_setup_task);
+	}
+}
+
 void __init pci_xen_swiotlb_init(void)
 {
 	if (xen_swiotlb) {
@@ -79,6 +314,16 @@ void __init pci_xen_swiotlb_init(void)
 		/* Make sure ACS will be enabled */
 		pci_request_acs();
 	}
+
+	/* Start the native swiotlb */
+	if (pv_iommu_1_to_1_offset) {
+		dma_ops = &xen_pv_iommu_dma_ops;
+		pci_request_acs();
+		swiotlb_init(0);
+		printk(KERN_INFO "XEN-PV-IOMMU: "
+		       "Using software bounce buffering for IO on 32bit DMA devices (SWIOTLB)\n");
+		swiotlb_print_info();
+	}
 }
 
 int pci_xen_swiotlb_init_late(void)
@@ -103,4 +348,4 @@ EXPORT_SYMBOL_GPL(pci_xen_swiotlb_init_late);
 IOMMU_INIT_FINISH(pci_xen_swiotlb_detect,
 		  NULL,
 		  pci_xen_swiotlb_init,
-		  NULL);
+		  pci_xen_pv_iommu_late_init);
diff --git a/arch/x86/xen/xen-head.S b/arch/x86/xen/xen-head.S
index 7faed58..8df5ef1 100644
--- a/arch/x86/xen/xen-head.S
+++ b/arch/x86/xen/xen-head.S
@@ -82,6 +82,8 @@ NEXT_HYPERCALL(arch_3)
 NEXT_HYPERCALL(arch_4)
 NEXT_HYPERCALL(arch_5)
 NEXT_HYPERCALL(arch_6)
+NEXT_HYPERCALL(arch_7)
+NEXT_HYPERCALL(iommu_op) /* 56 */
 	.balign PAGE_SIZE
 .popsection
 
diff --git a/drivers/xen/Makefile b/drivers/xen/Makefile
index 57d91f4..8427ca4 100644
--- a/drivers/xen/Makefile
+++ b/drivers/xen/Makefile
@@ -31,6 +31,7 @@ obj-$(CONFIG_XEN_SYS_HYPERVISOR)	+= sys-hypervisor.o
 obj-$(CONFIG_XEN_PVHVM)			+= platform-pci.o
 obj-$(CONFIG_XEN_TMEM)			+= tmem.o
 obj-$(CONFIG_SWIOTLB_XEN)		+= swiotlb-xen.o
+obj-$(CONFIG_SWIOTLB_XEN)		+= pv-iommu-xen.o
 obj-$(CONFIG_XEN_MCE_LOG)		+= mcelog.o
 obj-$(CONFIG_XEN_PCIDEV_BACKEND)	+= xen-pciback/
 obj-$(CONFIG_XEN_PRIVCMD)		+= xen-privcmd.o
diff --git a/drivers/xen/balloon.c b/drivers/xen/balloon.c
index 898ece41..d20718c 100644
--- a/drivers/xen/balloon.c
+++ b/drivers/xen/balloon.c
@@ -69,6 +69,10 @@
 #include <xen/features.h>
 #include <xen/page.h>
 
+extern int xen_iommu_map_page(unsigned long pfn, unsigned long mfn);
+extern int xen_iommu_unmap_page(unsigned long pfn);
+extern dma_addr_t pv_iommu_1_to_1_offset;
+
 /*
  * balloon_process() state:
  *
@@ -320,6 +324,15 @@ static enum bp_state reserve_additional_memory(void)
 
 static void xen_online_page(struct page *page)
 {
+#ifdef CONFIG_XEN_HAVE_PVMMU
+	/*
+	 * Clear any existing IOMMU mappings of the BFN (== PFN) for
+	 * this page, so the correct IOMMU mapping can be created when
+	 * the page is returned.
+	 */
+	if (pv_iommu_1_to_1_offset)
+		xen_iommu_unmap_page(page_to_pfn(page));
+#endif
 	__online_page_set_limits(page);
 
 	mutex_lock(&balloon_mutex);
@@ -390,6 +403,8 @@ static enum bp_state increase_reservation(unsigned long nr_pages)
 		return BP_EAGAIN;
 
 	for (i = 0; i < rc; i++) {
+		int ret;
+
 		page = balloon_retrieve(false);
 		BUG_ON(page == NULL);
 
@@ -397,10 +412,26 @@ static enum bp_state increase_reservation(unsigned long nr_pages)
 
 		set_phys_to_machine(pfn, frame_list[i]);
 
+		/*
+		 * Update the IOMMU mapping for this BFN (== PFN) now
+		 * that we have the new MFN.
+		 *
+		 * If this fails, leak the page as its not safe to use
+		 * it (any DMA will go somewhere unexpected causing
+		 * memory corruption).
+		 */
+		if (pv_iommu_1_to_1_offset) {
+			ret = xen_iommu_map_page(pfn, frame_list[i]);
+			if (ret < 0) {
+				pr_err("leaking pfn %lx (iommu map failed: %d)\n",
+				       pfn, ret);
+				continue;
+			}
+		}
+
 #ifdef CONFIG_XEN_HAVE_PVMMU
 		/* Link back into the page tables if not highmem. */
 		if (xen_pv_domain() && !PageHighMem(page)) {
-			int ret;
 			ret = HYPERVISOR_update_va_mapping(
 				(unsigned long)__va(pfn << PAGE_SHIFT),
 				mfn_pte(frame_list[i], PAGE_KERNEL),
@@ -466,6 +497,8 @@ static enum bp_state decrease_reservation(unsigned long nr_pages, gfp_t gfp)
 #endif
 		if (!xen_feature(XENFEAT_auto_translated_physmap)) {
 			__set_phys_to_machine(pfn, INVALID_P2M_ENTRY);
+			if (pv_iommu_1_to_1_offset)
+				xen_iommu_unmap_page(pfn);
 		}
 		put_balloon_scratch_page();
 
diff --git a/drivers/xen/ioemu.c b/drivers/xen/ioemu.c
index 369b897..7cb2584 100644
--- a/drivers/xen/ioemu.c
+++ b/drivers/xen/ioemu.c
@@ -51,4 +51,49 @@ int xen_ioemu_inject_msi(domid_t domid, uint64_t addr, uint32_t data)
 
 	return HYPERVISOR_hvm_op(HVMOP_inject_msi, &xinfo);
 }
+
+/**
+ * xen_ioemu_map_foreign_gfn_to_bfn: Returns the BFN's corresponding to GFN's.
+ * @pv_iommu_ops: pv_iommu_ops contains the struct_ map_foreign_page
+ * that will be used for lookup for BFN.
+ * @count: count of struct pv_iommu_ops.
+ *
+ * Its a wrapper function for getting BFN from GFN using IOMMU hypercall.
+*/
+int xen_ioemu_map_foreign_gfn_to_bfn(struct pv_iommu_op *ops, int count)
+{
+        int i;
+        int rc = 0;
+        for (i = 0; i < count; i++)
+        {
+                ops[i].subop_id = IOMMUOP_lookup_foreign_page;
+                ops[i].flags |= IOMMU_OP_writeable;
+        }
+        rc = HYPERVISOR_iommu_op(ops, count);
+        return rc;
+}
+
+/**
+ * xen_ioemu_unmap_foreign_gfn_to_bfn: Unmap BFN's corresponding to GFN's.
+ * @pv_iommu_ops: pv_iommu_ops contains the struct unmap_foreign_page
+ * that will be used to unmap BFNs.
+ * @count: count of struct pv_iommu_ops.
+ *
+ * Its a wrapper function to unmap foreign GFN's to BFN's .
+*/
+int xen_ioemu_unmap_foreign_gfn_to_bfn(struct pv_iommu_op *ops, int count)
+{
+        int i;
+        int rc = 0;
+        for (i = 0; i < count; i++)
+        {
+                ops[i].subop_id = IOMMUOP_unmap_foreign_page;
+        }
+        rc = HYPERVISOR_iommu_op(ops, count);
+        return rc;
+
+
+}
 EXPORT_SYMBOL(xen_ioemu_inject_msi);
+EXPORT_SYMBOL(xen_ioemu_map_foreign_gfn_to_bfn);
+EXPORT_SYMBOL(xen_ioemu_unmap_foreign_gfn_to_bfn);
diff --git a/drivers/xen/pv-iommu-xen.c b/drivers/xen/pv-iommu-xen.c
new file mode 100644
index 0000000..c9c1393
--- /dev/null
+++ b/drivers/xen/pv-iommu-xen.c
@@ -0,0 +1,158 @@
+/*
+ *  Copyright 2014
+ *  by Malcolm Crossley <malcolm.crossley@citrix.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License v2.0 as published by
+ * the Free Software Foundation
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ *
+ */
+
+#include <linux/bootmem.h>
+#include <linux/dma-mapping.h>
+#include <linux/export.h>
+#include <xen/swiotlb-xen.h>
+#include <asm/swiotlb.h>
+#include <xen/page.h>
+#include <xen/xen-ops.h>
+#include <xen/hvc-console.h>
+
+#include <trace/events/swiotlb.h>
+
+extern int xen_iommu_map_page(unsigned long pfn, unsigned long mfn);
+extern bool pv_iommu_1_to_1_setup_complete;
+
+extern phys_addr_t io_tlb_start, io_tlb_end;
+
+void *xen_pv_iommu_alloc_coherent(struct device *hwdev, size_t size,
+					dma_addr_t *dma_handle, gfp_t flags,
+					struct dma_attrs *attrs)
+{
+	void *vaddr;
+
+	vaddr = dma_generic_alloc_coherent(hwdev, size, dma_handle, flags,
+					   attrs);
+	if (vaddr)
+		return vaddr;
+
+	return swiotlb_alloc_coherent(hwdev, size, dma_handle, flags);
+}
+EXPORT_SYMBOL_GPL(xen_pv_iommu_alloc_coherent);
+
+void xen_pv_iommu_free_coherent(struct device *dev, size_t size,
+				      void *vaddr, dma_addr_t dma_addr,
+				      struct dma_attrs *attrs)
+{
+	swiotlb_free_coherent(dev, size, vaddr, dma_addr);
+}
+EXPORT_SYMBOL_GPL(xen_pv_iommu_free_coherent);
+
+
+dma_addr_t xen_pv_iommu_get_foreign_addr(unsigned long p2m_entry)
+{
+	dma_addr_t phys;
+	unsigned long mfn = p2m_entry & ~FOREIGN_FRAME_BIT;
+	/* If 1-1 has not completed being setup then map this page now */
+	if (unlikely(!pv_iommu_1_to_1_setup_complete))
+		xen_iommu_map_page(mfn + (pv_iommu_1_to_1_offset >> PAGE_SHIFT),
+					mfn);
+
+	phys = (mfn << PAGE_SHIFT) + pv_iommu_1_to_1_offset;
+	return phys;
+}
+
+/*
+ * Map a single buffer of the indicated size for DMA in streaming mode.  The
+ * physical address to use is returned.
+ *
+ * PV IOMMU version detects Xen foreign pages and use's the original MFN offset
+ * into previously setup IOMMU 1-to-1 offset mapping of host memory
+ */
+dma_addr_t xen_pv_iommu_map_page(struct device *dev, struct page *page,
+				unsigned long offset, size_t size,
+				enum dma_data_direction dir,
+				struct dma_attrs *attrs)
+{
+	unsigned long p2m_entry = get_phys_to_machine(page_to_pfn(page));
+
+	if (p2m_entry & FOREIGN_FRAME_BIT) {
+		dma_addr_t phys = xen_pv_iommu_get_foreign_addr(p2m_entry) + offset;
+		/* Check if device can DMA to 1-1 mapped foreign address */
+		if (dma_capable(dev, phys, size)) {
+			return phys;
+		} else {
+			phys_addr_t map = swiotlb_tbl_map_single(dev, io_tlb_start,
+							page_to_phys(page) + offset,
+							size, dir);
+			trace_swiotlb_bounced(dev, phys, size, 0);
+			return phys_to_dma(dev, map);
+		}
+	}
+
+	return swiotlb_map_page(dev, page, offset, size, dir, attrs);
+}
+EXPORT_SYMBOL_GPL(xen_pv_iommu_map_page);
+
+/*
+ * Map a set of buffers described by scatterlist in streaming mode for DMA.
+ * This is the scatter-gather version of the above xen_swiotlb_map_page
+ * interface.  Here the scatter gather list elements are each tagged with the
+ * appropriate dma address and length.  They are obtained via
+ * sg_dma_{address,length}(SG).
+ *
+ * NOTE: An implementation may be able to use a smaller number of
+ *       DMA address/length pairs than there are SG table elements.
+ *       (for example via virtual mapping capabilities)
+ *       The routine returns the number of addr/length pairs actually
+ *       used, at most nents.
+ *
+ * PV IOMMU version detects Xen foreign pages and use's the original MFN offset
+ * into previously setup IOMMU 1-to-1 offset mapping of host memory
+ *
+ */
+int
+xen_pv_iommu_map_sg_attrs(struct device *hwdev, struct scatterlist *sgl,
+			 int nelems, enum dma_data_direction dir,
+			 struct dma_attrs *attrs)
+{
+	struct scatterlist *sg;
+	int i;
+
+	for_each_sg(sgl, sg, nelems, i) {
+		phys_addr_t paddr = sg_phys(sg);
+		dma_addr_t dev_addr = phys_to_dma(hwdev, paddr);
+		unsigned long p2m_entry = get_phys_to_machine(PFN_DOWN(paddr));
+		if (p2m_entry & FOREIGN_FRAME_BIT)
+			dev_addr = xen_pv_iommu_get_foreign_addr(p2m_entry) +
+					(paddr & ~PAGE_MASK);
+
+		/* Check if device can DMA to bus address */
+		if (!dma_capable(hwdev, dev_addr, sg->length)){
+			phys_addr_t map = swiotlb_tbl_map_single(hwdev, io_tlb_start,
+						paddr, sg->length, dir);
+			trace_swiotlb_bounced(hwdev, dev_addr, sg->length, 0);
+			if (map == SWIOTLB_MAP_ERROR) {
+				/* Don't panic here, we expect map_sg users
+				   to do proper error handling. */
+				swiotlb_unmap_sg_attrs(hwdev, sgl, i, dir,
+						       attrs);
+				sgl[0].dma_length = 0;
+				return 0;
+			}
+			sg->dma_address = phys_to_dma(hwdev, map);
+
+		} else {
+			sg->dma_address = dev_addr;
+		}
+		sg->dma_length = sg->length;
+	}
+	return nelems;
+
+}
+EXPORT_SYMBOL_GPL(xen_pv_iommu_map_sg_attrs);
diff --git a/include/xen/interface/iommu.h b/include/xen/interface/iommu.h
new file mode 100644
index 0000000..ffe527c
--- /dev/null
+++ b/include/xen/interface/iommu.h
@@ -0,0 +1,38 @@
+/*
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to
+ * deal in the Software without restriction, including without limitation the
+ * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
+ * sell copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#ifndef __XEN_PUBLIC_IOMMU_H__
+#define __XEN_PUBLIC_IOMMU_H__
+
+#define IOMMUOP_map_page          1
+#define IOMMUOP_unmap_page        2
+
+
+struct iommu_map_op {
+    /* IN */
+    uint64_t gmfn;
+    uint64_t mfn;
+    uint32_t flags;
+    /* OUT */
+    int16_t status;
+};
+typedef struct iommu_map_op iommu_map_op_t;
+
+#endif
diff --git a/include/xen/interface/pv-iommu.h b/include/xen/interface/pv-iommu.h
new file mode 100644
index 0000000..0cc4ecc
--- /dev/null
+++ b/include/xen/interface/pv-iommu.h
@@ -0,0 +1,90 @@
+/*
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to
+ * deal in the Software without restriction, including without limitation the
+ * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
+ * sell copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#ifndef __XEN_PUBLIC_PV_IOMMU_H__
+#define __XEN_PUBLIC_PV_IOMMU_H__
+
+#define IOMMUOP_query_caps            1
+#define IOMMUOP_map_page              2
+#define IOMMUOP_unmap_page            3
+#define IOMMUOP_map_foreign_page      4
+#define IOMMUOP_lookup_foreign_page   5
+#define IOMMUOP_unmap_foreign_page    6
+
+struct pv_iommu_op {
+    uint16_t subop_id;
+
+#define IOMMU_page_order (0xf1 << 10)
+#define IOMMU_get_page_order(flags) ((flags & IOMMU_page_order) >> 10)
+#define IOMMU_QUERY_map_cap (1 << 0)
+#define IOMMU_QUERY_map_all_mfns (1 << 1)
+#define IOMMU_OP_readable (1 << 0)
+#define IOMMU_OP_writeable (1 << 1)
+#define IOMMU_MAP_OP_no_ref_cnt (1 << 2)
+#define IOMMU_MAP_OP_add_m2b (1 << 3)
+#define IOMMU_UNMAP_OP_remove_m2b (1 << 0)
+    uint16_t flags;
+    int32_t status;
+
+    union {
+        struct {
+            uint64_t bfn;
+            uint64_t gfn;
+        } map_page;
+
+        struct {
+            uint64_t bfn;
+        } unmap_page;
+
+        struct {
+            uint64_t bfn;
+            uint64_t gfn;
+            uint16_t domid;
+            uint16_t ioserver;
+        } map_foreign_page;
+
+        struct {
+            uint64_t bfn;
+            uint64_t gfn;
+            uint16_t domid;
+            uint16_t ioserver;
+        } lookup_foreign_page;
+
+        struct {
+            uint64_t bfn;
+            uint16_t ioserver;
+        } unmap_foreign_page;
+    } u;
+};
+
+
+typedef struct pv_iommu_op pv_iommu_op_t;
+
+#endif
+
+/*
+ * Local variables:
+ * mode: C
+ * c-file-style: "BSD"
+ * c-basic-offset: 4
+ * tab-width: 4
+ * indent-tabs-mode: nil
+ * End:
+ */
diff --git a/include/xen/interface/xen.h b/include/xen/interface/xen.h
index 679ef1d..856a0aa 100644
--- a/include/xen/interface/xen.h
+++ b/include/xen/interface/xen.h
@@ -71,6 +71,7 @@
 #define __HYPERVISOR_arch_6               54
 #define __HYPERVISOR_arch_7               55
 
+#define __HYPERVISOR_iommu_op             56
 /*
  * VIRTUAL INTERRUPTS
  *
diff --git a/include/xen/ioemu.h b/include/xen/ioemu.h
index 95eb4e7..21ca898 100644
--- a/include/xen/ioemu.h
+++ b/include/xen/ioemu.h
@@ -29,7 +29,9 @@
  */
 #ifndef _XEN_IOEMU_H
 #define _XEN_IOEMU_H
+#include <xen/interface/pv-iommu.h>
 
 int xen_ioemu_inject_msi(domid_t domid, uint64_t addr, uint32_t data);
+int xen_ioemu_map_foreign_gfn_to_bfn(struct pv_iommu_op *ops, int count);
 
 #endif /* #ifndef _XEN_IOEMU_H */
diff --git a/include/xen/swiotlb-xen.h b/include/xen/swiotlb-xen.h
index e08d8bd..ed9dd2e 100644
--- a/include/xen/swiotlb-xen.h
+++ b/include/xen/swiotlb-xen.h
@@ -58,4 +58,6 @@ xen_swiotlb_dma_supported(struct device *hwdev, u64 mask);
 extern u64
 xen_swiotlb_get_required_mask(struct device *dev);
 
+extern dma_addr_t pv_iommu_1_to_1_offset;
+
 #endif /* __LINUX_SWIOTLB_XEN_H */
diff --git a/lib/swiotlb.c b/lib/swiotlb.c
index d69ecbe..7942ade 100644
--- a/lib/swiotlb.c
+++ b/lib/swiotlb.c
@@ -59,7 +59,10 @@ int swiotlb_force;
  * swiotlb_tbl_sync_single_*, to see if the memory was in fact allocated by this
  * API.
  */
-static phys_addr_t io_tlb_start, io_tlb_end;
+phys_addr_t io_tlb_start, io_tlb_end;
+
+EXPORT_SYMBOL_GPL(io_tlb_start);
+EXPORT_SYMBOL_GPL(io_tlb_end);
 
 /*
  * The number of IO TLB blocks (in groups of 64) between io_tlb_start and
