diff --git a/XenGT-API.txt b/XenGT-API.txt
new file mode 100644
index 0000000..b4ee86c
--- /dev/null
+++ b/XenGT-API.txt
@@ -0,0 +1,573 @@
+------------------- XenGT interface ------------------------
+Index
+1 XenGT udev Interface
+	1.1 Introduction to XenGT udev interface
+	1.2 XenGT udev rules
+		1.2.1 udev rules for monitor hotplug (in & out)
+		1.2.2 udev rules for enabling/disabling VGA mode
+2 Access virtual GFX MMIO space of each VM (Virtual Machine)
+3 XenGT sysfs interface
+	3.1 XenGT sysfs layout
+	3.2 sysfs nodes
+		3.1.1 vgt instance creation
+		3.1.2 Display ownership switching
+		3.1.3 Foreground VM switching
+		3.1.4 Enable/disable rendering context switch (experimental)
+		3.1.5 Display PORT/Monitor management
+			3.1.5.1 monitor edid information
+			3.1.5.2 the virtual pipe information
+			3.1.5.3 virtual port to physical port mapping
+			3.1.5.4 monitor type information
+			3.1.5.5 monitor hotplug and connection information
+		3.1.6 Accessing physical MMIO registers
+		3.1.7 Remaining graphics memory size
+		      and the number of fence regs
+4 XenGT debugfs interface
+	4.1 Global statistics
+	4.2 Per VM statistics
+
+1 XenGT udev Interface
+
+1.1 Introduction to XenGT udev interface
+
+udev interface in XenGT is used to notify the userland daemon (like udevd)
+some events happened. After receiving such events, userland daemon uses defined
+rules or methods to take actions. In XenGT, userland daemon "udevd" is used for
+this purpose. Event matching and handling in udevd are based on rules
+defined in vgt.rules which should be placed under /etc/udev/rules.d.
+
+In vgt.rules, each line defines a matching and handling method of one uevent.
+Take a look at the line for handling uevent "VGT_DETECT_PORT_E" (PORT_E 
+corresponding monitor hotplug-in.
+):
+
+ACTION=="add", KERNEL=="control", ENV{VGT_DETECT_PORT_E}=="1", \
+RUN+="/bin/sh -c '/usr/bin/vgt_mgr --detect-display PORT_E'" 
+
+1.2 XenGT udev rules
+
+1.2.1 udev rules for monitor hotplug (in & out)
+
+XenGT uses uevent for monitor hotplug notification. The rules of handling CRT
+monitor insertion/removal are exemplified below:"
+
+ACTION=="add", KERNEL=="control", ENV{VGT_DETECT_PORT_E}=="1", \
+RUN+="/bin/sh -c '/usr/bin/vgt_mgr --detect-display PORT_E'" 
+
+Dom0 user space can decide its policy while receiving the monitor hotplug 
+events from kernel. In this example, "vgt_mgr" is called, and a virtual hotplug 
+interrupt will be finally injected into virtual machines. The hotplug injection 
+is described in 3.1.5 section.
+
+1.2.2 udev rules for enabling/disabling VGA mode
+
+Another usage for udev in XenGT is to indicate VGA mode changes. The rules for
+such uevents are also divided into matching and handling parts. These two
+rules are listed below:
+
+ACTION=="add", KERNEL=="control", ENV{VGT_ENABLE_VGA}=="1", \
+RUN+="/bin/sh -c 'echo VM_$env{VMID}_enable_VGA_mode >> /tmp/vgt-log'"
+ACTION=="add", KERNEL=="control", ENV{VGT_ENABLE_VGA}=="0", \
+RUN+="/bin/sh -c 'echo VM_$env{VMID}_disable_VGA_mode >> /tmp/vgt-log'"
+
+People can change the field "RUN" to enable their own handling methods.
+
+2 Access virtual GFX MMIO space of each VM (Virtual Machine)
+
+XenGT exposes all per VM virtual registers via debugfs:
+/sys/kernel/debug/vgt/vmX (here X is the VM ID).
+
+3 XenGT sysfs interface
+
+3.1 XenGT sysfs layout
+
+XenGT sysfs interfaces are located under /sys/kernel/vgt, the sub-directory
+"control" contains all the necessary switches for different purposes. After
+a VM is created, a new sub-directory "vmX" ("X" is the VM ID) will be
+created under /sys/kernel/vgt. This "vmX" includes the VM's
+graphics memory information. Detailed information for each entry(a.k.a node)
+is listed below.
+
+In below examples all accesses to these interfaces are via bash command 'echo'
+or 'cat'. This is a quick and easy way to get/control things. But when these
+operations fails, it is impossible to get respective error code by this way.
+
+When accessing sysfs entries, people should use library functions like read()
+or write().
+On success, the returned value of read() or write() indicates how many bytes
+have been transferred.
+On error, the returned value is -1 and the global 'errno' will be set
+appropriately -- this is the only way to figure out what kind of error occurs.
+
+3.2 sysfs nodes
+
+3.1.1 vgt instance creation
+PATH:		/sys/kernel/vgt/control/create_vgt_instance
+
+SYNOPSIS:	echo <vm_id> <low_gm_sz> <high_gm_sz> <fence_sz> <vgt_primary> \
+		> /sys/kernel/vgt/control/create_vgt_instance
+
+		echo <vm_id> <low_gm_sz> <high_gm_sz> <fence_sz>	\
+		> /sys/kernel/vgt/control/create_vgt_instance
+
+		echo -<vm_id>  >  \
+		/sys/kernel/vgt/control/create_vgt_instance
+
+DESCRIPTION:	It is used by QEMU to create a vgt instance when a new VM is
+		booting up. QEMU can also destroy a vgt instance by writing a
+		negative vm_id.	When QEMU uses this interface, actually it
+		uses the write() syscall, instead of "echo".
+PARAMETERS:
+vm_id		The new VM's ID. A valid value should be greater than 0.
+low_gm_sz	The size of CPU visible graphics memory allocated to this VM,
+		in MB (The default is 64MB. NOTES: The Windows7 graphics driver
+		for HSW requires a minimum value of 128MB)
+high_gm_sz	The size of CPU invisible graphics memory allocated to this VM,
+                in MB	(The default is 448MB)
+fence_sz	The number of fence registers assigned to this VM
+		(The default is 4)
+
+RETURNED CODE:	The 'errno' will be set to following values.
+		EINVAL:		If the parameters provided can not be applied
+				because of illegal combination of these
+				parameters.
+		0:		Succeed.
+
+EXAMPLES:	Three typical usage for this interface:
+		1) Create vgt instance of VM1 with XenGT as the primary
+		   VGA card.
+		   echo 1 128 384 4 1 >  \
+		   /sys/kernel/vgt/control/create_vgt_instance
+		2) Create vgt instance of VM1 with XenGT as the secondary
+		   VGA card.
+		   echo 1 128 384 4 0 >  \
+	           /sys/kernel/vgt/control/create_vgt_instance
+		3) Destroy vgt instance of VM 1.
+		   echo -1 > /sys/kernel/vgt/control/create_vgt_instance
+
+3.1.2 Display ownership switching
+PATH:		/sys/kernel/vgt/control/display_owner
+
+SYNOPSIS:	echo <vm_id> > /sys/kernel/vgt/control/display_owner
+
+DESCRIPTION:	It is used to set the current display-owner. The VM which is
+		display owner could have the direct access of display related
+		MMIOs (not including the display surface and cursor related
+		MMIOs). Right now only Dom0 is allowed to be the display owner.
+
+PARAMETERS:
+vm_id		The VM ID of a running VM that's associated with a vgt instance
+
+RETURNED CODE:	The 'errno' will be set to following values.
+		EINVAL:		If the <vm_id> is not an integer or the <vm_id>
+				is the same with current display-owner's
+				vm_id.
+		ENODEV:		Can not find the proper VM
+		EBUSY:		A pending request of display switch has not
+				be done yet.
+		0:		Succeed.
+
+EXAMPLES:	Set VM 1 as the display owner.
+		echo 1 > /sys/kernel/vgt/control/display_owner
+		Set VM 0 (i.e., Dom0) as the display owner.
+		echo 0 > /sys/kernel/vgt/control/display_owner
+
+3.1.3 Foreground VM switching
+PATH:		/sys/kernel/vgt/control/foreground_vm
+
+SYNOPSIS:	echo <vm_id> > /sys/kernel/vgt/control/foreground_vm
+
+DESCRIPTION:	It is used to set the current VM that is visible on display.
+		Notice that the foreground_vm does not necessarily equal to
+		display_owner. A foreground VM can have direct access of
+		display surface and cursor related MMIOs, hence visible on
+		display. Other display related MMIOs will be fully virtualized
+		if it is not display owner.
+
+PARAMETERS:
+vm_id		The VM ID of a running VM that's associated with a vgt instance
+
+RETURNED CODE:	The 'errno' will be set to following values.
+		EINVAL:		If the <vm_id> is not an integer or the <vm_id>
+				is the same with current display-owner's
+				vm_id.
+		ENODEV:		Can not find the proper VM
+		EBUSY:		A pending request of the switch has not be
+				done yet.
+		0:		Succeed.
+
+EXAMPLES:	Set VM 1 as the foreground VM.
+		echo 1 > /sys/kernel/vgt/control/foreground_vm
+		Set VM 0 (i.e., Dom0) as the foreground VM.
+		echo 0 > /sys/kernel/vgt/control/foreground_vm
+
+3.1.4 Enable/disable rendering context switch (experimental)
+PATH:		/sys/kernel/vgt/control/ctx_switch
+
+SYNOPSIS:	echo <render_switch> > /sys/kernel/vgt/control/ctx_switch
+
+DESCRIPTION:	It is used to enable/disable rendering context switch
+		dynamically. This feature was mainly used for debugging instead
+		of a formal feature.
+
+RETURNED CODE:	The 'errno' will be set to following values.
+		EINVAL:		If <render_switch> is not an integer.
+		0:		Succeed.
+PARAMETERS:
+render_switch	When it is non-zero, rendering context switch will be
+		enabled otherwise it will be disabled.
+
+3.1.5 Display PORT/Monitor management
+
+Below are per-VM sysfs interfaces for users to query and change display status. 
+Those  interfaces present virtual information which is not necessarily the same
+as physical one. Since we do not virtualize display for the VM who is display
+owner(usually it is dom0), the information is not meaningful for display owner
+VM. Instead, people could use drm interface to get information.
+
+3.1.5.1 monitor edid information
+
+PATH:		/sys/kernel/vgt/vm#/PORT_#/edid
+
+SYNOPSIS:	cat /sys/kernel/vgt/vm#/PORT_#/edid
+		echo <binary_stream> > /sys/kernel/vgt/vm#/PORT_#/edid
+
+DESCRIPTION:	It is used to show the EDID of the monitor to be presented on
+		that port; or give input to set the EDID. The new written EDID
+		value will take effect in next hotplug interrupt.
+
+RETURNS:	binary value of 128-byte EDID block. Extended EDID block is
+		not supported.
+
+3.1.5.2 the virtual pipe information
+
+PATH:           	/sys/kernel/vgt/vm#/PORT_#/pipe
+
+SYNOPSIS:	cat /sys/kernel/vgt/vm#/PORT_#/pipe
+
+DESCRIPTION:	It is used to query which virtual pipe is connected to that port.
+
+RETURNS:	value 0/1/2 for PIPE_A/B/C
+
+3.1.5.3 virtual port to physical port mapping
+
+PATH:		/sys/kernel/vgt/vm#/PORT_#/port_override
+
+SYNOPSIS:	cat /sys/kernel/vgt/vm#/PORT_#/port_override
+		echo <port_id> > /sys/kernel/vgt/vm#/PORT_#/port_override
+
+DESCRIPTION:	When a VM has virtual monitor presented on this virtual port,
+		but physical machine has physical monitor connected to another
+		physical port, this interface can be used to tell hypervisor to 
+		show the VM's framebuffer on this virtual port to the physical
+		port set in "port_override".
+
+PARAMETERS:	<port_id> is 0/1/2/3/4 for PIPE_A/B/C/D/E.
+
+RETURNS:	port id from "cat" command.
+
+3.1.5.4 monitor type information
+
+PATH:           	/sys/kernel/vgt/vm#/PORT_#/type
+
+SYNOPSIS:	cat /sys/kernel/vgt/vm#/PORT_#/type
+		echo <type> > /sys/kernel/vgt/vm#/PORT_#/type
+
+PARAMETERS:	<type> is a legacy value, and may change in future. Right now
+		it is from 0 to 7 for:
+		CRT     -       0
+		DP_A    -       1
+		DP_B    -       2
+		DP_C    -       3
+		DP_D    -       4
+		HDMI_B  -       5
+		HDMI_C  -       6
+		HDMI_D  -       7
+
+RETURNS:	type from the "cat" command.
+
+3.1.5.5 monitor hotplug and connection information
+
+PATH:		/sys/kernel/vgt/vm#/PORT_#/connection
+
+SYNOPSIS:	cat /sys/kernel/vgt/vm#/PORT_#/connection
+		echo "disconnect" > /sys/kernel/vgt/vm#/PORT_#/connection
+		echo "connect" > /sys/kernel/vgt/vm#/PORT_#/connection
+
+DESCRIPTION:	It is used to check current monitor connection status on the
+		port, or change the status. If current status is "disconnected" 
+		and there is another "connect" command written, the hypervisor 
+		will do nothing. 
+
+		If a "connect" command is written, it is required that users have 
+		set "edid", "port_override" and "type" in advance correctly. Then 
+		a virtual hotplug interrupt will be injected into the virtual machine.
+		Otherwise the command will be ignored.
+
+		If a "disconnect" command is written, it is not needed to do any 
+		manual changes for "edid" etc. The information will be cleared 
+		automatically.
+
+PARAMETERS:	N/A
+
+RETURNS:	The output of "cat" command could be "connected" or "disconnected",
+		indicating the monitor connection status.
+
+EXAMPLES:	N/A
+
+3.1.6 Accessing physical MMIO registers
+PATH:		/sys/kernel/vgt/control/igd_mmio
+
+DESCRIPTION:	This is used to read/write physical MMIO registers.
+		System calls like read()/write() can be used to access
+		this interface.
+
+RETURNED CODE:	The 'errno' will be set to following values
+		EINVAL:		The parameter passed to read()/write() is
+				illegal.
+		EIO:		Hypercall can't access the physical register.
+		0:		Succeed.
+
+3.1.7 Remaining graphics memory size,the number of fence regs
+PATH:		/sys/kernel/vgt/control/available_resources
+
+DESCRIPTION:	This entry shows remaining free CPU visible graphics memory size,
+                available CPU invisible graphics memory size and available
+		fence registers.It can be used to determine how many VMs with
+		VGT instance can still be created.
+		The output consists of 3 lines in hexadecimal and looks like this:
+		(Using "\" to represent the continuing of the same line)
+
+		0x00000200, 0x00000180, 0x00000600, 0x00000480, 0x00000020, \
+		0x0000000c
+
+		00000000,00000000,00000000,00000000,00000000,00000000,00000000,\
+		00000000,00000000,00000000,00000000,00000000,00000000,00000000,\
+		00000000,00000000,00000000,00000000,00000000,00000000,00000000,\
+		00000000,00000000,00000000,00000000,00000000,00000000,00000000,\
+		00000000,00000000,00000000,00000000,00000000,00000000,00000000,\
+		00000000,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,\
+		ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,\
+		ffffffff,00000000,00000000,00000000,00000000,00000000,00000000,\
+		00000000,00000000,00000000,00000000,00000000,00000000,ffffffff,\
+		ffffffff
+                000f
+
+		The first line shows 6 numbers: total CPU visible Graphics memory size,
+		free CPU visible graphics memory size, toal CPU invisible graphics memory size,
+		free CPU invisible graphics memory size, total number of fence register
+		and the number of free fence registers. (Note: the first 4 are in 'MB').
+		The second and third line show the bitmap of graphics memory and
+		fence register allocation. Bits of "1" mean the resources have
+		been taken.
+
+4 XenGT debugfs interface
+
+XenGT debugfs interfaces are used for debugging and
+performance tuning. All XenGT debugfs entries are read-only.
+The command 'cat' can be used to get their contents.
+
+4.1 Global statistics
+
+PATH:		/sys/kernel/debug/vgt/context_switch_cycles
+DESCRIPTION:	Aggregated CPU cycles used by the switching of
+		rendering contexts.
+
+PATH:		/sys/kernel/debug/vgt/context_switch_num
+DESCRIPTION:	Aggregated number of context switches.
+
+PATH:		/sys/kernel/debug/vgt/irqinfo
+DESCRIPTION:	Statistics for all physical and virtual interrupts
+		on each VMs.
+
+PATH:		/sys/kernel/debug/vgt/preg
+DESCRIPTION:	It is used to dump the contents of all physical MMIO
+		registers. It's always dangerous to read from physical MMIO
+		registers directly, since some read has side effect, e.g.
+		read-to-clear bit.So use it with caution only when debugging
+		hard GPU hang problem.
+
+PATH:		/sys/kernel/debug/vgt/reginfo
+DESCRIPTION:	Dumping "access model" of each MMIO registers.
+		This includes the "Flags", "Owner" and "Type" fields.
+		"Flags" is a DWORD its format like below:
+		bit 17 - 31	: Index into another auxiliary table.
+		bit 14 - 16	: Reserved.
+		bit 13		: This reg is saved/restored at context
+				  switch time.
+		bit 12		: This reg is virtualized, but accessible
+				  by Dom0 at boot time.
+		bit 11		: This reg has been accessed by a VM.
+		bit 10		: This reg has been tracked by XenGT.
+		bit 9		: VM has different settings on this reg.
+		bit 8		: Mode ctl reg with high 16 bits as the mask.
+		bit 7		: This reg is pure virtualized.
+		bit 6		: This reg contains status bit updated
+				  from HW.
+		bit 5		: This reg contains address requiring fix.
+		bit 4		: This is a workaround reg. It means: Allows
+				  physical MMIO  access from any VM but w/o
+				  save/restore regs  marked with this flag
+				  should be treated as unsafe.
+		bit 0 - 3	: Owner type of the reg, up to 16 owner type.
+
+		"Owner" is a string name of the owner type but only include 5
+		owner name:
+		"NONE"		: There is no ownership for this reg.
+		"Render"	: This reg is rendering related.
+		"Display"	: This reg is display related.
+		"PM"		: This reg is power management related.
+		"MGMT"		: This reg is management related.
+
+		"Type" is also a string telling main "Flags" of the reg and
+		include 4 type:
+		"MPT"		: Mediate pass-through
+		"Boot"		: This reg is virtualized, but accessible by
+				  Dom0 at boot time
+		"WA"		: This reg is a workaround reg. Check the above
+				  for detailed information.
+
+PATH:		/sys/kernel/debug/vgt/irqinfo
+DESCRIPTION:	Statistics for all physical interrupts. And also virtual
+		interrupts injected to each VMs. Its content looks
+		like below.
+		--------------------------
+		Total 7 interrupts logged:
+		#	WARNING: precisely this is the number of vGT
+		#	physical interrupt handler be called,
+		#	each calling several events can be
+		#	been handled, so usually this number
+		#	is less than the total events number.
+				       2: GSE
+				       5: Primary Plane A flip done
+		    616863224224: Last pirq
+		    616863246160: Last virq
+			   13129: Average pirq cycles
+			    3585: Average virq cycles
+			    8150: Average delay between pirq/virq handling
+
+		-->vgt-0:
+		    118848451768: Last virq propagation
+			       0: Last blocked virq propagation
+		    118848452508: Last injection
+		Total 3 virtual irq injection:
+				       2: GSE
+				       1: Primary Plane A flip done
+
+		-->vgt-1:
+		    616863247316: Last virq propagation
+		    616244474088: Last blocked virq propagation
+		    616863251092: Last injection
+		Total 3 virtual irq injection:
+				       3: Primary Plane A flip done
+
+		This interface show 3 kinds of statistics info:
+		1) Events timestamp;
+		2) CPU cycles used for handling pirq and virq;
+		3) Distribution of interrupt numbers;
+
+		These "Events" include:
+		"Last pirq":	Physical irq from Gen hardware
+		"Last virq":	Virtual irq generated from XenGT.
+				The difference between these two
+				timestamp is the cost of
+				handling physical interrupts.
+		"Last virq propagation":
+				Set virtual interrupt status when
+				the specific bit not masked by IMR.
+		"Last blocked virq propagation":
+				Set virtual interrupt status when
+				the specific bit masked by IMR.
+		"Last injection":
+				After the hypercall of injecting
+				virtual interrupts to some VM.
+		When the timestamp is 0, it means such events never
+		happened.
+
+PATH:		/sys/kernel/debug/vgt/ring_0_busy
+DESCRIPTION:	Statistics for ring 0 busy in oprofile. When ring_0
+		is busy, this counter will be increased.
+
+PATH:		/sys/kernel/debug/vgt/ring_0_idle
+DESCRIPTION:	Statistics for ring 0 idle in oprofile. When ring_0
+		is idle, this counter will be increased.
+
+PATH:		/sys/kernel/debug/vgt/ring_mmio_rcnt
+DESCRIPTION:	Statistics for ringbuffer reg read.
+		These registers include: TAIL, HEAD, START,
+		and CTL. This interface counts ringbuffer
+		regs for all ringbuffers.
+
+PATH:		/sys/kernel/debug/vgt/ring_mmio_wcnt
+DESCRIPTION:	Statistics for ringbuffer reg write.
+		These registers include: TAIL, HEAD, START,
+		and CTL. This interface counts ringbuffer
+		regs for all ringbuffers.
+
+PATH:		/sys/kernel/debug/vgt/ring_tail_mmio_wcnt
+DESCRIPTION:	Statistics for ringbuffer tail reg write.
+
+PATH:		/sys/kernel/debug/vgt/ring_tail_mmio_wcycles
+DESCRIPTION:	The total CPU cycles used for all tail writing.
+
+PATH:		/sys/kernel/debug/vgt/forcewake_count
+DESCRIPTION:	The counter should be usually 0 when dom0 and HVM guest are idle.
+
+PATH:		/sys/kernel/debug/vgt/device_reset
+DESCRIPTION:	Supports reset device with VGT.
+
+4.2 Per VM statistics
+
+In below descriptions, VM ID is represented as 'X'.
+
+PATH:		/sys/kernel/debug/vgt/vmX/gtt_mmio_rcnt
+DESCRIPTION:	Aggregated number of GTT MMIO read.
+
+PATH:		/sys/kernel/debug/vgt/vmX /gtt_mmio_rcycles
+DESCRIPTION:	Aggregated CPU cycles used by GTT MMIO read.
+
+PATH:		/sys/kernel/debug/vgt/vmX /gtt_mmio_wcnt
+DESCRIPTION:	Aggregated number of GTT MMIO write.
+
+PATH:		/sys/kernel/debug/vgt/vmX /gtt_mmio_wcycles
+DESCRIPTION:	Aggregated CPU cycles used by GTT MMIO write.
+
+PATH:		/sys/kernel/debug/vgt/vmX /ppgtt_wp_rcycles
+DESCRIPTION:	Aggregated CPU cycles used by PPGTT write protect.
+
+PATH:		/sys/kernel/debug/vgt/vmX /ppgtt_wp_cnt
+DESCRIPTION:	Aggregated number of PPGTT write protect.
+
+PATH:		/sys/kernel/debug/vgt/vmX /mmio_rcnt
+DESCRIPTION:	Aggregated number of MMIO register read.
+
+PATH:		/sys/kernel/debug/vgt/vmX/mmio_rcycles
+DESCRIPTION:	Aggregated CPU cycles used by MMIO register read.
+
+PATH:		/sys/kernel/debug/vgt/vmX/mmio_wcnt
+DESCRIPTION:	Aggregated number of MMIO register write.
+
+PATH:		/sys/kernel/debug/vgt/vmX/mmio_wcycles
+DESCRIPTION:	Aggregated CPU cycles used by MMIO register write.
+
+PATH:		/sys/kernel/debug/vgt/vmX/surfA_base
+		/sys/kernel/debug/vgt/vmX/surfB_base
+DESCRIPTION:	Surface A(B)'s base address in graphics memory space.
+
+PATH:		/sys/kernel/debug/vgt/vmX/shadow_mmio_space
+		/sys/kernel/debug/vgt/vmX/virtual_mmio_space
+DESCRIPTION:	Dumping shadow(virtual) MMIO space of a VM.
+
+PATH:		/sys/kernel/debug/vgt/vmX/allocated_cycles
+DESCRIPTION:	Total time vmX allocated to use rendering engines.
+		In old days, each VM will be assigned 16ms to use render
+		engines, in a round-robin way. But it usually takes more time
+		than given because of waiting for the idle state of render
+		engines.
+
+PATH:		/sys/kernel/debug/vgt/vmX/schedule_in_time
+DESCRIPTION:	Timestamp of the start of last context switch
+
+PATH:		/sys/kernel/debug/vgt/vmX/frame_buffer_format
+DESCRIPTION:	cat this node will dump all the frame buffer format information
+		for the vm.
diff --git a/arch/x86/include/asm/xen/hypercall.h b/arch/x86/include/asm/xen/hypercall.h
index a0b1799..b9a5200 100644
--- a/arch/x86/include/asm/xen/hypercall.h
+++ b/arch/x86/include/asm/xen/hypercall.h
@@ -49,6 +49,7 @@
 #include <xen/interface/physdev.h>
 #include <xen/interface/platform.h>
 #include <xen/interface/xen-mca.h>
+#include <xen/interface/domctl.h>
 
 /*
  * The hypercall asms have to meet several constraints:
@@ -466,6 +467,14 @@ HYPERVISOR_kexec_op(
 }
 
 static inline int
+HYPERVISOR_domctl(
+	struct xen_domctl *arg)
+{
+	return _hypercall1(int, domctl, arg);
+}
+
+
+static inline int
 HYPERVISOR_tmem_op(
 	struct tmem_op *op)
 {
diff --git a/arch/x86/include/asm/xen/hypervisor.h b/arch/x86/include/asm/xen/hypervisor.h
index 125f344..93c7f12 100644
--- a/arch/x86/include/asm/xen/hypervisor.h
+++ b/arch/x86/include/asm/xen/hypervisor.h
@@ -62,7 +62,11 @@ extern bool xen_hvm_need_lapic(void);
 
 static inline bool xen_x2apic_para_available(void)
 {
+#ifdef CONFIG_XEN_PVHVM
 	return xen_hvm_need_lapic();
+#else
+	return false;
+#endif
 }
 #else
 static inline bool xen_x2apic_para_available(void)
diff --git a/arch/x86/include/asm/xen/interface.h b/arch/x86/include/asm/xen/interface.h
index fd9cb76..0b687be 100644
--- a/arch/x86/include/asm/xen/interface.h
+++ b/arch/x86/include/asm/xen/interface.h
@@ -57,6 +57,7 @@ typedef unsigned long xen_ulong_t;
 /* Guest handles for primitive C types. */
 __DEFINE_GUEST_HANDLE(uchar, unsigned char);
 __DEFINE_GUEST_HANDLE(uint,  unsigned int);
+__DEFINE_GUEST_HANDLE(ulong,  unsigned long);
 DEFINE_GUEST_HANDLE(char);
 DEFINE_GUEST_HANDLE(int);
 DEFINE_GUEST_HANDLE(void);
diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index ab7c0dd..d818d96 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -615,6 +615,7 @@ dotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code)
 	exception_exit(prev_state);
 }
 
+
 dotraplinkage void
 do_simd_coprocessor_error(struct pt_regs *regs, long error_code)
 {
diff --git a/arch/x86/xen/mmu.c b/arch/x86/xen/mmu.c
index b80dd26..3ff2d89 100644
--- a/arch/x86/xen/mmu.c
+++ b/arch/x86/xen/mmu.c
@@ -2607,3 +2607,85 @@ int xen_unmap_domain_mfn_range(struct vm_area_struct *vma,
 	return -EINVAL;
 }
 EXPORT_SYMBOL_GPL(xen_unmap_domain_mfn_range);
+
+/* Note: here 'mfn' is actually gfn!!! */
+struct vm_struct * xen_remap_domain_mfn_range_in_kernel(unsigned long mfn,
+		int nr, unsigned domid)
+{
+	struct vm_struct *area;
+	struct remap_data rmd;
+	struct mmu_update mmu_update[REMAP_BATCH_SIZE];
+	int batch;
+	unsigned long range, addr;
+	pgprot_t prot;
+	int err;
+
+	WARN_ON(in_interrupt() || irqs_disabled());
+
+	area = alloc_vm_area(nr << PAGE_SHIFT, NULL);
+	if (!area)
+		return NULL;
+
+	addr = (unsigned long)area->addr;
+
+	prot = __pgprot(pgprot_val(PAGE_KERNEL));
+
+	rmd.mfn = mfn;
+	rmd.prot = prot;
+
+	while (nr) {
+		batch = min(REMAP_BATCH_SIZE, nr);
+		range = (unsigned long)batch << PAGE_SHIFT;
+
+		rmd.mmu_update = mmu_update;
+		err = apply_to_page_range(&init_mm, addr, range,
+					  remap_area_mfn_pte_fn, &rmd);
+		if (err || HYPERVISOR_mmu_update(mmu_update, batch, NULL, domid) < 0)
+			goto err;
+
+		nr -= batch;
+		addr += range;
+	}
+
+	xen_flush_tlb_all();
+	return area;
+err:
+	free_vm_area(area);
+	xen_flush_tlb_all();
+	return NULL;
+}
+EXPORT_SYMBOL_GPL(xen_remap_domain_mfn_range_in_kernel);
+
+void xen_unmap_domain_mfn_range_in_kernel(struct vm_struct *area, int nr,
+		unsigned domid)
+{
+	struct remap_data rmd;
+	struct mmu_update mmu_update[REMAP_BATCH_SIZE];
+	int batch;
+	unsigned long range, addr = (unsigned long)area->addr;
+	int err;
+
+	WARN_ON(in_interrupt() || irqs_disabled());
+
+#define INVALID_MFN (~0UL)
+	rmd.mfn = INVALID_MFN;
+	rmd.prot = PAGE_NONE;
+
+	while (nr) {
+		batch = min(REMAP_BATCH_SIZE, nr);
+		range = (unsigned long)batch << PAGE_SHIFT;
+
+		rmd.mmu_update = mmu_update;
+		err = apply_to_page_range(&init_mm, addr, range,
+					  remap_area_mfn_pte_fn, &rmd);
+		BUG_ON(err);
+		BUG_ON(HYPERVISOR_mmu_update(mmu_update, batch, NULL, domid) < 0);
+
+		nr -= batch;
+		addr += range;
+	}
+
+	free_vm_area(area);
+	xen_flush_tlb_all();
+}
+EXPORT_SYMBOL_GPL(xen_unmap_domain_mfn_range_in_kernel);
diff --git a/drivers/acpi/acpica/acmacros.h b/drivers/acpi/acpica/acmacros.h
index 53666bd..ea935af 100644
--- a/drivers/acpi/acpica/acmacros.h
+++ b/drivers/acpi/acpica/acmacros.h
@@ -44,6 +44,9 @@
 #ifndef __ACMACROS_H__
 #define __ACMACROS_H__
 
+#ifndef ACPI_NO_ERROR_MESSAGES
+#define ACPI_NO_ERROR_MESSAGES
+#endif
 /*
  * Extract data using a pointer. Any more than a byte and we
  * get into potential aligment issues -- see the STORE macros below.
diff --git a/drivers/gpu/drm/drm_irq.c b/drivers/gpu/drm/drm_irq.c
index 74ada28..a4bb5d8 100644
--- a/drivers/gpu/drm/drm_irq.c
+++ b/drivers/gpu/drm/drm_irq.c
@@ -42,6 +42,13 @@
 #include <linux/vgaarb.h>
 #include <linux/export.h>
 
+#ifdef CONFIG_I915_VGT
+extern bool i915_host_mediate __read_mostly;
+extern bool vgt_can_process_irq(void);
+extern bool vgt_can_process_timer(void *timer);
+extern void vgt_new_delay_event_timer(void *timer);
+#endif
+
 /* Access macro for slots in vblank timestamp ringbuffer. */
 #define vblanktimestamp(dev, crtc, count) \
 	((dev)->vblank[crtc].time[(count) % DRM_VBLANKTIME_RBSIZE])
@@ -252,6 +259,12 @@ static void vblank_disable_fn(unsigned long arg)
 	if (!dev->vblank_disable_allowed)
 		return;
 
+#ifdef CONFIG_I915_VGT
+	if (i915_host_mediate)
+		if (!vgt_can_process_timer(&vblank->disable_timer))
+			return;
+#endif
+
 	spin_lock_irqsave(&dev->vbl_lock, irqflags);
 	if (atomic_read(&vblank->refcount) == 0 && vblank->enabled) {
 		DRM_DEBUG("disabling vblank on crtc %d\n", crtc);
@@ -322,6 +335,10 @@ int drm_vblank_init(struct drm_device *dev, int num_crtcs)
 		init_waitqueue_head(&vblank->queue);
 		setup_timer(&vblank->disable_timer, vblank_disable_fn,
 			    (unsigned long)vblank);
+#ifdef CONFIG_I915_VGT
+		if (i915_host_mediate)
+			vgt_new_delay_event_timer(&vblank->disable_timer);
+#endif
 	}
 
 	DRM_INFO("Supports vblank timestamp caching Rev 2 (21.10.2013).\n");
diff --git a/drivers/gpu/drm/i915/Kconfig b/drivers/gpu/drm/i915/Kconfig
index 4e39ab3..b3cd776 100644
--- a/drivers/gpu/drm/i915/Kconfig
+++ b/drivers/gpu/drm/i915/Kconfig
@@ -69,3 +69,19 @@ config DRM_I915_PRELIMINARY_HW_SUPPORT
 	  option changes the default for that module option.
 
 	  If in doubt, say "N".
+
+config I915_VGT
+	tristate "iGVT-g vGT driver of i915"
+	depends on DRM_I915
+	select IRQ_WORK
+	default y
+	help
+	  Enabling vGT mediated graphics passthrough technique for Intel i915
+	  based integrated graphics card. With vGT, it's possible to have one
+	  integrated i915 device shared by multiple VMs. Performance critical
+	  opterations such as apperture accesses and ring buffer operations
+	  are pass-throughed to VM, with a minimal set of conflicting resources
+	  (e.g. display settings) mediated by vGT driver. The benefit of vGT
+	  is on both the performance, given that each VM could directly operate
+	  its aperture space and submit commands like running on native, and
+	  the feature completeness, given that a true GEN hardware is exposed.
diff --git a/drivers/gpu/drm/i915/Makefile b/drivers/gpu/drm/i915/Makefile
index e4083e4..92a4e7f 100644
--- a/drivers/gpu/drm/i915/Makefile
+++ b/drivers/gpu/drm/i915/Makefile
@@ -3,6 +3,7 @@
 # Direct Rendering Infrastructure (DRI) in XFree86 4.1.0 and higher.
 
 ccflags-y := -Iinclude/drm
+ccflags-$(CONFIG_I915_VGT) += -I$(src)/vgt
 
 # Please keep these build lists sorted!
 
@@ -30,6 +31,7 @@ i915-y += i915_cmd_parser.o \
 	  i915_gem_stolen.o \
 	  i915_gem_tiling.o \
 	  i915_gem_userptr.o \
+	  i915_gem_vgtbuffer.o \
 	  i915_gpu_error.o \
 	  i915_irq.o \
 	  i915_trace_points.o \
@@ -85,5 +87,6 @@ i915-y += i915_dma.o \
 	  i915_ums.o
 
 obj-$(CONFIG_DRM_I915)  += i915.o
+obj-$(CONFIG_I915_VGT)  += vgt/
 
 CFLAGS_i915_trace_points.o := -I$(src)
diff --git a/drivers/gpu/drm/i915/i915_debugfs.c b/drivers/gpu/drm/i915/i915_debugfs.c
index 779a275..a084347 100644
--- a/drivers/gpu/drm/i915/i915_debugfs.c
+++ b/drivers/gpu/drm/i915/i915_debugfs.c
@@ -649,6 +649,26 @@ static int i915_gem_seqno_info(struct seq_file *m, void *data)
 	return 0;
 }
 
+extern u64 i915_ring_0_idle;
+extern u64 i915_ring_0_busy;
+static int i915_ring_info(struct seq_file *m, void *data)
+{
+	struct drm_info_node *node = (struct drm_info_node *) m->private;
+	struct drm_device *dev = node->minor->dev;
+	int ret;
+
+	ret = mutex_lock_interruptible(&dev->struct_mutex);
+	if (ret)
+		return ret;
+
+	seq_printf(m, "i915_ring_0_idle %08lx busy %08lx\n",
+			(unsigned long) i915_ring_0_idle,
+			(unsigned long) i915_ring_0_busy);
+
+	mutex_unlock(&dev->struct_mutex);
+
+	return 0;
+}
 
 static int i915_interrupt_info(struct seq_file *m, void *data)
 {
@@ -2468,10 +2488,11 @@ static void intel_connector_info(struct seq_file *m,
 {
 	struct intel_connector *intel_connector = to_intel_connector(connector);
 	struct intel_encoder *intel_encoder = intel_connector->encoder;
+	struct intel_digital_port *dig_port = enc_to_dig_port(&intel_encoder->base);
 	struct drm_display_mode *mode;
 
-	seq_printf(m, "connector %d: type %s, status: %s\n",
-		   connector->base.id, connector->name,
+	seq_printf(m, "connector %d: type %s, port PORT_%c, status: %s\n",
+		   connector->base.id, connector->name, port_name(dig_port->port),
 		   drm_get_connector_status_name(connector->status));
 	if (connector->status == connector_status_connected) {
 		seq_printf(m, "\tname: %s\n", connector->display_info.name);
@@ -4292,6 +4313,7 @@ static const struct drm_info_list i915_debugfs_list[] = {
 	{"i915_gem_seqno", i915_gem_seqno_info, 0},
 	{"i915_gem_fence_regs", i915_gem_fence_regs_info, 0},
 	{"i915_gem_interrupt", i915_interrupt_info, 0},
+	{"i915_ring_info", i915_ring_info, 0},
 	{"i915_gem_hws", i915_hws_info, 0, (void *)RCS},
 	{"i915_gem_hws_blt", i915_hws_info, 0, (void *)BCS},
 	{"i915_gem_hws_bsd", i915_hws_info, 0, (void *)VCS},
diff --git a/drivers/gpu/drm/i915/i915_dma.c b/drivers/gpu/drm/i915/i915_dma.c
index ad9ce86..f8c6f9b 100644
--- a/drivers/gpu/drm/i915/i915_dma.c
+++ b/drivers/gpu/drm/i915/i915_dma.c
@@ -611,6 +611,8 @@ static void intel_device_info_runtime_init(struct drm_device *dev)
  *   - allocate initial config memory
  *   - setup the DRM framebuffer with the allocated memory
  */
+struct drm_i915_private *gpu_perf_dev_priv;
+
 int i915_driver_load(struct drm_device *dev, unsigned long flags)
 {
 	struct drm_i915_private *dev_priv;
@@ -635,7 +637,8 @@ int i915_driver_load(struct drm_device *dev, unsigned long flags)
 	if (dev_priv == NULL)
 		return -ENOMEM;
 
-	dev->dev_private = dev_priv;
+	dev->dev_private = (void *)dev_priv;
+	gpu_perf_dev_priv = (void *)dev_priv;
 	dev_priv->dev = dev;
 
 	/* Setup the write-once "constant" device info */
@@ -696,6 +699,14 @@ int i915_driver_load(struct drm_device *dev, unsigned long flags)
 
 	intel_uncore_init(dev);
 
+	if (i915_start_vgt(dev->pdev))
+		i915_host_mediate = true;
+	printk("i915_start_vgt: %s\n", i915_host_mediate ? "success" : "fail");
+
+	i915_check_vgt(dev_priv);
+	if (USES_VGT(dev))
+		i915.enable_fbc = 0;
+
 	ret = i915_gem_gtt_init(dev);
 	if (ret)
 		goto out_regs;
@@ -815,6 +826,17 @@ int i915_driver_load(struct drm_device *dev, unsigned long flags)
 			DRM_ERROR("failed to init modeset\n");
 			goto out_power_well;
 		}
+#ifdef DRM_I915_VGT_SUPPORT
+		if (USES_VGT(dev)) {
+			/*
+			 * Tell VGT that we have a valid surface to show
+			 * after modesetting. We doesn't distinguish DOM0 and
+			 * Linux guest here, The PVINFO write handler will
+			 * handle this.
+			 */
+			I915_WRITE(vgt_info_off(display_ready), 1);
+		}
+#endif
 	}
 
 	i915_setup_sysfs(dev);
@@ -1055,6 +1077,7 @@ const struct drm_ioctl_desc i915_ioctls[] = {
 	DRM_IOCTL_DEF_DRV(I915_REG_READ, i915_reg_read_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(I915_GET_RESET_STATS, i915_get_reset_stats_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(I915_GEM_USERPTR, i915_gem_userptr_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(I915_GEM_VGTBUFFER, i915_gem_vgtbuffer_ioctl, DRM_UNLOCKED),
 };
 
 int i915_max_ioctl = ARRAY_SIZE(i915_ioctls);
diff --git a/drivers/gpu/drm/i915/i915_drv.c b/drivers/gpu/drm/i915/i915_drv.c
index fc8cfdd..e20c94c 100644
--- a/drivers/gpu/drm/i915/i915_drv.c
+++ b/drivers/gpu/drm/i915/i915_drv.c
@@ -34,12 +34,15 @@
 #include "i915_drv.h"
 #include "i915_trace.h"
 #include "intel_drv.h"
+#include "vgt-if.h"
 
 #include <linux/console.h>
 #include <linux/module.h>
 #include <linux/pm_runtime.h>
 #include <drm/drm_crtc_helper.h>
 
+bool i915_host_mediate __read_mostly = false;
+
 static struct drm_driver driver;
 
 #define GEN_DEFAULT_PIPEOFFSETS \
@@ -422,6 +425,8 @@ void intel_detect_pch(struct drm_device *dev)
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct pci_dev *pch = NULL;
 
+	printk("i915: intel_detect_pch\n");
+
 	/* In all current cases, num_pipes is equivalent to the PCH_NOP setting
 	 * (which really amounts to a PCH but no South Display).
 	 */
@@ -674,6 +679,7 @@ int i915_suspend_legacy(struct drm_device *dev, pm_message_t state)
 static int i915_drm_resume(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
+	int ret = 0;
 
 	if (drm_core_check_feature(dev, DRIVER_MODESET)) {
 		mutex_lock(&dev->struct_mutex);
@@ -926,6 +932,7 @@ static int i915_pm_suspend(struct device *dev)
 {
 	struct pci_dev *pdev = to_pci_dev(dev);
 	struct drm_device *drm_dev = pci_get_drvdata(pdev);
+	int error;
 
 	if (!drm_dev || !drm_dev->dev_private) {
 		dev_err(dev, "DRM not initialized, aborting suspend.\n");
@@ -935,7 +942,18 @@ static int i915_pm_suspend(struct device *dev)
 	if (drm_dev->switch_power_state == DRM_SWITCH_POWER_OFF)
 		return 0;
 
-	return i915_drm_suspend(drm_dev);
+	error = i915_drm_suspend(drm_dev);
+	if (error)
+		return error;
+
+#ifdef CONFIG_I915_VGT
+	if (i915_host_mediate) {
+		error = vgt_suspend(pdev);
+		if (error)
+			return error;
+	}
+#endif
+	return 0;
 }
 
 static int i915_pm_suspend_late(struct device *dev)
@@ -977,6 +995,14 @@ static int i915_pm_resume(struct device *dev)
 	if (drm_dev->switch_power_state == DRM_SWITCH_POWER_OFF)
 		return 0;
 
+#ifdef DRM_I915_VGT_SUPPORT
+	if (i915_host_mediate) {
+		int error = vgt_resume(drm_dev->pdev);
+		if (error)
+			return error;
+	}
+#endif
+
 	return i915_drm_resume(drm_dev);
 }
 
diff --git a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
index 63bcda5..ebd2789 100644
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@ -38,6 +38,7 @@
 #include "intel_lrc.h"
 #include "i915_gem_gtt.h"
 #include "i915_gem_render_state.h"
+#include "i915_vgt.h"
 #include <linux/io-mapping.h>
 #include <linux/i2c.h>
 #include <linux/i2c-algo-bit.h>
@@ -50,6 +51,16 @@
 #include <linux/kref.h>
 #include <linux/pm_qos.h>
 
+#ifdef CONFIG_I915_VGT
+#include <linux/irq_work.h>
+#define DRM_I915_VGT_SUPPORT	1
+#endif
+
+#ifdef DRM_I915_VGT_SUPPORT
+#include "vgt-if.h"
+#include "fb_decoder.h"
+#endif
+
 /* General customization:
  */
 
@@ -1148,6 +1159,12 @@ struct i915_gem_mm {
 
 	/** PPGTT used for aliasing the PPGTT with the GTT */
 	struct i915_hw_ppgtt *aliasing_ppgtt;
+	/*
+	 * VGT:
+	 * Original i915 driver in 3.11.6 remove this entry,
+	 * whatever we need this for PPGTT ballooning.
+	 */
+	unsigned int first_ppgtt_pde_in_gtt;
 
 	struct notifier_block oom_notifier;
 	struct shrinker shrinker;
@@ -1200,6 +1217,14 @@ struct i915_gem_mm {
 	spinlock_t object_stat_lock;
 	size_t object_memory;
 	u32 object_count;
+
+#ifdef DRM_I915_VGT_SUPPORT
+	/* VGT balloon info */
+	unsigned long vgt_low_gm_base;
+	unsigned long vgt_low_gm_size;
+	unsigned long vgt_high_gm_base;
+	unsigned long vgt_high_gm_size;
+#endif
 };
 
 struct drm_i915_error_state_buf {
@@ -1758,6 +1783,18 @@ struct drm_i915_private {
 
 	uint32_t bios_vgacntr;
 
+#ifdef CONFIG_I915_VGT
+	/* vgt host-side mediation */
+	void *pgt;
+	struct irq_work irq_work;
+	struct {
+		irqreturn_t(*irq_handler) (int irq, void *arg);
+		void (*irq_preinstall) (struct drm_device *dev);
+		int (*irq_postinstall) (struct drm_device *dev);
+		void (*irq_uninstall) (struct drm_device *dev);
+	} irq_ops;
+#endif
+
 	/* Abstract the submission mechanism (legacy ringbuffer or execlists) away */
 	struct {
 		int (*do_execbuf)(struct drm_device *dev, struct drm_file *file,
@@ -1815,6 +1852,8 @@ struct drm_i915_gem_object_ops {
 	void (*put_pages)(struct drm_i915_gem_object *);
 	int (*dmabuf_export)(struct drm_i915_gem_object *);
 	void (*release)(struct drm_i915_gem_object *);
+	int (*pin)(struct drm_i915_gem_object *, uint32_t, bool, bool);
+	void (*unpin)(struct drm_i915_gem_object *);
 };
 
 /*
@@ -1917,6 +1956,8 @@ struct drm_i915_gem_object {
 
 	unsigned int has_dma_mapping:1;
 
+	unsigned int has_vmfb_mapping:1;
+
 	unsigned int frontbuffer_bits:INTEL_FRONTBUFFER_BITS;
 
 	struct sg_table *pages;
@@ -1963,6 +2004,7 @@ struct drm_i915_gem_object {
 		} userptr;
 	};
 };
+
 #define to_intel_bo(x) container_of(x, struct drm_i915_gem_object, base)
 
 void i915_gem_track_fb(struct drm_i915_gem_object *old,
@@ -2204,6 +2246,7 @@ struct drm_i915_cmd_table {
 				 __I915__(dev)->ellc_size)
 #define I915_NEED_GFX_HWS(dev)	(INTEL_INFO(dev)->need_gfx_hws)
 
+#define USES_VGT(dev)		(i915.enable_vgt)
 #define HAS_HW_CONTEXTS(dev)	(INTEL_INFO(dev)->gen >= 6)
 #define HAS_LOGICAL_RING_CONTEXTS(dev)	(INTEL_INFO(dev)->gen >= 8)
 #define USES_PPGTT(dev)		(i915.enable_ppgtt)
@@ -2243,8 +2286,8 @@ struct drm_i915_cmd_table {
 #define HAS_DDI(dev)		(INTEL_INFO(dev)->has_ddi)
 #define HAS_FPGA_DBG_UNCLAIMED(dev)	(INTEL_INFO(dev)->has_fpga_dbg)
 #define HAS_PSR(dev)		(IS_HASWELL(dev) || IS_BROADWELL(dev))
-#define HAS_RUNTIME_PM(dev)	(IS_GEN6(dev) || IS_HASWELL(dev) || \
-				 IS_BROADWELL(dev) || IS_VALLEYVIEW(dev))
+#define HAS_RUNTIME_PM(dev)	(!USES_VGT(dev) && (IS_GEN6(dev) || IS_HASWELL(dev) || \
+				 IS_BROADWELL(dev) || IS_VALLEYVIEW(dev)))
 #define HAS_RC6(dev)		(INTEL_INFO(dev)->gen >= 6)
 #define HAS_RC6p(dev)		(INTEL_INFO(dev)->gen == 6 || IS_IVYBRIDGE(dev))
 
@@ -2296,6 +2339,7 @@ struct i915_params {
 	int enable_rc6;
 	int enable_fbc;
 	int enable_ppgtt;
+	bool ctx_switch;
 	int enable_execlists;
 	int enable_psr;
 	unsigned int preliminary_hw_support;
@@ -2312,6 +2356,7 @@ struct i915_params {
 	bool disable_vtd_wa;
 	int use_mmio_flip;
 	bool mmio_debug;
+	int enable_vgt;
 };
 extern struct i915_params i915 __read_mostly;
 
@@ -2426,6 +2471,8 @@ int i915_gem_throttle_ioctl(struct drm_device *dev, void *data,
 			    struct drm_file *file_priv);
 int i915_gem_madvise_ioctl(struct drm_device *dev, void *data,
 			   struct drm_file *file_priv);
+int i915_gem_vgtbuffer_ioctl(struct drm_device *dev, void *data,
+			     struct drm_file *file);
 int i915_gem_set_tiling(struct drm_device *dev, void *data,
 			struct drm_file *file_priv);
 int i915_gem_get_tiling(struct drm_device *dev, void *data,
@@ -2484,11 +2531,15 @@ static inline struct page *i915_gem_object_get_page(struct drm_i915_gem_object *
 }
 static inline void i915_gem_object_pin_pages(struct drm_i915_gem_object *obj)
 {
+	if (obj->has_vmfb_mapping)
+		return;
 	BUG_ON(obj->pages == NULL);
 	obj->pages_pin_count++;
 }
 static inline void i915_gem_object_unpin_pages(struct drm_i915_gem_object *obj)
 {
+	if (obj->has_vmfb_mapping)
+		return;
 	BUG_ON(obj->pages_pin_count == 0);
 	obj->pages_pin_count--;
 }
@@ -2566,6 +2617,7 @@ int i915_gem_init_rings(struct drm_device *dev);
 int __must_check i915_gem_init_hw(struct drm_device *dev);
 int i915_gem_l3_remap(struct intel_engine_cs *ring, int slice);
 void i915_gem_init_swizzling(struct drm_device *dev);
+bool intel_enable_ppgtt(struct drm_device *dev);
 void i915_gem_cleanup_ringbuffer(struct drm_device *dev);
 int __must_check i915_gpu_idle(struct drm_device *dev);
 int __must_check i915_gem_suspend(struct drm_device *dev);
@@ -2934,6 +2986,18 @@ void gen6_gt_force_wake_get(struct drm_i915_private *dev_priv, int fw_engine);
 void gen6_gt_force_wake_put(struct drm_i915_private *dev_priv, int fw_engine);
 void assert_force_wake_inactive(struct drm_i915_private *dev_priv);
 
+#ifdef CONFIG_I915_VGT
+
+extern bool vgt_can_process_irq(void);
+extern bool vgt_can_process_timer(void *timer);
+extern void vgt_new_delay_event_timer(void *timer);
+#endif
+
+#ifdef DRM_I915_VGT_SUPPORT
+#define VGT_IF_VERSION	0x10000		/* 1.0 */
+extern void i915_check_vgt(struct drm_i915_private *dev_priv);
+#endif
+
 int sandybridge_pcode_read(struct drm_i915_private *dev_priv, u32 mbox, u32 *val);
 int sandybridge_pcode_write(struct drm_i915_private *dev_priv, u32 mbox, u32 val);
 
@@ -2970,27 +3034,40 @@ int vlv_freq_opcode(struct drm_i915_private *dev_priv, int val);
 					FORCEWAKE_BLITTER)
 
 
-#define I915_READ8(reg)		dev_priv->uncore.funcs.mmio_readb(dev_priv, (reg), true)
-#define I915_WRITE8(reg, val)	dev_priv->uncore.funcs.mmio_writeb(dev_priv, (reg), (val), true)
-
-#define I915_READ16(reg)	dev_priv->uncore.funcs.mmio_readw(dev_priv, (reg), true)
-#define I915_WRITE16(reg, val)	dev_priv->uncore.funcs.mmio_writew(dev_priv, (reg), (val), true)
-#define I915_READ16_NOTRACE(reg)	dev_priv->uncore.funcs.mmio_readw(dev_priv, (reg), false)
-#define I915_WRITE16_NOTRACE(reg, val)	dev_priv->uncore.funcs.mmio_writew(dev_priv, (reg), (val), false)
-
-#define I915_READ(reg)		dev_priv->uncore.funcs.mmio_readl(dev_priv, (reg), true)
-#define I915_WRITE(reg, val)	dev_priv->uncore.funcs.mmio_writel(dev_priv, (reg), (val), true)
-#define I915_READ_NOTRACE(reg)		dev_priv->uncore.funcs.mmio_readl(dev_priv, (reg), false)
-#define I915_WRITE_NOTRACE(reg, val)	dev_priv->uncore.funcs.mmio_writel(dev_priv, (reg), (val), false)
-
+#define __i915_read(x) \
+	u##x i915_read##x(struct drm_i915_private *dev_priv, u32 reg, bool trace);
+__i915_read(8)
+__i915_read(16)
+__i915_read(32)
+__i915_read(64)
+#undef __i915_read
+#define __i915_write(x) \
+	void i915_write##x(struct drm_i915_private *dev_priv, u32 reg, u##x val, bool trace);
+__i915_write(8)
+__i915_write(16)
+__i915_write(32)
+__i915_write(64)
+#undef __i915_write
+
+#define I915_READ8(reg)			i915_read8(dev_priv, (reg), true)
+#define I915_WRITE8(reg, val)		i915_write8(dev_priv, (reg), (val), true)
+#define I915_READ16(reg)		i915_read16(dev_priv, (reg), true)
+#define I915_WRITE16(reg, val)		i915_write16(dev_priv, (reg), (val), true)
+#define I915_READ16_NOTRACE(reg)	i915_read16(dev_priv, (reg), false)
+#define I915_WRITE16_NOTRACE(reg, val)	i915_write16(dev_priv, (reg), (val), false)
+#define I915_READ(reg)			i915_read32(dev_priv, (reg), true)
+#define I915_WRITE(reg, val)		i915_write32(dev_priv, (reg), (val), true)
+#define I915_READ_NOTRACE(reg)		i915_read32(dev_priv, (reg), false)
+#define I915_WRITE_NOTRACE(reg, val)	i915_write32(dev_priv, (reg), (val), false)
 /* Be very careful with read/write 64-bit values. On 32-bit machines, they
  * will be implemented using 2 32-bit writes in an arbitrary order with
  * an arbitrary delay between them. This can cause the hardware to
  * act upon the intermediate value, possibly leading to corruption and
  * machine death. You have been warned.
  */
-#define I915_WRITE64(reg, val)	dev_priv->uncore.funcs.mmio_writeq(dev_priv, (reg), (val), true)
-#define I915_READ64(reg)	dev_priv->uncore.funcs.mmio_readq(dev_priv, (reg), true)
+#define I915_WRITE64(reg, val)		i915_write64(dev_priv, (reg), (val), true)
+#define I915_READ64(reg)		i915_read64(dev_priv, (reg), true)
+
 
 #define I915_READ64_2x32(lower_reg, upper_reg) ({			\
 		u32 upper = I915_READ(upper_reg);			\
@@ -3006,6 +3083,57 @@ int vlv_freq_opcode(struct drm_i915_private *dev_priv, int val);
 #define POSTING_READ(reg)	(void)I915_READ_NOTRACE(reg)
 #define POSTING_READ16(reg)	(void)I915_READ16_NOTRACE(reg)
 
+
+#define GTT_READ32(addr)						\
+({									\
+	off_t reg = (unsigned long)(addr) -				\
+			(unsigned long)(dev_priv->gtt.gsm);		\
+	u32 __ret = 0;							\
+	if (i915_host_mediate)						\
+		vgt_host_read(reg, &__ret, sizeof(u32),			\
+						true, false);		\
+	else								\
+		__ret = readl(addr);					\
+	__ret;								\
+})
+
+#define GTT_READ64(addr)						\
+({									\
+	off_t reg = (unsigned long)(addr) -				\
+			(unsigned long)(dev_priv->gtt.gsm);		\
+	u64 __ret = 0;							\
+	if (i915_host_mediate)						\
+		vgt_host_read(reg, &__ret, sizeof(u64),			\
+						true, false);		\
+	else								\
+		__ret = readq(addr);					\
+	__ret;								\
+})
+
+#define GTT_WRITE32(val, addr)						\
+({									\
+	off_t reg = (unsigned long)(addr) -				\
+			(unsigned long)(dev_priv->gtt.gsm);		\
+	u32 __val = (val);						\
+	if (i915_host_mediate)						\
+		vgt_host_write(reg, &__val, sizeof(u32),		\
+						true, false);		\
+	else								\
+		writel((val), (addr));					\
+})
+
+#define GTT_WRITE64(val, addr)						\
+({									\
+	off_t reg = (unsigned long)(addr) -				\
+			(unsigned long)(dev_priv->gtt.gsm);		\
+	u64 __val = (val);						\
+	if (i915_host_mediate)						\
+		vgt_host_write(reg, &__val, sizeof(u64),		\
+						true, false);		\
+	else								\
+		writeq((val), (addr));					\
+})
+
 /* "Broadcast RGB" property */
 #define INTEL_BROADCAST_RGB_AUTO 0
 #define INTEL_BROADCAST_RGB_FULL 1
@@ -3073,4 +3201,6 @@ wait_remaining_ms_from_jiffies(unsigned long timestamp_jiffies, int to_wait_ms)
 	}
 }
 
+extern bool i915_host_mediate __read_mostly;
+
 #endif
diff --git a/drivers/gpu/drm/i915/i915_gem.c b/drivers/gpu/drm/i915/i915_gem.c
index fade594..c1fdf6a 100644
--- a/drivers/gpu/drm/i915/i915_gem.c
+++ b/drivers/gpu/drm/i915/i915_gem.c
@@ -136,6 +136,13 @@ i915_gem_wait_for_error(struct i915_gpu_error *error)
 	return 0;
 }
 
+int i915_wait_error_work_complete(struct drm_device *dev)
+{
+       struct drm_i915_private *dev_priv = dev->dev_private;
+
+       return i915_gem_wait_for_error(&dev_priv->gpu_error);
+}
+
 int i915_mutex_lock_interruptible(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -159,6 +166,19 @@ i915_gem_object_is_inactive(struct drm_i915_gem_object *obj)
 	return i915_gem_obj_bound_any(obj) && !obj->active;
 }
 
+#ifdef DRM_I915_VGT_SUPPORT
+/*
+ * Get the number of assigned fence registers.
+ * through the PV INFO page.
+ */
+static inline int vgt_avail_fence_num(struct drm_i915_private *dev_priv)
+{
+	unsigned long   avail_fences;
+	avail_fences = I915_READ(vgt_info_off(avail_rs.fence_num));
+	return avail_fences;
+}
+#endif
+
 int
 i915_gem_get_aperture_ioctl(struct drm_device *dev, void *data,
 			    struct drm_file *file)
@@ -3056,7 +3076,8 @@ int i915_vma_unbind(struct i915_vma *vma)
 	if (vma->pin_count)
 		return -EBUSY;
 
-	BUG_ON(obj->pages == NULL);
+	if (!obj->has_vmfb_mapping)
+		BUG_ON(obj->pages == NULL);
 
 	ret = i915_gem_object_finish_gpu(obj);
 	if (ret)
@@ -3069,7 +3090,7 @@ int i915_vma_unbind(struct i915_vma *vma)
 	/* Throw away the active reference before moving to the unbound list */
 	i915_gem_object_retire(obj);
 
-	if (i915_is_ggtt(vma->vm)) {
+	if (i915_is_ggtt(vma->vm) && !obj->has_vmfb_mapping) {
 		i915_gem_object_finish_gtt(obj);
 
 		/* release the fence reg _after_ flushing */
@@ -3086,13 +3107,16 @@ int i915_vma_unbind(struct i915_vma *vma)
 	if (i915_is_ggtt(vma->vm))
 		obj->map_and_fenceable = false;
 
-	drm_mm_remove_node(&vma->node);
+	if (!(obj->has_vmfb_mapping && i915_is_ggtt(vma->vm)))
+		drm_mm_remove_node(&vma->node);
+
 	i915_gem_vma_destroy(vma);
 
 	/* Since the unbound list is global, only move to that list if
 	 * no more VMAs exist. */
 	if (list_empty(&obj->vma_list)) {
-		i915_gem_gtt_finish_object(obj);
+		if (!obj->has_vmfb_mapping)
+			i915_gem_gtt_finish_object(obj);
 		list_move_tail(&obj->global_list, &dev_priv->mm.unbound_list);
 	}
 
@@ -3546,16 +3570,23 @@ i915_gem_object_bind_to_vm(struct drm_i915_gem_object *obj,
 		return ERR_PTR(-E2BIG);
 	}
 
-	ret = i915_gem_object_get_pages(obj);
-	if (ret)
-		return ERR_PTR(ret);
+	if (!obj->has_vmfb_mapping) {
+		ret = i915_gem_object_get_pages(obj);
+		if (ret)
+			return ERR_PTR(ret);
 
-	i915_gem_object_pin_pages(obj);
+		i915_gem_object_pin_pages(obj);
+	}
 
 	vma = i915_gem_obj_lookup_or_create_vma(obj, vm);
 	if (IS_ERR(vma))
 		goto err_unpin;
 
+	if (obj->has_vmfb_mapping && i915_is_ggtt(vm)) {
+		vma->node.allocated = 1;
+		goto bind;
+	}
+
 search_free:
 	ret = drm_mm_insert_node_in_range_generic(&vm->mm, &vma->node,
 						  size, alignment,
@@ -3571,6 +3602,12 @@ search_free:
 		if (ret == 0)
 			goto search_free;
 
+		DRM_ERROR("fail to allocate space from %s GM space, size: %u.\n",
+				obj->map_and_fenceable ? "low" : "whole",
+				size);
+
+		dump_stack();
+
 		goto err_free_vma;
 	}
 	if (WARN_ON(!i915_gem_valid_gtt_space(vma, obj->cache_level))) {
@@ -3585,6 +3622,7 @@ search_free:
 	list_move_tail(&obj->global_list, &dev_priv->mm.bound_list);
 	list_add_tail(&vma->mm_list, &vm->inactive_list);
 
+bind:
 	trace_i915_vma_bind(vma, flags);
 	vma->bind_vma(vma, obj->cache_level,
 		      flags & PIN_GLOBAL ? GLOBAL_BIND : 0);
@@ -5057,6 +5095,12 @@ i915_gem_load(struct drm_device *dev)
 	else
 		dev_priv->num_fence_regs = 8;
 
+#ifdef DRM_I915_VGT_SUPPORT
+	if (USES_VGT(dev))
+		dev_priv->num_fence_regs = vgt_avail_fence_num(dev_priv);
+	printk("i915: the number of the fence registers (%d)\n", dev_priv->num_fence_regs);
+#endif
+
 	/* Initialize fence registers to zero */
 	INIT_LIST_HEAD(&dev_priv->mm.fence_list);
 	i915_gem_restore_fences(dev);
diff --git a/drivers/gpu/drm/i915/i915_gem_context.c b/drivers/gpu/drm/i915/i915_gem_context.c
index d011ec8..0aba453 100644
--- a/drivers/gpu/drm/i915/i915_gem_context.c
+++ b/drivers/gpu/drm/i915/i915_gem_context.c
@@ -320,6 +320,9 @@ int i915_gem_context_init(struct drm_device *dev)
 	struct intel_context *ctx;
 	int i;
 
+	if (i915.ctx_switch == false)
+		return 0;
+
 	/* Init should only be called once per module load. Eventually the
 	 * restriction on the context_disabled check can be loosened. */
 	if (WARN_ON(dev_priv->ring[RCS].default_context))
@@ -492,7 +495,7 @@ mi_set_context(struct intel_engine_cs *ring,
 	}
 
 	/* These flags are for resource streamer on HSW+ */
-	if (!IS_HASWELL(ring->dev) && INTEL_INFO(ring->dev)->gen < 8)
+	if ((!IS_HASWELL(ring->dev) || USES_VGT(ring->dev)) && INTEL_INFO(ring->dev)->gen < 8)
 		flags |= (MI_SAVE_EXT_STATE_EN | MI_RESTORE_EXT_STATE_EN);
 
 
diff --git a/drivers/gpu/drm/i915/i915_gem_execbuffer.c b/drivers/gpu/drm/i915/i915_gem_execbuffer.c
index f06027b..e7e6c83 100644
--- a/drivers/gpu/drm/i915/i915_gem_execbuffer.c
+++ b/drivers/gpu/drm/i915/i915_gem_execbuffer.c
@@ -996,6 +996,44 @@ i915_gem_execbuffer_retire_commands(struct drm_device *dev,
 	(void)__i915_add_request(ring, file, obj, NULL);
 }
 
+static int debugon = 0;
+
+void i915_batchbuffer_print_debug_off(void)
+{
+	debugon=0;
+}
+EXPORT_SYMBOL_GPL(i915_batchbuffer_print_debug_off);
+
+void i915_batchbuffer_print_debug_on(void)
+{
+	debugon=1;
+}
+EXPORT_SYMBOL_GPL(i915_batchbuffer_print_debug_on);
+
+static void i915_batchbuffer_print(struct drm_device *dev,
+               struct drm_i915_gem_object *obj,
+               unsigned long start,
+               unsigned long len)
+{
+	struct drm_i915_private *dev_priv;
+	int i;
+	u32 *mem;
+
+	if (!debugon)
+		return;
+
+	printk("batch buffer: start=0x%lx len=%lx\n", start, len);
+
+	dev_priv = dev->dev_private;
+
+	mem = io_mapping_map_wc(dev_priv->gtt.mappable,
+			start);
+
+	for (i = 0; i < len ; i += 16)
+		printk("%08x :  %08x %08x %08x %08x\n", i, mem[i / 4], mem[i/4+1], mem[i/4+2], mem[i/4+3]);
+	io_mapping_unmap(mem);
+}
+
 static int
 i915_reset_gen7_sol_offsets(struct drm_device *dev,
 			    struct intel_engine_cs *ring)
@@ -1197,6 +1235,7 @@ i915_gem_ringbuffer_submission(struct drm_device *dev, struct drm_file *file,
 			if (ret)
 				goto error;
 
+			i915_batchbuffer_print(dev, batch_obj, exec_start,exec_len);
 			ret = ring->dispatch_execbuffer(ring,
 							exec_start, exec_len,
 							flags);
@@ -1204,6 +1243,7 @@ i915_gem_ringbuffer_submission(struct drm_device *dev, struct drm_file *file,
 				goto error;
 		}
 	} else {
+		i915_batchbuffer_print(dev, batch_obj, exec_start,exec_len);
 		ret = ring->dispatch_execbuffer(ring,
 						exec_start, exec_len,
 						flags);
diff --git a/drivers/gpu/drm/i915/i915_gem_gtt.c b/drivers/gpu/drm/i915/i915_gem_gtt.c
index 171f6ea..997ec8a 100644
--- a/drivers/gpu/drm/i915/i915_gem_gtt.c
+++ b/drivers/gpu/drm/i915/i915_gem_gtt.c
@@ -30,6 +30,260 @@
 #include "i915_trace.h"
 #include "intel_drv.h"
 
+#define GEN6_PPGTT_PD_ENTRIES 512
+#define I915_PPGTT_PT_ENTRIES (PAGE_SIZE / sizeof(gen6_gtt_pte_t))
+
+/* PPGTT stuff */
+#define GEN6_GTT_ADDR_ENCODE(addr)	((addr) | (((addr) >> 28) & 0xff0))
+#define HSW_GTT_ADDR_ENCODE(addr)	((addr) | (((addr) >> 28) & 0x7f0))
+
+#define GEN6_PDE_VALID			(1 << 0)
+/* gen6+ has bit 11-4 for physical addr bit 39-32 */
+#define GEN6_PDE_ADDR_ENCODE(addr)	GEN6_GTT_ADDR_ENCODE(addr)
+
+#define GEN6_PTE_VALID			(1 << 0)
+#define GEN6_PTE_UNCACHED		(1 << 1)
+#define HSW_PTE_UNCACHED		(0)
+#define GEN6_PTE_CACHE_LLC		(2 << 1)
+#define GEN7_PTE_CACHE_L3_LLC		(3 << 1)
+#define GEN6_PTE_ADDR_ENCODE(addr)	GEN6_GTT_ADDR_ENCODE(addr)
+#define HSW_PTE_ADDR_ENCODE(addr)	HSW_GTT_ADDR_ENCODE(addr)
+
+/* Cacheability Control is a 4-bit value. The low three bits are stored in *
+ * bits 3:1 of the PTE, while the fourth bit is stored in bit 11 of the PTE.
+ */
+#define HSW_CACHEABILITY_CONTROL(bits)	((((bits) & 0x7) << 1) | \
+					 (((bits) & 0x8) << (11 - 3)))
+#define HSW_WB_LLC_AGE3			HSW_CACHEABILITY_CONTROL(0x2)
+#define HSW_WB_LLC_AGE0			HSW_CACHEABILITY_CONTROL(0x3)
+#define HSW_WB_ELLC_LLC_AGE0		HSW_CACHEABILITY_CONTROL(0xb)
+#define HSW_WB_ELLC_LLC_AGE3		HSW_CACHEABILITY_CONTROL(0x8)
+#define HSW_WT_ELLC_LLC_AGE0		HSW_CACHEABILITY_CONTROL(0x6)
+#define HSW_WT_ELLC_LLC_AGE3		HSW_CACHEABILITY_CONTROL(0x7)
+
+#define GEN8_PTES_PER_PAGE		(PAGE_SIZE / sizeof(gen8_gtt_pte_t))
+#define GEN8_PDES_PER_PAGE		(PAGE_SIZE / sizeof(gen8_ppgtt_pde_t))
+#define GEN8_LEGACY_PDPS		4
+
+#define PPAT_UNCACHED_INDEX		(_PAGE_PWT | _PAGE_PCD)
+#define PPAT_CACHED_PDE_INDEX		0 /* WB LLC */
+#define PPAT_CACHED_INDEX		_PAGE_PAT /* WB LLCeLLC */
+#define PPAT_DISPLAY_ELLC_INDEX		_PAGE_PCD /* WT eLLC */
+
+#ifdef DRM_I915_VGT_SUPPORT
+struct _balloon_info_ {
+	/*
+	 * There are up to 2 regions per aperture/gmadr that 
+	 * might be ballooned, per assigned aperture/gmadr.
+	 * Here, ballooned gmadr doesn't include the
+	 * might-be overlap aperture areas, and index 0/1 is for 
+	 * aperture, 2/3 for gmadr.
+	 */
+	struct drm_mm_node space[4];
+} bl_info;
+
+static int i915_balloon_space(
+			struct drm_mm *mm,
+			struct drm_mm_node *node,
+			unsigned long start,
+			unsigned long end)
+{
+	unsigned long size = end - start;
+
+	if (start == end)
+		return -EEXIST;
+
+	printk("i915_balloon_space: range [ 0x%lx - 0x%lx ] %lu KB.\n",
+			start, end, size / 1024);
+
+	return drm_mm_insert_node_in_range_generic(mm, node, size, 0, 0, start, end,
+						  DRM_MM_SEARCH_DEFAULT,
+						  DRM_MM_CREATE_DEFAULT);
+}
+
+static void i915_deballoon(struct drm_i915_private *dev_priv)
+{
+	int i;
+
+	printk("i915 deballoon.\n");
+
+	for (i = 0; i < 4; i++) {
+		if (bl_info.space[i].allocated)
+			drm_mm_remove_node(&bl_info.space[i]);
+	}
+
+	memset (&bl_info, 0, sizeof(bl_info));
+}
+
+/*
+ *  return vgt version if it is, otherwise 0.
+ */
+void i915_check_vgt(struct drm_i915_private *dev_priv)
+{
+	uint64_t	magic;
+	uint32_t	version;
+
+	magic = I915_READ64(vgt_info_off(magic));
+	if (magic != VGT_MAGIC) {
+		printk(KERN_ERR "Wrong vgt_if magic number!\n");
+		return;
+	}
+	version = (I915_READ16(vgt_info_off(version_major)) << 16) |
+			I915_READ16(vgt_info_off(version_minor));
+
+	if (version == VGT_IF_VERSION)
+		i915.enable_vgt = 1;
+}
+
+static int sanitize_enable_ppgtt(struct drm_device *dev, int enable_ppgtt);
+static int i915_balloon(struct drm_i915_private *dev_priv)
+{
+	unsigned long low_gm_base, low_gm_size, low_gm_end;
+	unsigned long high_gm_base, high_gm_size, high_gm_end;
+	int fail = 0;
+
+	bool enable_ppgtt = sanitize_enable_ppgtt(dev_priv->dev, i915.enable_ppgtt);
+	bool ppgtt_pdes_allocated = false;
+
+	/* At the end of low_gm and high_gm there is a guard page,
+	 * respectively.
+	 *
+	 * And, if i915 wants to enable PPGTT, we also need to reserve
+	 * I915_PPGTT_PD_ENTRIES pages at the end of high_gm (in this
+	 * case, the guard page is the page that is just before the
+	 * I915_PPGTT_PD_ENTRIES pages).
+	 * If the size of high_gm is not big enough, we try to reserve
+	 * the I915_PPGTT_PD_ENTRIES pages at the end of low_gm.
+	 */
+
+	unsigned long guard_pg_sz = PAGE_SIZE;
+	unsigned long rsvd_pg_sz_for_ppgtt = GEN6_PPGTT_PD_ENTRIES * PAGE_SIZE;
+
+	printk("i915 ballooning.\n");
+
+	low_gm_base = I915_READ(vgt_info_off(avail_rs.low_gmadr.my_base));
+	low_gm_size = I915_READ(vgt_info_off(avail_rs.low_gmadr.my_size));
+	high_gm_base = I915_READ(vgt_info_off(avail_rs.high_gmadr.my_base));
+	high_gm_size = I915_READ(vgt_info_off(avail_rs.high_gmadr.my_size));
+
+	low_gm_end = low_gm_base + low_gm_size;
+	high_gm_end = high_gm_base + high_gm_size;
+
+	printk("Ballooning configuration:\n");
+	printk("Low GM: base 0x%lx size %ldKB\n", low_gm_base, low_gm_size / 1024);
+	printk("High GM: base 0x%lx size %ldKB\n", high_gm_base, high_gm_size / 1024);
+
+	if (low_gm_base < dev_priv->gtt.base.start
+			|| low_gm_end > dev_priv->gtt.mappable_end
+			|| high_gm_base < dev_priv->gtt.base.start
+			|| high_gm_end > dev_priv->gtt.base.start + dev_priv->gtt.base.total) {
+		printk(KERN_ERR "Invalid ballooning configuration!\n");
+		return -EINVAL;
+	}
+
+	dev_priv->mm.vgt_low_gm_base = low_gm_base;
+	dev_priv->mm.vgt_low_gm_size = low_gm_size;
+	dev_priv->mm.vgt_high_gm_base = high_gm_base;
+	dev_priv->mm.vgt_high_gm_size = high_gm_size;
+
+	memset (&bl_info, 0, sizeof(bl_info));
+
+	/* High GM ballooning */
+	if (high_gm_base > dev_priv->gtt.mappable_end) {
+	        fail = i915_balloon_space(
+			&dev_priv->gtt.base.mm,
+			&bl_info.space[2],
+			dev_priv->gtt.mappable_end, high_gm_base);
+
+		if (fail)
+			goto err;
+	}
+
+	if (high_gm_end <= dev_priv->gtt.base.start + dev_priv->gtt.base.total) {
+		if (enable_ppgtt && (high_gm_size >= rsvd_pg_sz_for_ppgtt)) {
+			/*
+			 * Allocated PPGTT PDES from High GM.
+			 */
+			high_gm_size -= rsvd_pg_sz_for_ppgtt;
+			ppgtt_pdes_allocated = true;
+		}
+
+		if (high_gm_size > guard_pg_sz) {
+			high_gm_size -= guard_pg_sz;
+		} else {
+			/* high_gm_size is in MB and rsvd_pg_sz_for_ppgtt is
+			 * actually 2M, so gmadr_size must be 0 here.
+			 */
+			BUG_ON(high_gm_size != 0);
+		}
+
+	        fail = i915_balloon_space(
+			&dev_priv->gtt.base.mm,
+			&bl_info.space[3],
+			high_gm_base + high_gm_size,
+			dev_priv->gtt.base.start + dev_priv->gtt.base.total);
+
+		if (fail)
+			goto err;
+	}
+
+	/* Low GM ballooning */
+	if (low_gm_base > dev_priv->gtt.base.start) {
+	        fail = i915_balloon_space(
+			&dev_priv->gtt.base.mm,
+			&bl_info.space[0],
+			dev_priv->gtt.base.start, low_gm_base);
+
+		if (fail)
+			goto err;
+	}
+
+	if (low_gm_end <= dev_priv->gtt.mappable_end) {
+		if (enable_ppgtt && !ppgtt_pdes_allocated &&
+				(low_gm_size >= rsvd_pg_sz_for_ppgtt)) {
+			/*
+			 * Allocate PPGTT PDES from low GM.
+			 */
+			low_gm_size -= rsvd_pg_sz_for_ppgtt;
+			ppgtt_pdes_allocated = true;
+		}
+
+		if (low_gm_size > guard_pg_sz) {
+			low_gm_size -= guard_pg_sz;
+		} else {
+			/* apert_size is in MB and rsvd_pg_sz_for_ppgtt is
+			 * actually 2M, so apert_size can only be 0 here.
+			 */
+			BUG_ON(low_gm_size != 0);
+		}
+
+	        fail = i915_balloon_space(
+			&dev_priv->gtt.base.mm,
+			&bl_info.space[1],
+			low_gm_base + low_gm_size,
+			dev_priv->gtt.mappable_end);
+
+		if (fail)
+			goto err;
+	}
+
+	if (enable_ppgtt) {
+		if (!ppgtt_pdes_allocated) {
+			printk("vGT: can not get space for PPGTT table!\n");
+			goto err;
+		}
+	}
+
+	printk("balloon successfully\n");
+	return 0;
+
+err:
+	printk(KERN_ERR "balloon fail!\n");
+	i915_deballoon(dev_priv);
+	return -ENOMEM;
+}
+#endif
+
 static void bdw_setup_private_ppat(struct drm_i915_private *dev_priv);
 static void chv_setup_private_ppat(struct drm_i915_private *dev_priv);
 
@@ -37,12 +291,20 @@ static int sanitize_enable_ppgtt(struct drm_device *dev, int enable_ppgtt)
 {
 	bool has_aliasing_ppgtt;
 	bool has_full_ppgtt;
+	/* Disable ppgtt on SNB since it isn't supported by vgt on SNB */
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	if (INTEL_INFO(dev)->gen == 6 && USES_VGT(dev))
+		return false;
+
 
 	has_aliasing_ppgtt = INTEL_INFO(dev)->gen >= 6;
 	has_full_ppgtt = INTEL_INFO(dev)->gen >= 7;
 	if (IS_GEN8(dev))
 		has_full_ppgtt = false; /* XXX why? */
-
+#ifdef DRM_I915_VGT_SUPPORT
+	if (USES_VGT(dev))
+		has_full_ppgtt = false;
+#endif
 	/*
 	 * We don't allow disabling PPGTT for gen9+ as it's a requirement for
 	 * execlists, the sole mechanism available to submit work.
@@ -388,11 +650,34 @@ static void gen8_ppgtt_unmap_pages(struct i915_hw_ppgtt *ppgtt)
 	}
 }
 
+static int gen8_ppgtt_notify_vgt(struct i915_hw_ppgtt *ppgtt, int msg)
+{
+	struct drm_i915_private *dev_priv = ppgtt->base.dev->dev_private;
+	int used_pd = ppgtt->num_pd_entries / GEN8_PDES_PER_PAGE;
+
+	int i;
+
+	for (i = 0; i < used_pd; i++) {
+		dma_addr_t addr = ppgtt->pd_dma_addr[i];
+		unsigned int off = vgt_info_off(pdp0_lo) + i * 8;
+
+		I915_WRITE(off, addr & 0xffffffff);
+		I915_WRITE(off + 4, addr >> 32);
+	}
+
+	I915_WRITE(vgt_info_off(g2v_notify), msg);
+
+	return 0;
+}
+
 static void gen8_ppgtt_cleanup(struct i915_address_space *vm)
 {
 	struct i915_hw_ppgtt *ppgtt =
 		container_of(vm, struct i915_hw_ppgtt, base);
 
+	if (USES_VGT(ppgtt->base.dev))
+		gen8_ppgtt_notify_vgt(ppgtt, VGT_G2V_PPGTT_L3_PAGE_TABLE_DESTROY);
+
 	gen8_ppgtt_unmap_pages(ppgtt);
 	gen8_ppgtt_free(ppgtt);
 }
@@ -616,6 +901,10 @@ static int gen8_ppgtt_init(struct i915_hw_ppgtt *ppgtt, uint64_t size)
 	DRM_DEBUG_DRIVER("Allocated %d pages for page tables (%lld wasted)\n",
 			 ppgtt->num_pd_entries,
 			 (ppgtt->num_pd_entries - min_pt_pages) + size % (1<<30));
+
+	if (USES_VGT(ppgtt->base.dev))
+		gen8_ppgtt_notify_vgt(ppgtt, VGT_G2V_PPGTT_L3_PAGE_TABLE_CREATE);
+
 	return 0;
 
 bail:
@@ -644,7 +933,7 @@ static void gen6_dump_ppgtt(struct i915_hw_ppgtt *ppgtt, struct seq_file *m)
 		u32 expected;
 		gen6_gtt_pte_t *pt_vaddr;
 		dma_addr_t pt_addr = ppgtt->pt_dma_addr[pde];
-		pd_entry = readl(pd_addr + pde);
+		pd_entry = GTT_READ32(pd_addr + pde);
 		expected = (GEN6_PDE_ADDR_ENCODE(pt_addr) | GEN6_PDE_VALID);
 
 		if (pd_entry != expected)
@@ -697,9 +986,9 @@ static void gen6_write_pdes(struct i915_hw_ppgtt *ppgtt)
 		pd_entry = GEN6_PDE_ADDR_ENCODE(pt_addr);
 		pd_entry |= GEN6_PDE_VALID;
 
-		writel(pd_entry, pd_addr + i);
+		GTT_WRITE32(pd_entry, pd_addr + i);
 	}
-	readl(pd_addr);
+	GTT_READ32(pd_addr);
 }
 
 static uint32_t get_pd_offset(struct i915_hw_ppgtt *ppgtt)
@@ -714,6 +1003,17 @@ static int hsw_mm_switch(struct i915_hw_ppgtt *ppgtt,
 {
 	int ret;
 
+#ifdef DRM_I915_VGT_SUPPORT
+	if (USES_VGT(ring->dev)) {
+		struct drm_i915_private *dev_priv = ring->dev->dev_private;
+
+		I915_WRITE(RING_PP_DIR_DCLV(ring), PP_DIR_DCLV_2G);
+		I915_WRITE(RING_PP_DIR_BASE(ring), get_pd_offset(ppgtt));
+
+		return 0;
+	}
+#endif
+
 	/* NB: TLBs must be flushed and invalidated before a switch */
 	ret = ring->flush(ring, I915_GEM_GPU_DOMAINS, I915_GEM_GPU_DOMAINS);
 	if (ret)
@@ -872,6 +1172,43 @@ static void gen6_ppgtt_clear_range(struct i915_address_space *vm,
 	}
 }
 
+static void gen6_ppgtt_insert_vmfb_entries(struct i915_address_space *vm,
+					   uint32_t num_pages,
+					   uint64_t start,
+					   unsigned vmfb_offset)
+{
+	struct i915_hw_ppgtt *ppgtt =
+		container_of(vm, struct i915_hw_ppgtt, base);
+	gen6_gtt_pte_t *pt_vaddr;
+	unsigned first_entry = start >> PAGE_SHIFT;
+	unsigned act_pt = first_entry / I915_PPGTT_PT_ENTRIES;
+	unsigned act_pte = first_entry % I915_PPGTT_PT_ENTRIES;
+
+	pt_vaddr = NULL;
+
+	struct drm_i915_private *dev_priv = ppgtt->base.dev->dev_private;
+	uint32_t __iomem *vmfb_start = dev_priv->gtt.gsm;
+	vmfb_start += vmfb_offset;
+
+	int i;
+	for (i = 0; i < num_pages; i++) {
+		if (pt_vaddr == NULL)
+			pt_vaddr = kmap_atomic(ppgtt->pt_pages[act_pt]);
+
+		pt_vaddr[act_pte] = GTT_READ32(vmfb_start);
+		vmfb_start++;
+
+		if (++act_pte == I915_PPGTT_PT_ENTRIES) {
+			kunmap_atomic(pt_vaddr);
+			pt_vaddr = NULL;
+			act_pt++;
+			act_pte = 0;
+		}
+	}
+	if (pt_vaddr)
+		kunmap_atomic(pt_vaddr);
+}
+
 static void gen6_ppgtt_insert_entries(struct i915_address_space *vm,
 				      struct sg_table *pages,
 				      uint64_t start,
@@ -1208,8 +1545,14 @@ ppgtt_bind_vma(struct i915_vma *vma,
 	if (vma->obj->gt_ro)
 		flags |= PTE_READ_ONLY;
 
-	vma->vm->insert_entries(vma->vm, vma->obj->pages, vma->node.start,
-				cache_level, flags);
+	if (vma->obj->has_vmfb_mapping)
+		gen6_ppgtt_insert_vmfb_entries(vma->vm,
+					       vma->obj->base.size >> PAGE_SHIFT,
+					       vma->node.start,
+					       i915_gem_obj_ggtt_offset(vma->obj) >> PAGE_SHIFT);
+	else
+		vma->vm->insert_entries(vma->vm, vma->obj->pages, vma->node.start,
+					cache_level, flags);
 }
 
 static void ppgtt_unbind_vma(struct i915_vma *vma)
@@ -1372,7 +1715,7 @@ void i915_gem_restore_gtt_mappings(struct drm_device *dev)
 
 int i915_gem_gtt_prepare_object(struct drm_i915_gem_object *obj)
 {
-	if (obj->has_dma_mapping)
+	if (obj->has_dma_mapping || obj->has_vmfb_mapping)
 		return 0;
 
 	if (!dma_map_sg(&obj->base.dev->pdev->dev,
@@ -1383,13 +1726,14 @@ int i915_gem_gtt_prepare_object(struct drm_i915_gem_object *obj)
 	return 0;
 }
 
-static inline void gen8_set_pte(void __iomem *addr, gen8_gtt_pte_t pte)
+static inline void gen8_set_pte(void __iomem *addr, gen8_gtt_pte_t pte,
+					struct drm_i915_private *dev_priv)
 {
 #ifdef writeq
-	writeq(pte, addr);
+	GTT_WRITE64(pte, addr);
 #else
-	iowrite32((u32)pte, addr);
-	iowrite32(pte >> 32, addr + 4);
+	GTT_WRITE32((u32)pte, addr);
+	GTT_WRITE32(pte >> 32, addr + 4);
 #endif
 }
 
@@ -1410,7 +1754,7 @@ static void gen8_ggtt_insert_entries(struct i915_address_space *vm,
 		addr = sg_dma_address(sg_iter.sg) +
 			(sg_iter.sg_pgoffset << PAGE_SHIFT);
 		gen8_set_pte(&gtt_entries[i],
-			     gen8_pte_encode(addr, level, true));
+			     gen8_pte_encode(addr, level, true), dev_priv);
 		i++;
 	}
 
@@ -1422,7 +1766,7 @@ static void gen8_ggtt_insert_entries(struct i915_address_space *vm,
 	 * hardware should work, we must keep this posting read for paranoia.
 	 */
 	if (i != 0)
-		WARN_ON(readq(&gtt_entries[i-1])
+		WARN_ON(GTT_READ64(&gtt_entries[i-1])
 			!= gen8_pte_encode(addr, level, true));
 
 	/* This next bit makes the above posting read even more important. We
@@ -1450,11 +1794,11 @@ static void gen6_ggtt_insert_entries(struct i915_address_space *vm,
 		(gen6_gtt_pte_t __iomem *)dev_priv->gtt.gsm + first_entry;
 	int i = 0;
 	struct sg_page_iter sg_iter;
-	dma_addr_t addr = 0;
+	dma_addr_t addr;
 
 	for_each_sg_page(st->sgl, &sg_iter, st->nents, 0) {
 		addr = sg_page_iter_dma_address(&sg_iter);
-		iowrite32(vm->pte_encode(addr, level, true, flags), &gtt_entries[i]);
+		GTT_WRITE32(vm->pte_encode(addr, level, true, flags), &gtt_entries[i]);
 		i++;
 	}
 
@@ -1465,7 +1809,7 @@ static void gen6_ggtt_insert_entries(struct i915_address_space *vm,
 	 * hardware should work, we must keep this posting read for paranoia.
 	 */
 	if (i != 0) {
-		unsigned long gtt = readl(&gtt_entries[i-1]);
+		unsigned long gtt = GTT_READ32(&gtt_entries[i-1]);
 		WARN_ON(gtt != vm->pte_encode(addr, level, true, flags));
 	}
 
@@ -1499,8 +1843,8 @@ static void gen8_ggtt_clear_range(struct i915_address_space *vm,
 				      I915_CACHE_LLC,
 				      use_scratch);
 	for (i = 0; i < num_entries; i++)
-		gen8_set_pte(&gtt_base[i], scratch_pte);
-	readl(gtt_base);
+		gen8_set_pte(&gtt_base[i], scratch_pte, dev_priv);
+	GTT_READ32(gtt_base);
 }
 
 static void gen6_ggtt_clear_range(struct i915_address_space *vm,
@@ -1524,11 +1868,10 @@ static void gen6_ggtt_clear_range(struct i915_address_space *vm,
 	scratch_pte = vm->pte_encode(vm->scratch.addr, I915_CACHE_LLC, use_scratch, 0);
 
 	for (i = 0; i < num_entries; i++)
-		iowrite32(scratch_pte, &gtt_base[i]);
-	readl(gtt_base);
+		GTT_WRITE32(scratch_pte, &gtt_base[i]);
+	GTT_READ32(gtt_base);
 }
 
-
 static void i915_ggtt_bind_vma(struct i915_vma *vma,
 			       enum i915_cache_level cache_level,
 			       u32 unused)
@@ -1538,7 +1881,8 @@ static void i915_ggtt_bind_vma(struct i915_vma *vma,
 		AGP_USER_MEMORY : AGP_USER_CACHED_MEMORY;
 
 	BUG_ON(!i915_is_ggtt(vma->vm));
-	intel_gtt_insert_sg_entries(vma->obj->pages, entry, flags);
+	if (!vma->obj->has_vmfb_mapping)
+		intel_gtt_insert_sg_entries(vma->obj->pages, entry, flags);
 	vma->bound = GLOBAL_BIND;
 }
 
@@ -1559,7 +1903,8 @@ static void i915_ggtt_unbind_vma(struct i915_vma *vma)
 
 	BUG_ON(!i915_is_ggtt(vma->vm));
 	vma->bound = 0;
-	intel_gtt_clear_range(first, size);
+	if (!vma->obj->has_vmfb_mapping)
+		intel_gtt_clear_range(first, size);
 }
 
 static void ggtt_bind_vma(struct i915_vma *vma,
@@ -1588,9 +1933,10 @@ static void ggtt_bind_vma(struct i915_vma *vma,
 	if (!dev_priv->mm.aliasing_ppgtt || flags & GLOBAL_BIND) {
 		if (!(vma->bound & GLOBAL_BIND) ||
 		    (cache_level != obj->cache_level)) {
-			vma->vm->insert_entries(vma->vm, obj->pages,
-						vma->node.start,
-						cache_level, flags);
+			if (!obj->has_vmfb_mapping)
+				vma->vm->insert_entries(vma->vm, obj->pages,
+							vma->node.start,
+							cache_level, flags);
 			vma->bound |= GLOBAL_BIND;
 		}
 	}
@@ -1599,10 +1945,16 @@ static void ggtt_bind_vma(struct i915_vma *vma,
 	    (!(vma->bound & LOCAL_BIND) ||
 	     (cache_level != obj->cache_level))) {
 		struct i915_hw_ppgtt *appgtt = dev_priv->mm.aliasing_ppgtt;
-		appgtt->base.insert_entries(&appgtt->base,
-					    vma->obj->pages,
-					    vma->node.start,
-					    cache_level, flags);
+		if (obj->has_vmfb_mapping)
+			gen6_ppgtt_insert_vmfb_entries(&appgtt->base,
+						       obj->base.size >> PAGE_SHIFT,
+						       vma->node.start,
+						       i915_gem_obj_ggtt_offset(obj) >> PAGE_SHIFT);
+		else
+			appgtt->base.insert_entries(&appgtt->base,
+							    vma->obj->pages,
+							    vma->node.start,
+							    cache_level, flags);
 		vma->bound |= LOCAL_BIND;
 	}
 }
@@ -1614,19 +1966,21 @@ static void ggtt_unbind_vma(struct i915_vma *vma)
 	struct drm_i915_gem_object *obj = vma->obj;
 
 	if (vma->bound & GLOBAL_BIND) {
-		vma->vm->clear_range(vma->vm,
-				     vma->node.start,
-				     obj->base.size,
-				     true);
+		if (!obj->has_vmfb_mapping)
+			vma->vm->clear_range(vma->vm,
+					     vma->node.start,
+					     obj->base.size,
+					     true);
 		vma->bound &= ~GLOBAL_BIND;
 	}
 
 	if (vma->bound & LOCAL_BIND) {
 		struct i915_hw_ppgtt *appgtt = dev_priv->mm.aliasing_ppgtt;
-		appgtt->base.clear_range(&appgtt->base,
-					 vma->node.start,
-					 obj->base.size,
-					 true);
+		if (!obj->has_vmfb_mapping)
+			appgtt->base.clear_range(&appgtt->base,
+						 vma->node.start,
+						 obj->base.size,
+						 true);
 		vma->bound &= ~LOCAL_BIND;
 	}
 }
@@ -1683,15 +2037,30 @@ static int i915_gem_setup_global_gtt(struct drm_device *dev,
 	struct drm_mm_node *entry;
 	struct drm_i915_gem_object *obj;
 	unsigned long hole_start, hole_end;
-	int ret;
+	int ret = 0;
 
 	BUG_ON(mappable_end > end);
 
-	/* Subtract the guard page ... */
-	drm_mm_init(&ggtt_vm->mm, start, end - start - PAGE_SIZE);
+	printk("Eddie: mappable_end %lx\n", mappable_end);
+      
+       if ( mappable_end > end )
+               mappable_end = end;
+
+	drm_mm_init(&ggtt_vm->mm, start, end - start);
 	if (!HAS_LLC(dev))
 		dev_priv->gtt.base.mm.color_adjust = i915_gtt_color_adjust;
 
+	dev_priv->gtt.base.start = start;
+	dev_priv->gtt.base.total = end - start;
+
+#ifdef DRM_I915_VGT_SUPPORT
+	/*
+	 * Do ballooning before touching GEM gtt space.
+	 */
+	if (USES_VGT(dev))
+		ret = i915_balloon(dev_priv);
+#endif
+
 	/* Mark any preallocated objects as occupied */
 	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list) {
 		struct i915_vma *vma = i915_gem_obj_to_vma(obj, ggtt_vm);
@@ -1708,9 +2077,6 @@ static int i915_gem_setup_global_gtt(struct drm_device *dev,
 		vma->bound |= GLOBAL_BIND;
 	}
 
-	dev_priv->gtt.base.start = start;
-	dev_priv->gtt.base.total = end - start;
-
 	/* Clear any non-preallocated blocks */
 	drm_mm_for_each_hole(entry, &ggtt_vm->mm, hole_start, hole_end) {
 		DRM_DEBUG_KMS("clearing unused GTT space: [%lx, %lx]\n",
@@ -2141,6 +2507,9 @@ int i915_gem_gtt_init(struct drm_device *dev)
 	if (ret)
 		return ret;
 
+	if (USES_VGT(dev))
+		gtt->stolen_size = 0;
+
 	gtt->base.dev = dev;
 
 	/* GMADR is the PCI mmio aperture into the global GTT. */
diff --git a/drivers/gpu/drm/i915/i915_gem_tiling.c b/drivers/gpu/drm/i915/i915_gem_tiling.c
index 4727a4e..91e2bd1 100644
--- a/drivers/gpu/drm/i915/i915_gem_tiling.c
+++ b/drivers/gpu/drm/i915/i915_gem_tiling.c
@@ -130,6 +130,12 @@ i915_gem_detect_bit_6_swizzle(struct drm_device *dev)
 				swizzle_y = I915_BIT_6_SWIZZLE_NONE;
 			}
 		}
+		/* FIXME: Linux and Windows have different swizzling setting
+		 * which would cause trouble. Now hardcode Linux side to sync
+		 * with Windows side. Need better cleanup in the future
+		 */
+		swizzle_x = I915_BIT_6_SWIZZLE_9_10;
+		swizzle_y = I915_BIT_6_SWIZZLE_9;
 	} else if (IS_GEN5(dev)) {
 		/* On Ironlake whatever DRAM config, GPU always do
 		 * same swizzling setup.
diff --git a/drivers/gpu/drm/i915/i915_gem_vgtbuffer.c b/drivers/gpu/drm/i915/i915_gem_vgtbuffer.c
new file mode 100644
index 0000000..789fb5c
--- /dev/null
+++ b/drivers/gpu/drm/i915/i915_gem_vgtbuffer.c
@@ -0,0 +1,223 @@
+/*
+ * Copyright  2012 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ *
+ */
+
+#include "drmP.h"
+#include "i915_drm.h"
+#include "i915_drv.h"
+#include "i915_trace.h"
+#include "intel_drv.h"
+#include <linux/swap.h>
+#include "fb_decoder.h"
+
+static int i915_gem_vgtbuffer_get_pages(struct drm_i915_gem_object *obj)
+{
+	return 0;
+}
+
+static void i915_gem_vgtbuffer_put_pages(struct drm_i915_gem_object *obj)
+{
+}
+
+static const struct drm_i915_gem_object_ops i915_gem_vgtbuffer_ops = {
+	.get_pages = i915_gem_vgtbuffer_get_pages,
+	.put_pages = i915_gem_vgtbuffer_put_pages,
+};
+
+/**
+ * Creates a new mm object that wraps some user memory.
+ */
+int
+i915_gem_vgtbuffer_ioctl(struct drm_device *dev, void *data,
+			 struct drm_file *file)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_gem_vgtbuffer *args = data;
+	struct drm_i915_gem_object *obj;
+	struct vgt_primary_plane_format *p;
+	struct vgt_cursor_plane_format *c;
+	struct vgt_fb_format fb;
+	struct vgt_pipe_format *pipe;
+
+	int ret;
+
+	int num_pages = 0;
+
+	u32 vmid;
+	u32 handle;
+
+	uint32_t __iomem *gtt_base = dev_priv->gtt.gsm;	/* mappable_base; */
+	uint32_t gtt_fbstart;
+	uint32_t gtt_pte;
+	uint32_t gtt_offset = 0;
+
+	/* Allocate the new object */
+	DRM_DEBUG_DRIVER("VGT: gem_vgtbuffer_ioctl\n");
+
+	if (!vgt_check_host())
+		return -EPERM;
+
+	obj = i915_gem_object_alloc(dev);
+	if (obj == NULL)
+		return -ENOMEM;
+
+	vmid = args->vmid;
+	DRM_DEBUG_DRIVER("VGT: calling decode\n");
+	if (vgt_decode_fb_format(vmid, &fb)) {
+		kfree(obj);
+		return -EINVAL;
+	}
+
+	pipe = ((args->pipe_id >= I915_MAX_PIPES) ?
+		NULL : &fb.pipes[args->pipe_id]);
+
+	/* If plane is not enabled, bail */
+	if (!pipe || !pipe->primary.enabled) {
+		kfree(obj);
+		return -ENOENT;
+	}
+
+	DRM_DEBUG_DRIVER("VGT: pipe = %d\n", args->pipe_id);
+	if ((args->plane_id) == I915_VGT_PLANE_PRIMARY) {
+		DRM_DEBUG_DRIVER("VGT: &pipe=0x%x\n", (&pipe));
+		p = &pipe->primary;
+		args->enabled = p->enabled;
+		args->x_offset = p->x_offset;
+		args->y_offset = p->y_offset;
+		args->start = p->base;
+		args->width = p->width;
+		args->height = p->height;
+		args->stride = p->stride;
+		args->bpp = p->bpp;
+		args->hw_format = p->hw_format;
+		args->drm_format = p->drm_format;
+		args->tiled = p->tiled;
+		args->size = (((p->width * p->height * p->bpp) / 8) +
+			      (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+
+		uint64_t range = p->base >> PAGE_SHIFT;
+		range += args->size;
+
+		if (range > gtt_total_entries(dev_priv->gtt)) {
+			DRM_DEBUG_DRIVER("VGT: Invalid GTT offset or size\n");
+			kfree(obj);
+			return -EINVAL;
+		}
+
+		if (args->flags & I915_VGTBUFFER_QUERY_ONLY) {
+			DRM_DEBUG_DRIVER("VGT: query only: primary");
+			kfree(obj);
+			return 0;
+		}
+
+		gtt_offset = p->base;
+		num_pages = args->size;
+
+		DRM_DEBUG_DRIVER("VGT GEM: Surface GTT Offset = %x\n", p->base);
+		obj->tiling_mode = p->tiled ? I915_TILING_X : 0;
+		obj->stride = p->tiled ? args->stride : 0;
+	}
+
+	if ((args->plane_id) == I915_VGT_PLANE_CURSOR) {
+		c = &pipe->cursor;
+		args->enabled = c->enabled;
+		args->x_offset = c->x_hot;
+		args->y_offset = c->y_hot;
+		args->x_pos = c->x_pos;
+		args->y_pos = c->y_pos;
+		args->start = c->base;
+		args->width = c->width;
+		args->height = c->height;
+		args->stride = c->width * (c->bpp / 8);
+		args->bpp = c->bpp;
+		args->tiled = 0;
+		args->size = (((c->width * c->height * c->bpp) / 8) +
+			      (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+
+		uint64_t range = c->base >> PAGE_SHIFT;
+		range += args->size;
+
+		if (range > gtt_total_entries(dev_priv->gtt)) {
+			DRM_DEBUG_DRIVER("VGT: Invalid GTT offset or size\n");
+			kfree(obj);
+			return -EINVAL;
+		}
+
+		if (args->flags & I915_VGTBUFFER_QUERY_ONLY) {
+			DRM_DEBUG_DRIVER("VGT: query only: cursor");
+			kfree(obj);
+			return 0;
+		}
+
+		gtt_offset = c->base;
+		num_pages = args->size;
+
+		DRM_DEBUG_DRIVER("VGT GEM: Surface GTT Offset = %x\n", c->base);
+		obj->tiling_mode = I915_TILING_NONE;
+	}
+
+	DRM_DEBUG_DRIVER("VGT GEM: Surface size = %d\n",
+			 (int)(num_pages * PAGE_SIZE));
+
+	gtt_fbstart = gtt_offset >> PAGE_SHIFT;
+
+	DRM_DEBUG_DRIVER("VGT GEM: gtt start addr %x\n",
+			 (unsigned int)gtt_base);
+	DRM_DEBUG_DRIVER("VGT GEM: fb start %x\n", (unsigned int)gtt_fbstart);
+
+	gtt_base += gtt_fbstart;
+
+	DRM_DEBUG_DRIVER("VGT GEM: gtt + fb start  %x\n", (uint32_t) gtt_base);
+
+	DRM_DEBUG_DRIVER("VGT: gtt_base=0x%x\n", gtt_base);
+
+	gtt_pte = readl(gtt_base);
+
+	DRM_DEBUG_DRIVER("VGT GEM: pte  %x\n", (uint32_t) gtt_pte);
+	DRM_DEBUG_DRIVER("VGT GEM: num_pages from fb decode=%d  \n",
+			 (uint32_t) num_pages);
+
+	drm_gem_private_object_init(dev, &obj->base, num_pages * PAGE_SIZE);
+
+	i915_gem_object_init(obj, &i915_gem_vgtbuffer_ops);
+	obj->cache_level = I915_CACHE_L3_LLC;
+
+	struct i915_address_space *ggtt_vm = &dev_priv->gtt.base;
+	struct i915_vma *vma = i915_gem_obj_lookup_or_create_vma(obj, ggtt_vm);
+	vma->node.start = gtt_offset;
+
+	obj->has_vmfb_mapping = true;
+	obj->pages = NULL;
+
+	ret = drm_gem_handle_create(file, &obj->base, &handle);
+	/* drop reference from allocate - handle holds it now */
+	drm_gem_object_unreference(&obj->base);
+	if (ret) {
+		kfree(obj);
+		i915_gem_vma_destroy(vma);
+		return ret;
+	}
+
+	args->handle = handle;
+	return 0;
+}
diff --git a/drivers/gpu/drm/i915/i915_irq.c b/drivers/gpu/drm/i915/i915_irq.c
index cd8232d..fcb0055 100644
--- a/drivers/gpu/drm/i915/i915_irq.c
+++ b/drivers/gpu/drm/i915/i915_irq.c
@@ -45,6 +45,26 @@
  * and related files, but that will be described in separate chapters.
  */
 
+#ifdef DRM_I915_VGT_SUPPORT
+#include "vgt-if.h"
+void i915_isr_wrapper(struct irq_work *work)
+{
+	struct drm_i915_private *dev_priv = container_of(work,
+				struct drm_i915_private, irq_work);
+
+	if (!vgt_can_process_irq())
+		return;
+
+	dev_priv->irq_ops.irq_handler(dev_priv->dev->pdev->irq, dev_priv->dev);
+}
+
+void vgt_schedule_host_isr(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	irq_work_queue_on(&dev_priv->irq_work, 0);
+}
+#endif
+
 static const u32 hpd_ibx[] = {
 	[HPD_CRT] = SDE_CRT_HOTPLUG,
 	[HPD_SDVO_B] = SDE_SDVOB_HOTPLUG,
@@ -2973,6 +2993,12 @@ static void i915_hangcheck_elapsed(unsigned long data)
 	if (!i915.enable_hangcheck)
 		return;
 
+#ifdef DRM_I915_VGT_SUPPORT
+	if (i915_host_mediate &&
+			!vgt_can_process_timer(&dev_priv->gpu_error.hangcheck_timer))
+		return;
+#endif
+
 	for_each_ring(ring, dev_priv, i) {
 		u64 acthd;
 		u32 seqno;
@@ -3064,8 +3090,14 @@ static void i915_hangcheck_elapsed(unsigned long data)
 		}
 	}
 
-	if (rings_hung)
-		return i915_handle_error(dev, true, "Ring hung");
+	if (rings_hung) {
+#ifdef DRM_I915_VGT_SUPPORT
+		if (i915_host_mediate && vgt_handle_dom0_device_reset())
+			return;
+#endif
+		i915_handle_error(dev, true, "Ring hung");
+		return;
+	}
 
 	if (busy_count)
 		/* Reset timer case chip hangs without another request
@@ -3082,7 +3114,10 @@ void i915_queue_hangcheck(struct drm_device *dev)
 		return;
 
 	/* Don't continually defer the hangcheck, but make sure it is active */
-	if (timer_pending(timer))
+	/* Disable the timer pending check temporary in vgt as it hasn't complete
+	 * QoS support, if context switch takes too long will trigger the Dom0
+	 * gfx reset */
+	if (!i915.enable_vgt && timer_pending(timer))
 		return;
 	mod_timer(timer,
 		  round_jiffies_up(jiffies + DRM_I915_HANGCHECK_JIFFIES));
@@ -3335,6 +3370,15 @@ static int ironlake_irq_postinstall(struct drm_device *dev)
 				DE_PLANEA_FLIP_DONE_IVB | DE_AUX_CHANNEL_A_IVB);
 		extra_mask = (DE_PIPEC_VBLANK_IVB | DE_PIPEB_VBLANK_IVB |
 			      DE_PIPEA_VBLANK_IVB | DE_ERR_INT_IVB);
+
+		I915_WRITE(GEN7_ERR_INT, I915_READ(GEN7_ERR_INT));
+
+		/*
+		 * Do not enable ERR_INT for VGT temporarily,
+		 * as VGT doesn't handle this.
+		 */
+		if (USES_VGT(dev))
+			extra_mask &= ~DE_ERR_INT_IVB;
 	} else {
 		display_mask = (DE_MASTER_IRQ_CONTROL | DE_GSE | DE_PCH_EVENT |
 				DE_PLANEA_FLIP_DONE | DE_PLANEB_FLIP_DONE |
@@ -4309,6 +4353,54 @@ static void intel_hpd_irq_reenable_work(struct work_struct *work)
 	intel_runtime_pm_put(dev_priv);
 }
 
+#ifdef CONFIG_I915_VGT
+void *i915_drm_to_pgt(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	return dev_priv->pgt;
+}
+
+static void vgt_irq_preinstall(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	dev_priv->pgt = vgt_init_irq(dev->pdev, dev);
+	if (!dev_priv->pgt) {
+		DRM_DEBUG_DRIVER("vgt_init_irq failed, turn vgt off\n");
+		i915_host_mediate = false;
+
+		dev->driver->irq_handler = dev_priv->irq_ops.irq_handler;
+		dev->driver->irq_preinstall = dev_priv->irq_ops.irq_preinstall;
+		dev->driver->irq_postinstall = dev_priv->irq_ops.irq_postinstall;
+		dev->driver->irq_uninstall = dev_priv->irq_ops.irq_uninstall;
+
+		/* still call it for this time */
+		dev_priv->irq_ops.irq_preinstall(dev);
+
+		return;
+	}
+
+	init_irq_work(&dev_priv->irq_work, i915_isr_wrapper);
+
+	dev_priv->irq_ops.irq_preinstall(dev);
+}
+
+static int vgt_irq_postinstall(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	return dev_priv->irq_ops.irq_postinstall(dev);
+}
+
+static void vgt_irq_uninstall(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	irq_work_sync(&dev_priv->irq_work);
+
+	dev_priv->irq_ops.irq_uninstall(dev);
+	vgt_fini_irq(dev->pdev);
+}
+#endif
+
 /**
  * intel_irq_init - initializes irq support
  * @dev_priv: i915 device instance
@@ -4336,6 +4428,11 @@ void intel_irq_init(struct drm_i915_private *dev_priv)
 	setup_timer(&dev_priv->gpu_error.hangcheck_timer,
 		    i915_hangcheck_elapsed,
 		    (unsigned long) dev);
+
+#ifdef CONFIG_I915_VGT
+	vgt_new_delay_event_timer(&dev_priv->gpu_error.hangcheck_timer);
+#endif
+
 	INIT_DELAYED_WORK(&dev_priv->hotplug_reenable_work,
 			  intel_hpd_irq_reenable_work);
 
@@ -4419,6 +4516,22 @@ void intel_irq_init(struct drm_i915_private *dev_priv)
 		dev->driver->enable_vblank = i915_enable_vblank;
 		dev->driver->disable_vblank = i915_disable_vblank;
 	}
+
+#ifdef CONFIG_I915_VGT
+	if (i915_host_mediate) {
+		/* save the original irq ops */
+		dev_priv->irq_ops.irq_handler = dev->driver->irq_handler;
+		dev_priv->irq_ops.irq_preinstall = dev->driver->irq_preinstall;
+		dev_priv->irq_ops.irq_postinstall = dev->driver->irq_postinstall;
+		dev_priv->irq_ops.irq_uninstall = dev->driver->irq_uninstall;
+
+		/* let drm take vgt as hardware interrupt handler */
+		dev->driver->irq_handler = vgt_interrupt;
+		dev->driver->irq_preinstall = vgt_irq_preinstall;
+		dev->driver->irq_postinstall = vgt_irq_postinstall;
+		dev->driver->irq_uninstall = vgt_irq_uninstall;
+	}
+#endif
 }
 
 /**
diff --git a/drivers/gpu/drm/i915/i915_params.c b/drivers/gpu/drm/i915/i915_params.c
index 139f490..44c54db 100644
--- a/drivers/gpu/drm/i915/i915_params.c
+++ b/drivers/gpu/drm/i915/i915_params.c
@@ -38,6 +38,7 @@ struct i915_params i915 __read_mostly = {
 	.enable_execlists = 0,
 	.enable_hangcheck = true,
 	.enable_ppgtt = -1,
+	.ctx_switch = true,
 	.enable_psr = 0,
 	.preliminary_hw_support = IS_ENABLED(CONFIG_DRM_I915_PRELIMINARY_HW_SUPPORT),
 	.disable_power_well = 1,
@@ -51,6 +52,7 @@ struct i915_params i915 __read_mostly = {
 	.disable_vtd_wa = 0,
 	.use_mmio_flip = 0,
 	.mmio_debug = 0,
+	.enable_vgt = 0,
 };
 
 module_param_named(modeset, i915.modeset, int, 0400);
@@ -119,6 +121,10 @@ MODULE_PARM_DESC(enable_ppgtt,
 	"Override PPGTT usage. "
 	"(-1=auto [default], 0=disabled, 1=aliasing, 2=full)");
 
+module_param_named(ctx_switch, i915.ctx_switch, bool, 0600);
+MODULE_PARM_DESC(ctx_switch,
+                "Enable HW context switch (default: true)");
+
 module_param_named(enable_execlists, i915.enable_execlists, int, 0400);
 MODULE_PARM_DESC(enable_execlists,
 	"Override execlists usage. "
diff --git a/drivers/gpu/drm/i915/i915_vgt.h b/drivers/gpu/drm/i915/i915_vgt.h
new file mode 100644
index 0000000..5dc3ae3
--- /dev/null
+++ b/drivers/gpu/drm/i915/i915_vgt.h
@@ -0,0 +1,77 @@
+#ifndef _I915_VGT_H_
+#define _I915_VGT_H_
+
+#include <linux/interrupt.h>
+
+struct drm_device;
+struct drm_i915_private;
+
+#ifdef CONFIG_I915_VGT
+
+bool i915_start_vgt(struct pci_dev *);
+void i915_vgt_record_priv(struct drm_i915_private *priv);
+bool vgt_host_read(u32, void *, int, bool, bool);
+bool vgt_host_write(u32, void *, int, bool, bool);
+void vgt_schedule_host_isr(struct drm_device *);
+void *vgt_init_irq(struct pci_dev *, struct drm_device *);
+void vgt_fini_irq(struct pci_dev *);
+irqreturn_t vgt_interrupt(int, void *);
+int vgt_suspend(struct pci_dev *pdev);
+int vgt_resume(struct pci_dev *pdev);
+bool vgt_check_host(void);
+
+#else /* !CONFIG_I915_VGT */
+
+static inline bool i915_start_vgt(struct pci_dev *pdev)
+{
+	return false;
+}
+
+static inline void i915_vgt_record_priv(struct drm_i915_private *priv)
+{
+}
+
+static inline bool vgt_host_read(u32 reg, void *val, int len,
+			bool is_gtt, bool trace)
+{
+	return false;
+}
+
+static inline bool vgt_host_write(u32 reg, void *val, int len,
+			bool is_gtt, bool trace)
+{
+	return false;
+}
+
+static inline void *vgt_init_irq(struct pci_dev *pdev, struct drm_device *dev)
+{
+	return NULL;
+}
+
+static inline void vgt_fini_irq(struct pci_dev *pdev)
+{
+}
+
+static inline irqreturn_t vgt_interrupt(int irq, void *data)
+{
+	return IRQ_NONE;
+}
+
+static inline int vgt_suspend(struct pci_dev *pdev)
+{
+	return 0;
+}
+
+static inline int vgt_resume(struct pci_dev *pdev)
+{
+	return 0;
+}
+
+static inline bool vgt_check_host(void)
+{
+	return false;
+}
+
+#endif /* CONFIG_I915_VGT */
+
+#endif
diff --git a/drivers/gpu/drm/i915/intel_display.c b/drivers/gpu/drm/i915/intel_display.c
index 03d0b0c..8fc9374 100644
--- a/drivers/gpu/drm/i915/intel_display.c
+++ b/drivers/gpu/drm/i915/intel_display.c
@@ -2113,6 +2113,8 @@ void intel_flush_primary_plane(struct drm_i915_private *dev_priv,
 	struct drm_device *dev = dev_priv->dev;
 	u32 reg = INTEL_INFO(dev)->gen >= 4 ? DSPSURF(plane) : DSPADDR(plane);
 
+	printk("i915: intel_flush_display_plane\n");
+
 	I915_WRITE(reg, I915_READ(reg));
 	POSTING_READ(reg);
 }
@@ -3321,7 +3323,7 @@ static void gen6_fdi_link_train(struct drm_crtc *crtc)
 	if (i == 4)
 		DRM_ERROR("FDI train 2 fail!\n");
 
-	DRM_DEBUG_KMS("FDI train done.\n");
+	printk("FDI train done.\n");
 }
 
 /* Manual link training for Ivy Bridge A0 parts */
@@ -3363,6 +3365,9 @@ static void ivb_manual_fdi_link_train(struct drm_crtc *crtc)
 		temp &= ~FDI_RX_ENABLE;
 		I915_WRITE(reg, temp);
 
+		POSTING_READ(reg);
+		udelay(150);
+
 		/* enable CPU FDI TX and PCH FDI RX */
 		reg = FDI_TX_CTL(pipe);
 		temp = I915_READ(reg);
@@ -3405,6 +3410,9 @@ static void ivb_manual_fdi_link_train(struct drm_crtc *crtc)
 			continue;
 		}
 
+		POSTING_READ(reg);
+		udelay(150);
+
 		/* Train 2 */
 		reg = FDI_TX_CTL(pipe);
 		temp = I915_READ(reg);
@@ -9239,7 +9247,10 @@ void intel_prepare_page_flip(struct drm_device *dev, int plane)
 	 * is also accompanied by a spurious intel_prepare_page_flip().
 	 */
 	spin_lock_irqsave(&dev->event_lock, flags);
-	if (intel_crtc->unpin_work && page_flip_finished(intel_crtc))
+	/* There is case that flip interrupts come early before intel_crtc
+	 * is initalized. Add a sanity check before use it.
+	 */
+	if (intel_crtc && intel_crtc->unpin_work && page_flip_finished(intel_crtc))
 		atomic_inc_not_zero(&intel_crtc->unpin_work->pending);
 	spin_unlock_irqrestore(&dev->event_lock, flags);
 }
@@ -11526,6 +11537,8 @@ static int intel_crtc_set_config(struct drm_mode_set *set)
 	struct intel_crtc_config *pipe_config;
 	unsigned modeset_pipes, prepare_pipes, disable_pipes;
 	int ret;
+	
+	struct drm_i915_private *dev_priv;
 
 	BUG_ON(!set);
 	BUG_ON(!set->crtc);
@@ -11544,6 +11557,7 @@ static int intel_crtc_set_config(struct drm_mode_set *set)
 	}
 
 	dev = set->crtc->dev;
+	dev_priv = dev->dev_private;
 
 	ret = -ENOMEM;
 	config = kzalloc(sizeof(*config), GFP_KERNEL);
diff --git a/drivers/gpu/drm/i915/intel_lrc.c b/drivers/gpu/drm/i915/intel_lrc.c
index dd39f7c..f97458c 100644
--- a/drivers/gpu/drm/i915/intel_lrc.c
+++ b/drivers/gpu/drm/i915/intel_lrc.c
@@ -224,7 +224,7 @@ int intel_sanitize_enable_execlists(struct drm_device *dev, int enable_execlists
 	if (INTEL_INFO(dev)->gen >= 9)
 		return 1;
 
-	if (enable_execlists == 0)
+	if (enable_execlists == 0 && !USES_VGT(dev))
 		return 0;
 
 	if (HAS_LOGICAL_RING_CONTEXTS(dev) && USES_PPGTT(dev) &&
@@ -1624,6 +1624,21 @@ out:
 	return ret;
 }
 
+static void intel_lr_context_notify_vgt(struct drm_i915_gem_object *ctx_obj,
+					struct drm_device *dev, int msg)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	u64 tmp = execlists_ctx_descriptor(ctx_obj);
+
+	I915_WRITE(vgt_info_off(execlist_context_descriptor_lo),
+			tmp & 0xffffffff);
+	I915_WRITE(vgt_info_off(execlist_context_descriptor_hi),
+			tmp >> 32);
+
+	I915_WRITE(vgt_info_off(g2v_notify), msg);
+}
+
 static int
 populate_lr_context(struct intel_context *ctx, struct drm_i915_gem_object *ctx_obj,
 		    struct intel_engine_cs *ring, struct intel_ringbuffer *ringbuf)
@@ -1730,6 +1745,19 @@ populate_lr_context(struct intel_context *ctx, struct drm_i915_gem_object *ctx_o
 		reg_state[CTX_R_PWR_CLK_STATE+1] = 0;
 	}
 
+	if (USES_VGT(ring->dev)) {
+		/* Allocate VMA instantly. */
+		ret = i915_gem_obj_ggtt_pin(ctx_obj,
+				GEN8_LR_CONTEXT_ALIGN, 0);
+		if (ret) {
+			DRM_DEBUG_DRIVER("Pin LRC backing obj failed: %d\n",
+					ret);
+			return ret;
+		}
+		intel_lr_context_notify_vgt(ctx_obj, ring->dev,
+				VGT_G2V_EXECLIST_CONTEXT_ELEMENT_CREATE);
+	}
+
 	kunmap_atomic(reg_state);
 
 	ctx_obj->dirty = 1;
@@ -1759,6 +1787,12 @@ void intel_lr_context_free(struct intel_context *ctx)
 					ctx->engine[i].ringbuf;
 			struct intel_engine_cs *ring = ringbuf->ring;
 
+			if (USES_VGT(ringbuf->ring->dev)) {
+				intel_lr_context_notify_vgt(ctx_obj, ringbuf->ring->dev,
+						VGT_G2V_EXECLIST_CONTEXT_ELEMENT_DESTROY);
+				i915_gem_object_ggtt_unpin(ctx_obj);
+			}
+
 			if (ctx == ring->default_context) {
 				intel_unpin_ringbuffer_obj(ringbuf);
 				i915_gem_object_ggtt_unpin(ctx_obj);
diff --git a/drivers/gpu/drm/i915/intel_pm.c b/drivers/gpu/drm/i915/intel_pm.c
index 3242f3c..2025cff 100644
--- a/drivers/gpu/drm/i915/intel_pm.c
+++ b/drivers/gpu/drm/i915/intel_pm.c
@@ -6673,6 +6673,14 @@ static void broadwell_init_clock_gating(struct drm_device *dev)
 	I915_WRITE(GEN8_UCGCTL6, I915_READ(GEN8_UCGCTL6) |
 		   GEN8_SDEUNIT_CLOCK_GATE_DISABLE);
 
+	I915_WRITE(GEN7_ROW_CHICKEN2,
+			_MASKED_BIT_ENABLE(DOP_CLOCK_GATING_DISABLE));
+
+	I915_WRITE(GEN6_UCGCTL1, I915_READ(GEN6_UCGCTL1) |
+			GEN6_EU_TCUNIT_CLOCK_GATE_DISABLE);
+
+	I915_WRITE(0x20e4, _MASKED_BIT_ENABLE(0x2));
+
 	lpt_init_clock_gating(dev);
 }
 
diff --git a/drivers/gpu/drm/i915/intel_ringbuffer.c b/drivers/gpu/drm/i915/intel_ringbuffer.c
index c7bc93d..c722531 100644
--- a/drivers/gpu/drm/i915/intel_ringbuffer.c
+++ b/drivers/gpu/drm/i915/intel_ringbuffer.c
@@ -44,8 +44,10 @@ intel_ring_initialized(struct intel_engine_cs *ring)
 	if (i915.enable_execlists) {
 		struct intel_context *dctx = ring->default_context;
 		struct intel_ringbuffer *ringbuf = dctx->engine[ring->id].ringbuf;
-
-		return ringbuf->obj;
+		if (!ringbuf)
+			return false;
+		else
+			return ringbuf->obj;
 	} else
 		return ring->buffer && ring->buffer->obj;
 }
@@ -2724,3 +2726,37 @@ intel_stop_ring_buffer(struct intel_engine_cs *ring)
 
 	stop_ring(ring);
 }
+
+static bool is_rendering_engine_empty(struct drm_i915_private *dev_priv, int ring_id)
+{
+	static	int ring_mi_regs[3] = { 0x0209C, 0x1209C, 0x2209C };
+	u32	mi_mode;
+
+	mi_mode = I915_READ(ring_mi_regs[ring_id]);
+	if ( mi_mode & (1 << 9) )
+		return true;
+	else
+		return false;
+}
+
+extern struct drm_i915_private *gpu_perf_dev_priv;
+u64	i915_ring_0_idle = 0;
+u64	i915_ring_0_busy = 0;
+void gpu_perf_sample(void)
+{
+	struct drm_i915_private *dev_priv = gpu_perf_dev_priv;
+	int	ring_id = 0;
+	static  int count = 0;
+
+	if ( dev_priv ) {
+		if ( count ++ % 1000 == 0 )
+			printk("dev_priv->regs: %p\n", dev_priv->regs);
+		if ( spin_is_locked (&dev_priv->uncore.lock) )
+			return;
+		if ( is_rendering_engine_empty(dev_priv, ring_id) )
+			i915_ring_0_idle++;
+		else
+			i915_ring_0_busy++;
+	}
+}
+EXPORT_SYMBOL_GPL(gpu_perf_sample);
diff --git a/drivers/gpu/drm/i915/intel_uncore.c b/drivers/gpu/drm/i915/intel_uncore.c
index 46de8d7..46c086e 100644
--- a/drivers/gpu/drm/i915/intel_uncore.c
+++ b/drivers/gpu/drm/i915/intel_uncore.c
@@ -40,6 +40,40 @@
 
 #define __raw_posting_read(dev_priv__, reg__) (void)__raw_i915_read32(dev_priv__, reg__)
 
+
+/* Intel GVT-g MMIO mediation */
+#define __i915_read(x, y) \
+u##x i915_read##x(struct drm_i915_private *dev_priv, u32 reg,			\
+		bool trace) {							\
+	u##x val = 0;								\
+	if (i915_host_mediate)							\
+		vgt_host_read((reg), &val, sizeof(u##x), false, trace);		\
+	else									\
+		val = dev_priv->uncore.funcs.mmio_read##y(dev_priv,		\
+				reg, trace);					\
+	return val;								\
+}
+__i915_read(8, b)
+__i915_read(16, w)
+__i915_read(32, l)
+__i915_read(64, q)
+#undef __i915_read
+#define __i915_write(x, y) \
+void i915_write##x(struct drm_i915_private *dev_priv, u32 reg,			\
+		u##x val, bool trace)						\
+{										\
+	if (i915_host_mediate)							\
+		vgt_host_write(reg, &val, sizeof(u##x), false, trace);		\
+	else									\
+		dev_priv->uncore.funcs.mmio_write##y(dev_priv,			\
+				(reg), (val), trace);				\
+}
+__i915_write(8, b)
+__i915_write(16, w)
+__i915_write(32, l)
+__i915_write(64, q)
+#undef __i915_write
+
 static void
 assert_device_not_suspended(struct drm_i915_private *dev_priv)
 {
@@ -630,7 +664,8 @@ void assert_force_wake_inactive(struct drm_i915_private *dev_priv)
 
 /* We give fast paths for the really cool registers */
 #define NEEDS_FORCE_WAKE(dev_priv, reg) \
-	 ((reg) < 0x40000 && (reg) != FORCEWAKE)
+	 (!(USES_VGT(dev_priv->dev) && !i915_host_mediate) && \
+		(reg) < 0x40000 && (reg) != FORCEWAKE)
 
 #define REG_RANGE(reg, start, end) ((reg) >= (start) && (reg) < (end))
 
@@ -731,7 +766,7 @@ hsw_unclaimed_reg_debug(struct drm_i915_private *dev_priv, u32 reg, bool read,
 static void
 hsw_unclaimed_reg_detect(struct drm_i915_private *dev_priv)
 {
-	if (i915.mmio_debug)
+	if (i915.mmio_debug || i915_host_mediate)
 		return;
 
 	if (__raw_i915_read32(dev_priv, FPGA_DBG) & FPGA_DBG_RM_NOCLAIM) {
@@ -744,6 +779,13 @@ hsw_unclaimed_reg_detect(struct drm_i915_private *dev_priv)
 	unsigned long irqflags; \
 	u##x val = 0; \
 	assert_device_not_suspended(dev_priv); \
+	if (USES_VGT(dev_priv->dev) && !i915_host_mediate) {     \
+		spin_lock_irqsave(&dev_priv->uncore.lock, irqflags); \
+		val = __raw_i915_read##x(dev_priv, reg); \
+		spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags); \
+		trace_i915_reg_rw(false, reg, val, sizeof(val), trace); \
+		return val; \
+	} \
 	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags)
 
 #define REG_READ_FOOTER \
@@ -901,10 +943,16 @@ __gen4_read(64)
 #undef REG_READ_FOOTER
 #undef REG_READ_HEADER
 
-#define REG_WRITE_HEADER \
+#define REG_WRITE_HEADER(x) \
 	unsigned long irqflags; \
 	trace_i915_reg_rw(true, reg, val, sizeof(val), trace); \
 	assert_device_not_suspended(dev_priv); \
+	if (USES_VGT(dev_priv->dev) && !i915_host_mediate) { \
+		spin_lock_irqsave(&dev_priv->uncore.lock, irqflags); \
+		__raw_i915_write##x(dev_priv, reg, val); \
+		spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags); \
+		return; \
+	} \
 	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags)
 
 #define REG_WRITE_FOOTER \
@@ -913,7 +961,7 @@ __gen4_read(64)
 #define __gen4_write(x) \
 static void \
 gen4_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
-	REG_WRITE_HEADER; \
+	REG_WRITE_HEADER(x); \
 	__raw_i915_write##x(dev_priv, reg, val); \
 	REG_WRITE_FOOTER; \
 }
@@ -921,7 +969,7 @@ gen4_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace
 #define __gen5_write(x) \
 static void \
 gen5_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
-	REG_WRITE_HEADER; \
+	REG_WRITE_HEADER(x); \
 	ilk_dummy_write(dev_priv); \
 	__raw_i915_write##x(dev_priv, reg, val); \
 	REG_WRITE_FOOTER; \
@@ -931,7 +979,7 @@ gen5_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace
 static void \
 gen6_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
 	u32 __fifo_ret = 0; \
-	REG_WRITE_HEADER; \
+	REG_WRITE_HEADER(x); \
 	if (NEEDS_FORCE_WAKE((dev_priv), (reg))) { \
 		__fifo_ret = __gen6_gt_wait_for_fifo(dev_priv); \
 	} \
@@ -946,7 +994,7 @@ gen6_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace
 static void \
 hsw_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
 	u32 __fifo_ret = 0; \
-	REG_WRITE_HEADER; \
+	REG_WRITE_HEADER(x); \
 	if (NEEDS_FORCE_WAKE((dev_priv), (reg))) { \
 		__fifo_ret = __gen6_gt_wait_for_fifo(dev_priv); \
 	} \
@@ -984,7 +1032,7 @@ static bool is_gen8_shadowed(struct drm_i915_private *dev_priv, u32 reg)
 #define __gen8_write(x) \
 static void \
 gen8_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
-	REG_WRITE_HEADER; \
+	REG_WRITE_HEADER(x); \
 	hsw_unclaimed_reg_debug(dev_priv, reg, false, true); \
 	if (reg < 0x40000 && !is_gen8_shadowed(dev_priv, reg)) { \
 		if (dev_priv->uncore.forcewake_count == 0) \
@@ -1007,7 +1055,7 @@ static void \
 chv_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
 	unsigned fwengine = 0; \
 	bool shadowed = is_gen8_shadowed(dev_priv, reg); \
-	REG_WRITE_HEADER; \
+	REG_WRITE_HEADER(x); \
 	if (!shadowed) { \
 		if (FORCEWAKE_CHV_RENDER_RANGE_OFFSET(reg)) { \
 			if (dev_priv->uncore.fw_rendercount == 0) \
@@ -1057,7 +1105,7 @@ static bool is_gen9_shadowed(struct drm_i915_private *dev_priv, u32 reg)
 static void \
 gen9_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, \
 		bool trace) { \
-	REG_WRITE_HEADER; \
+	REG_WRITE_HEADER(x); \
 	if (!SKL_NEEDS_FORCE_WAKE((dev_priv), (reg)) || \
 			is_gen9_shadowed(dev_priv, reg)) { \
 		__raw_i915_write##x(dev_priv, reg, val); \
@@ -1245,6 +1293,7 @@ void intel_uncore_init(struct drm_device *dev)
 	}
 
 	i915_check_and_clear_faults(dev);
+	i915_vgt_record_priv(dev_priv);
 }
 #undef ASSIGN_WRITE_MMIO_VFUNCS
 #undef ASSIGN_READ_MMIO_VFUNCS
@@ -1439,15 +1488,10 @@ static int gen6_do_reset(struct drm_device *dev)
 	int	ret;
 
 	/* Reset the chip */
-
-	/* GEN6_GDRST is not in the gt power well, no need to check
-	 * for fifo space for the write or forcewake the chip for
-	 * the read
-	 */
-	__raw_i915_write32(dev_priv, GEN6_GDRST, GEN6_GRDOM_FULL);
+	I915_WRITE(GEN6_GDRST, GEN6_GRDOM_FULL);
 
 	/* Spin waiting for the device to ack the reset request */
-	ret = wait_for((__raw_i915_read32(dev_priv, GEN6_GDRST) & GEN6_GRDOM_FULL) == 0, 500);
+	ret = wait_for((I915_READ(GEN6_GDRST) & GEN6_GRDOM_FULL) == 0, 500);
 
 	intel_uncore_forcewake_reset(dev, true);
 
@@ -1474,6 +1518,9 @@ void intel_uncore_check_errors(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 
+	if (!i915.mmio_debug)
+		return;
+
 	if (HAS_FPGA_DBG_UNCLAIMED(dev) &&
 	    (__raw_i915_read32(dev_priv, FPGA_DBG) & FPGA_DBG_RM_NOCLAIM)) {
 		DRM_ERROR("Unclaimed register before interrupt\n");
diff --git a/drivers/gpu/drm/i915/vgt/Makefile b/drivers/gpu/drm/i915/vgt/Makefile
new file mode 100644
index 0000000..d5ec8bb
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/Makefile
@@ -0,0 +1,8 @@
+VGT_SOURCE := vgt.o render.o mmio.o handlers.o interrupt.o  \
+	sysfs.o display.o debugfs.o edid.o gtt.o aperture_gm.o utility.o \
+	klog.o dev.o cmd_parser.o sched.o instance.o cfg_space.o \
+	fb_decoder.o vbios.o host.o execlists.o
+
+ccflags-y				+= -I$(src) -I$(src)/.. -Wall -Werror
+xen_vgt-y				:= $(VGT_SOURCE)
+obj-$(CONFIG_I915_VGT)			+= xen_vgt.o
diff --git a/drivers/gpu/drm/i915/vgt/aperture_gm.c b/drivers/gpu/drm/i915/vgt/aperture_gm.c
new file mode 100644
index 0000000..25dea5b
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/aperture_gm.c
@@ -0,0 +1,352 @@
+/*
+ * Aperture and Graphics Memory (GM) virtualization
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/highmem.h>
+#include "vgt.h"
+
+/*
+ * Guest to host GMADR (include aperture) converting.
+ *
+ * handle in 4 bytes granule
+ */
+vgt_reg_t mmio_g2h_gmadr(struct vgt_device *vgt, unsigned long reg, vgt_reg_t g_value)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	uint64_t h_value;
+	vgt_reg_t mask;
+	uint32_t size;
+	int ret;
+
+	if (!reg_addr_fix(pdev, reg))
+		return g_value;
+
+	ASSERT((reg < _REG_FENCE_0_LOW) || (reg >= _REG_FENCE_0_LOW + VGT_FENCE_REGION_SIZE));
+
+	mask = reg_aux_addr_mask(pdev, reg);
+	vgt_dbg(VGT_DBG_MEM, "vGT: address fix g->h for reg (0x%lx) value (0x%x) mask (0x%x)\n", reg, g_value, mask);
+	/*
+	 * NOTE: address ZERO is special, and sometimes the driver may hard
+	 * code address ZERO, e.g. in curbase setting (when the cursor becomes
+	 * invisible). So we always translate address ZERO into the valid
+	 * range of the VM. If this doesn't work, we need change the driver!
+	 */
+	if (!(g_value & mask)) {
+		vgt_dbg(VGT_DBG_MEM, "vGT(%d): translate address ZERO for reg (%lx)\n",
+			vgt->vgt_id, reg);
+		g_value = (vgt_guest_visible_gm_base(vgt) & mask) |
+			  (g_value & ~mask);
+	}
+
+	h_value = g_value & mask;
+	size = reg_aux_addr_size(pdev, reg);
+	ret = g2h_gm_range(vgt, &h_value, size);
+
+	/*
+	 *  Note: ASSERT_VM should be placed outside, e.g. after lock is released in
+	 *  vgt_emulate_write(). Will fix this later.
+	 */
+	ASSERT_VM(!ret, vgt);
+	vgt_dbg(VGT_DBG_MEM, "....(g)%x->(h)%llx\n", g_value, (h_value & mask) | (g_value & ~mask));
+
+	return (h_value & mask) | (g_value & ~mask);
+}
+
+/*
+ * Host to guest GMADR (include aperture) converting.
+ *
+ * handle in 4 bytes granule
+ */
+vgt_reg_t mmio_h2g_gmadr(struct vgt_device *vgt, unsigned long reg, vgt_reg_t h_value)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	vgt_reg_t g_value;
+	vgt_reg_t mask;
+
+	if (!reg_addr_fix(pdev, reg))
+		return h_value;
+
+	vgt_dbg(VGT_DBG_MEM, "vGT: address fix h->g for reg (%lx)(%x)\n", reg, h_value);
+	mask = reg_aux_addr_mask(pdev, reg);
+
+	/*
+	 * it's possible the initial state may not contain a valid address
+	 * vm's range. In such case fake a valid address since the value there
+	 * doesn't matter.
+	 */
+	if (!h_gm_is_valid(vgt, h_value & mask)) {
+		vgt_dbg(VGT_DBG_MEM, "!!!vGT: reg (%lx) doesn't contain a valid host address (%x)\n", reg, h_value);
+		h_value = (vgt_visible_gm_base(vgt) & mask) | (h_value & ~mask);
+	}
+
+	g_value = h2g_gm(vgt, h_value & mask);
+	vgt_dbg(VGT_DBG_MEM, "....(h)%x->(g)%x\n", h_value, (g_value & mask) | (h_value & ~mask));
+	return (g_value & mask) | (h_value & ~mask);
+}
+
+/* Allocate pages in reserved aperture.
+ * TODO: rsvd_aperture_alloc() and rsvd_aperture_free() are invoked on both vgt
+ * driver initialization/destroy and vgt instance creation/destroy: for the
+ * latter case, we use vgt_sysfs_lock to achieve mutual exclusive. However,
+ * it looks vgt_sysfs_lock is not the correct mechanism: we should lock the
+ * the data, not the sysfs code. We could need use small granularity locks for
+ * different GFX resources and data structures.
+ */
+unsigned long rsvd_aperture_alloc(struct pgt_device *pdev, unsigned long size)
+{
+	unsigned long start, nr_pages;
+
+	ASSERT(size > 0);
+
+	nr_pages = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;
+	start = bitmap_find_next_zero_area( pdev->rsvd_aperture_bitmap,
+			VGT_RSVD_APERTURE_BITMAP_BITS, 0, nr_pages, 0 );
+
+	if (start >= VGT_RSVD_APERTURE_BITMAP_BITS) {
+		vgt_err("Out of memory for reserved aperture allocation "
+				"of size 0x%lx!\n", size);
+		BUG();
+	}
+
+	bitmap_set(pdev->rsvd_aperture_bitmap, start, nr_pages);
+
+	return pdev->rsvd_aperture_base + (start << PAGE_SHIFT);
+}
+
+/* free pages in reserved aperture */
+void rsvd_aperture_free(struct pgt_device *pdev, unsigned long start, unsigned long size)
+{
+	unsigned long nr_pages = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;
+
+	if ( (start >= pdev->rsvd_aperture_base) &&
+			(start + size <= pdev->rsvd_aperture_base + pdev->rsvd_aperture_sz) )
+	{
+		bitmap_clear(pdev->rsvd_aperture_bitmap,
+				(start - pdev->rsvd_aperture_base)>>PAGE_SHIFT, nr_pages);
+	} else {
+		vgt_err("Out of range parameter for rsvd_aperture_free(pdev, "
+			"start[0x%lx], size[0x%lx])!\n", start, size);
+	}
+}
+
+ssize_t get_avl_vm_aperture_gm_and_fence(struct pgt_device *pdev, char *buf,
+		ssize_t buf_sz)
+{
+	unsigned long aperture_guard = phys_aperture_sz(pdev) / SIZE_1MB;
+	unsigned long gm_guard = gm_sz(pdev) / SIZE_1MB;
+	unsigned long fence_guard = VGT_FENCE_BITMAP_BITS;
+	unsigned long available_low_gm_sz = 0;
+	unsigned long available_high_gm_sz = 0;
+	int i;
+	ssize_t buf_len = 0;
+#define MAX_NR_RES 2
+	unsigned long *bitmap[MAX_NR_RES];
+	unsigned long bitmap_sz[MAX_NR_RES];
+
+	available_low_gm_sz = aperture_guard - bitmap_weight(pdev->gm_bitmap,
+	  aperture_guard);
+	available_high_gm_sz = gm_guard - bitmap_weight(pdev->gm_bitmap, gm_guard)
+	  - available_low_gm_sz;
+	buf_len = snprintf(buf, buf_sz, "0x%08lx, 0x%08lx, 0x%08lx, "
+			"0x%08lx, 0x%08lx, 0x%08lx\n",
+			aperture_guard,
+			available_low_gm_sz,
+			gm_guard - aperture_guard,
+			available_high_gm_sz,
+			fence_guard,
+			fence_guard - bitmap_weight(pdev->fence_bitmap, fence_guard)
+			);
+
+#define init_resource_bitmap(i, map, sz) \
+	ASSERT((i) <  MAX_NR_RES);	\
+	bitmap[i] = map;	\
+	bitmap_sz[i] = sz;
+	/* gm */
+	init_resource_bitmap(0, pdev->gm_bitmap, gm_guard);
+	/* fence registers */
+	init_resource_bitmap(1, pdev->fence_bitmap, fence_guard);
+
+	for (i = 0; i < MAX_NR_RES; i++) {
+		buf_len += bitmap_scnprintf(buf + buf_len, buf_sz - buf_len,
+				bitmap[i], bitmap_sz[i]);
+		buf_len += snprintf(buf + buf_len, buf_sz - buf_len, "\n");
+	}
+
+	return buf_len;
+}
+
+int allocate_vm_aperture_gm_and_fence(struct vgt_device *vgt, vgt_params_t vp)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	struct vgt_device_info *info = &pdev->device_info;
+
+	unsigned long *gm_bitmap = pdev->gm_bitmap;
+	unsigned long *fence_bitmap = pdev->fence_bitmap;
+	unsigned long guard = hidden_gm_base(vgt->pdev)/SIZE_1MB;
+	unsigned long gm_bitmap_total_bits = info->max_gtt_gm_sz >> 20;
+	unsigned long aperture_search_start = 0;
+	unsigned long visable_gm_start, hidden_gm_start = guard;
+	unsigned long fence_base;
+
+	ASSERT(vgt->aperture_base == 0); /* not allocated yet*/
+	ASSERT(vp.aperture_sz > 0 && vp.aperture_sz <= vp.gm_sz);
+	ASSERT(vp.fence_sz > 0);
+
+	visable_gm_start = bitmap_find_next_zero_area(gm_bitmap, guard,
+				aperture_search_start, vp.aperture_sz, 0);
+	if (visable_gm_start >= guard)
+		return -ENOMEM;
+
+	if (vp.gm_sz > vp.aperture_sz) {
+		hidden_gm_start = bitmap_find_next_zero_area(gm_bitmap,
+				gm_bitmap_total_bits, guard, vp.gm_sz - vp.aperture_sz, 0);
+		if (hidden_gm_start >= gm_bitmap_total_bits)
+			return -ENOMEM;
+	}
+	fence_base = bitmap_find_next_zero_area(fence_bitmap,
+				VGT_FENCE_BITMAP_BITS, 0, vp.fence_sz, 0);
+	if (fence_base >= VGT_MAX_NUM_FENCES)
+		return -ENOMEM;
+
+	vgt->aperture_base = phys_aperture_base(vgt->pdev) +
+			(visable_gm_start * SIZE_1MB);
+	vgt->aperture_sz = vp.aperture_sz * SIZE_1MB;
+	vgt->gm_sz = vp.gm_sz * SIZE_1MB;
+	vgt->hidden_gm_offset = hidden_gm_start * SIZE_1MB;
+	vgt->fence_base = fence_base;
+	vgt->fence_sz = vp.fence_sz;
+
+	/* mark the related areas as BUSY. */
+	bitmap_set(gm_bitmap, visable_gm_start, vp.aperture_sz);
+	if (vp.gm_sz > vp.aperture_sz)
+		bitmap_set(gm_bitmap, hidden_gm_start, vp.gm_sz - vp.aperture_sz);
+	bitmap_set(fence_bitmap, fence_base, vp.fence_sz);
+	return 0;
+}
+
+void free_vm_aperture_gm_and_fence(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	unsigned long *gm_bitmap = pdev->gm_bitmap;
+	unsigned long *fence_bitmap = pdev->fence_bitmap;
+	unsigned long visable_gm_start =
+		aperture_2_gm(vgt->pdev, vgt->aperture_base)/SIZE_1MB;
+	unsigned long hidden_gm_start = vgt->hidden_gm_offset/SIZE_1MB;
+
+	ASSERT(vgt->aperture_sz > 0 && vgt->aperture_sz <= vgt->gm_sz);
+
+	/* mark the related areas as available */
+	bitmap_clear(gm_bitmap, visable_gm_start, vgt->aperture_sz/SIZE_1MB);
+	if (vgt->gm_sz > vgt->aperture_sz)
+		bitmap_clear(gm_bitmap, hidden_gm_start,
+			(vgt->gm_sz - vgt->aperture_sz)/SIZE_1MB);
+	bitmap_clear(fence_bitmap, vgt->fence_base,  vgt->fence_sz);
+}
+
+int alloc_vm_rsvd_aperture(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	int i;
+
+	for (i=0; i< pdev->max_engines; i++) {
+		vgt_state_ring_t *rb;
+		struct vgt_rsvd_ring *ring = &pdev->ring_buffer[i];
+
+		rb = &vgt->rb[i];
+		rb->context_save_area = aperture_2_gm(pdev,
+				rsvd_aperture_alloc(pdev, SZ_CONTEXT_AREA_PER_RING) );
+
+		printk("VM%d Ring%d context_save_area is allocated at gm(%llx)\n", vgt->vm_id, i,
+				rb->context_save_area);
+		rb->active_vm_context = 0;
+
+		/*
+		 * copy NULL context as the initial content. This update is
+		 * only for non-dom0 instance. Dom0's context is updated when
+		 * NULL context is created
+		 */
+		if (vgt->vgt_id && (i == RING_BUFFER_RCS)) {
+			memcpy((char *)v_aperture(pdev, rb->context_save_area),
+			       (char *)v_aperture(pdev, ring->null_context),
+			       SZ_CONTEXT_AREA_PER_RING);
+		}
+
+		vgt_init_cmd_info(rb);
+	}
+
+	return 0;
+}
+
+void free_vm_rsvd_aperture(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	vgt_state_ring_t *rb;
+	int i;
+
+	for (i=0; i< pdev->max_engines; i++) {
+		rb = &vgt->rb[i];
+		rsvd_aperture_free(pdev, rb->context_save_area + phys_aperture_base(pdev),
+				SZ_CONTEXT_AREA_PER_RING);
+	}
+}
+
+void initialize_gm_fence_allocation_bitmaps(struct pgt_device *pdev)
+{
+	struct vgt_device_info *info = &pdev->device_info;
+	unsigned long *gm_bitmap = pdev->gm_bitmap;
+
+	vgt_info("total aperture: 0x%x bytes, total GM space: 0x%llx bytes\n",
+		phys_aperture_sz(pdev), gm_sz(pdev));
+
+	ASSERT(phys_aperture_sz(pdev) % SIZE_1MB == 0);
+	ASSERT(gm_sz(pdev) % SIZE_1MB == 0);
+	ASSERT(phys_aperture_sz(pdev) <= gm_sz(pdev) && gm_sz(pdev) <= info->max_gtt_gm_sz);
+	ASSERT(info->max_gtt_gm_sz <= VGT_MAX_GM_SIZE);
+
+	// mark the non-available space as non-available.
+	if (gm_sz(pdev) < info->max_gtt_gm_sz)
+		bitmap_set(gm_bitmap, gm_sz(pdev) / SIZE_1MB,
+			(info->max_gtt_gm_sz - gm_sz(pdev)) / SIZE_1MB);
+
+	pdev->rsvd_aperture_sz = VGT_RSVD_APERTURE_SZ;
+	pdev->rsvd_aperture_base = phys_aperture_base(pdev) + hidden_gm_base(pdev) -
+								pdev->rsvd_aperture_sz;
+
+	// mark the rsvd aperture as not-available.
+	bitmap_set(gm_bitmap, aperture_2_gm(pdev, pdev->rsvd_aperture_base)/SIZE_1MB,
+				pdev->rsvd_aperture_sz/SIZE_1MB);
+
+	vgt_info("reserved aperture: [0x%llx, 0x%llx)\n",
+			pdev->rsvd_aperture_base,
+			pdev->rsvd_aperture_base + pdev->rsvd_aperture_sz);
+}
+
+void vgt_init_reserved_aperture(struct pgt_device *pdev)
+{
+	/* setup the scratch page for the context switch */
+	pdev->scratch_page = aperture_2_gm(pdev, rsvd_aperture_alloc(pdev,
+				VGT_APERTURE_PER_INSTANCE_SZ));
+	printk("scratch page is allocated at gm(0x%llx)\n", pdev->scratch_page);
+	/* reserve the 1st trunk for vGT's general usage */
+}
diff --git a/drivers/gpu/drm/i915/vgt/cfg_space.c b/drivers/gpu/drm/i915/vgt/cfg_space.c
new file mode 100644
index 0000000..18ed7be
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/cfg_space.c
@@ -0,0 +1,398 @@
+/*
+ * PCI Configuration Space virtualization
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+
+#include "vgt.h"
+
+typedef union _SCI_REG_DATA{
+	uint16_t data;
+	struct {
+		uint16_t trigger:1; /* bit 0: trigger SCI */
+		uint16_t reserve:14;
+		uint16_t method:1; /* bit 15: 1 - SCI, 0 - SMI */
+	};
+} SCI_REG_DATA;
+
+static bool vgt_cfg_sci_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, int bytes)
+{
+	printk("VM%d Read SCI Trigger Register, bytes=%d value=0x%x\n", vgt->vm_id, bytes, *(uint16_t*)p_data);
+
+	return true;
+}
+
+static bool vgt_cfg_sci_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, int bytes)
+{
+	SCI_REG_DATA sci_reg;
+
+	printk("VM%d Write SCI Trigger Register, bytes=%d value=0x%x\n", vgt->vm_id, bytes, *(uint32_t*)p_data);
+
+	if( (bytes == 2) || (bytes == 4)){
+		memcpy (&vgt->state.cfg_space[offset], p_data, bytes);
+	} else {
+		printk("Warning: VM%d vgt_cfg_sci_write invalid bytes=%d, ignore it\n", vgt->vm_id, bytes);
+		return false;
+	}
+
+	sci_reg.data = *(uint16_t*)(vgt->state.cfg_space + offset);
+	sci_reg.method = 1; /* set method to SCI */
+	if (sci_reg.trigger == 1){
+		printk("SW SCI Triggered by VM%d\n", vgt->vm_id);
+		/* TODO: add SCI emulation */
+		sci_reg.trigger = 0; /* SCI completion indicator */
+	}
+
+	memcpy (&vgt->state.cfg_space[offset], &sci_reg.data , 2);
+
+	return true;
+}
+
+#define VGT_OPREGION_FUNC(scic)						\
+	({								\
+		uint32_t __ret;						\
+		__ret = (scic & _REGBIT_OPREGION_SCIC_FUNC_MASK) >>	\
+		_REGBIT_OPREGION_SCIC_FUNC_SHIFT;			\
+		__ret;							\
+	})
+
+#define VGT_OPREGION_SUBFUNC(scic)					\
+	({								\
+		uint32_t __ret;						\
+		__ret = (scic & _REGBIT_OPREGION_SCIC_SUBFUNC_MASK) >>	\
+			_REGBIT_OPREGION_SCIC_SUBFUNC_SHIFT;		\
+		__ret;							\
+	})
+
+static const char *vgt_opregion_func_name(uint32_t func)
+{
+	const char *name = NULL;
+
+	switch (func) {
+	case 0 ... 3:
+	case 5:
+	case 7 ... 15:
+		name = "Reserved";
+		break;
+
+	case 4:
+		name = "Get BIOS Data";
+		break;
+
+	case 6:
+		name = "System BIOS Callbacks";
+		break;
+
+	default:
+		name = "Unknown";
+		break;
+	}
+	return name;
+}
+
+static const char *vgt_opregion_subfunc_name(uint32_t subfunc)
+{
+	const char *name = NULL;
+	switch (subfunc) {
+	case 0:
+		name = "Supported Calls";
+		break;
+
+	case 1:
+		name = "Requested Callbacks";
+		break;
+
+	case 2 ... 3:
+	case 8 ... 9:
+		name = "Reserved";
+		break;
+
+	case 5:
+		name = "Boot Display";
+		break;
+
+	case 6:
+		name = "TV-Standard/Video-Connector";
+		break;
+
+	case 7:
+		name = "Internal Graphics";
+		break;
+
+	case 10:
+		name = "Spread Spectrum Clocks";
+		break;
+
+	case 11:
+		name = "Get AKSV";
+		break;
+
+	default:
+		name = "Unknown";
+		break;
+	}
+	return name;
+};
+
+/* Only allowing capability queries */
+static bool vgt_opregion_is_capability_get(uint32_t scic)
+{
+	uint32_t func, subfunc;
+
+	func = VGT_OPREGION_FUNC(scic);
+	subfunc = VGT_OPREGION_SUBFUNC(scic);
+
+	if ((func == VGT_OPREGION_SCIC_F_GETBIOSDATA &&
+			subfunc == VGT_OPREGION_SCIC_SF_SUPPRTEDCALLS) ||
+			(func == VGT_OPREGION_SCIC_F_GETBIOSDATA &&
+			 subfunc == VGT_OPREGION_SCIC_SF_REQEUSTEDCALLBACKS) ||
+			(func == VGT_OPREGION_SCIC_F_GETBIOSCALLBACKS &&
+			 subfunc == VGT_OPREGION_SCIC_SF_SUPPRTEDCALLS)) {
+		return true;
+	}
+
+	return false;
+}
+/*
+ * emulate multiple capability query requests
+ */
+static void vgt_hvm_opregion_handle_request(struct vgt_device *vgt, uint32_t swsci)
+{
+	uint32_t *scic, *parm;
+	uint32_t func, subfunc;
+	scic = vgt->state.opregion_va + VGT_OPREGION_REG_SCIC;
+	parm = vgt->state.opregion_va + VGT_OPREGION_REG_PARM;
+
+	if (!(swsci & _REGBIT_CFG_SWSCI_SCI_SELECT)) {
+		vgt_warn("VM%d requesting SMI service\n", vgt->vm_id);
+		return;
+	}
+	/* ignore non 0->1 trasitions */
+	if ((vgt->state.cfg_space[VGT_REG_CFG_SWSCI_TRIGGER] &
+				_REGBIT_CFG_SWSCI_SCI_TRIGGER) ||
+			!(swsci & _REGBIT_CFG_SWSCI_SCI_TRIGGER)) {
+		return;
+	}
+
+	func = VGT_OPREGION_FUNC(*scic);
+	subfunc = VGT_OPREGION_SUBFUNC(*scic);
+	if (!vgt_opregion_is_capability_get(*scic)) {
+		vgt_warn("VM%d requesting runtime service: func \"%s\", subfunc \"%s\"\n",
+				vgt->vm_id,
+				vgt_opregion_func_name(func),
+				vgt_opregion_subfunc_name(subfunc));
+
+		/*
+		 * emulate exit status of function call, '0' means
+		 * "failure, generic, unsupported or unkown cause"
+		 */
+		*scic &= ~_REGBIT_OPREGION_SCIC_EXIT_MASK;
+		return;
+	}
+
+	*scic = 0;
+	*parm = 0;
+}
+
+bool vgt_emulate_cfg_read(struct vgt_device *vgt, unsigned int offset, void *p_data, int bytes)
+{
+
+	ASSERT ((offset + bytes) <= VGT_CFG_SPACE_SZ);
+	memcpy(p_data, &vgt->state.cfg_space[offset], bytes);
+
+	/* TODO: hooks */
+	offset &= ~3;
+	switch (offset) {
+		case 0:
+		case 4:
+			break;
+		case VGT_REG_CFG_SWSCI_TRIGGER:
+			vgt_cfg_sci_read(vgt, offset, p_data, bytes);
+			break;
+		default:
+			break;
+	}
+	return true;
+}
+
+bool vgt_emulate_cfg_write(struct vgt_device *vgt, unsigned int off,
+	void *p_data, int bytes)
+{
+	char *cfg_space = &vgt->state.cfg_space[0];
+	uint32_t *cfg_reg, new, size;
+	u8 old_cmd, cmd_changed; /* we don't care the high 8 bits */
+	bool rc = true;
+	uint32_t low_mem_max_gpfn;
+
+	ASSERT ((off + bytes) <= VGT_CFG_SPACE_SZ);
+	cfg_reg = (uint32_t*)(cfg_space + (off & ~3));
+	switch (off & ~3) {
+		case VGT_REG_CFG_VENDOR_ID:
+			low_mem_max_gpfn = *(uint32_t *)p_data;
+			vgt_info("low_mem_max_gpfn: 0x%x\n", low_mem_max_gpfn);
+			if (bytes != 4 ||
+				low_mem_max_gpfn >= (1UL << (32-PAGE_SHIFT))) {
+				vgt_warn("invalid low_mem_max_gpfn!\n");
+				break;
+			}
+			if (vgt->low_mem_max_gpfn == 0)
+				vgt->low_mem_max_gpfn = low_mem_max_gpfn;
+			break;
+
+		case VGT_REG_CFG_COMMAND:
+			old_cmd = vgt->state.cfg_space[off];
+			cmd_changed = old_cmd ^ (*(u8*)p_data);
+			memcpy (&vgt->state.cfg_space[off], p_data, bytes);
+			if (cmd_changed & _REGBIT_CFG_COMMAND_MEMORY) {
+				if (old_cmd & _REGBIT_CFG_COMMAND_MEMORY) {
+					 vgt_hvm_map_aperture(vgt, 0);
+				} else {
+					if(!vgt->state.bar_mapped[1]) {
+						vgt_hvm_map_aperture(vgt, 1);
+						vgt_hvm_set_trap_area(vgt, 1);
+					}
+				}
+			} else {
+				vgt_dbg(VGT_DBG_GENERIC, "need to trap the PIO BAR? "
+					"old_cmd=0x%x, cmd_changed=%0x",
+					old_cmd, cmd_changed);
+			}
+			break;
+		case VGT_REG_CFG_SPACE_BAR0:	/* GTTMMIO */
+		case VGT_REG_CFG_SPACE_BAR1:	/* GMADR */
+		case VGT_REG_CFG_SPACE_BAR2:	/* IO */
+			ASSERT((bytes == 4) && (off & 3) == 0);
+
+			new = *(uint32_t *)p_data;
+			printk("Programming bar 0x%x with 0x%x\n", off, new);
+			size = vgt->state.bar_size[(off - VGT_REG_CFG_SPACE_BAR0)/8];
+			if (new == 0xFFFFFFFF) {
+				/*
+				 * Power-up software can determine how much address
+				 * space the device requires by writing a value of
+				 * all 1's to the register and then reading the value
+				 * back. The device will return 0's in all don't-care
+				 * address bits.
+				 */
+				new = new & ~(size-1);
+				if ((off & ~3) == VGT_REG_CFG_SPACE_BAR1)
+					vgt_hvm_map_aperture(vgt, 0);
+				if ((off & ~3) == VGT_REG_CFG_SPACE_BAR0)
+					vgt_hvm_set_trap_area(vgt, 0);
+				vgt_pci_bar_write_32(vgt, off, new);
+			} else {
+				if ((off & ~3) == VGT_REG_CFG_SPACE_BAR1)
+					vgt_hvm_map_aperture(vgt, 0);
+				if ((off & ~3) == VGT_REG_CFG_SPACE_BAR0)
+					vgt_hvm_set_trap_area(vgt, 0);
+				vgt_pci_bar_write_32(vgt, off, new);
+				if ((off & ~3) == VGT_REG_CFG_SPACE_BAR1)
+					vgt_hvm_map_aperture(vgt, 1);
+				if ((off & ~3) == VGT_REG_CFG_SPACE_BAR0)
+					vgt_hvm_set_trap_area(vgt, 1);
+			}
+			break;
+
+		case VGT_REG_CFG_SPACE_MSAC:
+			printk("Guest write MSAC %x, %d: Not supported yet\n",
+					*(char *)p_data, bytes);
+			break;
+
+		case VGT_REG_CFG_SWSCI_TRIGGER:
+			new = *(uint32_t *)p_data;
+			if (vgt->vm_id == 0)
+				rc = vgt_cfg_sci_write(vgt, off, p_data, bytes);
+			else
+				vgt_hvm_opregion_handle_request(vgt, new);
+			break;
+
+		case VGT_REG_CFG_OPREGION:
+			new = *(uint32_t *)p_data;
+			if (vgt->vm_id == 0) {
+				/* normally domain 0 shouldn't write this reg */
+				memcpy(&vgt->state.cfg_space[off], p_data, bytes);
+			} else if (vgt->state.opregion_va == NULL) {
+				vgt_hvm_opregion_init(vgt, new);
+				memcpy(&vgt->state.cfg_space[off], p_data, bytes);
+			} else
+				vgt_warn("VM%d write OPREGION multiple times",
+						vgt->vm_id);
+			break;
+
+		case VGT_REG_CFG_SPACE_BAR1+4:
+		case VGT_REG_CFG_SPACE_BAR0+4:
+		case VGT_REG_CFG_SPACE_BAR2+4:
+			ASSERT((bytes == 4) && (off & 3) == 0);
+			if (*(uint32_t *)p_data == 0xFFFFFFFF)
+				/* BAR size is not beyond 4G, so return all-0 in uppper 32 bit */
+				*cfg_reg = 0;
+			else
+				*cfg_reg = *(uint32_t*)p_data;
+			break;
+		case 0x90:
+		case 0x94:
+		case 0x98:
+			printk("vGT: write to MSI capa(%x) with val (%x)\n", off, *(uint32_t *)p_data);
+		default:
+			memcpy (&vgt->state.cfg_space[off], p_data, bytes);
+			break;
+	}
+	/*
+	 * Assume most Dom0's cfg writes should be propagated to
+	 * the real conf space. In the case where propagation is required
+	 * but value needs be changed (sReg), do it here
+	 */
+	return rc;
+}
+
+bool vgt_hvm_write_cfg_space(struct vgt_device *vgt,
+	uint64_t addr, unsigned int bytes, unsigned long val)
+{
+	/* Low 32 bit of addr is real address, high 32 bit is bdf */
+	unsigned int port = addr & 0xffffffff;
+
+	vgt_dbg(VGT_DBG_GENERIC, "vgt_hvm_write_cfg_space %x %d %lx\n", port, bytes, val);
+	ASSERT(((bytes == 4) && ((port & 3) == 0)) ||
+		((bytes == 2) && ((port & 1) == 0)) || (bytes == 1));
+	vgt_emulate_cfg_write (vgt, port, &val, bytes);
+
+	return true;
+}
+
+bool vgt_hvm_read_cfg_space(struct vgt_device *vgt,
+	uint64_t addr, unsigned int bytes, unsigned long *val)
+{
+	unsigned long data;
+	/* Low 32 bit of addr is real address, high 32 bit is bdf */
+	unsigned int port = addr & 0xffffffff;
+
+	ASSERT (((bytes == 4) && ((port & 3) == 0)) ||
+		((bytes == 2) && ((port & 1) == 0)) || (bytes == 1));
+	vgt_emulate_cfg_read(vgt, port, &data, bytes);
+	memcpy(val, &data, bytes);
+	vgt_dbg(VGT_DBG_GENERIC, "VGT: vgt_hvm_read_cfg_space port %x bytes %x got %lx\n",
+			port, bytes, *val);
+	return true;
+}
diff --git a/drivers/gpu/drm/i915/vgt/cmd_parser.c b/drivers/gpu/drm/i915/vgt/cmd_parser.c
new file mode 100644
index 0000000..c8542f5
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/cmd_parser.c
@@ -0,0 +1,2533 @@
+/*
+ * vGT command parser
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/slab.h>
+#include "vgt.h"
+#include "trace.h"
+#include "fb_decoder.h"
+
+/* vgt uses below bits in NOOP_ID:
+ *	    bit 21 - 16 is command type.
+ *	    bit 15 - 0  holds command specific information.
+ *
+ * Assumption: Linux/Windows guest will not use bits 21 - bits 16 with
+ * non-zero value.
+ */
+#define VGT_NOOP_ID_CMD_SHIFT	16
+#define VGT_NOOP_ID_CMD_MASK	(0x3f << VGT_NOOP_ID_CMD_SHIFT)
+#define CMD_LENGTH_MASK		0xff
+#define gmadr_dw_number(s)	\
+	(s->vgt->pdev->device_info.gmadr_bytes_in_cmd >> 2)
+/*
+ * new cmd parser
+ */
+
+DEFINE_HASHTABLE(vgt_cmd_table, VGT_CMD_HASH_BITS);
+
+static void vgt_add_cmd_entry(struct vgt_cmd_entry *e)
+{
+	hash_add(vgt_cmd_table, &e->hlist, e->info->opcode);
+}
+
+static struct cmd_info* vgt_find_cmd_entry(unsigned int opcode, int ring_id)
+{
+	struct vgt_cmd_entry *e;
+
+	hash_for_each_possible(vgt_cmd_table, e, hlist, opcode) {
+		if ((opcode == e->info->opcode) && (e->info->rings & (1<<ring_id)))
+			return e->info;
+	}
+	return NULL;
+}
+
+static struct cmd_info* vgt_find_cmd_entry_any_ring(unsigned int opcode, int rings)
+{
+	struct cmd_info* info = NULL;
+	unsigned int ring;
+	for_each_set_bit(ring, (unsigned long*)&rings, MAX_ENGINES) {
+		info = vgt_find_cmd_entry(opcode, ring);
+		if (info)
+			break;
+	}
+	return info;
+}
+
+void vgt_clear_cmd_table(void)
+{
+	int i;
+	struct hlist_node *tmp;
+	struct vgt_cmd_entry *e;
+
+	hash_for_each_safe(vgt_cmd_table, i, tmp, e, hlist)
+		kfree(e);
+
+	hash_init(vgt_cmd_table);
+}
+
+void vgt_init_cmd_info(vgt_state_ring_t *rs)
+{
+	memset(&rs->patch_list, 0, sizeof(struct cmd_general_info));
+	rs->patch_list.head = 0;
+	rs->patch_list.tail = 0;
+	rs->patch_list.count = CMD_PATCH_NUM;
+	memset(&rs->handler_list, 0, sizeof(struct cmd_general_info));
+	rs->handler_list.head = 0;
+	rs->handler_list.tail = 0;
+	rs->handler_list.count = CMD_HANDLER_NUM;
+	memset(&rs->tail_list, 0, sizeof(struct cmd_general_info));
+	rs->tail_list.head = 0;
+	rs->tail_list.tail = 0;
+	rs->tail_list.count = CMD_TAIL_NUM;
+}
+
+static int get_next_entry(struct cmd_general_info *list)
+{
+	int next;
+
+	next = list->tail + 1;
+	if (next == list->count)
+		next = 0;
+
+	if (next == list->head)
+		next = list->count;
+
+	return next;
+}
+
+/* TODO: support incremental patching */
+static int add_patch_entry(struct parser_exec_state *s,
+	void *addr, uint32_t val)
+{
+	vgt_state_ring_t *rs = &s->vgt->rb[s->ring_id];
+	struct cmd_general_info *list = &rs->patch_list;
+	struct cmd_patch_info *patch;
+	int next;
+
+	ASSERT(addr != NULL);
+
+	next = get_next_entry(list);
+	if (next == list->count) {
+		vgt_err("CMD_SCAN: no free patch entry\n");
+		return -ENOSPC;
+	}
+
+	vgt_dbg(VGT_DBG_CMD, "VM(%d): Add patch entry-%d (addr: %llx, val: %x, id: %lld\n",
+		s->vgt->vm_id, next, (uint64_t)addr, val, s->request_id);
+	patch = &list->patch[next];
+	patch->addr = addr;
+	patch->new_val = val;
+
+	hypervisor_read_va(s->vgt, addr, &patch->old_val,
+				sizeof(patch->old_val), 1);
+
+	patch->request_id = s->request_id;
+
+	list->tail = next;
+	return 0;
+}
+
+static int add_post_handle_entry(struct parser_exec_state *s,
+	parser_cmd_handler handler)
+{
+	vgt_state_ring_t* rs = &s->vgt->rb[s->ring_id];
+	struct cmd_general_info *list = &rs->handler_list;
+	struct cmd_handler_info *entry;
+	int next;
+
+	next = get_next_entry(list);
+	if (next == list->count) {
+		vgt_err("CMD_SCAN: no free post-handle entry\n");
+		return -ENOSPC;
+	}
+
+	entry = &list->handler[next];
+	/* two pages mapping are always valid */
+	memcpy(&entry->exec_state, s, sizeof(struct parser_exec_state));
+	entry->handler = handler;
+	entry->request_id = s->request_id;
+
+	list->tail = next;
+	return 0;
+
+}
+
+static int add_tail_entry(struct parser_exec_state *s,
+	uint32_t tail, uint32_t cmd_nr, uint32_t flags, uint32_t ip_offset)
+{
+	vgt_state_ring_t* rs = &s->vgt->rb[s->ring_id];
+	struct cmd_general_info *list = &rs->tail_list;
+	struct cmd_tail_info *entry;
+	int next;
+
+	next = get_next_entry(list);
+	if (next == list->count) {
+		vgt_err("CMD_SCAN: no free tail entry\n");
+		return -ENOSPC;
+	}
+
+	entry = &list->cmd[next];
+	entry->request_id = s->request_id;
+	entry->tail = tail;
+	entry->cmd_nr = cmd_nr;
+	entry->flags = flags;
+	entry->ip_offset = ip_offset;
+
+	list->tail = next;
+	return 0;
+
+}
+
+static void apply_patch_entry(struct vgt_device *vgt, struct cmd_patch_info *patch)
+{
+	ASSERT(patch->addr);
+
+	hypervisor_write_va(vgt, patch->addr, &patch->new_val,
+				sizeof(patch->new_val), 1);
+	clflush(patch->addr);
+}
+
+#if 0
+static void revert_batch_entry(struct batch_info *info)
+{
+	ASSERT(info->addr);
+
+	*(uint32_t *)info->addr = info->old_val;
+}
+#endif
+
+/*
+ * Apply all patch entries with request ID before or
+ * equal to the submission ID
+ */
+static void apply_patch_list(struct vgt_device *vgt, vgt_state_ring_t *rs,
+		uint64_t submission_id)
+{
+	int next;
+	struct cmd_general_info *list = &rs->patch_list;
+	struct cmd_patch_info *patch;
+
+	next = list->head;
+	while (next != list->tail) {
+		next++;
+		if (next == list->count)
+			next = 0;
+		patch = &list->patch[next];
+		/* TODO: handle id wrap */
+		if (patch->request_id > submission_id)
+			break;
+
+		vgt_dbg(VGT_DBG_CMD, "submission-%lld: apply patch entry-%d (addr: %llx, val: %x->%x, id: %lld\n",
+			submission_id, next, (uint64_t)patch->addr,
+			patch->old_val, patch->new_val, patch->request_id);
+		apply_patch_entry(vgt, patch);
+		list->head = next;
+	}
+}
+
+/*
+ * Invoke all post-handle entries with request ID before or
+ * equal to the submission ID
+ */
+static void apply_post_handle_list(vgt_state_ring_t *rs, uint64_t submission_id)
+{
+	int next;
+	struct cmd_general_info *list = &rs->handler_list;
+	struct cmd_handler_info *entry;
+
+	next = list->head;
+	while (next != list->tail) {
+		next++;
+		if (next == list->count)
+			next = 0;
+		entry = &list->handler[next];
+		/* TODO: handle id wrap */
+		if (entry->request_id > submission_id)
+			break;
+
+		entry->handler(&entry->exec_state);
+		list->head = next;
+	}
+}
+
+/* submit tails according to submission id */
+void apply_tail_list(struct vgt_device *vgt, int ring_id,
+	uint64_t submission_id)
+{
+	int next;
+	struct pgt_device *pdev = vgt->pdev;
+	vgt_state_ring_t *rs = &vgt->rb[ring_id];
+	struct cmd_general_info *list = &rs->tail_list;
+	struct cmd_tail_info *entry;
+
+	next = list->head;
+	while (next != list->tail) {
+		next++;
+		if (next == list->count)
+			next = 0;
+		entry = &list->cmd[next];
+		/* TODO: handle id wrap */
+		if (entry->request_id > submission_id)
+			break;
+
+		apply_post_handle_list(rs, entry->request_id);
+		apply_patch_list(vgt, rs, entry->request_id);
+
+		if (!pdev->enable_execlist) {
+			if ((rs->uhptr & _REGBIT_UHPTR_VALID) &&
+					(rs->uhptr_id < entry->request_id)) {
+				rs->uhptr &= ~_REGBIT_UHPTR_VALID;
+				VGT_MMIO_WRITE(pdev, VGT_UHPTR(ring_id), rs->uhptr);
+			}
+			VGT_WRITE_TAIL(pdev, ring_id, entry->tail);
+		}
+		list->head = next;
+	}
+}
+
+/* find allowable tail info based on cmd budget */
+int get_submission_id(vgt_state_ring_t *rs, int budget,
+	uint64_t *submission_id)
+{
+	int next, cmd_nr = 0;
+	struct cmd_general_info *list = &rs->tail_list;
+	struct cmd_tail_info *entry, *target = NULL;
+
+	next = list->head;
+	while (next != list->tail) {
+		next++;
+		if (next == list->count)
+			next = 0;
+		entry = &list->cmd[next];
+		budget -= entry->cmd_nr;
+		if (budget < 0)
+			break;
+		target = entry;
+		cmd_nr += entry->cmd_nr;
+	}
+
+	if (target) {
+		*submission_id = target->request_id;
+		return cmd_nr;
+	} else
+		return MAX_CMD_BUDGET;
+}
+
+/* ring ALL, type = 0 */
+static struct sub_op_bits sub_op_mi[] = {
+	{31, 29},
+	{28, 23},
+};
+
+static struct decode_info decode_info_mi = {
+	"MI",
+	OP_LEN_MI,
+	ARRAY_SIZE(sub_op_mi),
+	sub_op_mi,
+};
+
+
+/* ring RCS, command type 2 */
+static struct sub_op_bits sub_op_2d[] = {
+	{31, 29},
+	{28, 22},
+};
+
+static struct decode_info decode_info_2d = {
+	"2D",
+	OP_LEN_2D,
+	ARRAY_SIZE(sub_op_2d),
+	sub_op_2d,
+};
+
+/* ring RCS, command type 3 */
+static struct sub_op_bits sub_op_3d_media[] = {
+	{31, 29},
+	{28, 27},
+	{26, 24},
+	{23, 16},
+};
+
+static struct decode_info decode_info_3d_media = {
+	"3D_Media",
+	OP_LEN_3D_MEDIA,
+	ARRAY_SIZE(sub_op_3d_media),
+	sub_op_3d_media,
+};
+
+/* ring VCS, command type 3 */
+static struct sub_op_bits sub_op_mfx_vc[] = {
+	{31, 29},
+	{28, 27},
+	{26, 24},
+	{23, 21},
+	{20, 16},
+};
+
+static struct decode_info decode_info_mfx_vc = {
+	"MFX_VC",
+	OP_LEN_MFX_VC,
+	ARRAY_SIZE(sub_op_mfx_vc),
+	sub_op_mfx_vc,
+};
+
+/* ring VECS, command type 3 */
+static struct sub_op_bits sub_op_vebox[] = {
+	{31, 29},
+	{28, 27},
+	{26, 24},
+	{23, 21},
+	{20, 16},
+};
+
+static struct decode_info decode_info_vebox = {
+	"VEBOX",
+	OP_LEN_VEBOX,
+	ARRAY_SIZE(sub_op_vebox),
+	sub_op_vebox,
+};
+
+static struct decode_info* ring_decode_info[MAX_ENGINES][8] = {
+	[RING_BUFFER_RCS] = {
+		&decode_info_mi,
+		NULL,
+		NULL,
+		&decode_info_3d_media,
+		NULL,
+		NULL,
+		NULL,
+		NULL,
+	},
+
+	[RING_BUFFER_VCS] = {
+		&decode_info_mi,
+		NULL,
+		NULL,
+		&decode_info_mfx_vc,
+		NULL,
+		NULL,
+		NULL,
+		NULL,
+	},
+
+	[RING_BUFFER_BCS] = {
+		&decode_info_mi,
+		NULL,
+		&decode_info_2d,
+		NULL,
+		NULL,
+		NULL,
+		NULL,
+		NULL,
+	},
+
+	[RING_BUFFER_VECS] = {
+		&decode_info_mi,
+		NULL,
+		NULL,
+		&decode_info_vebox,
+		NULL,
+		NULL,
+		NULL,
+		NULL,
+	},
+
+	[RING_BUFFER_VCS2] = {
+		&decode_info_mi,
+		NULL,
+		NULL,
+		&decode_info_mfx_vc,
+		NULL,
+		NULL,
+		NULL,
+		NULL,
+	},
+};
+
+uint32_t vgt_get_opcode(uint32_t cmd, int ring_id)
+{
+	struct decode_info * d_info;
+
+	if (ring_id >= MAX_ENGINES)
+		return INVALID_OP;
+
+	d_info = ring_decode_info[ring_id][CMD_TYPE(cmd)];
+	if (d_info == NULL)
+		return INVALID_OP;
+
+	return cmd >> (32 - d_info->op_len);
+}
+
+static inline uint32_t sub_op_val(uint32_t cmd, uint32_t hi, uint32_t low)
+{
+	return (cmd >> low) & ((1U << (hi-low+1)) - 1);
+}
+
+static void vgt_print_opcode(uint32_t cmd, int ring_id)
+{
+	struct decode_info * d_info;
+	int i;
+
+	if (ring_id >= MAX_ENGINES)
+		return;
+
+	d_info = ring_decode_info[ring_id][CMD_TYPE(cmd)];
+	if (d_info == NULL)
+		return;
+
+	vgt_err("opcode=0x%x %s sub_ops:", cmd >> (32 - d_info->op_len), d_info->name);
+	for (i = 0; i< d_info->nr_sub_op; i++) {
+		vgt_err("0x%x ", sub_op_val(cmd, d_info->sub_op[i].hi,  d_info->sub_op[i].low));
+	}
+	vgt_err("\n");
+}
+
+static inline struct cmd_info* vgt_get_cmd_info(uint32_t cmd, int ring_id)
+{
+	uint32_t opcode;
+
+	opcode = vgt_get_opcode(cmd, ring_id);
+	if (opcode == INVALID_OP) {
+		return NULL;
+	}
+
+	return vgt_find_cmd_entry(opcode, ring_id);
+}
+
+static inline uint32_t *cmd_ptr(struct parser_exec_state *s, int index)
+{
+	if (index < s->ip_buf_len)
+		return s->ip_va + index;
+	else
+		return s->ip_va_next_page + (index - s->ip_buf_len);
+}
+
+static inline uint32_t cmd_val(struct parser_exec_state *s, int index)
+{
+	uint32_t *addr = cmd_ptr(s, index);
+	uint32_t ret = 0;
+
+	hypervisor_read_va(s->vgt, addr, &ret, sizeof(ret), 1);
+
+	return ret;
+}
+
+static void parser_exec_state_dump(struct parser_exec_state *s)
+{
+	vgt_err("  vgt%d RING%d: ring_start(%08lx) ring_end(%08lx)"
+			" ring_head(%08lx) ring_tail(%08lx)\n", s->vgt->vgt_id,
+			s->ring_id, s->ring_start, s->ring_start + s->ring_size, s->ring_head, s->ring_tail);
+
+	vgt_err("  %s %s ip_gma(%08lx) ",
+			s->buf_type == RING_BUFFER_INSTRUCTION ? "RING_BUFFER": "BATCH_BUFFER",
+			s->buf_addr_type == GTT_BUFFER ? "GTT" : "PPGTT", s->ip_gma);
+
+	if (s->ip_va == NULL) {
+		vgt_err(" ip_va(NULL)\n");
+	} else {
+		vgt_err("  ip_va=%p: %08x %08x %08x %08x \n",
+				s->ip_va, cmd_val(s, 0), cmd_val(s, 1), cmd_val(s, 2), cmd_val(s, 3));
+		vgt_print_opcode(cmd_val(s, 0), s->ring_id);
+	}
+}
+#define RING_BUF_WRAP(s, ip_gma)	(((s)->buf_type == RING_BUFFER_INSTRUCTION) && \
+		((ip_gma) >= (s)->ring_start + (s)->ring_size))
+
+static inline struct vgt_mm *parser_exec_state_to_mm(struct parser_exec_state *s)
+{
+	if (s->buf_addr_type != PPGTT_BUFFER)
+		return s->vgt->gtt.ggtt_mm;
+	else
+		return s->vgt->rb[s->ring_id].active_ppgtt_mm;
+}
+
+static int ip_gma_set(struct parser_exec_state *s, unsigned long ip_gma)
+{
+	unsigned long gma_next_page;
+
+	ASSERT(VGT_REG_IS_ALIGNED(ip_gma, 4));
+
+	/* set ip_gma */
+
+	if (RING_BUF_WRAP(s, ip_gma)) {
+		ip_gma = ip_gma - s->ring_size;
+	}
+
+	s->ip_gma = ip_gma;
+	s->ip_va = vgt_gma_to_va(parser_exec_state_to_mm(s), ip_gma);
+	if (s->ip_va == NULL) {
+		vgt_err("ERROR: gma %lx is invalid, fail to set\n", s->ip_gma);
+		dump_stack();
+		parser_exec_state_dump(s);
+		return -EFAULT;
+	}
+
+	s->ip_buf_len = (PAGE_SIZE - (ip_gma & (PAGE_SIZE-1)))
+		/ sizeof(uint32_t);
+
+	/* set ip of next page */
+
+	if (RING_BUF_WRAP(s, ip_gma + PAGE_SIZE))
+		gma_next_page = s->ring_start;
+	else
+		gma_next_page = ((ip_gma >> PAGE_SHIFT) + 1) << PAGE_SHIFT;
+
+	s->ip_va_next_page = vgt_gma_to_va(parser_exec_state_to_mm(s), gma_next_page);
+	if (s->ip_va_next_page == NULL) {
+		vgt_err("ERROR: next page gma %lx is invalid, fail to set\n",gma_next_page);
+		dump_stack();
+		parser_exec_state_dump(s);
+		return -EFAULT;
+	}
+
+	return 0;
+}
+
+static inline int ip_gma_advance(struct parser_exec_state *s, unsigned int len)
+{
+	int rc = 0;
+	if (s->ip_buf_len > len) {
+		/* not cross page, advance ip inside page */
+		s->ip_gma += len*sizeof(uint32_t);
+		s->ip_va += len;
+		s->ip_buf_len -= len;
+	} else {
+		/* cross page, reset ip_va */
+		rc = ip_gma_set(s, s->ip_gma + len*sizeof(uint32_t));
+	}
+	return rc;
+}
+
+static inline int cmd_length(struct parser_exec_state *s)
+{
+	struct cmd_info *info = s->info;
+
+	/*
+	 * MI_NOOP is special as the replacement elements. It's fixed
+	 * length in definition, but variable length when using for
+	 * replacement purpose. Instead of having the same handler
+	 * invoke twice (may be postponed), special case length
+	 * handling for MI_NOOP.
+	 */
+	if (info->opcode == OP_MI_NOOP) {
+		unsigned int cmd, length = info->len;
+		cmd = (cmd_val(s, 0) & VGT_NOOP_ID_CMD_MASK) >>
+			VGT_NOOP_ID_CMD_SHIFT;
+		if (cmd)
+			length = cmd_val(s, 0) & CMD_LENGTH_MASK;
+
+		return length;
+	} else if ((info->flag & F_LEN_MASK) == F_LEN_CONST) {
+		return info->len;
+	} else /* F_LEN_VAR */{
+		return (cmd_val(s, 0) & ((1U << s->info->len) - 1)) + 2;
+	}
+}
+
+static bool addr_audit_32(struct parser_exec_state *s, int index)
+{
+	/* TODO:
+	 * Add the address audit implementation here. Right now do nothing
+	 */
+	return true;
+}
+
+static int vgt_cmd_handler_mi_set_context(struct parser_exec_state* s)
+{
+	struct vgt_device *vgt = s->vgt;
+	addr_audit_32(s, 1);
+	if (!vgt->has_context) {
+		printk("VM %d activate context\n", vgt->vm_id);
+		vgt->has_context = 1;
+	}
+
+	return 0;
+}
+
+static int cmd_reg_handler(struct parser_exec_state *s,
+	unsigned int offset, unsigned int index, char *cmd)
+{
+	struct vgt_device *vgt = s->vgt;
+	struct pgt_device *pdev = vgt->pdev;
+	int rc = -1;
+
+	/*Enabled for HSW at this moment to test,  disabled for BDW*/
+	if (!IS_HSW(pdev)) {
+		rc = 0;
+		goto reg_handle;
+	}
+
+	if (!reg_is_mmio(pdev, offset + 4)){
+		rc = -1;
+		goto reg_handle;
+	}
+
+	if ( reg_is_render(pdev, offset) ||
+	     reg_passthrough(pdev, offset) ||
+	     (!vgt->vm_id && reg_is_config(pdev, offset)) ) {
+		rc = 0;
+	}
+	else if (offset == _REG_DE_RRMR || offset == _REG_MUL_FORCEWAKE){
+		rc = 0;
+	}/*TODO: for registers like rmrr or other tricky registers, continue using current
+		temporary exception before developing full solution for them.*/
+	else if ((offset == 0x138064) || (offset == 0x42008)) {
+		rc = 0;
+	}
+
+reg_handle:
+	if (!rc)
+		reg_set_cmd_access(pdev, offset);
+	else {
+		vgt_err("%s access to non-render register (%x)\n", cmd, offset);
+		//ASSERT_VM(0,vgt);
+	}
+
+	return 0;
+}
+#define BIT_RANGE_MASK(a, b)	\
+	((1UL << ((a) + 1)) - (1UL << (b)))
+static int vgt_cmd_handler_lri(struct parser_exec_state *s)
+{
+	int i, rc = 0;
+	int cmd_len = cmd_length(s);
+
+	for (i = 1; i < cmd_len; i += 2) {
+		rc |= cmd_reg_handler(s,
+			cmd_val(s, i) & BIT_RANGE_MASK(22, 2), i, "lri");
+	}
+
+	return rc;
+}
+
+static int vgt_cmd_handler_lrr(struct parser_exec_state *s)
+{
+	int i, rc = 0;
+	int cmd_len = cmd_length(s);
+
+	for (i = 1; i < cmd_len; i += 2) {
+		rc = cmd_reg_handler(s,
+			cmd_val(s, i) & BIT_RANGE_MASK(22, 2), i, "lrr-src");
+		rc |= cmd_reg_handler(s,
+			cmd_val(s, i+1) & BIT_RANGE_MASK(22, 2), i, "lrr-dst");
+	}
+
+	return rc;
+}
+
+static int vgt_cmd_handler_lrm(struct parser_exec_state *s)
+{
+	int i, rc = 0;
+	int cmd_len = cmd_length(s);
+
+	for (i = 1; i < cmd_len;) {
+		rc |= cmd_reg_handler(s,
+			cmd_val(s, i) & BIT_RANGE_MASK(22, 2), i, "lrm");
+		i += gmadr_dw_number(s) + 1;
+	}
+
+	return rc;
+}
+
+static int vgt_cmd_handler_srm(struct parser_exec_state *s)
+{
+	int i, rc = 0;
+	int cmd_len = cmd_length(s);
+
+	for (i = 1; i < cmd_len;) {
+		rc |= cmd_reg_handler(s,
+			cmd_val(s, i) & BIT_RANGE_MASK(22, 2), i, "srm");
+		i += gmadr_dw_number(s) + 1;
+	}
+
+	return rc;
+}
+
+static int vgt_cmd_handler_pipe_control(struct parser_exec_state *s)
+{
+	int i, rc = 0;
+	int cmd_len = cmd_length(s);
+
+
+	for (i = 1; i < cmd_len;) {
+		if (cmd_val(s, i) & PIPE_CONTROL_POST_SYNC)
+			rc |= cmd_reg_handler(s,
+				cmd_val(s, i+1) & BIT_RANGE_MASK(22, 2), i, "pipe_ctrl");
+		else if (cmd_val(s, i) & (2 << 14))
+			rc |= cmd_reg_handler(s, 0x2350, i, "pipe_ctrl");
+		else if (cmd_val(s, i) & (3 << 14))
+			rc |= cmd_reg_handler(s, _REG_RCS_TIMESTAMP, i, "pipe_ctrl");
+
+		if (!rc)
+			s->cmd_issue_irq |= (cmd_val(s, i) & PIPE_CONTROL_NOTIFY) ? true : false;
+
+		i += gmadr_dw_number(s) + 3;
+	}
+
+	return rc;
+}
+
+static int vgt_cmd_handler_mi_user_interrupt(struct parser_exec_state *s)
+{
+	s->cmd_issue_irq = true;
+	return 0;
+}
+
+static int vgt_cmd_advance_default(struct parser_exec_state *s)
+{
+	return ip_gma_advance(s, cmd_length(s));
+}
+
+
+static int vgt_cmd_handler_mi_batch_buffer_end(struct parser_exec_state *s)
+{
+	int rc;
+
+	if (s->buf_type == BATCH_BUFFER_2ND_LEVEL) {
+		s->buf_type = BATCH_BUFFER_INSTRUCTION;
+		rc = ip_gma_set(s, s->ret_ip_gma_bb);
+		s->buf_addr_type = s->saved_buf_addr_type;
+	} else {
+		s->buf_type = RING_BUFFER_INSTRUCTION;
+		s->buf_addr_type = GTT_BUFFER;
+		rc = ip_gma_set(s, s->ret_ip_gma_ring);
+	}
+
+	return rc;
+}
+
+/* TODO
+ *
+ * The mi_display_flip handler below is just a workaround. The completed
+ * handling is under discussion. The current approach is to NOOP the
+ * MI_DISPLAY_FLIP command and then do pre-emulation. The pre-emulation
+ * cannot exactly emulate command's behavior since it happens before
+ * the command is issued. Consider the following two cases: one is that
+ * right after the command-scan, display switch happens. Another case
+ * is that commands inside ring buffer has some dependences.
+ *
+ * The interrupt is another consideration. mi_display_flip can trigger
+ * interrupt for completion. VM's gfx driver may rely on that. Whether
+ * we should inject virtual interrupt and when is the right time.
+ *
+ * The user space could resubmit ring/batch buffer with partially updated
+ * MI_DISPLAY_FLIP. So special handling is needed in command parser for
+ * such scenario.
+ *
+ * And we did not update HW state for the display flip.
+ *
+ */
+#define PLANE_SELECT_SHIFT	19
+#define PLANE_SELECT_MASK	(0x7 << PLANE_SELECT_SHIFT)
+#define SURF_MASK		0xFFFFF000
+#define PITCH_MASK		0x0000FFC0
+#define TILE_PARA_SHIFT		0x0
+#define TILE_PARA_MASK		0x1
+/* Primary plane and sprite plane has the same tile shift in control reg */
+#define PLANE_TILE_SHIFT	_PRI_PLANE_TILE_SHIFT
+#define PLANE_TILE_MASK		(0x1 << PLANE_TILE_SHIFT)
+#define FLIP_TYPE_MASK		0x3
+
+#define DISPLAY_FLIP_PLANE_A  0x0
+#define DISPLAY_FLIP_PLANE_B  0x1
+#define DISPLAY_FLIP_SPRITE_A  0x2
+#define DISPLAY_FLIP_SPRITE_B  0x3
+#define DISPLAY_FLIP_PLANE_C  0x4
+#define DISPLAY_FLIP_SPRITE_C  0x5
+
+
+/* The NOOP for MI_DISPLAY_FLIP has below information stored in NOOP_ID:
+ *
+ *	bit 21 - bit 16 is 0x14, opcode of MI_DISPLAY_FLIP;
+ *	bit 10 - bit 8  is plane select;
+ *	bit 7  - bit 0  is the cmd length
+ */
+#define PLANE_INFO_SHIFT	8
+#define PLANE_INFO_MASK		(0x7 << PLANE_INFO_SHIFT)
+
+static bool display_flip_decode_plane_info(uint32_t  plane_code, enum vgt_pipe *pipe, enum vgt_plane_type *plane )
+{
+	switch (plane_code) {
+		case DISPLAY_FLIP_PLANE_A:
+			*pipe = PIPE_A;
+			*plane = PRIMARY_PLANE;
+			break;
+		case DISPLAY_FLIP_PLANE_B:
+			*pipe = PIPE_B;
+			*plane = PRIMARY_PLANE;
+			break;
+		case DISPLAY_FLIP_SPRITE_A:
+			*pipe = PIPE_A;
+			*plane = SPRITE_PLANE;
+			break;
+		case DISPLAY_FLIP_SPRITE_B:
+			*pipe = PIPE_B;
+			*plane = SPRITE_PLANE;
+			break;
+		case DISPLAY_FLIP_PLANE_C:
+			*pipe = PIPE_C;
+			*plane = PRIMARY_PLANE;
+			break;
+		case DISPLAY_FLIP_SPRITE_C:
+			*pipe = PIPE_C;
+			*plane = SPRITE_PLANE;
+			break;
+		default:
+			return false;
+	}
+
+	return true;
+
+}
+
+static bool display_flip_encode_plane_info(enum vgt_pipe pipe, enum vgt_plane_type plane, uint32_t * plane_code)
+{
+
+	if (pipe == PIPE_A && plane == PRIMARY_PLANE)
+		*plane_code = DISPLAY_FLIP_PLANE_A;
+	else if (pipe == PIPE_B && plane == PRIMARY_PLANE)
+		*plane_code = DISPLAY_FLIP_PLANE_B;
+	else if (pipe == PIPE_A && plane == SPRITE_PLANE)
+		*plane_code = DISPLAY_FLIP_SPRITE_A;
+	else if (pipe == PIPE_B && plane == SPRITE_PLANE)
+		*plane_code = DISPLAY_FLIP_SPRITE_B;
+	else if (pipe == PIPE_C && plane == PRIMARY_PLANE)
+		*plane_code = DISPLAY_FLIP_PLANE_C;
+	else if (pipe == PIPE_C && plane == SPRITE_PLANE)
+		*plane_code = DISPLAY_FLIP_SPRITE_C;
+	else
+		return false;
+
+	return true;
+
+}
+
+#define GET_INFO_FOR_FLIP(pipe, plane, 					\
+			ctrl_reg, surf_reg, stride_reg, stride_mask)	\
+do{									\
+	if (plane == PRIMARY_PLANE) {					\
+		ctrl_reg = VGT_DSPCNTR(pipe);				\
+		surf_reg = VGT_DSPSURF(pipe);				\
+		stride_reg = VGT_DSPSTRIDE(pipe);			\
+		stride_mask = _PRI_PLANE_STRIDE_MASK;			\
+	} else {							\
+		ASSERT (plane == SPRITE_PLANE);				\
+		ctrl_reg = VGT_SPRCTL(pipe);				\
+		surf_reg = VGT_SPRSURF(pipe);				\
+		stride_reg = VGT_SPRSTRIDE(pipe);			\
+		stride_mask = _SPRITE_STRIDE_MASK;			\
+	}								\
+}while(0);
+
+static bool vgt_flip_parameter_check(struct parser_exec_state *s,
+				uint32_t plane_code,
+				uint32_t stride_val,
+				uint32_t surf_val)
+{
+	struct pgt_device *pdev = s->vgt->pdev;
+	enum vgt_pipe pipe = I915_MAX_PIPES;
+	enum vgt_plane_type plane = MAX_PLANE;
+	uint32_t surf_reg, ctrl_reg;
+	uint32_t stride_reg, stride_mask, phys_stride;
+	uint32_t tile_para, tile_in_ctrl;
+	bool async_flip;
+
+	if (!display_flip_decode_plane_info(plane_code, &pipe, &plane))
+		return false;
+
+	GET_INFO_FOR_FLIP(pipe, plane,
+			ctrl_reg, surf_reg, stride_reg, stride_mask);
+
+	async_flip = ((surf_val & FLIP_TYPE_MASK) == 0x1);
+	tile_para = ((stride_val & TILE_PARA_MASK) >> TILE_PARA_SHIFT);
+	tile_in_ctrl = (__vreg(s->vgt, ctrl_reg) & PLANE_TILE_MASK)
+				>> PLANE_TILE_SHIFT;
+
+	phys_stride = __vreg(current_display_owner(pdev), stride_reg);
+	if ((s->vgt != current_display_owner(pdev)) && !enable_panel_fitting &&
+		(plane == PRIMARY_PLANE) &&
+		((stride_val & PITCH_MASK) !=
+			(phys_stride & stride_mask))) {
+		vgt_dbg(VGT_DBG_CMD, "Stride value may not match display timing! "
+			"MI_DISPLAY_FLIP will be ignored!\n");
+		return false;
+	}
+
+	if ((__vreg(s->vgt, stride_reg) & stride_mask)
+		!= (stride_val & PITCH_MASK)) {
+
+		if (async_flip) {
+			vgt_warn("Cannot change stride in async flip!\n");
+			return false;
+		}
+	}
+
+	if (tile_para != tile_in_ctrl) {
+
+		if (async_flip) {
+			vgt_warn("Cannot change tiling in async flip!\n");
+			return false;
+		}
+	}
+
+	return true;
+}
+
+static int vgt_handle_mi_display_flip(struct parser_exec_state *s, bool resubmitted)
+{
+	uint32_t surf_reg, surf_val, ctrl_reg;
+	uint32_t stride_reg, stride_val, stride_mask;
+	uint32_t tile_para;
+	uint32_t opcode, plane_code, real_plane_code;
+	enum vgt_pipe pipe;
+	enum vgt_pipe real_pipe;
+	enum vgt_plane_type plane;
+	int i, length, rc = 0;
+	struct fb_notify_msg msg;
+	uint32_t value;
+
+	opcode = cmd_val(s, 0);
+	stride_val = cmd_val(s, 1);
+	surf_val = cmd_val(s, 2);
+
+	if (resubmitted) {
+		plane_code = (opcode & PLANE_INFO_MASK) >> PLANE_INFO_SHIFT;
+		length = opcode & CMD_LENGTH_MASK;
+	} else {
+		plane_code = (opcode & PLANE_SELECT_MASK) >> PLANE_SELECT_SHIFT;
+		length = cmd_length(s);
+	}
+
+
+	if (!display_flip_decode_plane_info(plane_code, &pipe, &plane)) {
+		goto wrong_command;
+	}
+
+	real_pipe = s->vgt->pipe_mapping[pipe];
+
+	if (length == 4) {
+		vgt_warn("Page flip of Stereo 3D is not supported!\n");
+		goto wrong_command;
+	} else if (length != 3) {
+		vgt_warn("Flip length not equal to 3, ignore handling flipping");
+		goto wrong_command;
+	}
+
+	if ((pipe == I915_MAX_PIPES) || (plane == MAX_PLANE)) {
+		vgt_warn("Invalid pipe/plane in MI_DISPLAY_FLIP!\n");
+		goto wrong_command;
+	}
+
+	if (!resubmitted) {
+		if (!vgt_flip_parameter_check(s, plane_code, stride_val, surf_val))
+			goto wrong_command;
+
+		GET_INFO_FOR_FLIP(pipe, plane,
+			ctrl_reg, surf_reg, stride_reg, stride_mask);
+		tile_para = ((stride_val & TILE_PARA_MASK) >> TILE_PARA_SHIFT);
+
+		__vreg(s->vgt, stride_reg) = (stride_val & stride_mask) |
+				(__vreg(s->vgt, stride_reg) & (~stride_mask));
+		__vreg(s->vgt, ctrl_reg) = (tile_para << PLANE_TILE_SHIFT) |
+				(__vreg(s->vgt, ctrl_reg) & (~PLANE_TILE_MASK));
+		__vreg(s->vgt, surf_reg) = (surf_val & SURF_MASK) |
+				(__vreg(s->vgt, surf_reg) & (~SURF_MASK));
+		__sreg(s->vgt, stride_reg) = __vreg(s->vgt, stride_reg);
+		__sreg(s->vgt, ctrl_reg) = __vreg(s->vgt, ctrl_reg);
+		__sreg(s->vgt, surf_reg) = __vreg(s->vgt, surf_reg);
+	}
+
+	__vreg(s->vgt, VGT_PIPE_FLIPCOUNT(pipe))++;
+
+	msg.vm_id = s->vgt->vm_id;
+	msg.pipe_id = pipe;
+	msg.plane_id = plane;
+	vgt_fb_notifier_call_chain(FB_DISPLAY_FLIP, &msg);
+
+	if ((s->vgt == current_foreground_vm(s->vgt->pdev)) && !resubmitted) {
+		if(!display_flip_encode_plane_info(real_pipe, plane, &real_plane_code))
+			goto wrong_command;
+
+		value = cmd_val(s, 0);
+		add_patch_entry(s,
+			cmd_ptr(s, 0),
+			((value & ~PLANE_SELECT_MASK) |
+			 (real_plane_code << PLANE_SELECT_SHIFT)));
+		return 0;
+	}
+
+	vgt_dbg(VGT_DBG_CMD, "VM %d: mi_display_flip to be ignored\n",
+		s->vgt->vm_id);
+
+	for (i = 1; i < length; i ++) {
+		rc |= add_patch_entry(s, cmd_ptr(s, i), MI_NOOP |
+			(OP_MI_DISPLAY_FLIP << VGT_NOOP_ID_CMD_SHIFT));
+	}
+
+	rc |= add_patch_entry(s, cmd_ptr(s, 0), MI_NOOP |
+			(OP_MI_DISPLAY_FLIP << VGT_NOOP_ID_CMD_SHIFT) |
+			(plane_code << PLANE_INFO_SHIFT) |
+			(length & CMD_LENGTH_MASK));
+
+	vgt_inject_flip_done(s->vgt, pipe);
+
+	return rc;
+
+wrong_command:
+	for (i = 0; i < length; i ++)
+		rc |= add_patch_entry(s, cmd_ptr(s, i), MI_NOOP);
+	return rc;
+}
+
+static int vgt_cmd_handler_mi_display_flip(struct parser_exec_state *s)
+{
+	addr_audit_32(s, 2);
+	return vgt_handle_mi_display_flip(s, false);
+}
+static bool is_wait_for_flip_pending(uint32_t cmd)
+{
+	return cmd & (MI_WAIT_FOR_PLANE_A_FLIP_PENDING |
+		MI_WAIT_FOR_PLANE_B_FLIP_PENDING |
+		MI_WAIT_FOR_PLANE_C_FLIP_PENDING |
+		MI_WAIT_FOR_SPRITE_A_FLIP_PENDING |
+		MI_WAIT_FOR_SPRITE_B_FLIP_PENDING |
+		MI_WAIT_FOR_SPRITE_C_FLIP_PENDING);
+}
+
+static int vgt_handle_mi_wait_for_event(struct parser_exec_state *s)
+{
+	int rc = 0;
+	enum vgt_pipe virtual_pipe = I915_MAX_PIPES;
+	enum vgt_pipe real_pipe = I915_MAX_PIPES;
+	uint32_t cmd = cmd_val(s, 0);
+	uint32_t new_cmd = cmd;
+	enum vgt_plane_type plane_type = MAX_PLANE;
+
+	if (!is_wait_for_flip_pending(cmd))
+		return rc;
+
+	if (s->vgt != current_foreground_vm(s->vgt->pdev)) {
+		rc |= add_patch_entry(s, cmd_ptr(s, 0), MI_NOOP);
+		vgt_dbg(VGT_DBG_CMD, "VM %d: mi_wait_for_event to be ignored\n", s->vgt->vm_id);
+		return rc;
+	}
+
+	if (cmd & MI_WAIT_FOR_PLANE_A_FLIP_PENDING) {
+		virtual_pipe = PIPE_A;
+		plane_type = PRIMARY_PLANE;
+		new_cmd &= ~MI_WAIT_FOR_PLANE_A_FLIP_PENDING;
+	} else if (cmd & MI_WAIT_FOR_PLANE_B_FLIP_PENDING) {
+		virtual_pipe = PIPE_B;
+		plane_type = PRIMARY_PLANE;
+		new_cmd &= ~MI_WAIT_FOR_PLANE_B_FLIP_PENDING;
+	} else if (cmd & MI_WAIT_FOR_PLANE_C_FLIP_PENDING) {
+		virtual_pipe = PIPE_C;
+		plane_type = PRIMARY_PLANE;
+		new_cmd &= ~MI_WAIT_FOR_PLANE_C_FLIP_PENDING;
+	} else if (cmd & MI_WAIT_FOR_SPRITE_A_FLIP_PENDING) {
+		virtual_pipe = PIPE_A;
+		plane_type = SPRITE_PLANE;
+		new_cmd &= ~MI_WAIT_FOR_SPRITE_A_FLIP_PENDING;
+	} else if (cmd & MI_WAIT_FOR_SPRITE_B_FLIP_PENDING) {
+		virtual_pipe = PIPE_B;
+		plane_type = SPRITE_PLANE;
+		new_cmd &= ~MI_WAIT_FOR_SPRITE_B_FLIP_PENDING;
+	} else  if(cmd & MI_WAIT_FOR_SPRITE_C_FLIP_PENDING) {
+		virtual_pipe = PIPE_C;
+		plane_type = SPRITE_PLANE;
+		new_cmd &= ~MI_WAIT_FOR_SPRITE_C_FLIP_PENDING;
+	} else {
+		ASSERT(0);
+	}
+
+	real_pipe = s->vgt->pipe_mapping[virtual_pipe];
+
+	if (real_pipe == PIPE_A && plane_type == PRIMARY_PLANE) {
+		new_cmd |= MI_WAIT_FOR_PLANE_A_FLIP_PENDING;
+	} else if (real_pipe == PIPE_B && plane_type == PRIMARY_PLANE) {
+		new_cmd |= MI_WAIT_FOR_PLANE_B_FLIP_PENDING;
+	} else if (real_pipe == PIPE_C && plane_type == PRIMARY_PLANE) {
+		new_cmd |= MI_WAIT_FOR_PLANE_C_FLIP_PENDING;
+	} else if (real_pipe == PIPE_A && plane_type == SPRITE_PLANE) {
+		new_cmd |= MI_WAIT_FOR_SPRITE_A_FLIP_PENDING;
+	} else if (real_pipe == PIPE_B && plane_type == SPRITE_PLANE) {
+		new_cmd |= MI_WAIT_FOR_SPRITE_B_FLIP_PENDING;
+	} else if (real_pipe == PIPE_C && plane_type == SPRITE_PLANE) {
+		new_cmd |= MI_WAIT_FOR_SPRITE_C_FLIP_PENDING;
+	} else {
+		rc = add_patch_entry(s, cmd_ptr(s, 0), MI_NOOP);
+		return rc;
+	}
+	rc = add_patch_entry(s, cmd_ptr(s, 0), new_cmd);
+	return rc;
+}
+
+static unsigned long get_gma_bb_from_cmd(struct parser_exec_state *s, int index)
+{
+	unsigned long addr;
+	int32_t gma_high, gma_low;
+	int gmadr_bytes = s->vgt->pdev->device_info.gmadr_bytes_in_cmd;
+
+	ASSERT(gmadr_bytes == 4 || gmadr_bytes == 8);
+
+	gma_low = cmd_val(s, index) & BATCH_BUFFER_ADDR_MASK;
+
+	if (gmadr_bytes == 4) {
+		addr = gma_low;
+	} else {
+		gma_high = cmd_val(s, index + 1) & BATCH_BUFFER_ADDR_HIGH_MASK;
+		addr = (((unsigned long)gma_high) << 32) | gma_low;
+	}
+
+	return addr;
+}
+
+static bool address_audit(struct parser_exec_state *s, int index)
+{
+	int gmadr_bytes = s->vgt->pdev->device_info.gmadr_bytes_in_cmd;
+
+	/* TODO:
+	 * Add the address audit implementation here. Right now do nothing
+	 */
+	ASSERT(gmadr_bytes == 4 || gmadr_bytes == 8);
+
+	return true;
+}
+
+static bool vgt_cmd_addr_audit_with_bitmap(struct parser_exec_state *s,
+			unsigned long addr_bitmap)
+{
+	unsigned int bit;
+	unsigned int delta = 0;
+	int cmd_len = cmd_length(s);
+
+	for_each_set_bit(bit, &addr_bitmap, sizeof(addr_bitmap)*8) {
+		if (bit + delta >= cmd_len)
+			return false;
+		address_audit(s, bit + delta);
+		delta = delta + gmadr_dw_number(s) - 1;
+	}
+
+	return true;
+}
+
+static int vgt_cmd_handler_mi_update_gtt(struct parser_exec_state *s)
+{
+	vgt_err("Unexpectted mi_update_gtt in VM command buffer\n");
+	return -1;
+}
+
+static int vgt_cmd_handler_mi_flush_dw(struct parser_exec_state* s)
+{
+	int i, len;
+	int offset = 1;
+
+	/* Check post-sync bit */
+	if ((cmd_val(s, 0) >> 14) & 0x3)
+		address_audit(s, offset);
+	offset += gmadr_dw_number(s);
+
+	/* Check notify bit */
+	s->cmd_issue_irq = ( cmd_val(s,0) & (1 << 8)) ? true : false;
+
+	len = cmd_length(s);
+	for (i=2; i<len; i++) {
+		address_audit(s, offset);
+		offset += gmadr_dw_number(s);
+	}
+
+	return 0;
+}
+
+static void addr_type_update_snb(struct parser_exec_state* s)
+{
+	if ((s->buf_type == RING_BUFFER_INSTRUCTION) &&
+			(s->vgt->rb[s->ring_id].has_ppgtt_mode_enabled) &&
+			(BATCH_BUFFER_ADR_SPACE_BIT(cmd_val(s, 0)) == 1)) {
+		s->buf_addr_type = PPGTT_BUFFER;
+	}
+}
+
+/*
+ * Check whether a batch buffer needs to be scanned. Currently
+ * the only criteria is based on privilege.
+ */
+static int batch_buffer_needs_scan(struct parser_exec_state *s)
+{
+	struct pgt_device *pdev = s->vgt->pdev;
+
+	if (IS_BDW(pdev)) {
+		/* BDW decides privilege based on address space */
+		if (cmd_val(s, 0) & (1 << 8))
+			return 0;
+	} else if (IS_HSW(pdev)) {
+		/* pre-BDW has dedicated privilege bit */
+		if (cmd_val(s, 0) & (1 << 13))
+			return 0;
+	}
+
+	return 1;
+}
+
+static int vgt_cmd_handler_mi_batch_buffer_start(struct parser_exec_state *s)
+{
+	int rc=0;
+	bool second_level;
+
+	if (s->buf_type == BATCH_BUFFER_2ND_LEVEL) {
+		vgt_err("MI_BATCH_BUFFER_START not allowd in 2nd level batch buffer\n");
+		return -EINVAL;
+	}
+
+	second_level = BATCH_BUFFER_2ND_LEVEL_BIT(cmd_val(s, 0)) == 1;
+	if (second_level && (s->buf_type != BATCH_BUFFER_INSTRUCTION)) {
+		vgt_err("Jumping to 2nd level batch buffer from ring buffer is not allowd\n");
+		return -EINVAL;
+	}
+
+	s->saved_buf_addr_type = s->buf_addr_type;
+
+	/* FIXME: add IVB/HSW code */
+	addr_type_update_snb(s);
+
+	if (s->buf_type == RING_BUFFER_INSTRUCTION) {
+		s->ret_ip_gma_ring = s->ip_gma + cmd_length(s) * sizeof(uint32_t);
+		s->buf_type = BATCH_BUFFER_INSTRUCTION;
+	} else if (second_level) {
+		s->buf_type = BATCH_BUFFER_2ND_LEVEL;
+		s->ret_ip_gma_bb = s->ip_gma + cmd_length(s) * sizeof(uint32_t);
+	}
+
+	klog_printk("MI_BATCH_BUFFER_START: Addr=%x ClearCommandBufferEnable=%d\n",
+			cmd_val(s, 1), (cmd_val(s, 0) >> 11) & 1);
+
+	address_audit(s, 1);
+
+	if (batch_buffer_needs_scan(s)) {
+		rc = ip_gma_set(s, get_gma_bb_from_cmd(s, 1));
+		if (rc < 0)
+			vgt_warn("invalid batch buffer addr, so skip scanning it\n");
+	} else {
+		struct vgt_statistics *stat = &s->vgt->stat;
+
+		stat->skip_bb_cnt++;
+		/* emulate a batch buffer end to do return right */
+		rc = vgt_cmd_handler_mi_batch_buffer_end(s);
+		if (rc < 0)
+			vgt_err("skip batch buffer error\n");
+	}
+
+	return rc;
+}
+
+static int vgt_cmd_handler_3dstate_vertex_buffers(struct parser_exec_state *s)
+{
+	int length, offset;
+
+	length = cmd_length(s);
+
+	for (offset = 1; offset < length; offset += 4) {
+		address_audit(s, offset + 1);
+		address_audit(s, offset + 2);
+	}
+
+	return 0;
+}
+
+static int vgt_cmd_handler_3dstate_vertex_buffers_bdw(struct parser_exec_state *s)
+{
+	int length, offset;
+
+	length = cmd_length(s);
+
+	for (offset = 1; offset < length; offset += 4) {
+		address_audit(s, offset + 1);
+	}
+
+	return 0;
+}
+
+static int vgt_cmd_handler_3dstate_index_buffer(struct parser_exec_state *s)
+{
+	address_audit(s, 1);
+
+	if (cmd_val(s, 2) != 0)
+		address_audit(s, 2);
+
+	return 0;
+}
+
+static unsigned int constant_buffer_address_offset_disable(struct parser_exec_state *s)
+{
+	/* return the "CONSTANT_BUFFER Address Offset Disable" bit
+	  in "INSTPMInstruction Parser Mode Register"
+	  0 - use as offset
+	  1 - use as graphics address
+	 */
+
+	return __vreg(s->vgt, _REG_RCS_INSTPM) & INSTPM_CONS_BUF_ADDR_OFFSET_DIS;
+}
+
+static int vgt_cmd_handler_3dstate_constant_hsw(struct parser_exec_state *s)
+{
+	if (constant_buffer_address_offset_disable(s) == 1)
+		address_audit(s, 3);
+
+	address_audit(s, 4);
+	address_audit(s, 5);
+	address_audit(s, 6);
+
+	return 0;
+}
+
+static int vgt_cmd_handler_3dstate_constant_bdw(struct parser_exec_state *s)
+{
+	int offset = 3;
+	int cmd_len = cmd_length(s);
+
+	while (offset < cmd_len) {
+		address_audit(s, offset);
+		offset += gmadr_dw_number(s);
+	}
+
+	return 0;
+}
+
+static int vgt_cmd_handler_state_base_address(struct parser_exec_state *s)
+{
+	address_audit(s, 1);
+	address_audit(s, 2);
+	address_audit(s, 3);
+	address_audit(s, 4);
+	address_audit(s, 5);
+	/* Zero Bound is ignore */
+	if (cmd_val(s, 6) >> 12)
+		address_audit(s, 6);
+	if (cmd_val(s, 7) >> 12)
+		address_audit(s, 7);
+	if (cmd_val(s, 8) >> 12)
+		address_audit(s, 8);
+	if (cmd_val(s, 9) >> 12)
+		address_audit(s, 9);
+	return 0;
+}
+
+static inline int base_and_upper_addr_fix(struct parser_exec_state *s)
+{
+	address_audit(s, 1);
+	/* Zero Bound is ignore */
+	if (cmd_val(s, 2) >> 12)
+		address_audit(s, 2);
+	return 0;
+}
+
+static int vgt_cmd_handler_3dstate_binding_table_pool_alloc(struct parser_exec_state *s)
+{
+	return base_and_upper_addr_fix(s);
+}
+
+static int vgt_cmd_handler_3dstate_gather_pool_alloc(struct parser_exec_state *s)
+{
+	return base_and_upper_addr_fix(s);
+}
+
+static int vgt_cmd_handler_3dstate_dx9_constant_buffer_pool_alloc(struct parser_exec_state *s)
+{
+	return base_and_upper_addr_fix(s);
+}
+
+static int vgt_cmd_handler_mfx_pipe_buf_addr_state_hsw(struct parser_exec_state *s)
+{
+	int i;
+	int offset = 1;
+	for (i = 1; i <= 24; i++) {
+		address_audit(s, offset);
+		offset += gmadr_dw_number(s);
+	}
+	return 0;
+}
+
+static int vgt_cmd_handler_mfx_pipe_buf_addr_state_bdw(struct parser_exec_state *s)
+{
+	/*  address pattern of the command is like below:
+	 *  from bit0: "01010101 01010111 11111111 11111010 1010"
+	 */
+	vgt_cmd_addr_audit_with_bitmap(s, 0x055fffeaaaUL);
+
+	return 0;
+}
+
+static int vgt_cmd_handler_mfx_ind_obj_base_addr_state_hsw(struct parser_exec_state *s)
+{
+	int i;
+	int offset = 1;
+	for (i = 1; i <= 10; i++) {
+		address_audit(s, offset);
+		offset += gmadr_dw_number(s);
+	}
+	return 0;
+}
+
+static int vgt_cmd_handler_mfx_ind_obj_base_addr_state_bdw(struct parser_exec_state *s)
+{
+	/*  address pattern of the command is like below:
+	 *  from bit0: "10110110 11011010"
+	 */
+	vgt_cmd_addr_audit_with_bitmap(s, 0x5b6d);
+
+	return 0;
+}
+
+static int vgt_cmd_handler_mfx_2_6_0_0(struct parser_exec_state *s)
+{
+	base_and_upper_addr_fix(s);
+	address_audit(s, 2);
+	return 0;
+}
+
+static int vgt_cmd_handler_mi_noop(struct parser_exec_state* s)
+{
+	unsigned int cmd;
+	cmd = (cmd_val(s, 0) & VGT_NOOP_ID_CMD_MASK) >> VGT_NOOP_ID_CMD_SHIFT;
+
+	if (cmd) {
+		if (cmd == OP_MI_DISPLAY_FLIP) {
+			vgt_handle_mi_display_flip(s, true);
+		} else {
+			vgt_err("VM %d: Guest reuse cmd buffer that is not handled!\n",
+					s->vgt->vm_id);
+			parser_exec_state_dump(s);
+		}
+	}
+
+	return 0;
+}
+
+static struct cmd_info cmd_info[] = {
+	{"MI_NOOP", OP_MI_NOOP, F_LEN_CONST|F_POST_HANDLE, R_ALL, D_ALL, 0, 1, vgt_cmd_handler_mi_noop},
+
+	{"MI_SET_PREDICATE", OP_MI_SET_PREDICATE, F_LEN_CONST, R_ALL, D_HSW_PLUS,
+		0, 1, NULL},
+
+	{"MI_USER_INTERRUPT", OP_MI_USER_INTERRUPT, F_LEN_CONST, R_ALL, D_ALL, 0, 1, vgt_cmd_handler_mi_user_interrupt},
+
+	{"MI_WAIT_FOR_EVENT", OP_MI_WAIT_FOR_EVENT, F_LEN_CONST | F_POST_HANDLE, R_RCS | R_BCS,
+		D_ALL, 0, 1, vgt_handle_mi_wait_for_event},
+
+	{"MI_FLUSH", OP_MI_FLUSH, F_LEN_CONST, R_ALL, D_ALL, 0, 1, NULL},
+
+	{"MI_ARB_CHECK", OP_MI_ARB_CHECK, F_LEN_CONST, R_ALL, D_ALL, 0, 1, NULL},
+
+	{"MI_RS_CONTROL", OP_MI_RS_CONTROL, F_LEN_CONST, R_RCS, D_HSW_PLUS, 0, 1, NULL},
+
+	{"MI_REPORT_HEAD", OP_MI_REPORT_HEAD, F_LEN_CONST, R_ALL, D_ALL, 0, 1, NULL},
+
+	{"MI_ARB_ON_OFF", OP_MI_ARB_ON_OFF, F_LEN_CONST, R_ALL, D_ALL, 0, 1, NULL},
+
+	{"MI_URB_ATOMIC_ALLOC", OP_MI_URB_ATOMIC_ALLOC, F_LEN_CONST, R_RCS,
+		D_HSW_PLUS, 0, 1, NULL},
+
+	{"MI_BATCH_BUFFER_END", OP_MI_BATCH_BUFFER_END, F_IP_ADVANCE_CUSTOM|F_LEN_CONST,
+		R_ALL, D_ALL, 0, 1, vgt_cmd_handler_mi_batch_buffer_end},
+
+	{"MI_SUSPEND_FLUSH", OP_MI_SUSPEND_FLUSH, F_LEN_CONST, R_ALL, D_ALL, 0, 1, NULL},
+
+	{"MI_PREDICATE", OP_MI_PREDICATE, F_LEN_CONST, R_RCS, D_IVB_PLUS, 0, 1, NULL},
+
+	{"MI_TOPOLOGY_FILTER", OP_MI_TOPOLOGY_FILTER, F_LEN_CONST, R_ALL,
+		D_IVB_PLUS, 0, 1, NULL},
+
+	{"MI_SET_APPID", OP_MI_SET_APPID, F_LEN_CONST, R_ALL, D_IVB_PLUS, 0, 1, NULL},
+
+	{"MI_RS_CONTEXT", OP_MI_RS_CONTEXT, F_LEN_CONST, R_RCS, D_HSW_PLUS, 0, 1, NULL},
+
+	{"MI_DISPLAY_FLIP", OP_MI_DISPLAY_FLIP, F_LEN_VAR|F_POST_HANDLE, R_RCS | R_BCS,
+		D_ALL, 0, 8, vgt_cmd_handler_mi_display_flip},
+
+	{"MI_SEMAPHORE_MBOX", OP_MI_SEMAPHORE_MBOX, F_LEN_VAR, R_ALL, D_ALL, 0, 8, NULL },
+
+	{"MI_SET_CONTEXT", OP_MI_SET_CONTEXT, F_LEN_VAR, R_ALL, D_ALL,
+		0, 8, vgt_cmd_handler_mi_set_context},
+
+	{"MI_MATH", OP_MI_MATH, F_LEN_VAR, R_ALL, D_ALL, 0, 8, NULL},
+
+	{"MI_URB_CLEAR", OP_MI_URB_CLEAR, F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"ME_SEMAPHORE_SIGNAL", OP_MI_SEMAPHORE_SIGNAL, F_LEN_VAR, R_ALL, D_BDW, 0, 8, NULL},
+
+	{"ME_SEMAPHORE_WAIT", OP_MI_SEMAPHORE_WAIT, F_LEN_VAR, R_ALL, D_BDW, ADDR_FIX_1(2), 8, NULL},
+
+	{"MI_STORE_DATA_IMM", OP_MI_STORE_DATA_IMM, F_LEN_VAR, R_ALL, D_HSW,
+		ADDR_FIX_1(2), 10, NULL},
+
+	{"MI_STORE_DATA_IMM", OP_MI_STORE_DATA_IMM, F_LEN_VAR, R_ALL, D_BDW,
+		ADDR_FIX_1(1), 10, NULL},
+
+	{"MI_STORE_DATA_INDEX", OP_MI_STORE_DATA_INDEX, F_LEN_VAR, R_ALL, D_ALL,
+		0, 8, NULL},
+
+	{"MI_LOAD_REGISTER_IMM", OP_MI_LOAD_REGISTER_IMM, F_LEN_VAR, R_ALL, D_ALL, 0, 8, vgt_cmd_handler_lri},
+
+	{"MI_UPDATE_GTT", OP_MI_UPDATE_GTT, F_LEN_VAR, R_RCS, D_PRE_BDW,
+		0, 8, vgt_cmd_handler_mi_update_gtt},
+
+	{"MI_UPDATE_GTT", OP_MI_UPDATE_GTT, F_LEN_VAR, (R_VCS | R_BCS | R_VECS), D_PRE_BDW,
+		0, 6, vgt_cmd_handler_mi_update_gtt},
+
+	{"MI_UPDATE_GTT", OP_MI_UPDATE_GTT, F_LEN_VAR, R_ALL, D_BDW,
+		0, 10, vgt_cmd_handler_mi_update_gtt},
+
+	{"MI_STORE_REGISTER_MEM", OP_MI_STORE_REGISTER_MEM, F_LEN_VAR, R_ALL, D_ALL,
+		ADDR_FIX_1(2), 8, vgt_cmd_handler_srm},
+
+	{"MI_FLUSH_DW", OP_MI_FLUSH_DW, F_LEN_VAR, R_ALL, D_ALL,
+		0, 6, vgt_cmd_handler_mi_flush_dw},
+
+	{"MI_CLFLUSH", OP_MI_CLFLUSH, F_LEN_VAR, R_ALL, D_ALL,
+		ADDR_FIX_1(1), 10, NULL},
+
+	{"MI_REPORT_PERF_COUNT", OP_MI_REPORT_PERF_COUNT, F_LEN_VAR, R_ALL, D_ALL,
+		ADDR_FIX_1(1), 6, NULL},
+
+	{"MI_LOAD_REGISTER_MEM", OP_MI_LOAD_REGISTER_MEM, F_LEN_VAR, R_ALL, D_GEN7PLUS,
+		ADDR_FIX_1(2), 8, vgt_cmd_handler_lrm},
+
+	{"MI_LOAD_REGISTER_REG", OP_MI_LOAD_REGISTER_REG, F_LEN_VAR, R_ALL, D_HSW_PLUS,
+		0, 8, vgt_cmd_handler_lrr},
+
+	{"MI_RS_STORE_DATA_IMM", OP_MI_RS_STORE_DATA_IMM, F_LEN_VAR, R_RCS, D_HSW_PLUS,
+		0, 8, NULL},
+
+	{"MI_LOAD_URB_MEM", OP_MI_LOAD_URB_MEM, F_LEN_VAR, R_RCS, D_HSW_PLUS,
+		ADDR_FIX_1(2), 8, NULL},
+
+	{"MI_STORE_URM_MEM", OP_MI_STORE_URM_MEM, F_LEN_VAR, R_RCS, D_HSW_PLUS,
+		ADDR_FIX_1(2), 8, NULL},
+
+	{"MI_OP_2E", OP_MI_2E, F_LEN_VAR, R_ALL, D_BDW, ADDR_FIX_2(1, 2), 8, NULL},
+
+	{"MI_OP_2F", OP_MI_2F, F_LEN_VAR, R_ALL, D_BDW, ADDR_FIX_1(1), 8, NULL},
+
+	{"MI_BATCH_BUFFER_START", OP_MI_BATCH_BUFFER_START, F_IP_ADVANCE_CUSTOM,
+		R_ALL, D_ALL, 0, 8, vgt_cmd_handler_mi_batch_buffer_start},
+
+	{"MI_CONDITIONAL_BATCH_BUFFER_END", OP_MI_CONDITIONAL_BATCH_BUFFER_END,
+		F_LEN_VAR, R_ALL, D_ALL, ADDR_FIX_1(2), 8, NULL},
+
+	{"MI_LOAD_SCAN_LINES_INCL", OP_MI_LOAD_SCAN_LINES_INCL, F_LEN_CONST, R_RCS | R_BCS, D_HSW_PLUS,
+		0, 2, NULL},
+
+	{"XY_SETUP_BLT", OP_XY_SETUP_BLT, F_LEN_VAR, R_BCS, D_ALL,
+		ADDR_FIX_2(4, 7), 8, NULL},
+
+	{"XY_SETUP_CLIP_BLT", OP_XY_SETUP_CLIP_BLT, F_LEN_VAR, R_BCS, D_ALL,
+		0, 8, NULL},
+
+	{"XY_SETUP_MONO_PATTERN_SL_BLT", OP_XY_SETUP_MONO_PATTERN_SL_BLT, F_LEN_VAR,
+		R_BCS, D_ALL, ADDR_FIX_1(4), 8, NULL},
+
+	{"XY_PIXEL_BLT", OP_XY_PIXEL_BLT, F_LEN_VAR, R_BCS, D_ALL, 0, 8, NULL},
+
+	{"XY_SCANLINES_BLT", OP_XY_SCANLINES_BLT, F_LEN_VAR, R_BCS, D_ALL,
+		0, 8, NULL},
+
+	{"XY_TEXT_BLT", OP_XY_TEXT_BLT, F_LEN_VAR, R_BCS, D_ALL,
+		ADDR_FIX_1(3), 8, NULL},
+
+	{"XY_TEXT_IMMEDIATE_BLT", OP_XY_TEXT_IMMEDIATE_BLT, F_LEN_VAR, R_BCS,
+		D_ALL, 0, 8, NULL},
+
+	{"COLOR_BLT", OP_COLOR_BLT, F_LEN_VAR, R_BCS, D_PRE_BDW, ADDR_FIX_1(3), 6, NULL},
+
+	{"SRC_COPY_BLT", OP_SRC_COPY_BLT, F_LEN_VAR, R_BCS, D_PRE_BDW,
+		ADDR_FIX_1(3), 6, NULL},
+
+	{"XY_COLOR_BLT", OP_XY_COLOR_BLT, F_LEN_VAR, R_BCS, D_ALL,
+		ADDR_FIX_1(4), 8, NULL},
+
+	{"XY_PAT_BLT", OP_XY_PAT_BLT, F_LEN_VAR, R_BCS, D_ALL,
+		ADDR_FIX_2(4, 5), 8, NULL},
+
+	{"XY_MONO_PAT_BLT", OP_XY_MONO_PAT_BLT, F_LEN_VAR, R_BCS, D_ALL,
+		ADDR_FIX_1(4), 8, NULL},
+
+	{"XY_SRC_COPY_BLT", OP_XY_SRC_COPY_BLT, F_LEN_VAR, R_BCS, D_ALL,
+		ADDR_FIX_2(4, 7), 8, NULL},
+
+	{"XY_MONO_SRC_COPY_BLT", OP_XY_MONO_SRC_COPY_BLT, F_LEN_VAR, R_BCS,
+		D_ALL, ADDR_FIX_2(4, 5), 8, NULL},
+
+	{"XY_FULL_BLT", OP_XY_FULL_BLT, F_LEN_VAR, R_BCS, D_ALL, 0, 8, NULL},
+
+	{"XY_FULL_MONO_SRC_BLT", OP_XY_FULL_MONO_SRC_BLT, F_LEN_VAR, R_BCS, D_ALL,
+		ADDR_FIX_3(4, 5, 8), 8, NULL},
+
+	{"XY_FULL_MONO_PATTERN_BLT", OP_XY_FULL_MONO_PATTERN_BLT, F_LEN_VAR,
+		R_BCS, D_ALL, ADDR_FIX_2(4, 7), 8, NULL},
+
+	{"XY_FULL_MONO_PATTERN_MONO_SRC_BLT", OP_XY_FULL_MONO_PATTERN_MONO_SRC_BLT,
+		F_LEN_VAR, R_BCS, D_ALL, ADDR_FIX_2(4, 5), 8, NULL},
+
+	{"XY_MONO_PAT_FIXED_BLT", OP_XY_MONO_PAT_FIXED_BLT, F_LEN_VAR, R_BCS, D_ALL,
+		ADDR_FIX_1(4), 8, NULL},
+
+	{"XY_MONO_SRC_COPY_IMMEDIATE_BLT", OP_XY_MONO_SRC_COPY_IMMEDIATE_BLT,
+		F_LEN_VAR, R_BCS, D_ALL, ADDR_FIX_1(4), 8, NULL},
+
+	{"XY_PAT_BLT_IMMEDIATE", OP_XY_PAT_BLT_IMMEDIATE, F_LEN_VAR, R_BCS,
+		D_ALL, ADDR_FIX_1(4), 8, NULL},
+
+	{"XY_SRC_COPY_CHROMA_BLT", OP_XY_SRC_COPY_CHROMA_BLT, F_LEN_VAR, R_BCS,
+		D_ALL, ADDR_FIX_2(4, 7), 8, NULL},
+
+	{"XY_FULL_IMMEDIATE_PATTERN_BLT", OP_XY_FULL_IMMEDIATE_PATTERN_BLT,
+		F_LEN_VAR, R_BCS, D_ALL, ADDR_FIX_2(4, 7), 8, NULL},
+
+	{"XY_FULL_MONO_SRC_IMMEDIATE_PATTERN_BLT", OP_XY_FULL_MONO_SRC_IMMEDIATE_PATTERN_BLT,
+		F_LEN_VAR, R_BCS, D_ALL, ADDR_FIX_2(4, 5), 8, NULL},
+
+	{"XY_PAT_CHROMA_BLT", OP_XY_PAT_CHROMA_BLT, F_LEN_VAR, R_BCS, D_ALL,
+		ADDR_FIX_2(4, 5), 8, NULL},
+
+	{"XY_PAT_CHROMA_BLT_IMMEDIATE", OP_XY_PAT_CHROMA_BLT_IMMEDIATE, F_LEN_VAR,
+		R_BCS, D_ALL, ADDR_FIX_1(4), 8, NULL},
+
+	{"3DSTATE_BINDING_TABLE_POINTERS", OP_3DSTATE_BINDING_TABLE_POINTERS,
+		F_LEN_VAR, R_RCS, D_SNB, 0, 8, NULL},
+
+	{"3DSTATE_VIEWPORT_STATE_POINTERS_SF_CLIP", OP_3DSTATE_VIEWPORT_STATE_POINTERS_SF_CLIP,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_VIEWPORT_STATE_POINTERS_CC", OP_3DSTATE_VIEWPORT_STATE_POINTERS_CC,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_BLEND_STATE_POINTERS", OP_3DSTATE_BLEND_STATE_POINTERS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_DEPTH_STENCIL_STATE_POINTERS", OP_3DSTATE_DEPTH_STENCIL_STATE_POINTERS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_BINDING_TABLE_POINTERS_VS", OP_3DSTATE_BINDING_TABLE_POINTERS_VS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_BINDING_TABLE_POINTERS_HS", OP_3DSTATE_BINDING_TABLE_POINTERS_HS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_BINDING_TABLE_POINTERS_DS", OP_3DSTATE_BINDING_TABLE_POINTERS_DS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_BINDING_TABLE_POINTERS_GS", OP_3DSTATE_BINDING_TABLE_POINTERS_GS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_BINDING_TABLE_POINTERS_PS", OP_3DSTATE_BINDING_TABLE_POINTERS_PS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_SAMPLER_STATE_POINTERS_VS", OP_3DSTATE_SAMPLER_STATE_POINTERS_VS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_SAMPLER_STATE_POINTERS_HS", OP_3DSTATE_SAMPLER_STATE_POINTERS_HS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_SAMPLER_STATE_POINTERS_DS", OP_3DSTATE_SAMPLER_STATE_POINTERS_DS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_SAMPLER_STATE_POINTERS_GS", OP_3DSTATE_SAMPLER_STATE_POINTERS_GS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_SAMPLER_STATE_POINTERS_PS", OP_3DSTATE_SAMPLER_STATE_POINTERS_PS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_URB_VS", OP_3DSTATE_URB_VS, F_LEN_VAR, R_RCS, D_GEN7PLUS,
+		0, 8, NULL},
+
+	{"3DSTATE_URB_HS", OP_3DSTATE_URB_HS, F_LEN_VAR, R_RCS, D_GEN7PLUS,
+		0, 8, NULL},
+
+	{"3DSTATE_URB_DS", OP_3DSTATE_URB_DS, F_LEN_VAR, R_RCS, D_GEN7PLUS,
+		0, 8, NULL},
+
+	{"3DSTATE_URB_GS", OP_3DSTATE_URB_GS, F_LEN_VAR, R_RCS, D_GEN7PLUS,
+		0, 8, NULL},
+
+	{"3DSTATE_GATHER_CONSTANT_VS", OP_3DSTATE_GATHER_CONSTANT_VS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_GATHER_CONSTANT_GS", OP_3DSTATE_GATHER_CONSTANT_GS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_GATHER_CONSTANT_HS", OP_3DSTATE_GATHER_CONSTANT_HS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_GATHER_CONSTANT_DS", OP_3DSTATE_GATHER_CONSTANT_DS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_GATHER_CONSTANT_PS", OP_3DSTATE_GATHER_CONSTANT_PS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_DX9_CONSTANTF_VS", OP_3DSTATE_DX9_CONSTANTF_VS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 11, NULL},
+
+	{"3DSTATE_DX9_CONSTANTF_PS", OP_3DSTATE_DX9_CONSTANTF_PS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 11, NULL},
+
+	{"3DSTATE_DX9_CONSTANTI_VS", OP_3DSTATE_DX9_CONSTANTI_VS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_DX9_CONSTANTI_PS", OP_3DSTATE_DX9_CONSTANTI_PS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_DX9_CONSTANTB_VS", OP_3DSTATE_DX9_CONSTANTB_VS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_DX9_CONSTANTB_PS", OP_3DSTATE_DX9_CONSTANTB_PS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_DX9_LOCAL_VALID_VS", OP_3DSTATE_DX9_LOCAL_VALID_VS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_DX9_LOCAL_VALID_PS", OP_3DSTATE_DX9_LOCAL_VALID_PS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_DX9_GENERATE_ACTIVE_VS", OP_3DSTATE_DX9_GENERATE_ACTIVE_VS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_DX9_GENERATE_ACTIVE_PS", OP_3DSTATE_DX9_GENERATE_ACTIVE_PS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_BINDING_TABLE_EDIT_VS", OP_3DSTATE_BINDING_TABLE_EDIT_VS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 9, NULL},
+
+	{"3DSTATE_BINDING_TABLE_EDIT_GS", OP_3DSTATE_BINDING_TABLE_EDIT_GS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 9, NULL},
+
+	{"3DSTATE_BINDING_TABLE_EDIT_HS", OP_3DSTATE_BINDING_TABLE_EDIT_HS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 9, NULL},
+
+	{"3DSTATE_BINDING_TABLE_EDIT_DS", OP_3DSTATE_BINDING_TABLE_EDIT_DS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 9, NULL},
+
+	{"3DSTATE_BINDING_TABLE_EDIT_PS", OP_3DSTATE_BINDING_TABLE_EDIT_PS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 9, NULL},
+
+	{"3DSTATE_VF_INSTANCING", OP_3DSTATE_VF_INSTANCING, F_LEN_VAR, R_RCS, D_BDW, 0, 8, NULL},
+
+	{"3DSTATE_VF_SGVS", OP_3DSTATE_VF_SGVS, F_LEN_VAR, R_RCS, D_BDW, 0, 8, NULL},
+
+	{"3DSTATE_VF_TOPOLOGY", OP_3DSTATE_VF_TOPOLOGY, F_LEN_VAR, R_RCS, D_BDW, 0, 8, NULL},
+
+	{"3DSTATE_WM_CHROMAKEY", OP_3DSTATE_WM_CHROMAKEY, F_LEN_VAR, R_RCS, D_BDW, 0, 8, NULL},
+
+	{"3DSTATE_PS_BLEND", OP_3DSTATE_PS_BLEND, F_LEN_VAR, R_RCS, D_BDW, 0, 8, NULL},
+
+	{"3DSTATE_WM_DEPTH_STENCIL", OP_3DSTATE_WM_DEPTH_STENCIL, F_LEN_VAR, R_RCS, D_BDW, 0, 8, NULL},
+
+	{"3DSTATE_PS_EXTRA", OP_3DSTATE_PS_EXTRA, F_LEN_VAR, R_RCS, D_BDW, 0, 8, NULL},
+
+	{"3DSTATE_RASTER", OP_3DSTATE_RASTER, F_LEN_VAR, R_RCS, D_BDW, 0, 8, NULL},
+
+	{"3DSTATE_SBE_SWIZ", OP_3DSTATE_SBE_SWIZ, F_LEN_VAR, R_RCS, D_BDW, 0, 8, NULL},
+
+	{"3DSTATE_WM_HZ_OP", OP_3DSTATE_WM_HZ_OP, F_LEN_VAR, R_RCS, D_BDW, 0, 8, NULL},
+
+	{"3DSTATE_SAMPLER_STATE_POINTERS", OP_3DSTATE_SAMPLER_STATE_POINTERS,
+		F_LEN_VAR, R_RCS, D_SNB, 0, 8, NULL},
+
+	{"3DSTATE_URB", OP_3DSTATE_URB, F_LEN_VAR, R_RCS, D_SNB, 0, 8, NULL},
+
+	{"3DSTATE_VERTEX_BUFFERS", OP_3DSTATE_VERTEX_BUFFERS, F_LEN_VAR, R_RCS,
+		D_PRE_BDW, 0, 8, vgt_cmd_handler_3dstate_vertex_buffers},
+
+	{"3DSTATE_VERTEX_BUFFERS", OP_3DSTATE_VERTEX_BUFFERS, F_LEN_VAR, R_RCS,
+		D_BDW, 0, 8, vgt_cmd_handler_3dstate_vertex_buffers_bdw},
+
+	{"3DSTATE_VERTEX_ELEMENTS", OP_3DSTATE_VERTEX_ELEMENTS, F_LEN_VAR, R_RCS,
+		D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_INDEX_BUFFER", OP_3DSTATE_INDEX_BUFFER, F_LEN_VAR, R_RCS,
+		D_PRE_BDW, 0, 8, vgt_cmd_handler_3dstate_index_buffer},
+
+	{"3DSTATE_INDEX_BUFFER", OP_3DSTATE_INDEX_BUFFER, F_LEN_VAR, R_RCS,
+		D_BDW, ADDR_FIX_1(2), 8, NULL},
+
+	{"3DSTATE_VF_STATISTICS", OP_3DSTATE_VF_STATISTICS, F_LEN_CONST,
+		R_RCS, D_ALL, 0, 1, NULL},
+
+	{"3DSTATE_VF", OP_3DSTATE_VF, F_LEN_VAR, R_RCS, D_GEN75PLUS, 0, 8, NULL},
+
+	{"3DSTATE_VIEWPORT_STATE_POINTERS", OP_3DSTATE_VIEWPORT_STATE_POINTERS,
+		F_LEN_VAR, R_RCS, D_SNB, 0, 8, NULL},
+
+	{"3DSTATE_CC_STATE_POINTERS", OP_3DSTATE_CC_STATE_POINTERS, F_LEN_VAR,
+		R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_SCISSOR_STATE_POINTERS", OP_3DSTATE_SCISSOR_STATE_POINTERS,
+		F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_GS", OP_3DSTATE_GS, F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_CLIP", OP_3DSTATE_CLIP, F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_WM", OP_3DSTATE_WM, F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_CONSTANT_GS", OP_3DSTATE_CONSTANT_GS, F_LEN_VAR, R_RCS,
+		D_PRE_BDW, 0, 8, vgt_cmd_handler_3dstate_constant_hsw},
+
+	{"3DSTATE_CONSTANT_GS", OP_3DSTATE_CONSTANT_GS, F_LEN_VAR, R_RCS,
+		D_BDW, 0, 8, vgt_cmd_handler_3dstate_constant_bdw},
+
+	{"3DSTATE_CONSTANT_PS", OP_3DSTATE_CONSTANT_PS, F_LEN_VAR, R_RCS,
+		D_PRE_BDW, 0, 8, vgt_cmd_handler_3dstate_constant_hsw},
+
+	{"3DSTATE_CONSTANT_PS", OP_3DSTATE_CONSTANT_PS, F_LEN_VAR, R_RCS,
+		D_BDW, 0, 8, vgt_cmd_handler_3dstate_constant_bdw},
+
+	{"3DSTATE_SAMPLE_MASK", OP_3DSTATE_SAMPLE_MASK, F_LEN_VAR, R_RCS,
+		D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_CONSTANT_HS", OP_3DSTATE_CONSTANT_HS, F_LEN_VAR, R_RCS,
+		D_IVB|D_HSW, 0, 8, vgt_cmd_handler_3dstate_constant_hsw},
+
+	{"3DSTATE_CONSTANT_HS", OP_3DSTATE_CONSTANT_HS, F_LEN_VAR, R_RCS,
+		D_BDW, 0, 8, vgt_cmd_handler_3dstate_constant_bdw},
+
+	{"3DSTATE_CONSTANT_DS", OP_3DSTATE_CONSTANT_DS, F_LEN_VAR, R_RCS,
+		D_IVB|D_HSW, 0, 8, vgt_cmd_handler_3dstate_constant_hsw},
+
+	{"3DSTATE_CONSTANT_DS", OP_3DSTATE_CONSTANT_DS, F_LEN_VAR, R_RCS,
+		D_BDW, 0, 8, vgt_cmd_handler_3dstate_constant_bdw},
+
+	{"3DSTATE_HS", OP_3DSTATE_HS, F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_TE", OP_3DSTATE_TE, F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_DS", OP_3DSTATE_DS, F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_STREAMOUT", OP_3DSTATE_STREAMOUT, F_LEN_VAR, R_RCS,
+		D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_SBE", OP_3DSTATE_SBE, F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_PS", OP_3DSTATE_PS, F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_DRAWING_RECTANGLE", OP_3DSTATE_DRAWING_RECTANGLE, F_LEN_VAR,
+		R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_SAMPLER_PALETTE_LOAD0", OP_3DSTATE_SAMPLER_PALETTE_LOAD0,
+		F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_CHROMA_KEY", OP_3DSTATE_CHROMA_KEY, F_LEN_VAR, R_RCS, D_ALL,
+		0, 8, NULL},
+
+	{"3DSTATE_DEPTH_BUFFER", OP_SNB_3DSTATE_DEPTH_BUFFER, F_LEN_VAR, R_RCS,
+		D_SNB, ADDR_FIX_1(2), 8, NULL},
+
+	{"3DSTATE_DEPTH_BUFFER", OP_3DSTATE_DEPTH_BUFFER, F_LEN_VAR, R_RCS,
+		D_GEN7PLUS, ADDR_FIX_1(2), 8, NULL},
+
+	{"3DSTATE_POLY_STIPPLE_OFFSET", OP_3DSTATE_POLY_STIPPLE_OFFSET,
+		F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_POLY_STIPPLE_PATTERN", OP_3DSTATE_POLY_STIPPLE_PATTERN,
+		F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_LINE_STIPPLE", OP_3DSTATE_LINE_STIPPLE, F_LEN_VAR, R_RCS,
+		D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_AA_LINE_PARAMS", OP_3DSTATE_AA_LINE_PARAMS, F_LEN_VAR, R_RCS,
+		D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_GS_SVB_INDEX", OP_3DSTATE_GS_SVB_INDEX, F_LEN_VAR, R_RCS, D_ALL,
+		0, 8, NULL},
+
+	{"3DSTATE_SAMPLER_PALETTE_LOAD1", OP_3DSTATE_SAMPLER_PALETTE_LOAD1,
+		F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_MULTISAMPLE", OP_3DSTATE_MULTISAMPLE, F_LEN_VAR, R_RCS, D_PRE_BDW,
+		0, 8, NULL},
+
+	{"3DSTATE_MULTISAMPLE", OP_3DSTATE_MULTISAMPLE_BDW, F_LEN_VAR, R_RCS, D_BDW,
+		0, 8, NULL},
+
+	{"3DSTATE_RAST_MULTISAMPLE", OP_3DSTATE_RAST_MULTISAMPLE, F_LEN_VAR, R_RCS,
+		D_HSW, 0, 8, NULL},
+
+	{"3DSTATE_STENCIL_BUFFER", OP_SNB_3DSTATE_STENCIL_BUFFER, F_LEN_VAR, R_RCS,
+		D_SNB, ADDR_FIX_1(2), 8, NULL},
+
+	{"3DSTATE_STENCIL_BUFFER", OP_3DSTATE_STENCIL_BUFFER, F_LEN_VAR, R_RCS,
+		D_GEN7PLUS, ADDR_FIX_1(2), 8, NULL},
+
+	{"3DSTATE_HIER_DEPTH_BUFFER", OP_SNB_3DSTATE_HIER_DEPTH_BUFFER, F_LEN_VAR,
+		R_RCS, D_SNB, 0, 8, NULL},
+
+	{"3DSTATE_HIER_DEPTH_BUFFER", OP_3DSTATE_HIER_DEPTH_BUFFER, F_LEN_VAR,
+		R_RCS, D_GEN7PLUS, ADDR_FIX_1(2), 8, NULL},
+
+	{"3DSTATE_CLEAR_PARAMS", OP_SNB_3DSTATE_CLEAR_PARAMS, F_LEN_VAR, R_RCS, D_SNB,
+		0, 8, NULL},
+
+	{"3DSTATE_CLEAR_PARAMS", OP_3DSTATE_CLEAR_PARAMS, F_LEN_VAR,
+		R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_PUSH_CONSTANT_ALLOC_VS", OP_3DSTATE_PUSH_CONSTANT_ALLOC_VS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_PUSH_CONSTANT_ALLOC_HS", OP_3DSTATE_PUSH_CONSTANT_ALLOC_HS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_PUSH_CONSTANT_ALLOC_DS", OP_3DSTATE_PUSH_CONSTANT_ALLOC_DS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_PUSH_CONSTANT_ALLOC_GS", OP_3DSTATE_PUSH_CONSTANT_ALLOC_GS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_PUSH_CONSTANT_ALLOC_PS", OP_3DSTATE_PUSH_CONSTANT_ALLOC_PS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_MONOFILTER_SIZE", OP_3DSTATE_MONOFILTER_SIZE, F_LEN_VAR, R_RCS,
+		D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_SO_DECL_LIST", OP_3DSTATE_SO_DECL_LIST, F_LEN_VAR, R_RCS, D_ALL,
+		0, 9, NULL},
+
+	{"3DSTATE_SO_BUFFER", OP_3DSTATE_SO_BUFFER, F_LEN_VAR, R_RCS, D_IVB|D_HSW,
+		ADDR_FIX_2(2, 3), 8, NULL},
+
+	{"3DSTATE_SO_BUFFER", OP_3DSTATE_SO_BUFFER, F_LEN_VAR, R_RCS, D_BDW,
+		ADDR_FIX_2(2, 4), 8, NULL},
+
+	{"3DSTATE_BINDING_TABLE_POOL_ALLOC", OP_3DSTATE_BINDING_TABLE_POOL_ALLOC,
+		F_LEN_VAR, R_RCS, D_HSW, 0, 8, vgt_cmd_handler_3dstate_binding_table_pool_alloc},
+
+	{"3DSTATE_BINDING_TABLE_POOL_ALLOC", OP_3DSTATE_BINDING_TABLE_POOL_ALLOC,
+		F_LEN_VAR, R_RCS, D_BDW, ADDR_FIX_1(1), 8, NULL},
+
+	{"3DSTATE_GATHER_POOL_ALLOC", OP_3DSTATE_GATHER_POOL_ALLOC,
+		F_LEN_VAR, R_RCS, D_HSW, 0, 8, vgt_cmd_handler_3dstate_gather_pool_alloc},
+
+	{"3DSTATE_GATHER_POOL_ALLOC", OP_3DSTATE_GATHER_POOL_ALLOC,
+		F_LEN_VAR, R_RCS, D_BDW, ADDR_FIX_1(1), 8, NULL},
+
+	{"3DSTATE_DX9_CONSTANT_BUFFER_POOL_ALLOC", OP_3DSTATE_DX9_CONSTANT_BUFFER_POOL_ALLOC,
+		F_LEN_VAR, R_RCS, D_HSW, 0, 8, vgt_cmd_handler_3dstate_dx9_constant_buffer_pool_alloc},
+
+	{"3DSTATE_DX9_CONSTANT_BUFFER_POOL_ALLOC", OP_3DSTATE_DX9_CONSTANT_BUFFER_POOL_ALLOC,
+		F_LEN_VAR, R_RCS, D_BDW, ADDR_FIX_1(1), 8, NULL},
+
+	{"3DSTATE_SAMPLE_PATTERN", OP_3DSTATE_SAMPLE_PATTERN, F_LEN_VAR, R_RCS, D_BDW, 0, 8, NULL},
+
+	{"PIPE_CONTROL", OP_PIPE_CONTROL, F_LEN_VAR, R_RCS, D_ALL,
+		ADDR_FIX_1(2), 8, vgt_cmd_handler_pipe_control},
+
+	{"3DPRIMITIVE", OP_3DPRIMITIVE, F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"PIPELINE_SELECT", OP_PIPELINE_SELECT, F_LEN_CONST, R_RCS, D_ALL, 0, 1, NULL},
+
+	{"STATE_PREFETCH", OP_STATE_PREFETCH, F_LEN_VAR, R_RCS, D_ALL,
+		ADDR_FIX_1(1), 8, NULL},
+
+	{"STATE_SIP", OP_STATE_SIP, F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"STATE_BASE_ADDRESS", OP_STATE_BASE_ADDRESS, F_LEN_VAR, R_RCS, D_PRE_BDW,
+		0, 8, vgt_cmd_handler_state_base_address},
+
+	{"STATE_BASE_ADDRESS", OP_STATE_BASE_ADDRESS, F_LEN_VAR, R_RCS, D_BDW,
+		ADDR_FIX_5(1, 3, 4, 5, 6), 8, NULL},
+
+	{"OP_3D_MEDIA_0_1_4", OP_3D_MEDIA_0_1_4, F_LEN_VAR, R_RCS, D_HSW_PLUS,
+		ADDR_FIX_1(1), 8, NULL},
+
+	{"3DSTATE_VS", OP_3DSTATE_VS, F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_SF", OP_3DSTATE_SF, F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_CONSTANT_VS", OP_3DSTATE_CONSTANT_VS, F_LEN_VAR, R_RCS, D_PRE_BDW,
+		0, 8, vgt_cmd_handler_3dstate_constant_hsw},
+
+	{"3DSTATE_CONSTANT_VS", OP_3DSTATE_CONSTANT_VS, F_LEN_VAR, R_RCS, D_BDW,
+		0, 8, vgt_cmd_handler_3dstate_constant_bdw},
+
+	{"MEDIA_INTERFACE_DESCRIPTOR_LOAD", OP_MEDIA_INTERFACE_DESCRIPTOR_LOAD,
+		F_LEN_VAR, R_RCS, D_ALL, 0, 16, NULL},
+
+	{"MEDIA_GATEWAY_STATE", OP_MEDIA_GATEWAY_STATE, F_LEN_VAR, R_RCS, D_ALL,
+		0, 16, NULL},
+
+	{"MEDIA_STATE_FLUSH", OP_MEDIA_STATE_FLUSH, F_LEN_VAR, R_RCS, D_ALL,
+		0, 16, NULL},
+
+	{"MEDIA_OBJECT", OP_MEDIA_OBJECT, F_LEN_VAR, R_RCS, D_ALL, 0, 16, NULL},
+
+	{"MEDIA_CURBE_LOAD", OP_MEDIA_CURBE_LOAD, F_LEN_VAR, R_RCS, D_ALL,
+		0, 16, NULL},
+
+	{"MEDIA_OBJECT_PRT", OP_MEDIA_OBJECT_PRT, F_LEN_VAR, R_RCS, D_ALL,
+		0, 16, NULL},
+
+	{"MEDIA_OBJECT_WALKER", OP_MEDIA_OBJECT_WALKER, F_LEN_VAR, R_RCS, D_ALL,
+		0, 16, NULL},
+
+	{"GPGPU_WALKER", OP_GPGPU_WALKER, F_LEN_VAR, R_RCS, D_ALL,
+		0, 8, NULL},
+
+	{"MEDIA_VFE_STATE", OP_MEDIA_VFE_STATE, F_LEN_VAR, R_RCS, D_ALL, 0, 16, NULL},
+
+	{"3DSTATE_VF_STATISTICS_GM45", OP_3DSTATE_VF_STATISTICS_GM45, F_LEN_CONST,
+		R_ALL, D_ALL, 0, 1, NULL},
+
+	{"MFX_PIPE_MODE_SELECT", OP_MFX_PIPE_MODE_SELECT, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_SURFACE_STATE", OP_MFX_SURFACE_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_PIPE_BUF_ADDR_STATE", OP_MFX_PIPE_BUF_ADDR_STATE, F_LEN_VAR,
+		R_VCS, D_PRE_BDW, 0, 12, vgt_cmd_handler_mfx_pipe_buf_addr_state_hsw},
+
+	{"MFX_PIPE_BUF_ADDR_STATE", OP_MFX_PIPE_BUF_ADDR_STATE, F_LEN_VAR,
+		R_VCS, D_BDW, 0, 12, vgt_cmd_handler_mfx_pipe_buf_addr_state_bdw},
+
+	{"MFX_IND_OBJ_BASE_ADDR_STATE", OP_MFX_IND_OBJ_BASE_ADDR_STATE, F_LEN_VAR,
+		R_VCS, D_PRE_BDW, 0, 12, vgt_cmd_handler_mfx_ind_obj_base_addr_state_hsw},
+
+	{"MFX_IND_OBJ_BASE_ADDR_STATE", OP_MFX_IND_OBJ_BASE_ADDR_STATE, F_LEN_VAR,
+		R_VCS, D_BDW, 0, 12, vgt_cmd_handler_mfx_ind_obj_base_addr_state_bdw},
+
+	{"MFX_BSP_BUF_BASE_ADDR_STATE", OP_MFX_BSP_BUF_BASE_ADDR_STATE, F_LEN_VAR,
+		R_VCS, D_PRE_BDW, ADDR_FIX_3(1, 2, 3), 12, NULL},
+
+	{"MFX_BSP_BUF_BASE_ADDR_STATE", OP_MFX_BSP_BUF_BASE_ADDR_STATE, F_LEN_VAR,
+		R_VCS, D_BDW, ADDR_FIX_3(1, 3, 5), 12, NULL},
+
+	{"OP_2_0_0_5", OP_2_0_0_5, F_LEN_VAR,
+		R_VCS, D_PRE_BDW, ADDR_FIX_1(6), 12, NULL},
+
+	{"OP_2_0_0_5", OP_2_0_0_5, F_LEN_VAR, R_VCS, D_BDW, 0, 12, NULL},
+
+	{"MFX_STATE_POINTER", OP_MFX_STATE_POINTER, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_QM_STATE", OP_MFX_QM_STATE, F_LEN_VAR,
+		R_VCS, D_GEN7PLUS, 0, 12, NULL},
+
+	{"MFX_FQM_STATE", OP_MFX_FQM_STATE, F_LEN_VAR,
+		R_VCS, D_GEN7PLUS, 0, 12, NULL},
+
+	{"MFX_PAK_INSERT_OBJECT", OP_MFX_PAK_INSERT_OBJECT, F_LEN_VAR,
+		R_VCS, D_GEN7PLUS, 0, 12, NULL},
+
+	{"MFX_STITCH_OBJECT", OP_MFX_STITCH_OBJECT, F_LEN_VAR,
+		R_VCS, D_GEN7PLUS, 0, 12, NULL},
+
+	{"MFD_IT_OBJECT", OP_MFD_IT_OBJECT, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_WAIT", OP_MFX_WAIT, F_LEN_VAR,
+		R_VCS, D_GEN7PLUS, 0, 6, NULL},
+
+	{"MFX_AVC_IMG_STATE", OP_MFX_AVC_IMG_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_AVC_QM_STATE", OP_MFX_AVC_QM_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	/* to check: is "Direct MV Buffer Base Address" GMA ? */
+	{"MFX_AVC_DIRECTMODE_STATE", OP_MFX_AVC_DIRECTMODE_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_AVC_SLICE_STATE", OP_MFX_AVC_SLICE_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_AVC_REF_IDX_STATE", OP_MFX_AVC_REF_IDX_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_AVC_WEIGHTOFFSET_STATE", OP_MFX_AVC_WEIGHTOFFSET_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFD_AVC_PICID_STATE", OP_MFD_AVC_PICID_STATE, F_LEN_VAR,
+		R_VCS, D_GEN75PLUS, 0, 12, NULL},
+	{"MFD_AVC_DPB_STATE", OP_MFD_AVC_DPB_STATE, F_LEN_VAR,
+		R_VCS, D_IVB_PLUS, 0, 12, NULL},
+
+	{"MFD_AVC_BSD_OBJECT", OP_MFD_AVC_BSD_OBJECT, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFD_AVC_SLICEADDR", OP_MFD_AVC_SLICEADDR, F_LEN_VAR,
+		R_VCS, D_IVB_PLUS, ADDR_FIX_1(2), 12, NULL},
+
+	{"MFC_AVC_FQM_STATE", OP_MFC_AVC_FQM_STATE, F_LEN_VAR,
+		R_VCS, D_SNB, 0, 12, NULL},
+
+	{"MFC_AVC_PAK_INSERT_OBJECT", OP_MFC_AVC_PAK_INSERT_OBJECT, F_LEN_VAR,
+		R_VCS, D_SNB, 0, 12, NULL},
+
+	{"MFC_AVC_PAK_OBJECT", OP_MFC_AVC_PAK_OBJECT, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_VC1_PIC_STATE", OP_MFX_VC1_PIC_STATE, F_LEN_VAR,
+		R_VCS, D_SNB, 0, 12, NULL},
+
+	{"MFX_VC1_PRED_PIPE_STATE", OP_MFX_VC1_PRED_PIPE_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_VC1_DIRECTMODE_STATE", OP_MFX_VC1_DIRECTMODE_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFD_VC1_SHORT_PIC_STATE", OP_MFD_VC1_SHORT_PIC_STATE, F_LEN_VAR,
+		R_VCS, D_GEN7PLUS, 0, 12, NULL},
+
+	{"MFD_VC1_LONG_PIC_STATE", OP_MFD_VC1_LONG_PIC_STATE, F_LEN_VAR,
+		R_VCS, D_GEN7PLUS, 0, 12, NULL},
+
+	{"MFD_VC1_BSD_OBJECT", OP_MFD_VC1_BSD_OBJECT, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFC_MPEG2_SLICEGROUP_STATE", OP_MFC_MPEG2_SLICEGROUP_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFC_MPEG2_PAK_OBJECT", OP_MFC_MPEG2_PAK_OBJECT, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_MPEG2_PIC_STATE", OP_MFX_MPEG2_PIC_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_MPEG2_QM_STATE", OP_MFX_MPEG2_QM_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFD_MPEG2_BSD_OBJECT", OP_MFD_MPEG2_BSD_OBJECT, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_2_6_0_0", OP_MFX_2_6_0_0, F_LEN_VAR, R_VCS, D_ALL,
+		0, 16, vgt_cmd_handler_mfx_2_6_0_0},
+
+	{"MFX_2_6_0_9", OP_MFX_2_6_0_9, F_LEN_VAR, R_VCS, D_ALL, 0, 16, NULL},
+
+	{"MFX_2_6_0_8", OP_MFX_2_6_0_8, F_LEN_VAR, R_VCS, D_ALL, 0, 16, NULL},
+
+	{"MFX_JPEG_PIC_STATE", OP_MFX_JPEG_PIC_STATE, F_LEN_VAR,
+		R_VCS, D_GEN7PLUS, 0, 12, NULL},
+
+	{"MFX_JPEG_HUFF_TABLE_STATE", OP_MFX_JPEG_HUFF_TABLE_STATE, F_LEN_VAR,
+		R_VCS, D_GEN7PLUS, 0, 12, NULL},
+
+	{"MFD_JPEG_BSD_OBJECT", OP_MFD_JPEG_BSD_OBJECT, F_LEN_VAR,
+		R_VCS, D_GEN7PLUS, 0, 12, NULL},
+
+	{"VEBOX_STATE", OP_VEB_STATE, F_LEN_VAR, R_VECS, D_GEN75PLUS, 0, 12, NULL},
+
+	{"VEBOX_SURFACE_STATE", OP_VEB_SURFACE_STATE, F_LEN_VAR, R_VECS, D_HSW_PLUS, 0, 12, NULL},
+
+	{"VEB_DI_IECP", OP_VEB_DNDI_IECP_STATE, F_LEN_VAR, R_VECS, D_HSW, 0, 12, NULL},
+
+	{"VEB_DI_IECP", OP_VEB_DNDI_IECP_STATE, F_LEN_VAR, R_VECS, D_BDW_PLUS, 0, 20, NULL},
+};
+
+static int cmd_hash_init(struct pgt_device *pdev)
+{
+	int i;
+	struct vgt_cmd_entry *e;
+	struct cmd_info	*info;
+	unsigned int gen_type;
+
+	gen_type = vgt_gen_dev_type(pdev);
+
+	for (i = 0; i < ARRAY_SIZE(cmd_info); i++) {
+		if (!(cmd_info[i].devices & gen_type)) {
+			vgt_dbg(VGT_DBG_CMD, "CMD[%-30s] op[%04x] flag[%x] devs[%02x] rings[%02x] not registered\n",
+					cmd_info[i].name, cmd_info[i].opcode, cmd_info[i].flag,
+					cmd_info[i].devices, cmd_info[i].rings);
+			continue;
+		}
+
+		e = kmalloc(sizeof(*e), GFP_KERNEL);
+		if (e == NULL) {
+			vgt_err("Insufficient memory in %s\n", __FUNCTION__);
+			return -ENOMEM;
+		}
+		e->info = &cmd_info[i];
+
+		info = vgt_find_cmd_entry_any_ring(e->info->opcode, e->info->rings);
+		if (info) {
+			vgt_err("%s %s duplicated\n", e->info->name, info->name);
+			return -EINVAL;
+		}
+
+		INIT_HLIST_NODE(&e->hlist);
+		vgt_add_cmd_entry(e);
+		vgt_dbg(VGT_DBG_CMD, "CMD[%-30s] op[%04x] flag[%x] devs[%02x] rings[%02x] registered\n",
+				e->info->name,e->info->opcode, e->info->flag, e->info->devices,
+				e->info->rings);
+	}
+	return 0;
+}
+
+static void trace_cs_command(struct parser_exec_state *s)
+{
+	/* This buffer is used by ftrace to store all commands copied from guest gma
+	* space. Sometimes commands can cross pages, this should not be handled in
+	 * ftrace logic. So this is just used as a 'bounce buffer' */
+	u32 cmd_trace_buf[VGT_MAX_CMD_LENGTH];
+	int i;
+	u32 cmd_len = cmd_length(s);
+	/* The chosen value of VGT_MAX_CMD_LENGTH are just based on
+	 * following two considerations:
+	 * 1) From observation, most common ring commands is not that long.
+	 *    But there are execeptions. So it indeed makes sence to observe
+	 *    longer commands.
+	 * 2) From the performance and debugging point of view, dumping all
+	 *    contents of very commands is not necessary.
+	 * We mgith shrink VGT_MAX_CMD_LENGTH or remove this trace event in
+	 * future for performance considerations.
+	 */
+	if (unlikely(cmd_len > VGT_MAX_CMD_LENGTH)) {
+		vgt_dbg(VGT_DBG_CMD, "cmd length exceed tracing limitation!\n");
+		cmd_len = VGT_MAX_CMD_LENGTH;
+	}
+
+	for (i = 0; i < cmd_len; i++)
+		cmd_trace_buf[i] = cmd_val(s, i);
+
+	trace_vgt_command(s->vgt->vm_id, s->ring_id, s->ip_gma, cmd_trace_buf,
+			cmd_len, s->buf_type == RING_BUFFER_INSTRUCTION);
+
+}
+
+/* call the cmd handler, and advance ip */
+static int vgt_cmd_parser_exec(struct parser_exec_state *s)
+{
+	struct cmd_info *info;
+	uint32_t cmd;
+	int rc = 0;
+
+	hypervisor_read_va(s->vgt, s->ip_va, &cmd, sizeof(cmd), 1);
+
+	info = vgt_get_cmd_info(cmd, s->ring_id);
+	if (info == NULL) {
+		vgt_err("ERROR: unknown cmd 0x%x, opcode=0x%x\n", cmd,
+				vgt_get_opcode(cmd, s->ring_id));
+		parser_exec_state_dump(s);
+		klog_printk("ERROR: unknown cmd %x, ring%d[%lx, %lx] gma[%lx] va[%p]\n",
+				cmd, s->ring_id, s->ring_start,
+				s->ring_start + s->ring_size, s->ip_gma, s->ip_va);
+
+		return -EFAULT;
+	}
+
+	s->info = info;
+
+	vgt_cmd_addr_audit_with_bitmap(s, info->addr_bitmap);
+
+	/* Let's keep this logic here. Someone has special needs for dumping
+	 * commands can customize this code snippet.
+	 */
+#if 0
+	klog_printk("%s ip(%08lx): ",
+			s->buf_type == RING_BUFFER_INSTRUCTION ?
+			"RB" : "BB",
+			s->ip_gma);
+	for (i = 0; i < cmd_length(s); i++) {
+		klog_printk("%08x ", cmd_val(s, i));
+	}
+	klog_printk("\n");
+#endif
+	trace_cs_command(s);
+
+	if (info->handler) {
+		int post_handle = 0;
+
+		if (info->flag & F_POST_HANDLE) {
+			post_handle = 1;
+
+			/* Post handle special case.*/
+			/*
+			 * OP_MI_NOOP: only handles nooped MI_DISPLAY_FILP
+			 * to prevent the heavy usage of patch list.
+			 */
+			if (info->opcode == OP_MI_NOOP && cmd_length(s) == 1)
+				post_handle = 0;
+		}
+
+		if (!post_handle)
+			rc = info->handler(s);
+		else
+			rc = add_post_handle_entry(s, info->handler);
+
+		if (rc < 0) {
+			vgt_err("%s handler error", info->name);
+			return rc;
+		}
+	}
+
+	if (!(info->flag & F_IP_ADVANCE_CUSTOM)) {
+		rc = vgt_cmd_advance_default(s);
+		if (rc < 0) {
+			vgt_err("%s IP advance error", info->name);
+			return rc;
+		}
+	}
+
+	return rc;
+}
+
+static inline bool gma_out_of_range(unsigned long gma, unsigned long gma_head, unsigned gma_tail)
+{
+	if ( gma_tail >= gma_head)
+		return (gma < gma_head) || (gma > gma_tail);
+	else
+		return (gma > gma_tail) && (gma < gma_head);
+
+}
+
+#define MAX_PARSER_ERROR_NUM	10
+
+static int __vgt_scan_vring(struct vgt_device *vgt, int ring_id, vgt_reg_t head, vgt_reg_t tail, vgt_reg_t base, vgt_reg_t size)
+{
+	unsigned long gma_head, gma_tail, gma_bottom;
+	struct parser_exec_state s;
+	int rc=0;
+	uint64_t cmd_nr = 0;
+	vgt_state_ring_t *rs = &vgt->rb[ring_id];
+	u32 cmd_flags = 0;
+	unsigned long ip_gma = 0;
+
+	/* ring base is page aligned */
+	ASSERT((base & (PAGE_SIZE-1)) == 0);
+
+	gma_head = base + head;
+	gma_tail = base + tail;
+	gma_bottom = base + size;
+
+	s.buf_type = RING_BUFFER_INSTRUCTION;
+	s.buf_addr_type = GTT_BUFFER;
+	s.vgt = vgt;
+	s.ring_id = ring_id;
+	s.ring_start = base;
+	s.ring_size = size;
+	s.ring_head = gma_head;
+	s.ring_tail = gma_tail;
+
+	s.request_id = rs->request_id;
+
+	if (bypass_scan_mask & (1 << ring_id)) {
+		add_tail_entry(&s, tail, 100, 0, 0);
+		return 0;
+	}
+
+	rc = ip_gma_set(&s, base + head);
+	if (rc < 0)
+		return rc;
+
+	klog_printk("ring buffer scan start on ring %d\n", ring_id);
+	vgt_dbg(VGT_DBG_CMD, "scan_start: start=%lx end=%lx\n", gma_head, gma_tail);
+	while(s.ip_gma != gma_tail){
+		s.cmd_issue_irq = false;
+		if (s.buf_type == RING_BUFFER_INSTRUCTION){
+			ASSERT((s.ip_gma >= base) && (s.ip_gma < gma_bottom));
+			if (gma_out_of_range(s.ip_gma, gma_head, gma_tail)) {
+				vgt_err("ERROR: ip_gma %lx out of range."
+					"(base:0x%x, head: 0x%x, tail: 0x%x)\n",
+					s.ip_gma, base, head, tail);
+				break;
+			}
+		}
+
+		cmd_nr++;
+
+		rc = vgt_cmd_parser_exec(&s);
+		if (rc < 0) {
+			vgt_err("cmd parser error\n");
+			break;
+		}
+
+		if (irq_based_ctx_switch &&
+			(s.buf_type == RING_BUFFER_INSTRUCTION)) {
+		/* record the status of irq instruction in ring buffer */
+			if (s.cmd_issue_irq) {
+				cmd_flags |= F_CMDS_ISSUE_IRQ;
+				ip_gma = s.ip_gma;
+			}
+		}
+	}
+
+	if (!rc) {
+		/*
+		 * Set flag to indicate the command buffer is end with user interrupt,
+		 * and save the instruction's offset in ring buffer.
+		 */
+		if (cmd_flags & F_CMDS_ISSUE_IRQ)
+			ip_gma = ip_gma - base;
+		add_tail_entry(&s, tail, cmd_nr, cmd_flags, ip_gma);
+		rs->cmd_nr++;
+	}
+
+	klog_printk("ring buffer scan end on ring %d\n", ring_id);
+	vgt_dbg(VGT_DBG_CMD, "scan_end\n");
+	return rc;
+}
+
+/*
+ * Scan the guest ring.
+ *   Return 0: success
+ *         <0: Address violation.
+ */
+int vgt_scan_vring(struct vgt_device *vgt, int ring_id)
+{
+	vgt_state_ring_t *rs = &vgt->rb[ring_id];
+	vgt_ringbuffer_t *vring = &rs->vring;
+	int ret;
+	cycles_t t0, t1;
+	struct vgt_statistics *stat = &vgt->stat;
+
+	t0 = get_cycles();
+
+	if (!(vring->ctl & _RING_CTL_ENABLE)) {
+		/* Ring is enabled */
+		vgt_dbg(VGT_DBG_CMD, "VGT-Parser.c vring is disabled. head %x tail %x ctl %x\n",
+			vring->head, vring->tail, vring->ctl);
+		return 0;
+	}
+
+	stat->vring_scan_cnt++;
+	rs->request_id++;
+	ret = __vgt_scan_vring(vgt, ring_id, rs->last_scan_head,
+		vring->tail & RB_TAIL_OFF_MASK,
+		vring->start, _RING_CTL_BUF_SIZE(vring->ctl));
+
+	rs->last_scan_head = vring->tail;
+
+	t1 = get_cycles();
+	stat->vring_scan_cycles += t1 - t0;
+	ASSERT_VM(!ret, vgt);
+	return ret;
+}
+
+int vgt_cmd_parser_init(struct pgt_device *pdev)
+{
+	return cmd_hash_init(pdev);
+}
+
+void vgt_cmd_parser_exit(void)
+{
+	vgt_clear_cmd_table();
+}
diff --git a/drivers/gpu/drm/i915/vgt/cmd_parser.h b/drivers/gpu/drm/i915/vgt/cmd_parser.h
new file mode 100644
index 0000000..09c61cb
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/cmd_parser.h
@@ -0,0 +1,525 @@
+/*
+ * cmd_parser.h: core header file for vGT command parser
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#define VGT_UNHANDLEABLE 1
+
+#define INVALID_OP    (~0U)
+
+#define OP_LEN_MI           9
+#define OP_LEN_2D           10
+#define OP_LEN_3D_MEDIA     16
+#define OP_LEN_MFX_VC       16
+#define OP_LEN_VEBOX	    16
+
+#define CMD_TYPE(cmd)	(((cmd) >> 29) & 7)
+
+struct sub_op_bits{
+	int hi;
+	int low;
+};
+struct decode_info{
+	char* name;
+	int op_len;
+	int nr_sub_op;
+	struct sub_op_bits *sub_op;
+};
+
+/* Render Command Map */
+
+/* MI_* command Opcode (28:23) */
+#define OP_MI_NOOP                          0x0
+#define OP_MI_SET_PREDICATE                 0x1  /* HSW+ */
+#define OP_MI_USER_INTERRUPT                0x2
+#define OP_MI_WAIT_FOR_EVENT                0x3
+#define OP_MI_FLUSH                         0x4
+#define OP_MI_ARB_CHECK                     0x5
+#define OP_MI_RS_CONTROL                    0x6  /* HSW+ */
+#define OP_MI_REPORT_HEAD                   0x7
+#define OP_MI_ARB_ON_OFF                    0x8
+#define OP_MI_URB_ATOMIC_ALLOC              0x9  /* HSW+ */
+#define OP_MI_BATCH_BUFFER_END              0xA
+#define OP_MI_SUSPEND_FLUSH                 0xB
+#define OP_MI_PREDICATE                     0xC  /* IVB+ */
+#define OP_MI_TOPOLOGY_FILTER               0xD  /* IVB+ */
+#define OP_MI_SET_APPID                     0xE  /* IVB+ */
+#define OP_MI_RS_CONTEXT                    0xF  /* HSW+ */
+#define OP_MI_LOAD_SCAN_LINES_INCL          0x12 /* HSW+ */
+#define OP_MI_DISPLAY_FLIP                  0x14
+#define OP_MI_SEMAPHORE_MBOX                0x16
+#define OP_MI_SET_CONTEXT                   0x18
+#define OP_MI_MATH                          0x1A
+#define OP_MI_URB_CLEAR                     0x19
+#define OP_MI_SEMAPHORE_SIGNAL		    0x1B  /* BDW+ */
+#define OP_MI_SEMAPHORE_WAIT		    0x1C  /* BDW+ */
+
+#define OP_MI_STORE_DATA_IMM                0x20
+#define OP_MI_STORE_DATA_INDEX              0x21
+#define OP_MI_LOAD_REGISTER_IMM             0x22
+#define OP_MI_UPDATE_GTT                    0x23
+#define OP_MI_STORE_REGISTER_MEM            0x24
+#define OP_MI_FLUSH_DW                      0x26
+#define OP_MI_CLFLUSH                       0x27
+#define OP_MI_REPORT_PERF_COUNT             0x28
+#define OP_MI_LOAD_REGISTER_MEM             0x29  /* HSW+ */
+#define OP_MI_LOAD_REGISTER_REG             0x2A  /* HSW+ */
+#define OP_MI_RS_STORE_DATA_IMM             0x2B  /* HSW+ */
+#define OP_MI_LOAD_URB_MEM                  0x2C  /* HSW+ */
+#define OP_MI_STORE_URM_MEM                 0x2D  /* HSW+ */
+#define OP_MI_2E			    0x2E  /* BDW+ */
+#define OP_MI_2F			    0x2F  /* BDW+ */
+#define OP_MI_BATCH_BUFFER_START            0x31
+
+/* Bit definition for dword 0 */
+#define _CMDBIT_BB_START_IN_PPGTT	(1UL << 8)
+
+#define OP_MI_CONDITIONAL_BATCH_BUFFER_END  0x36
+
+#define BATCH_BUFFER_ADDR_MASK ((1UL << 32) - (1U <<2))
+#define BATCH_BUFFER_ADDR_HIGH_MASK ((1UL << 16) - (1U))
+#define BATCH_BUFFER_ADR_SPACE_BIT(x)	(((x)>>8) & 1U)
+#define BATCH_BUFFER_2ND_LEVEL_BIT(x)   ((x)>>22 & 1U)
+
+/* 2D command: Opcode (28:22) */
+#define OP_2D(x)    ((2<<7) | x)
+
+#define OP_XY_SETUP_BLT                             OP_2D(0x1)
+#define OP_XY_SETUP_CLIP_BLT                        OP_2D(0x3)
+#define OP_XY_SETUP_MONO_PATTERN_SL_BLT             OP_2D(0x11)
+#define OP_XY_PIXEL_BLT                             OP_2D(0x24)
+#define OP_XY_SCANLINES_BLT                         OP_2D(0x25)
+#define OP_XY_TEXT_BLT                              OP_2D(0x26)
+#define OP_XY_TEXT_IMMEDIATE_BLT                    OP_2D(0x31)
+#define OP_COLOR_BLT                                OP_2D(0x40)
+#define OP_SRC_COPY_BLT                             OP_2D(0x43)
+#define OP_XY_COLOR_BLT                             OP_2D(0x50)
+#define OP_XY_PAT_BLT                               OP_2D(0x51)
+#define OP_XY_MONO_PAT_BLT                          OP_2D(0x52)
+#define OP_XY_SRC_COPY_BLT                          OP_2D(0x53)
+#define OP_XY_MONO_SRC_COPY_BLT                     OP_2D(0x54)
+#define OP_XY_FULL_BLT                              OP_2D(0x55)
+#define OP_XY_FULL_MONO_SRC_BLT                     OP_2D(0x56)
+#define OP_XY_FULL_MONO_PATTERN_BLT                 OP_2D(0x57)
+#define OP_XY_FULL_MONO_PATTERN_MONO_SRC_BLT        OP_2D(0x58)
+#define OP_XY_MONO_PAT_FIXED_BLT                    OP_2D(0x59)
+#define OP_XY_MONO_SRC_COPY_IMMEDIATE_BLT           OP_2D(0x71)
+#define OP_XY_PAT_BLT_IMMEDIATE                     OP_2D(0x72)
+#define OP_XY_SRC_COPY_CHROMA_BLT                   OP_2D(0x73)
+#define OP_XY_FULL_IMMEDIATE_PATTERN_BLT            OP_2D(0x74)
+#define OP_XY_FULL_MONO_SRC_IMMEDIATE_PATTERN_BLT   OP_2D(0x75)
+#define OP_XY_PAT_CHROMA_BLT                        OP_2D(0x76)
+#define OP_XY_PAT_CHROMA_BLT_IMMEDIATE              OP_2D(0x77)
+
+/* 3D/Media Command: Pipeline Type(28:27) Opcode(26:24) Sub Opcode(23:16) */
+#define OP_3D_MEDIA(sub_type, opcode, sub_opcode) \
+	( (3<<13) | ((sub_type)<<11) | ((opcode) <<8) | (sub_opcode))
+
+#define OP_STATE_PREFETCH                       OP_3D_MEDIA(0x0, 0x0, 0x03)
+
+#define OP_STATE_BASE_ADDRESS                   OP_3D_MEDIA(0x0, 0x1, 0x01)
+#define OP_STATE_SIP                            OP_3D_MEDIA(0x0, 0x1, 0x02)
+#define OP_3D_MEDIA_0_1_4			OP_3D_MEDIA(0x0, 0x1, 0x04)
+
+#define OP_3DSTATE_VF_STATISTICS_GM45           OP_3D_MEDIA(0x1, 0x0, 0x0B)
+
+#define OP_PIPELINE_SELECT                      OP_3D_MEDIA(0x1, 0x1, 0x04)
+
+#define OP_MEDIA_VFE_STATE                      OP_3D_MEDIA(0x2, 0x0, 0x0)
+#define OP_MEDIA_CURBE_LOAD                     OP_3D_MEDIA(0x2, 0x0, 0x1)
+#define OP_MEDIA_INTERFACE_DESCRIPTOR_LOAD      OP_3D_MEDIA(0x2, 0x0, 0x2)
+#define OP_MEDIA_GATEWAY_STATE                  OP_3D_MEDIA(0x2, 0x0, 0x3)
+#define OP_MEDIA_STATE_FLUSH                    OP_3D_MEDIA(0x2, 0x0, 0x4)
+
+#define OP_MEDIA_OBJECT                         OP_3D_MEDIA(0x2, 0x1, 0x0)
+#define OP_MEDIA_OBJECT_PRT                     OP_3D_MEDIA(0x2, 0x1, 0x2)
+#define OP_MEDIA_OBJECT_WALKER                  OP_3D_MEDIA(0x2, 0x1, 0x3)
+#define OP_GPGPU_WALKER                         OP_3D_MEDIA(0x2, 0x1, 0x5)
+
+#define OP_3DSTATE_BINDING_TABLE_POINTERS       OP_3D_MEDIA(0x3, 0x0, 0x01)
+#define OP_3DSTATE_SAMPLER_STATE_POINTERS       OP_3D_MEDIA(0x3, 0x0, 0x02)
+#define OP_3DSTATE_CLEAR_PARAMS                 OP_3D_MEDIA(0x3, 0x0, 0x04) /* IVB+ */
+#define OP_3DSTATE_DEPTH_BUFFER                 OP_3D_MEDIA(0x3, 0x0, 0x05) /* IVB+ */
+#define OP_3DSTATE_URB                          OP_3D_MEDIA(0x3, 0x0, 0x05) /* SNB  */
+#define OP_3DSTATE_STENCIL_BUFFER               OP_3D_MEDIA(0x3, 0x0, 0x06) /* IVB+ */
+#define OP_3DSTATE_HIER_DEPTH_BUFFER            OP_3D_MEDIA(0x3, 0x0, 0x07) /* IVB+ */
+#define OP_3DSTATE_VERTEX_BUFFERS               OP_3D_MEDIA(0x3, 0x0, 0x08)
+#define OP_3DSTATE_VERTEX_ELEMENTS              OP_3D_MEDIA(0x3, 0x0, 0x09)
+#define OP_3DSTATE_INDEX_BUFFER                 OP_3D_MEDIA(0x3, 0x0, 0x0A)
+#define OP_3DSTATE_VF_STATISTICS                OP_3D_MEDIA(0x3, 0x0, 0x0B)
+#define OP_3DSTATE_VF                           OP_3D_MEDIA(0x3, 0x0, 0x0C)  /* HSW+ */
+#define OP_3DSTATE_VIEWPORT_STATE_POINTERS      OP_3D_MEDIA(0x3, 0x0, 0x0D)
+#define OP_3DSTATE_CC_STATE_POINTERS            OP_3D_MEDIA( 0x3 ,0x0, 0x0E )
+#define OP_3DSTATE_SCISSOR_STATE_POINTERS       OP_3D_MEDIA( 0x3 ,0x0, 0x0F )
+#define OP_3DSTATE_VS                           OP_3D_MEDIA( 0x3 ,0x0, 0x10)
+#define OP_3DSTATE_GS                           OP_3D_MEDIA( 0x3 ,0x0, 0x11 )
+#define OP_3DSTATE_CLIP                         OP_3D_MEDIA( 0x3 ,0x0, 0x12 )
+#define OP_3DSTATE_SF                           OP_3D_MEDIA( 0x3 ,0x0, 0x13)
+#define OP_3DSTATE_WM                           OP_3D_MEDIA( 0x3 ,0x0, 0x14 )
+#define OP_3DSTATE_CONSTANT_VS                  OP_3D_MEDIA( 0x3 ,0x0, 0x15)
+#define OP_3DSTATE_CONSTANT_GS                  OP_3D_MEDIA( 0x3 ,0x0, 0x16 )
+#define OP_3DSTATE_CONSTANT_PS                  OP_3D_MEDIA( 0x3 ,0x0, 0x17 )
+#define OP_3DSTATE_SAMPLE_MASK                  OP_3D_MEDIA( 0x3 ,0x0, 0x18 )
+#define OP_3DSTATE_CONSTANT_HS                  OP_3D_MEDIA( 0x3 ,0x0, 0x19 ) /* IVB+ */
+#define OP_3DSTATE_CONSTANT_DS                  OP_3D_MEDIA( 0x3 ,0x0, 0x1A ) /* IVB+ */
+#define OP_3DSTATE_HS                           OP_3D_MEDIA( 0x3 ,0x0, 0x1B ) /* IVB+ */
+#define OP_3DSTATE_TE                           OP_3D_MEDIA( 0x3 ,0x0, 0x1C ) /* IVB+ */
+#define OP_3DSTATE_DS                           OP_3D_MEDIA( 0x3 ,0x0, 0x1D ) /* IVB+ */
+#define OP_3DSTATE_STREAMOUT                    OP_3D_MEDIA( 0x3 ,0x0, 0x1E ) /* IVB+ */
+#define OP_3DSTATE_SBE                          OP_3D_MEDIA( 0x3 ,0x0, 0x1F ) /* IVB+ */
+#define OP_3DSTATE_PS                           OP_3D_MEDIA( 0x3 ,0x0, 0x20 ) /* IVB+ */
+#define OP_3DSTATE_VIEWPORT_STATE_POINTERS_SF_CLIP OP_3D_MEDIA(0x3, 0x0, 0x21) /* IVB+ */
+#define OP_3DSTATE_VIEWPORT_STATE_POINTERS_CC   OP_3D_MEDIA(0x3, 0x0, 0x23) /* IVB+ */
+#define OP_3DSTATE_BLEND_STATE_POINTERS         OP_3D_MEDIA(0x3, 0x0, 0x24) /* IVB+ */
+#define OP_3DSTATE_DEPTH_STENCIL_STATE_POINTERS OP_3D_MEDIA(0x3, 0x0, 0x25) /* IVB+ */
+#define OP_3DSTATE_BINDING_TABLE_POINTERS_VS    OP_3D_MEDIA(0x3, 0x0, 0x26) /* IVB+ */
+#define OP_3DSTATE_BINDING_TABLE_POINTERS_HS    OP_3D_MEDIA(0x3, 0x0, 0x27) /* IVB+ */
+#define OP_3DSTATE_BINDING_TABLE_POINTERS_DS    OP_3D_MEDIA(0x3, 0x0, 0x28) /* IVB+ */
+#define OP_3DSTATE_BINDING_TABLE_POINTERS_GS    OP_3D_MEDIA(0x3, 0x0, 0x29) /* IVB+ */
+#define OP_3DSTATE_BINDING_TABLE_POINTERS_PS    OP_3D_MEDIA(0x3, 0x0, 0x2A) /* IVB+ */
+#define OP_3DSTATE_SAMPLER_STATE_POINTERS_VS    OP_3D_MEDIA(0x3, 0x0, 0x2B) /* IVB+ */
+#define OP_3DSTATE_SAMPLER_STATE_POINTERS_HS    OP_3D_MEDIA(0x3, 0x0, 0x2C) /* IVB+ */
+#define OP_3DSTATE_SAMPLER_STATE_POINTERS_DS    OP_3D_MEDIA(0x3, 0x0, 0x2D) /* IVB+ */
+#define OP_3DSTATE_SAMPLER_STATE_POINTERS_GS    OP_3D_MEDIA(0x3, 0x0, 0x2E) /* IVB+ */
+#define OP_3DSTATE_SAMPLER_STATE_POINTERS_PS    OP_3D_MEDIA(0x3, 0x0, 0x2F) /* IVB+ */
+#define OP_3DSTATE_URB_VS                       OP_3D_MEDIA(0x3, 0x0, 0x30) /* IVB+ */
+#define OP_3DSTATE_URB_HS                       OP_3D_MEDIA(0x3, 0x0, 0x31) /* IVB+ */
+#define OP_3DSTATE_URB_DS                       OP_3D_MEDIA(0x3, 0x0, 0x32) /* IVB+ */
+#define OP_3DSTATE_URB_GS                       OP_3D_MEDIA(0x3, 0x0, 0x33) /* IVB+ */
+#define OP_3DSTATE_GATHER_CONSTANT_VS           OP_3D_MEDIA(0x3, 0x0, 0x34) /* HSW+ */
+#define OP_3DSTATE_GATHER_CONSTANT_GS           OP_3D_MEDIA(0x3, 0x0, 0x35) /* HSW+ */
+#define OP_3DSTATE_GATHER_CONSTANT_HS           OP_3D_MEDIA(0x3, 0x0, 0x36) /* HSW+ */
+#define OP_3DSTATE_GATHER_CONSTANT_DS           OP_3D_MEDIA(0x3, 0x0, 0x37) /* HSW+ */
+#define OP_3DSTATE_GATHER_CONSTANT_PS           OP_3D_MEDIA(0x3, 0x0, 0x38) /* HSW+ */
+#define OP_3DSTATE_DX9_CONSTANTF_VS             OP_3D_MEDIA(0x3, 0x0, 0x39) /* HSW+ */
+#define OP_3DSTATE_DX9_CONSTANTF_PS             OP_3D_MEDIA(0x3, 0x0, 0x3A) /* HSW+ */
+#define OP_3DSTATE_DX9_CONSTANTI_VS             OP_3D_MEDIA(0x3, 0x0, 0x3B) /* HSW+ */
+#define OP_3DSTATE_DX9_CONSTANTI_PS             OP_3D_MEDIA(0x3, 0x0, 0x3C) /* HSW+ */
+#define OP_3DSTATE_DX9_CONSTANTB_VS             OP_3D_MEDIA(0x3, 0x0, 0x3D) /* HSW+ */
+#define OP_3DSTATE_DX9_CONSTANTB_PS             OP_3D_MEDIA(0x3, 0x0, 0x3E) /* HSW+ */
+#define OP_3DSTATE_DX9_LOCAL_VALID_VS           OP_3D_MEDIA(0x3, 0x0, 0x3F) /* HSW+ */
+#define OP_3DSTATE_DX9_LOCAL_VALID_PS           OP_3D_MEDIA(0x3, 0x0, 0x40) /* HSW+ */
+#define OP_3DSTATE_DX9_GENERATE_ACTIVE_VS       OP_3D_MEDIA(0x3, 0x0, 0x41) /* HSW+ */
+#define OP_3DSTATE_DX9_GENERATE_ACTIVE_PS       OP_3D_MEDIA(0x3, 0x0, 0x42) /* HSW+ */
+#define OP_3DSTATE_BINDING_TABLE_EDIT_VS        OP_3D_MEDIA(0x3, 0x0, 0x43) /* HSW+ */
+#define OP_3DSTATE_BINDING_TABLE_EDIT_GS        OP_3D_MEDIA(0x3, 0x0, 0x44) /* HSW+ */
+#define OP_3DSTATE_BINDING_TABLE_EDIT_HS        OP_3D_MEDIA(0x3, 0x0, 0x45) /* HSW+ */
+#define OP_3DSTATE_BINDING_TABLE_EDIT_DS        OP_3D_MEDIA(0x3, 0x0, 0x46) /* HSW+ */
+#define OP_3DSTATE_BINDING_TABLE_EDIT_PS        OP_3D_MEDIA(0x3, 0x0, 0x47) /* HSW+ */
+
+#define OP_3DSTATE_VF_INSTANCING 		OP_3D_MEDIA(0x3, 0x0, 0x49) /* BDW+ */
+#define OP_3DSTATE_VF_SGVS  			OP_3D_MEDIA(0x3, 0x0, 0x4A) /* BDW+ */
+#define OP_3DSTATE_VF_TOPOLOGY   		OP_3D_MEDIA(0x3, 0x0, 0x4B) /* BDW+ */
+#define OP_3DSTATE_WM_CHROMAKEY   		OP_3D_MEDIA(0x3, 0x0, 0x4C) /* BDW+ */
+#define OP_3DSTATE_PS_BLEND   			OP_3D_MEDIA(0x3, 0x0, 0x4D) /* BDW+ */
+#define OP_3DSTATE_WM_DEPTH_STENCIL   		OP_3D_MEDIA(0x3, 0x0, 0x4E) /* BDW+ */
+#define OP_3DSTATE_PS_EXTRA   			OP_3D_MEDIA(0x3, 0x0, 0x4F) /* BDW+ */
+#define OP_3DSTATE_RASTER   			OP_3D_MEDIA(0x3, 0x0, 0x50) /* BDW+ */
+#define OP_3DSTATE_SBE_SWIZ   			OP_3D_MEDIA(0x3, 0x0, 0x51) /* BDW+ */
+#define OP_3DSTATE_WM_HZ_OP   			OP_3D_MEDIA(0x3, 0x0, 0x52) /* BDW+ */
+
+#define OP_3DSTATE_DRAWING_RECTANGLE            OP_3D_MEDIA( 0x3 ,0x1, 0x00 )
+#define OP_3DSTATE_SAMPLER_PALETTE_LOAD0        OP_3D_MEDIA( 0x3 ,0x1, 0x02 )
+#define OP_3DSTATE_CHROMA_KEY                   OP_3D_MEDIA( 0x3 ,0x1, 0x04 )
+#define OP_SNB_3DSTATE_DEPTH_BUFFER             OP_3D_MEDIA( 0x3 ,0x1, 0x05 )
+#define OP_3DSTATE_POLY_STIPPLE_OFFSET          OP_3D_MEDIA( 0x3 ,0x1, 0x06 )
+#define OP_3DSTATE_POLY_STIPPLE_PATTERN         OP_3D_MEDIA( 0x3 ,0x1, 0x07 )
+#define OP_3DSTATE_LINE_STIPPLE                 OP_3D_MEDIA( 0x3 ,0x1, 0x08 )
+#define OP_3DSTATE_AA_LINE_PARAMS               OP_3D_MEDIA( 0x3 ,0x1, 0x0A )
+#define OP_3DSTATE_GS_SVB_INDEX                 OP_3D_MEDIA( 0x3 ,0x1, 0x0B )
+#define OP_3DSTATE_SAMPLER_PALETTE_LOAD1        OP_3D_MEDIA( 0x3 ,0x1, 0x0C )
+#define OP_3DSTATE_MULTISAMPLE                  OP_3D_MEDIA( 0x3 ,0x1, 0x0D )
+#define OP_3DSTATE_MULTISAMPLE_BDW		OP_3D_MEDIA( 0x3 ,0x0, 0x0D )
+#define OP_3DSTATE_RAST_MULTISAMPLE             OP_3D_MEDIA( 0x3 ,0x1, 0x0E )
+#define OP_SNB_3DSTATE_STENCIL_BUFFER           OP_3D_MEDIA( 0x3 ,0x1, 0x0E )
+#define OP_SNB_3DSTATE_HIER_DEPTH_BUFFER        OP_3D_MEDIA( 0x3 ,0x1, 0x0F )
+#define OP_SNB_3DSTATE_CLEAR_PARAMS             OP_3D_MEDIA( 0x3 ,0x1, 0x10 )
+#define OP_3DSTATE_MONOFILTER_SIZE              OP_3D_MEDIA( 0x3 ,0x1, 0x11 )
+#define OP_3DSTATE_PUSH_CONSTANT_ALLOC_VS       OP_3D_MEDIA(0x3, 0x1, 0x12) /* IVB+ */
+#define OP_3DSTATE_PUSH_CONSTANT_ALLOC_HS       OP_3D_MEDIA(0x3, 0x1, 0x13) /* IVB+ */
+#define OP_3DSTATE_PUSH_CONSTANT_ALLOC_DS       OP_3D_MEDIA(0x3, 0x1, 0x14) /* IVB+ */
+#define OP_3DSTATE_PUSH_CONSTANT_ALLOC_GS       OP_3D_MEDIA(0x3, 0x1, 0x15) /* IVB+ */
+#define OP_3DSTATE_PUSH_CONSTANT_ALLOC_PS       OP_3D_MEDIA(0x3, 0x1, 0x16) /* IVB+ */
+#define OP_3DSTATE_SO_DECL_LIST                 OP_3D_MEDIA( 0x3 ,0x1, 0x17 )
+#define OP_3DSTATE_SO_BUFFER                    OP_3D_MEDIA( 0x3 ,0x1, 0x18 )
+#define OP_3DSTATE_BINDING_TABLE_POOL_ALLOC     OP_3D_MEDIA( 0x3 ,0x1, 0x19 ) /* HSW+ */
+#define OP_3DSTATE_GATHER_POOL_ALLOC            OP_3D_MEDIA( 0x3 ,0x1, 0x1A ) /* HSW+ */
+#define OP_3DSTATE_DX9_CONSTANT_BUFFER_POOL_ALLOC OP_3D_MEDIA( 0x3 ,0x1, 0x1B ) /* HSW+ */
+#define OP_3DSTATE_SAMPLE_PATTERN               OP_3D_MEDIA (0x3 ,0x1, 0x1C )
+#define OP_3DSTATE_URB_CLEAR                    OP_3D_MEDIA (0x3 ,0x1, 0x1D )
+
+#define OP_PIPE_CONTROL                         OP_3D_MEDIA( 0x3 ,0x2, 0x00 )
+
+#define OP_3DPRIMITIVE                          OP_3D_MEDIA( 0x3 ,0x3, 0x00 )
+
+/* VCCP Command Parser */
+
+/*
+ * Below MFX and VBE cmd definition is from vaapi intel driver project (BSD License)
+ * git://anongit.freedesktop.org/vaapi/intel-driver
+ * src/i965_defines.h
+ *
+ */
+
+#define OP_MFX(pipeline, op, sub_opa, sub_opb)     \
+     (3 << 13 |                                  \
+     (pipeline) << 11 |                         \
+     (op) << 8 |                               \
+     (sub_opa) << 5 |                          \
+     (sub_opb))
+
+#define OP_MFX_PIPE_MODE_SELECT                    OP_MFX(2, 0, 0, 0)  /* ALL */
+#define OP_MFX_SURFACE_STATE                       OP_MFX(2, 0, 0, 1)  /* ALL */
+#define OP_MFX_PIPE_BUF_ADDR_STATE                 OP_MFX(2, 0, 0, 2)  /* ALL */
+#define OP_MFX_IND_OBJ_BASE_ADDR_STATE             OP_MFX(2, 0, 0, 3)  /* ALL */
+#define OP_MFX_BSP_BUF_BASE_ADDR_STATE             OP_MFX(2, 0, 0, 4)  /* ALL */
+#define OP_2_0_0_5                                 OP_MFX(2, 0, 0, 5)  /* ALL */
+#define OP_MFX_STATE_POINTER                       OP_MFX(2, 0, 0, 6)  /* ALL */
+#define OP_MFX_QM_STATE                            OP_MFX(2, 0, 0, 7)  /* IVB+ */
+#define OP_MFX_FQM_STATE                           OP_MFX(2, 0, 0, 8)  /* IVB+ */
+
+#define OP_MFX_PAK_INSERT_OBJECT                   OP_MFX(2, 0, 2, 8)  /* IVB+ */
+#define OP_MFX_STITCH_OBJECT                       OP_MFX(2, 0, 2, 0xA)  /* IVB+ */
+
+#define OP_MFD_IT_OBJECT                           OP_MFX(2, 0, 1, 9) /* ALL */
+
+#define OP_MFX_WAIT                                OP_MFX(1, 0, 0, 0) /* IVB+ */
+
+#define OP_MFX_AVC_IMG_STATE                       OP_MFX(2, 1, 0, 0) /* ALL */
+#define OP_MFX_AVC_QM_STATE                        OP_MFX(2, 1, 0, 1) /* ALL */
+#define OP_MFX_AVC_DIRECTMODE_STATE                OP_MFX(2, 1, 0, 2) /* ALL */
+#define OP_MFX_AVC_SLICE_STATE                     OP_MFX(2, 1, 0, 3) /* ALL */
+#define OP_MFX_AVC_REF_IDX_STATE                   OP_MFX(2, 1, 0, 4) /* ALL */
+#define OP_MFX_AVC_WEIGHTOFFSET_STATE              OP_MFX(2, 1, 0, 5) /* ALL */
+
+#define OP_MFD_AVC_PICID_STATE                     OP_MFX(2, 1, 1, 5) /* HSW+ */
+#define OP_MFD_AVC_DPB_STATE			   OP_MFX(2, 1, 1, 6) /* IVB+ */
+#define OP_MFD_AVC_SLICEADDR                       OP_MFX(2, 1, 1, 7) /* IVB+ */
+#define OP_MFD_AVC_BSD_OBJECT                      OP_MFX(2, 1, 1, 8) /* ALL */
+
+#define OP_MFC_AVC_FQM_STATE                       OP_MFX(2, 1, 2, 2) /* SNB */
+#define OP_MFC_AVC_PAK_INSERT_OBJECT               OP_MFX(2, 1, 2, 8) /* SNB */
+#define OP_MFC_AVC_PAK_OBJECT                      OP_MFX(2, 1, 2, 9) /* ALL */
+
+#define OP_MFX_VC1_PIC_STATE                       OP_MFX(2, 2, 0, 0) /* SNB */
+#define OP_MFX_VC1_PRED_PIPE_STATE                 OP_MFX(2, 2, 0, 1) /* ALL */
+#define OP_MFX_VC1_DIRECTMODE_STATE                OP_MFX(2, 2, 0, 2) /* ALL */
+
+#define OP_MFD_VC1_SHORT_PIC_STATE                 OP_MFX(2, 2, 1, 0) /* IVB+ */
+#define OP_MFD_VC1_LONG_PIC_STATE                  OP_MFX(2, 2, 1, 1) /* IVB+ */
+
+#define OP_MFD_VC1_BSD_OBJECT                      OP_MFX(2, 2, 1, 8) /* ALL */
+
+#define OP_MFX_MPEG2_PIC_STATE                     OP_MFX(2, 3, 0, 0) /* ALL */
+#define OP_MFX_MPEG2_QM_STATE                      OP_MFX(2, 3, 0, 1) /* ALL */
+
+#define OP_MFD_MPEG2_BSD_OBJECT                    OP_MFX(2, 3, 1, 8) /* ALL */
+
+#define OP_MFC_MPEG2_SLICEGROUP_STATE              OP_MFX(2, 3, 2, 3) /* ALL */
+#define OP_MFC_MPEG2_PAK_OBJECT                    OP_MFX(2, 3, 2, 9) /* ALL */
+
+#define OP_MFX_2_6_0_0                             OP_MFX(2, 6, 0, 0) /* IVB+ */
+#define OP_MFX_2_6_0_8                             OP_MFX(2, 6, 0, 8) /* IVB+ */
+#define OP_MFX_2_6_0_9                             OP_MFX(2, 6, 0, 9) /* IVB+ */
+
+#define OP_MFX_JPEG_PIC_STATE                      OP_MFX(2, 7, 0, 0)
+#define OP_MFX_JPEG_HUFF_TABLE_STATE               OP_MFX(2, 7, 0, 2)
+
+#define OP_MFD_JPEG_BSD_OBJECT                     OP_MFX(2, 7, 1, 8)
+
+/* copy from vaapi, but not found definition in PRM yet */
+#define OP_VEB(pipeline, op, sub_opa, sub_opb)     \
+     (3 << 13 |                                 \
+     (pipeline) << 11 |                         \
+     (op) << 8 |                               \
+     (sub_opa) << 5 |                          \
+     (sub_opb))
+
+#define OP_VEB_SURFACE_STATE                       OP_VEB(2, 4, 0, 0)
+#define OP_VEB_STATE                               OP_VEB(2, 4, 0, 2)
+#define OP_VEB_DNDI_IECP_STATE                     OP_VEB(2, 4, 0, 3)
+
+extern int vgt_scan_vring_2(struct vgt_device *vgt, int ring_id);
+
+struct parser_exec_state;
+
+typedef int (*parser_cmd_handler)(struct parser_exec_state *s);
+
+#define VGT_CMD_HASH_BITS   7
+
+/* which DWords need address fix */
+#define ADDR_FIX_1(x1)                  (1<<(x1))
+#define ADDR_FIX_2(x1,x2)               (ADDR_FIX_1(x1) | ADDR_FIX_1(x2))
+#define ADDR_FIX_3(x1,x2,x3)            (ADDR_FIX_1(x1) | ADDR_FIX_2(x2,x3))
+#define ADDR_FIX_4(x1,x2,x3,x4)         (ADDR_FIX_1(x1) | ADDR_FIX_3(x2,x3,x4))
+#define ADDR_FIX_5(x1,x2,x3,x4,x5)      (ADDR_FIX_1(x1) | ADDR_FIX_4(x2,x3,x4,x5))
+
+struct cmd_info{
+	char* name;
+	uint32_t opcode;
+
+#define F_LEN_MASK	(1U<<0)
+#define F_LEN_CONST  1U
+#define F_LEN_VAR    0U
+
+/* command has its own ip advance logic
+   e.g. MI_BATCH_START, MI_BATCH_END
+*/
+#define F_IP_ADVANCE_CUSTOM (1<<1)
+
+#define F_POST_HANDLE	(1<<2)
+	uint32_t flag;
+
+#define R_RCS	(1 << RING_BUFFER_RCS )
+#define R_VCS1  (1 << RING_BUFFER_VCS)
+#define R_VCS2  (1 << RING_BUFFER_VCS2)
+#define R_VCS	( R_VCS1 | R_VCS2)
+#define R_BCS	(1 << RING_BUFFER_BCS )
+#define R_VECS	(1 << RING_BUFFER_VECS )
+#define R_ALL (R_RCS | R_VCS | R_BCS | R_VECS)
+	/* rings that support this cmd: BLT/RCS/VCS/VECS */
+	uint16_t rings;
+
+	/* devices that support this cmd: SNB/IVB/HSW/... */
+	uint16_t devices;
+
+	/* which DWords are address that need fix up.
+	 * bit 0 means a 32-bit non address operand in command
+	 * bit 1 means address operand, which could be 32-bit
+	 * or 64-bit depending on different architectures.(
+	 * defined by "gmadr_bytes_in_cmd" in pgt_device.
+	 * No matter the address length, each address only takes
+	 * one bit in the bitmap.
+	 */
+	uint16_t addr_bitmap;
+
+	/*	flag == F_LEN_CONST : command length
+		flag == F_LEN_VAR : lenght bias bits
+		Note: length is in DWord
+	 */
+	uint8_t	len;
+
+	parser_cmd_handler handler;
+};
+#define VGT_MAX_CMD_LENGTH	20  /* In Dword */
+struct vgt_cmd_entry {
+	struct hlist_node hlist;
+	struct cmd_info* info;
+};
+
+typedef enum {
+	RING_BUFFER_INSTRUCTION,
+	BATCH_BUFFER_INSTRUCTION,
+	BATCH_BUFFER_2ND_LEVEL,
+}cmd_buf_t;
+
+typedef enum{
+	GTT_BUFFER,
+	PPGTT_BUFFER
+}gtt_addr_t;
+
+struct parser_exec_state {
+	struct vgt_device *vgt;
+	int ring_id;
+
+	uint64_t request_id;
+
+	cmd_buf_t buf_type;
+
+	/* batch buffer address type */
+	gtt_addr_t buf_addr_type;
+
+	/* graphics memory address of ring buffer start */
+	unsigned long ring_start;
+	unsigned long ring_size;
+	unsigned long ring_head;
+	unsigned long ring_tail;
+
+	/* instruction graphics memory address */
+	unsigned long ip_gma;
+
+	/* mapped va of the instr_gma */
+	uint32_t *ip_va;
+
+	/* length of free buffer in current page, in qword */
+	unsigned long ip_buf_len;
+
+	/* mapped va of the next page near instr_gma */
+	uint32_t *ip_va_next_page;
+
+	/* next instruction when return from  batch buffer to ring buffer */
+	unsigned long ret_ip_gma_ring;
+
+	/* next instruction when return from 2nd batch buffer to batch buffer */
+	unsigned long ret_ip_gma_bb;
+
+	/* batch buffer address type (GTT or PPGTT)
+	   used when ret from 2nd level batch buffer */
+	gtt_addr_t saved_buf_addr_type;
+
+	/* indicate the command has user interrupt*/
+	bool cmd_issue_irq;
+
+	struct cmd_info* info;
+};
+
+#define CMD_TAIL_NUM	1024
+#define CMD_HANDLER_NUM	1024
+#define CMD_PATCH_NUM	CMD_HANDLER_NUM * 8
+/* a DW based structure to avoid cross-page trickiness */
+struct cmd_patch_info {
+	uint64_t request_id;
+	void *addr;
+	uint32_t old_val;
+	uint32_t new_val;
+};
+
+struct cmd_handler_info {
+	uint64_t request_id;
+	struct parser_exec_state exec_state;
+	parser_cmd_handler handler;
+};
+
+struct cmd_tail_info {
+	uint64_t request_id;
+	uint32_t tail;
+	uint32_t cmd_nr;
+	uint32_t ip_offset;
+
+#define F_CMDS_ISSUE_IRQ (1<<0)
+	uint32_t flags;
+};
+
+struct cmd_general_info {
+	union {
+		struct cmd_patch_info patch[CMD_PATCH_NUM];
+		struct cmd_handler_info handler[CMD_HANDLER_NUM];
+		struct cmd_tail_info cmd[CMD_TAIL_NUM];
+	};
+	int	head;
+	int	tail;
+	int	count;
+};
+
+extern uint32_t vgt_get_opcode(uint32_t cmd, int ring_id );
+extern void vgt_cmd_name(uint32_t cmd, int ring_id, int gen);
diff --git a/drivers/gpu/drm/i915/vgt/debugfs.c b/drivers/gpu/drm/i915/vgt/debugfs.c
new file mode 100644
index 0000000..a51dfa4
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/debugfs.c
@@ -0,0 +1,1155 @@
+/*
+ * Debugfs interfaces
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of Version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+/* TODO: this file's code copied from arch/x86/xen/debugfs.c and
+ * fs/debugfs/file.c. Can we clean up and/or minimize this file???
+ */
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/debugfs.h>
+#include "fb_decoder.h"
+
+#include "vgt.h"
+
+/*
+ * Dump buffer
+ *
+ * dump buffer provides users the ability to dump contents into a text
+ * buffer first, so that the contents could be later printed by various
+ * ways, like printk to stdout, or seq_printf to files.
+ *
+ * buffer overflow is handled inside.
+ */
+#define MAX_DUMP_BUFFER_SIZE 4096
+
+int create_dump_buffer(struct dump_buffer *buf, int buf_size)
+{
+	buf->buffer = NULL;
+	buf->buf_len = buf->buf_size = 0;
+
+	if ((buf_size > MAX_DUMP_BUFFER_SIZE) || (buf_size <= 0)) {
+		vgt_err ("Invalid dump buffer size!\n");
+		return -EINVAL;
+	}
+
+	buf->buffer = vzalloc(buf_size);
+	if (!buf->buffer) {
+		vgt_err(
+		"Buffer allocation failed for frame buffer format dump!\n");
+		return -EINVAL;
+	}
+
+	buf->buf_size = buf_size;
+	return 0;
+}
+
+void destroy_dump_buffer(struct dump_buffer *buf)
+{
+	if (buf->buffer)
+		vfree(buf->buffer);
+
+	buf->buffer = NULL;
+	buf->buf_len = buf->buf_size = 0;
+}
+
+void dump_string(struct dump_buffer *buf, const char *fmt, ...)
+{
+	va_list args;
+	int n;
+
+	if (buf->buf_len >= buf->buf_size - 1) {
+		vgt_warn("dump buffer is full! Contents will be ignored!\n");
+		return;
+	}
+
+	va_start(args, fmt);
+	n = vsnprintf(&buf->buffer[buf->buf_len],
+			buf->buf_size - buf->buf_len, fmt, args);
+	va_end(args);
+
+	if (buf->buf_len + n >= buf->buf_size) {
+		buf->buf_len = buf->buf_size - 1;
+		vgt_warn("dump buffer is full! Content is truncated!\n");
+	} else {
+		buf->buf_len += n;
+	}
+}
+
+/*************** end of dump buffer implementation **************/
+
+/* Maximum lenth of stringlized integer is 10 */
+#define MAX_VM_NAME_LEN (3 + 10)
+enum vgt_debugfs_entry_t
+{
+	VGT_DEBUGFS_VIRTUAL_MMIO = 0,
+	VGT_DEBUGFS_SHADOW_MMIO,
+	VGT_DEBUGFS_FB_FORMAT,
+	VGT_DEBUGFS_DPY_INFO,
+	VGT_DEBUGFS_VIRTUAL_GTT,
+	VGT_DEBUGFS_ENTRY_MAX
+};
+
+static debug_statistics_t  stat_info [] = {
+	{ "context_switch_cycles", &context_switch_cost },
+	{ "context_switch_num", &context_switch_num },
+	{ "ring_idle_wait", &ring_idle_wait },
+	{ "ring_0_busy", &ring_0_busy },
+	{ "ring_0_idle", &ring_0_idle },
+	{ "", NULL}
+};
+
+#define debugfs_create_u64_node(name, perm, parent, u64_ptr) \
+	do { \
+		struct dentry *__dentry = debugfs_create_u64( \
+		(name),\
+		(perm), \
+		(parent), \
+		(u64_ptr) \
+		); \
+		if (!__dentry) \
+			printk(KERN_ERR "Failed to create debugfs node: %s\n", (name)); \
+	} while (0)
+
+static struct dentry *d_vgt_debug;
+static struct dentry *d_per_vgt[VGT_MAX_VMS];
+static struct dentry *d_debugfs_entry[VGT_MAX_VMS][VGT_DEBUGFS_ENTRY_MAX];
+static char vm_dir_name[VGT_MAX_VMS][MAX_VM_NAME_LEN];
+
+struct array_data
+{
+	void *array;
+	unsigned elements;
+};
+struct array_data vgt_debugfs_data[VGT_MAX_VMS][VGT_DEBUGFS_ENTRY_MAX];
+
+static int u32_array_open(struct inode *inode, struct file *file)
+{
+	file->private_data = NULL;
+	return nonseekable_open(inode, file);
+}
+
+/* This is generic function, used to format ring_buffer and etc. */
+static size_t format_array(char *buf, size_t bufsize, const char *fmt,
+				u32 *array, unsigned array_size)
+{
+	size_t ret = 0;
+	unsigned i;
+
+	for(i = 0; i < array_size; i++) {
+		size_t len;
+
+		if (i % 16 == 0) {
+			len = snprintf(buf, bufsize, "0x%x:",i*4);
+			ret += len;
+
+			if (buf) {
+				buf += len;
+				bufsize -= len;
+			}
+		}
+
+		len = snprintf(buf, bufsize, fmt, array[i]);
+		len++;	/* ' ' or '\n' */
+		ret += len;
+
+		if (buf) {
+			buf += len;
+			bufsize -= len;
+			buf[-1] = ((i + 1) % 16 == 0) ? '\n' : ' ';
+		}
+	}
+
+	ret++;		/* \0 */
+	if (buf)
+		*buf = '\0';
+
+	return ret;
+}
+
+static char *format_array_alloc(const char *fmt, u32 *array, unsigned array_size)
+{
+	/* very tricky way */
+	size_t len = format_array(NULL, 0, fmt, array, array_size);
+	char *ret;
+
+	ret = vmalloc(len);
+	if (ret == NULL) {
+		vgt_err("failed to alloc memory!");
+		return NULL;
+	}
+
+	format_array(ret, len, fmt, array, array_size);
+	return ret;
+}
+
+/* data copied from kernel space to user space */
+static ssize_t u32_array_read(struct file *file, char __user *buf, size_t len,
+				loff_t *ppos)
+{
+	struct inode *inode = file->f_path.dentry->d_inode;
+	struct array_data *data = inode->i_private;
+	size_t size;
+
+	if (*ppos == 0) {
+		if (file->private_data) {
+			vfree(file->private_data);
+			file->private_data = NULL;
+		}
+
+		file->private_data = format_array_alloc("%x", data->array, data->elements);
+	}
+
+	size = 0;
+	if (file->private_data)
+		size = strlen(file->private_data);
+
+	return simple_read_from_buffer(buf, len, ppos, file->private_data, size);
+}
+
+static int vgt_array_release(struct inode *inode, struct file *file)
+{
+	vfree(file->private_data);
+	return 0;
+}
+
+static const struct file_operations u32_array_fops = {
+	.owner	= THIS_MODULE,
+	.open	= u32_array_open,
+	.release= vgt_array_release,
+	.read	= u32_array_read,
+	.llseek = no_llseek,
+};
+
+static struct dentry *vgt_debugfs_create_blob(const char *name, mode_t mode,
+					struct dentry *parent,
+					struct array_data *p)
+{
+	if (!p || !(p->array))
+		return NULL;
+	return debugfs_create_file(name, mode, parent, p, &u32_array_fops);
+}
+
+static inline char *reg_show_reg_owner(struct pgt_device *pdev, int i)
+{
+	char *str;
+	switch (reg_get_owner(pdev, i)) {
+		case VGT_OT_NONE:
+			str = "NONE";
+			break;
+		case VGT_OT_RENDER:
+			str = "Render";
+			break;
+		case VGT_OT_DISPLAY:
+			str = "Display";
+			break;
+		case VGT_OT_CONFIG:
+			str = "Config";
+			break;
+		default:
+			str = "";
+			break;
+	}
+	return str;
+}
+
+static inline char *reg_show_reg_type(struct pgt_device *pdev, int i)
+{
+	if (reg_get_owner(pdev, i) != VGT_OT_NONE)
+		return "MPT";
+	else if (reg_passthrough(pdev, i))
+		return "PT";
+	else if (reg_virt(pdev, i))
+		return "Virt";
+	else
+		return "";
+}
+
+static int vgt_show_regs(struct seq_file *m, void *data)
+{
+	int i, tot;
+	struct pgt_device *pdev = (struct pgt_device *)m->private;
+
+	tot = 0;
+	seq_printf(m, "------------------------------------------\n");
+	seq_printf(m, "MGMT - Management context\n");
+	seq_printf(m, "MPT - Mediated Pass-Through based on owner type\n");
+	seq_printf(m, "PT - passthrough regs with special risk\n");
+	seq_printf(m, "%8s: %8s (%-8s %-4s)\n",
+			"Reg", "Flags", "Owner", "Type");
+	for (i = 0; i < pdev->mmio_size; i +=  REG_SIZE) {
+		if (!reg_is_accessed(pdev, i) && !reg_is_tracked(pdev, i))
+			continue;
+
+		tot++;
+		seq_printf(m, "%8x: %8x (%-8s %-4s)\n",
+			i, pdev->reg_info[REG_INDEX(i)],
+			reg_show_reg_owner(pdev, i),
+			reg_show_reg_type(pdev, i));
+	}
+	seq_printf(m, "------------------------------------------\n");
+	seq_printf(m, "Total %d accessed registers are shown\n", tot);
+	return 0;
+}
+
+static int vgt_reginfo_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, vgt_show_regs, inode->i_private);
+}
+
+static const struct file_operations reginfo_fops = {
+	.open = vgt_reginfo_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+/*
+ * It's always dangerious to read from pReg directly, since some
+ * read has side effect e.g. read-to-clear bit.
+ *
+ * So use it with caution only when debugging hard GPU hang problem
+ */
+static int vgt_show_pregs(struct seq_file *m, void *data)
+{
+	u64 i;
+	struct pgt_device *pdev = (struct pgt_device *)m->private;
+
+	seq_printf(m, "Use this interface with caution b/c side effect may be caused by reading hw status\n");
+	for(i = 0; i < pdev->reg_num; i++) {
+		if (!(i % 16))
+			seq_printf(m, "\n%8llx:", i * REG_SIZE);
+		seq_printf(m, " %x", VGT_MMIO_READ(pdev, i * REG_SIZE));
+	}
+
+	seq_printf(m, "\n");
+	return 0;
+}
+
+static int vgt_preg_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, vgt_show_pregs, inode->i_private);
+}
+
+static const struct file_operations preg_fops = {
+	.open = vgt_preg_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+static int vgt_show_irqinfo(struct seq_file *m, void *data)
+{
+	struct pgt_device *pdev = (struct pgt_device *)m->private;
+	struct vgt_device *vgt;
+	struct pgt_statistics *pstat = &pdev->stat;
+	struct vgt_statistics *vstat;
+	int i, j;
+
+	if (!pstat->irq_num) {
+		seq_printf(m, "No irq logged\n");
+		return 0;
+	}
+	seq_printf(m, "--------------------------\n");
+	seq_printf(m, "Interrupt control status:\n");
+
+	show_interrupt_regs(pdev, m);
+
+	seq_printf(m, "Total %lld interrupts logged:\n", pstat->irq_num);
+	seq_printf(m, "#	WARNING: precisely this is the number of vGT \n"
+			"#	physical interrupt handler be called,\n"
+			"#	each calling several events can be\n"
+			"#	been handled, so usually this number\n"
+			"#	is less than the total events number.\n");
+	for (i = 0; i < EVENT_MAX; i++) {
+		if (!pstat->events[i])
+			continue;
+		seq_printf(m, "\t%16lld: %s\n", pstat->events[i],
+				vgt_irq_name[i]);
+	}
+
+	seq_printf(m, "%16lld: Last pirq\n", pstat->last_pirq);
+	seq_printf(m, "%16lld: Last virq\n", pstat->last_virq);
+	seq_printf(m, "%16lld: Average pirq cycles\n",
+		pstat->pirq_cycles / pstat->irq_num);
+	seq_printf(m, "%16lld: Average virq cycles\n",
+		pstat->virq_cycles / pstat->irq_num);
+	seq_printf(m, "%16lld: Average delay between pirq/virq handling\n",
+		pstat->irq_delay_cycles / pstat->irq_num);
+	/* TODO: hold lock */
+	for (i = 0; i < VGT_MAX_VMS; i++) {
+		if (!pdev->device[i])
+			continue;
+
+		seq_printf(m, "\n-->vgt-%d:\n", pdev->device[i]->vgt_id);
+		vgt = pdev->device[i];
+		vstat = &vgt->stat;
+
+		show_virtual_interrupt_regs(vgt, m);
+
+		seq_printf(m, "%16lld: Last injection\n",
+			vstat->last_injection);
+
+		if (!vstat->irq_num)
+			continue;
+
+		seq_printf(m, "Total %lld virtual irq injection:\n",
+			vstat->irq_num);
+		for (j = 0; j < EVENT_MAX; j++) {
+			if (!vstat->events[j])
+				continue;
+			seq_printf(m, "\t%16lld: %s\n", vstat->events[j],
+					vgt_irq_name[j]);
+		}
+
+		if (vstat->pending_events)
+			seq_printf(m, "\t%16lld: %s\n", vstat->pending_events,
+					"pending virt events");
+	}
+	return 0;
+}
+
+static int vgt_irqinfo_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, vgt_show_irqinfo, inode->i_private);
+}
+
+static const struct file_operations irqinfo_fops = {
+	.open = vgt_irqinfo_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+int vgt_dump_fb_format(struct dump_buffer *buf, struct vgt_fb_format *fb);
+static int vgt_show_fbinfo(struct seq_file *m, void *data)
+{
+	struct vgt_device *vgt =  (struct vgt_device *)m->private;
+	struct vgt_fb_format fb;
+	int rc;
+
+	rc = vgt_decode_fb_format(vgt->vm_id, &fb);
+	if (rc != 0) {
+		seq_printf(m, "Failed to get frame buffer information!\n");
+	} else {
+		struct dump_buffer buf;
+		if ((rc = create_dump_buffer(&buf, 2048) < 0))
+			return rc;
+		vgt_dump_fb_format(&buf, &fb);
+		seq_printf(m, "-----------FB format (VM-%d)--------\n",
+					vgt->vm_id);
+		seq_printf(m, "%s", buf.buffer);
+		destroy_dump_buffer(&buf);
+	}
+
+	return 0;
+}
+
+static int vgt_fbinfo_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, vgt_show_fbinfo, inode->i_private);
+}
+
+static const struct file_operations fbinfo_fops = {
+	.open = vgt_fbinfo_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+static inline vgt_reg_t vgt_get_mmio_value(struct pgt_device *pdev,
+		struct vgt_device *vgt, unsigned int reg)
+{
+	ASSERT(pdev || vgt);
+	return (vgt ? __vreg(vgt, reg) : VGT_MMIO_READ(pdev, reg));
+}
+
+static void vgt_dump_dpy_mmio(struct seq_file *m, struct pgt_device *pdev,
+		struct vgt_device *vgt)
+{
+	enum vgt_pipe pipe;
+	enum vgt_port port;
+	const char *str;
+	unsigned int reg;
+	vgt_reg_t val;
+	bool enabled;
+
+	seq_printf(m, "----General CTL:\n");
+
+	reg = _REG_CPU_VGACNTRL;
+	val = vgt_get_mmio_value(pdev, vgt, reg);
+	enabled = !(val & _REGBIT_VGA_DISPLAY_DISABLE);
+	seq_printf(m,"\tVGA_CONTROL(0x%x):0x%08x (VGA Mode %s)\n",
+		reg, val, (enabled ? "enabled" : "disabled"));
+
+	reg = _REG_HSW_FUSE_STRAP;
+	val = vgt_get_mmio_value(pdev, vgt, reg);
+	seq_printf(m,"\tFUSE_STRAP(0x%x):0x%08x(RO)\n", reg, val);
+
+	reg = _REG_SHOTPLUG_CTL;
+	val = vgt_get_mmio_value(pdev, vgt, reg);
+	seq_printf(m,"\tSHOTPLUG_CTL(0x%x):0x%08x\n", reg, val);
+
+	seq_printf(m, "\n");
+
+	seq_printf(m, "----plane:\n");
+	for (pipe = PIPE_A; pipe < I915_MAX_PIPES; ++ pipe) {
+		char P = VGT_PIPE_CHAR(pipe);
+		reg = VGT_DSPCNTR(pipe);
+		val = vgt_get_mmio_value(pdev, vgt, reg);
+		enabled = !!(val & _PRI_PLANE_ENABLE);
+		seq_printf(m, "\tDSPCTL_%c(0x%x): 0x%08x (%s)\n",
+			P, reg, val, (enabled ? "enabled" : "disabled"));
+		if (enabled) {
+			reg = VGT_DSPSURF(pipe);
+			seq_printf(m, "\tDSPSURF_%c(0x%x): 0x%08x\n",
+				P, reg, vgt_get_mmio_value(pdev, vgt, reg));
+		}
+		seq_printf(m, "\n");
+	}
+
+	seq_printf(m, "----pipe:\n");
+	for (pipe = PIPE_A; pipe < I915_MAX_PIPES; ++ pipe) {
+		char P = VGT_PIPE_CHAR(pipe);
+		reg = VGT_PIPECONF(pipe);
+		val = vgt_get_mmio_value(pdev, vgt, reg);
+		enabled = !!(val & _REGBIT_PIPE_ENABLE);
+		seq_printf(m, "\tPIPECONF_%c(0x%x): 0x%08x (%s)\n",
+			P, reg, val, (enabled ? "enabled" : "disabled"));
+
+		if (enabled) {
+			reg = VGT_PIPESRC(pipe);
+			val = vgt_get_mmio_value(pdev, vgt, reg);
+			seq_printf(m, "\tPIPE_SRC_%c(0x%x): 0x%08x "
+					"(width : %d, height: %d)\n",
+				P, reg, val, ((val >> 16) & 0xfff) + 1,
+						((val & 0xfff) + 1));
+			reg = VGT_HTOTAL(pipe);
+			val = vgt_get_mmio_value(pdev, vgt, reg);
+			seq_printf(m, "\tPIPE_HTOTAL_%c(0x%x): 0x%08x (total: %d)\n",
+				P, reg, val, (val & 0xfff) + 1);
+			reg = VGT_VTOTAL(pipe);
+			val = vgt_get_mmio_value(pdev, vgt, reg);
+			seq_printf(m, "\tPIPE_VTOTAL_%c(0x%x): 0x%08x (total: %d)\n",
+				P, reg, val, (val & 0xfff) + 1);
+		}
+
+		reg = _VGT_TRANS_DDI_FUNC_CTL(pipe);
+		val = vgt_get_mmio_value(pdev, vgt, reg);
+		enabled = !!(val & _REGBIT_TRANS_DDI_FUNC_ENABLE);
+		seq_printf(m, "\tTRANS_DDI_FUNC_CTL_%c(0x%x): 0x%08x (%s)\n",
+			P, reg, val, (enabled ? "enabled" : "disabled"));
+
+		if (enabled) {
+			vgt_reg_t ddi_select, mode_select;
+
+			ddi_select = val & _REGBIT_TRANS_DDI_PORT_MASK;
+			mode_select = val & _REGBIT_TRANS_DDI_MODE_SELECT_MASK;
+
+			switch (ddi_select >> _TRANS_DDI_PORT_SHIFT) {
+				case 0:
+					str = "No Port Connected"; break;
+				case 1:
+					str = "DDI_B"; break;
+				case 2:
+					str = "DDI_C"; break;
+				case 3:
+					str = "DDI_D"; break;
+				case 4:
+					str = "DDI_E"; break;
+				default:
+					str = "Port INV";
+			}
+			seq_printf(m, "\t\tmapping to port: %s\n", str);
+
+			switch (mode_select >> _TRANS_DDI_MODE_SELECT_HIFT) {
+				case 0:
+					str = "HDMI"; break;
+				case 1:
+					str = "DVI"; break;
+				case 2:
+					str = "DP SST"; break;
+				case 3:
+					str = "DP MST"; break;
+				case 4:
+					str = "FDI"; break;
+				default:
+					str = "Mode INV";
+			}
+			seq_printf(m, "\t\tMode type: %s\n", str);
+		}
+		seq_printf(m, "\n");
+	}
+
+	reg = _REG_PIPE_EDP_CONF;
+	val = vgt_get_mmio_value(pdev, vgt, reg);
+	enabled = !!(val & _REGBIT_PIPE_ENABLE);
+	seq_printf(m, "\tPIPECONF_EDP(0x%x): 0x%08x (%s)\n",
+		reg, val, (enabled ? "enabled" : "disabled"));
+
+	if (enabled) {
+		reg = _REG_HTOTAL_EDP;
+		val = vgt_get_mmio_value(pdev, vgt, reg);
+		seq_printf(m, "\tPIPE_HTOTAL_EDP(0x%x): 0x%08x (total: %d)\n",
+			reg, val, (val & 0xfff) + 1);
+		reg = _REG_VTOTAL_EDP;
+		val = vgt_get_mmio_value(pdev, vgt, reg);
+		seq_printf(m, "\tPIPE_VTOTAL_EDP(0x%x): 0x%08x (total: %d)\n",
+			reg, val, (val & 0xfff) + 1);
+	}
+
+	reg = _REG_TRANS_DDI_FUNC_CTL_EDP;
+	val = vgt_get_mmio_value(pdev, vgt, reg);
+	enabled = !!(val & _REGBIT_TRANS_DDI_FUNC_ENABLE);
+	seq_printf(m, "\tTRANS_DDI_FUNC_CTL_EDP(0x%x): 0x%08x (%s)\n",
+		reg, val, (enabled ? "enabled" : "disabled"));
+	if (enabled) {
+		vgt_reg_t edp_input = val &_REGBIT_TRANS_DDI_EDP_INPUT_MASK;
+		switch (edp_input >> _TRANS_DDI_EDP_INPUT_SHIFT) {
+			case 0:
+				str = "Plane A 0"; break;
+			case 4:
+				str = "Plane A 4"; break;
+			case 5:
+				str = "Plane B"; break;
+			case 6:
+				str = "Plane C"; break;
+			default:
+				str = "Plane INV";
+		}
+		seq_printf(m, "\t\teDP select: %s\n", str);
+	}
+	seq_printf(m, "\n");
+	
+	if (is_current_display_owner(vgt)) {
+		return;
+	}
+
+	seq_printf(m, "---- virtual port:\n");
+
+	for (port = PORT_A; port < I915_MAX_PORTS; ++ port) {
+		if (!dpy_has_monitor_on_port(vgt, port))
+			continue;
+
+		seq_printf(m, "\t%s connected to monitors.\n",
+			VGT_PORT_NAME(port));
+
+		if (port == PORT_E) {
+			reg = _REG_PCH_ADPA;
+			val = vgt_get_mmio_value(pdev, vgt, reg);
+			enabled = !!(val & _REGBIT_ADPA_DAC_ENABLE);
+			seq_printf(m, "\tDAC_CTL(0x%x): 0x%08x (%s)\n",
+				reg, val, (enabled ? "enabled" : "disabled"));
+			if (enabled) {
+				pipe = (val & PORT_TRANS_SEL_MASK)
+						>> PORT_TRANS_SEL_SHIFT;
+				seq_printf(m, "\t\t Transcoder %c selected.\n",
+					VGT_PIPE_CHAR(pipe));
+			}
+			reg = _REG_TRANSACONF;
+			val = vgt_get_mmio_value(pdev, vgt, reg);
+			enabled = !!(val & _REGBIT_TRANS_ENABLE);
+			seq_printf(m, "\tPCH TRANS_CONF(0x%x): 0x%08x (%s)\n",
+				reg, val, (enabled ? "enabled" : "disabled"));
+		}
+	}
+}
+
+static int vgt_show_phys_dpyinfo(struct seq_file *m, void *data)
+{
+	struct pgt_device *pdev =  (struct pgt_device *)m->private;
+
+	seq_printf(m, "----------Physical DPY info ----------\n");
+	vgt_dump_dpy_mmio(m, pdev, NULL);
+	seq_printf(m, "\n");
+
+	return 0;
+}
+
+static int vgt_show_virt_dpyinfo(struct seq_file *m, void *data)
+{
+	struct vgt_device *vgt =  (struct vgt_device *)m->private;
+	enum vgt_pipe pipe;
+
+	seq_printf(m, "----------DPY info (VM-%d)----------\n", vgt->vm_id);
+	vgt_dump_dpy_mmio(m, NULL, vgt);
+	seq_printf(m, "\n");
+
+	seq_printf(m, "---- physical/virtual mapping:\n");
+	for (pipe = PIPE_A; pipe < I915_MAX_PIPES; ++ pipe) {
+		enum vgt_pipe physical_pipe = vgt->pipe_mapping[pipe];
+		if (physical_pipe == I915_MAX_PIPES) {
+			seq_printf(m, "\t virtual pipe %d no mapping available yet\n", pipe);
+		} else {
+			seq_printf(m, "\t virtual pipe %d to physical pipe %d\n", pipe, physical_pipe);
+		}
+	}
+
+	return 0;
+}
+
+static int vgt_phys_dpyinfo_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, vgt_show_phys_dpyinfo, inode->i_private);
+}
+
+static const struct file_operations phys_dpyinfo_fops = {
+	.open = vgt_phys_dpyinfo_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+static int vgt_virt_dpyinfo_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, vgt_show_virt_dpyinfo, inode->i_private);
+}
+
+static const struct file_operations virt_dpyinfo_fops = {
+	.open = vgt_virt_dpyinfo_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+static int vgt_device_reset_show(struct seq_file *m, void *data)
+{
+	struct pgt_device *pdev = (struct pgt_device *)m->private;
+
+	unsigned long flags;
+	int i;
+
+	seq_printf(m, "switch: ");
+	seq_printf(m, enable_reset ? "enable" : "disable");
+	seq_printf(m, "\n");
+
+	seq_printf(m, "status: ");
+
+	if (test_bit(RESET_INPROGRESS, &pdev->device_reset_flags))
+		seq_printf(m, "resetting");
+	else {
+		if (get_seconds() - vgt_dom0->last_reset_time < 6)
+			seq_printf(m, "hold");
+		else
+			seq_printf(m, "idle");
+	}
+
+	seq_printf(m, "\n");
+
+	seq_printf(m, "waiting vm reset: ");
+
+	spin_lock_irqsave(&pdev->lock, flags);
+
+	for (i = 0; i < VGT_MAX_VMS; i++) {
+		if (pdev->device[i]
+			&& test_bit(WAIT_RESET, &pdev->device[i]->reset_flags))
+		seq_printf(m, "%d ", pdev->device[i]->vm_id);
+	}
+
+	spin_unlock_irqrestore(&pdev->lock, flags);
+
+	seq_printf(m, "\n");
+
+	return 0;
+}
+
+static int vgt_device_reset_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, vgt_device_reset_show, inode->i_private);
+}
+
+static ssize_t vgt_device_reset_write(struct file *file,
+		const char __user *ubuf, size_t count, loff_t *ppos)
+{
+	struct seq_file *s = file->private_data;
+	struct pgt_device *pdev = (struct pgt_device *)s->private;
+	struct vgt_device *vgt;
+	struct list_head *pos, *n;
+	unsigned long flags;
+	char buf[32];
+
+	if (*ppos && count > sizeof(buf))
+		return -EINVAL;
+
+	if (copy_from_user(buf, ubuf, count))
+		return -EFAULT;
+
+	if (!enable_reset) {
+		vgt_err("VGT device reset is not enabled.\n");
+		return -ENODEV;
+	}
+
+	/*
+	 * Prevent the protection logic bites ourself.
+	 */
+	if (get_seconds() - vgt_dom0->last_reset_time < 6)
+		return -EAGAIN;
+
+	if (!strncmp(buf, "normal", 6)) {
+		vgt_info("Trigger device reset under normal situation.\n");
+
+		vgt_raise_request(pdev, VGT_REQUEST_DEVICE_RESET);
+	} else if (!strncmp(buf, "invalid head", 12)) {
+		spin_lock_irqsave(&pdev->lock, flags);
+
+		list_for_each_safe(pos, n, &pdev->rendering_runq_head) {
+			vgt = list_entry(pos, struct vgt_device, list);
+
+			if (vgt != current_render_owner(pdev)) {
+				vgt_info("Inject an invalid RCS ring head pointer to VM: %d.\n",
+						vgt->vm_id);
+
+				vgt->rb[0].sring.head = 0xdeadbeef;
+			}
+		}
+
+		spin_unlock_irqrestore(&pdev->lock, flags);
+	}
+
+	return count;
+}
+
+static const struct file_operations vgt_device_reset_fops = {
+	.open = vgt_device_reset_open,
+	.read = seq_read,
+	.write = vgt_device_reset_write,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+static int vgt_debug_show(struct seq_file *m, void *data)
+{
+	struct pgt_device *pdev = (struct pgt_device *)m->private;
+	unsigned long flags;
+
+	spin_lock_irqsave(&pdev->lock, flags);
+	show_debug(pdev);
+	spin_unlock_irqrestore(&pdev->lock, flags);
+
+	seq_printf(m, "\n");
+
+	return 0;
+}
+
+static int vgt_debug_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, vgt_debug_show, inode->i_private);
+}
+
+static const struct file_operations vgt_debug_fops = {
+	.open = vgt_debug_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+static int vgt_el_status_show(struct seq_file *m, void *data)
+{
+	struct pgt_device *pdev = (struct pgt_device *)m->private;
+	unsigned long flags;
+
+	spin_lock_irqsave(&pdev->lock, flags);
+	dump_el_status(pdev);
+	spin_unlock_irqrestore(&pdev->lock, flags);
+
+	seq_printf(m, "\n");
+
+	return 0;
+}
+
+static int vgt_el_status_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, vgt_el_status_show, inode->i_private);
+}
+
+static const struct file_operations vgt_el_status_fops = {
+	.open = vgt_el_status_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+static int vgt_el_context_show(struct seq_file *m, void *data)
+{
+	struct pgt_device *pdev = (struct pgt_device *)m->private;
+	unsigned long flags;
+
+	spin_lock_irqsave(&pdev->lock, flags);
+	dump_all_el_contexts(pdev);
+	spin_unlock_irqrestore(&pdev->lock, flags);
+
+	seq_printf(m, "\n");
+
+	return 0;
+}
+
+static int vgt_el_context_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, vgt_el_context_show, inode->i_private);
+}
+
+static const struct file_operations vgt_el_context_fops = {
+	.open = vgt_el_context_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+/* initialize vGT debufs top directory */
+struct dentry *vgt_init_debugfs(struct pgt_device *pdev)
+{
+	struct dentry *temp_d;
+	int   i;
+
+	if (!d_vgt_debug) {
+		d_vgt_debug = debugfs_create_dir("vgt", NULL);
+
+		if (!d_vgt_debug) {
+			pr_warning("Could not create 'vgt' debugfs directory\n");
+			return NULL;
+		}
+	}
+
+	temp_d = debugfs_create_file("device_reset", 0444, d_vgt_debug,
+		pdev, &vgt_device_reset_fops);
+	if (!temp_d)
+		return NULL;
+
+	temp_d = debugfs_create_file("show_debug", 0444, d_vgt_debug,
+		pdev, &vgt_debug_fops);
+	if (!temp_d)
+		return NULL;
+
+	temp_d = debugfs_create_file("show_el_status", 0444, d_vgt_debug,
+		pdev, &vgt_el_status_fops);
+	if (!temp_d)
+		return NULL;
+
+	temp_d = debugfs_create_file("show_el_context", 0444, d_vgt_debug,
+		pdev, &vgt_el_context_fops);
+	if (!temp_d)
+		return NULL;
+
+	for ( i = 0; stat_info[i].stat != NULL; i++ ) {
+		temp_d = debugfs_create_u64(stat_info[i].node_name,
+			0444,
+			d_vgt_debug,
+			stat_info[i].stat);
+		if (!temp_d)
+			printk(KERN_ERR "Failed to create debugfs node %s\n",
+				stat_info[i].node_name);
+	}
+
+	temp_d = debugfs_create_file("reginfo", 0444, d_vgt_debug,
+		pdev, &reginfo_fops);
+	if (!temp_d)
+		return NULL;
+
+	temp_d = debugfs_create_file("preg", 0444, d_vgt_debug,
+		pdev, &preg_fops);
+	if (!temp_d)
+		return NULL;
+
+	temp_d = debugfs_create_file("irqinfo", 0444, d_vgt_debug,
+		pdev, &irqinfo_fops);
+	if (!temp_d)
+		return NULL;
+
+	temp_d = debugfs_create_file("dpyinfo", 0444, d_vgt_debug,
+		pdev, &phys_dpyinfo_fops);
+	if (!temp_d)
+		return NULL;
+
+	return d_vgt_debug;
+}
+
+static void vgt_create_cmdstat_per_ring(struct vgt_device *vgt, int ring_id, struct dentry *parent)
+{
+	char *ring_name;
+	struct dentry *ring_dir_entry;
+	switch (ring_id) {
+		case RING_BUFFER_RCS:
+			ring_name = "render";
+			break;
+		case RING_BUFFER_VCS:
+			ring_name = "video";
+			break;
+		case RING_BUFFER_BCS:
+			ring_name = "blitter";
+			break;
+		case RING_BUFFER_VECS:
+			ring_name = "ve";
+			break;
+		default:
+			return;
+	}
+	ring_dir_entry = debugfs_create_dir(ring_name, parent);
+	if (!ring_dir_entry)
+		printk(KERN_ERR "vGT(%d): failed to create debugfs directory: %s\n", vgt->vgt_id, ring_name);
+	else {
+		debugfs_create_u64_node("cmd_nr", 0444, ring_dir_entry, &(vgt->rb[ring_id].cmd_nr));
+	}
+}
+
+int vgt_create_debugfs(struct vgt_device *vgt)
+{
+	int retval,i;
+	struct array_data *p;
+	int vgt_id;
+	struct pgt_device *pdev;
+	struct dentry *perf_dir_entry, *cmdstat_dir_entry;
+
+	if (!vgt || !d_vgt_debug)
+		return -EINVAL;
+
+	vgt_id = vgt->vgt_id;
+	pdev = vgt->pdev;
+
+	retval = sprintf(vm_dir_name[vgt_id], "vm%d", vgt->vm_id);
+	if (retval <= 0) {
+		printk(KERN_ERR "vGT: failed to generating dirname:  vm%d\n", vgt->vm_id);
+		return -EINVAL;
+	}
+	/* create vm directory */
+	d_per_vgt[vgt_id] = debugfs_create_dir(vm_dir_name[vgt_id], d_vgt_debug);
+	if (d_per_vgt[vgt_id] == NULL) {
+		printk(KERN_ERR "vGT: creation faiure for debugfs directory: vm%d\n", vgt->vm_id);
+		return -EINVAL;
+	}
+
+	/* virtual mmio space dump */
+	p = &vgt_debugfs_data[vgt_id][VGT_DEBUGFS_VIRTUAL_MMIO];
+	p->array = (u32 *)(vgt->state.vReg);
+	p->elements = pdev->reg_num;
+	d_debugfs_entry[vgt_id][VGT_DEBUGFS_VIRTUAL_MMIO] = vgt_debugfs_create_blob("virtual_mmio_space",
+			0444,
+			d_per_vgt[vgt_id],
+			p);
+
+	if (!d_debugfs_entry[vgt_id][VGT_DEBUGFS_VIRTUAL_MMIO])
+		printk(KERN_ERR "vGT(%d): failed to create debugfs node: virtual_mmio_space\n", vgt_id);
+	else
+		printk("vGT(%d): create debugfs node: virtual_mmio_space\n", vgt_id);
+
+	p = &vgt_debugfs_data[vgt_id][VGT_DEBUGFS_SHADOW_MMIO];
+	p->array = (u32 *)(vgt->state.sReg);
+	p->elements = pdev->reg_num;
+	d_debugfs_entry[vgt_id][VGT_DEBUGFS_SHADOW_MMIO] = vgt_debugfs_create_blob("shadow_mmio_space",
+			0444,
+			d_per_vgt[vgt_id],
+			p);
+
+	if (!d_debugfs_entry[vgt_id][VGT_DEBUGFS_SHADOW_MMIO])
+		printk(KERN_ERR "vGT(%d): failed to create debugfs node: shadow_mmio_space\n", vgt_id);
+	else
+		printk("vGT(%d): create debugfs node: shadow_mmio_space\n", vgt_id);
+
+	/* virtual gtt space dump */
+	p = &vgt_debugfs_data[vgt_id][VGT_DEBUGFS_VIRTUAL_GTT];
+	p->array = (u32 *)(vgt->gtt.ggtt_mm->virtual_page_table);
+	p->elements = 2* SIZE_1MB;
+	d_debugfs_entry[vgt_id][VGT_DEBUGFS_VIRTUAL_GTT] =
+		vgt_debugfs_create_blob("virtual_gtt_space",
+			0444,
+			d_per_vgt[vgt_id],
+			p);
+
+	if (!d_debugfs_entry[vgt_id][VGT_DEBUGFS_VIRTUAL_GTT])
+		printk(KERN_ERR "vGT(%d): failed to create debugfs node: "
+				"virtual_mmio_space\n", vgt_id);
+	else
+		printk("vGT(%d): create debugfs node: virtual_mmio_space\n", vgt_id);
+	/* end of virtual gtt space dump */
+
+	d_debugfs_entry[vgt_id][VGT_DEBUGFS_FB_FORMAT] = debugfs_create_file("frame_buffer_format",
+			0444, d_per_vgt[vgt_id], vgt, &fbinfo_fops);
+
+	if (!d_debugfs_entry[vgt_id][VGT_DEBUGFS_FB_FORMAT])
+		printk(KERN_ERR "vGT(%d): failed to create debugfs node: frame_buffer_format\n", vgt_id);
+	else
+		printk("vGT(%d): create debugfs node: frame_buffer_format\n", vgt_id);
+
+	d_debugfs_entry[vgt_id][VGT_DEBUGFS_DPY_INFO] = debugfs_create_file("dpyinfo",
+			0444, d_per_vgt[vgt_id], vgt, &virt_dpyinfo_fops);
+
+	if (!d_debugfs_entry[vgt_id][VGT_DEBUGFS_FB_FORMAT])
+		printk(KERN_ERR "vGT(%d): failed to create debugfs node: frame_buffer_format\n", vgt_id);
+	else
+		printk("vGT(%d): create debugfs node: frame_buffer_format\n", vgt_id);
+
+	/* perf vm perfermance statistics */
+	perf_dir_entry = debugfs_create_dir("perf", d_per_vgt[vgt_id]);
+	if (!perf_dir_entry)
+		printk(KERN_ERR "vGT(%d): failed to create debugfs directory: perf\n", vgt_id);
+	else {
+		debugfs_create_u64_node ("schedule_in_time", 0444, perf_dir_entry, &(vgt->stat.schedule_in_time));
+		debugfs_create_u64_node ("allocated_cycles", 0444, perf_dir_entry, &(vgt->stat.allocated_cycles));
+		//debugfs_create_u64_node ("used_cycles", 0444, perf_dir_entry, &(vgt->stat.used_cycles));
+
+		debugfs_create_u64_node ("gtt_mmio_rcnt", 0444, perf_dir_entry, &(vgt->stat.gtt_mmio_rcnt));
+		debugfs_create_u64_node ("gtt_mmio_wcnt", 0444, perf_dir_entry, &(vgt->stat.gtt_mmio_wcnt));
+		debugfs_create_u64_node ("gtt_mmio_wcycles", 0444, perf_dir_entry, &(vgt->stat.gtt_mmio_wcycles));
+		debugfs_create_u64_node ("gtt_mmio_rcycles", 0444, perf_dir_entry, &(vgt->stat.gtt_mmio_rcycles));
+		debugfs_create_u64_node ("mmio_rcnt", 0444, perf_dir_entry, &(vgt->stat.mmio_rcnt));
+		debugfs_create_u64_node ("mmio_wcnt", 0444, perf_dir_entry, &(vgt->stat.mmio_wcnt));
+		debugfs_create_u64_node ("mmio_wcycles", 0444, perf_dir_entry, &(vgt->stat.mmio_wcycles));
+		debugfs_create_u64_node ("mmio_rcycles", 0444, perf_dir_entry, &(vgt->stat.mmio_rcycles));
+		debugfs_create_u64_node ("ring_mmio_rcnt", 0444, perf_dir_entry, &(vgt->stat.ring_mmio_rcnt));
+		debugfs_create_u64_node ("ring_mmio_wcnt", 0444, perf_dir_entry, &(vgt->stat.ring_mmio_wcnt));
+		debugfs_create_u64_node ("ring_tail_mmio_wcnt", 0444, perf_dir_entry, &(vgt->stat.ring_tail_mmio_wcnt));
+		debugfs_create_u64_node ("ring_tail_mmio_wcycles", 0444, perf_dir_entry, &(vgt->stat.ring_tail_mmio_wcycles));
+		debugfs_create_u64_node ("total_cmds", 0444, perf_dir_entry, &(vgt->total_cmds));
+		debugfs_create_u64_node ("vring_scan_cnt", 0444, perf_dir_entry, &(vgt->stat.vring_scan_cnt));
+		debugfs_create_u64_node ("vring_scan_cycles", 0444, perf_dir_entry, &(vgt->stat.vring_scan_cycles));
+		debugfs_create_u64_node ("ppgtt_wp_cnt", 0444, perf_dir_entry, &(vgt->stat.ppgtt_wp_cnt));
+		debugfs_create_u64_node ("ppgtt_wp_cycles", 0444, perf_dir_entry, &(vgt->stat.ppgtt_wp_cycles));
+		debugfs_create_u64_node ("skip_bb_cnt", 0444, perf_dir_entry, &(vgt->stat.skip_bb_cnt));
+
+		/* cmd statistics for ring/batch buffers */
+		cmdstat_dir_entry = debugfs_create_dir("ring", perf_dir_entry);
+		if (!cmdstat_dir_entry)
+			printk(KERN_ERR "vGT(%d): failed to create debugfs directory: ringbuffer\n", vgt_id);
+		else
+			/* for each ring */
+			for (i = 0; i < pdev->max_engines; i++)
+				vgt_create_cmdstat_per_ring(vgt, i, cmdstat_dir_entry);
+	}
+
+	return 0;
+}
+
+/* debugfs_remove_recursive has no return value, this fuction
+ * also return nothing */
+void vgt_destroy_debugfs(struct vgt_device *vgt)
+{
+	int vgt_id = vgt->vgt_id;
+
+	if(!d_per_vgt[vgt_id])
+		return;
+
+	debugfs_remove_recursive(d_per_vgt[vgt_id]);
+	d_per_vgt[vgt_id] = NULL;
+}
+
+void vgt_release_debugfs(void)
+{
+	if (!d_vgt_debug)
+		return;
+
+	debugfs_remove_recursive(d_vgt_debug);
+}
diff --git a/drivers/gpu/drm/i915/vgt/dev.c b/drivers/gpu/drm/i915/vgt/dev.c
new file mode 100644
index 0000000..65b3281
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/dev.c
@@ -0,0 +1,213 @@
+/*
+ * Per-instance device node
+ *
+ * This is used for userland program to access MMIO of each vgt
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of Version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#include <linux/module.h>
+#include <linux/device.h>
+#include <linux/mm.h>
+#include <linux/fs.h>
+#include "vgt.h"
+
+static int vgt_mmio_dev_pgfault(struct vm_area_struct *vma, struct vm_fault *vmf)
+{
+	struct file *vm_file = vma->vm_file;
+	struct vgt_device *vgt = vm_file->private_data;
+	struct page *page = NULL;
+	unsigned long offset = 0;
+	void *req_addr;
+
+	offset = (vmf->pgoff + vma->vm_pgoff) << PAGE_SHIFT;
+
+	if(offset >= VGT_MMIO_SPACE_SZ)
+		return -EACCES;
+
+	req_addr = vgt_vreg(vgt, offset);
+	page = vmalloc_to_page(req_addr);
+
+	ASSERT(page);
+
+	get_page(page);
+	vmf->page = page;
+
+	return 0;
+}
+
+static const struct vm_operations_struct vgt_mmio_dev_vm_ops = {
+	.fault = vgt_mmio_dev_pgfault,
+};
+
+static int vgt_mmio_dev_release(struct inode *inode, struct file *filp)
+{
+	return 0;
+}
+
+static int vgt_mmio_dev_mmap(struct file *filp, struct vm_area_struct *vma)
+{
+	/* Since
+	 * 1) remap_pfn_range() can only be used
+	 *	  for memory allocated by kmalloc
+	 * 2) Most of the time, user may only access part
+	 *    of vreg space, so page fault handling for
+	 *    page requested is more efficient
+	 */
+	vma->vm_ops = &vgt_mmio_dev_vm_ops;
+	/* mark the page as read-only */
+	pgprot_val(vma->vm_page_prot) &= ~_PAGE_RW;
+	return 0;
+}
+
+static int vgt_mmio_dev_open(struct inode *inode, struct file *filp)
+{
+	unsigned int vgt_id = iminor(inode);
+	ASSERT(vgt_id < VGT_MAX_VMS);
+
+	if (!(default_device.device[vgt_id]))
+		return -ENODEV;
+
+	/* point to device(data) */
+	filp->private_data = default_device.device[vgt_id];
+	//ASSERT(filp->private_data == vgt_dom0);
+
+	return 0;
+}
+
+static struct file_operations vgt_mmio_dev_fops = {
+	.owner = THIS_MODULE,
+	.open = vgt_mmio_dev_open,
+	.mmap = vgt_mmio_dev_mmap,
+	.llseek = no_llseek,
+	.release = vgt_mmio_dev_release,
+};
+
+
+int vgt_init_mmio_device(struct pgt_device *pdev)
+{
+	int retval, devid;
+	struct vgt_mmio_dev *mmio_dev = NULL;
+
+	if ((retval = alloc_chrdev_region(&devid, 0,
+			VGT_MAX_VMS, VGT_MMIO_DEV_NAME)) < 0) {
+		vgt_err("failed to alloc chrdev region!\n");
+		return retval;
+	}
+
+	if ((mmio_dev = vmalloc(sizeof(struct vgt_mmio_dev))) == NULL) {
+		vgt_err("failed to alloc struct vgt_mmio_dev!\n");
+		return -ENOMEM;
+	}
+
+	mmio_dev->devid_major = MAJOR(devid);
+	mmio_dev->dev_name = VGT_MMIO_DEV_NAME;
+	pdev->mmio_dev = mmio_dev;
+
+	cdev_init(&mmio_dev->cdev, &vgt_mmio_dev_fops);
+
+	if ((retval = cdev_add(&mmio_dev->cdev, devid, VGT_MAX_VMS)) < 0) {
+		vgt_err("failed to add char device vgt_mmio_dev!\n");
+		goto free_chrdev_region;
+	}
+
+	mmio_dev->class = class_create(THIS_MODULE,
+			mmio_dev->dev_name);
+	if (IS_ERR_OR_NULL(mmio_dev->class)) {
+		vgt_err("mmio device class creation failed!\n");
+		retval = -EINVAL;
+		goto delete_cdev;
+	}
+
+	return 0;
+
+delete_cdev:
+	cdev_del(&mmio_dev->cdev);
+free_chrdev_region:
+	unregister_chrdev_region(
+			MKDEV(mmio_dev->devid_major, 0),
+			VGT_MAX_VMS);
+	vfree(mmio_dev);
+
+	return retval;
+}
+
+int vgt_create_mmio_dev(struct vgt_device *vgt)
+{
+	int vgt_id = vgt->vgt_id;
+	struct vgt_mmio_dev *mmio_dev = vgt->pdev->mmio_dev;
+	struct device *devnode;
+
+	ASSERT(mmio_dev->class);
+	devnode = device_create(mmio_dev->class,
+			NULL,
+			MKDEV(mmio_dev->devid_major, vgt_id),
+			NULL,
+			"%s%d",
+			mmio_dev->dev_name,
+			vgt_id);
+	if (IS_ERR_OR_NULL(devnode))
+		return -EINVAL;
+
+	mmio_dev->devnode[vgt_id] = devnode;
+
+	return 0;
+}
+
+void vgt_destroy_mmio_dev(struct vgt_device *vgt)
+{
+	struct vgt_mmio_dev *mmio_dev = vgt->pdev->mmio_dev;
+	int vgt_id = vgt->vgt_id;
+
+	if (!mmio_dev)
+		return;
+
+	if (mmio_dev->devnode[vgt_id] && mmio_dev->class) {
+		device_destroy(mmio_dev->class,
+				MKDEV(mmio_dev->devid_major, vgt_id));
+		mmio_dev->devnode[vgt_id] = NULL;
+	}
+}
+
+void vgt_cleanup_mmio_dev(struct pgt_device *pdev)
+{
+	struct vgt_mmio_dev *mmio_dev = pdev->mmio_dev;
+	int id;
+
+	if (!pdev->mmio_dev)
+		return;
+
+	for (id = 0; id < VGT_MAX_VMS; id++)
+		if (pdev->device[id])
+			vgt_destroy_mmio_dev(pdev->device[id]);
+
+	if (mmio_dev->class) {
+		class_destroy(mmio_dev->class);
+		mmio_dev->class = NULL;
+	}
+
+	cdev_del(&mmio_dev->cdev);
+
+	if (mmio_dev->devid_major != -EINVAL) {
+		unregister_chrdev_region(
+				MKDEV(mmio_dev->devid_major, 0),
+				VGT_MAX_VMS);
+		mmio_dev->devid_major = -EINVAL;
+	}
+
+	vfree(mmio_dev);
+	pdev->mmio_dev = NULL;
+}
diff --git a/drivers/gpu/drm/i915/vgt/devtable.h b/drivers/gpu/drm/i915/vgt/devtable.h
new file mode 100644
index 0000000..3389d46
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/devtable.h
@@ -0,0 +1,147 @@
+/*
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef _VGT_DEVTABLE_H
+#define _VGT_DEVTABLE_H
+
+static inline int _is_sandybridge(int devid)
+{
+	int ret = 0;
+
+	switch (devid) {
+	case 0x0102:
+	case 0x0112:
+	case 0x0122:
+	case 0x0106:
+	case 0x0116:
+	case 0x0126:
+	case 0x010A:
+		ret = 1;
+		break;
+	default:
+		break;
+	}
+	return ret;
+}
+
+static inline int _is_ivybridge(int devid)
+{
+	int ret = 0;
+
+	switch (devid) {
+	case 0x0156:
+	case 0x0166:
+	case 0x0152:
+	case 0x0162:
+	case 0x015a:
+	case 0x016a:
+		ret = 1;
+		break;
+	default:
+		break;
+	}
+	return ret;
+}
+
+static inline int _is_haswell(int devid)
+{
+	int ret = 0;
+
+	switch (devid) {
+	case 0x0400:
+	case 0x0402:
+	case 0x0404:
+	case 0x0406:
+	case 0x0408:
+	case 0x040a:
+	case 0x0412:
+	case 0x0416:
+	case 0x041a:
+	case 0x0422:
+	case 0x0426:
+	case 0x042a:
+	case 0x0a02:
+	case 0x0a06:
+	case 0x0a0a:
+	case 0x0a12:
+	case 0x0a16:
+	case 0x0a1a:
+	case 0x0a22:
+	case 0x0a26:
+	case 0x0a2a:
+	case 0x0c02:
+	case 0x0c04:
+	case 0x0c06:
+	case 0x0c0a:
+	case 0x0c12:
+	case 0x0c16:
+	case 0x0c1a:
+	case 0x0c22:
+	case 0x0c26:
+	case 0x0c2a:
+	case 0x0d12:
+	case 0x0d16:
+	case 0x0d1a:
+	case 0x0d22:
+	case 0x0d26:
+	case 0x0d2a:
+	case 0x0d32:
+	case 0x0d36:
+	case 0x0d3a:
+		ret = 1;
+		break;
+	default:
+		break;
+	}
+	return ret;
+}
+
+static inline int _is_broadwell(int devid)
+{
+	switch ((devid >> 4) & 0xf) {
+		case 0:
+		case 1:
+		case 2:
+			break;
+		default:
+			return 0;
+	}
+
+	devid &= ~0xf0;
+
+	switch (devid) {
+		case 0x1602:
+		case 0x1606:
+		case 0x160B:
+		case 0x160E:
+		case 0x160A:
+		case 0x160D:
+			break;
+		default:
+			return 0;
+	}
+
+	return 1;
+}
+
+#endif  /* _VGT_DEVTABLE_H */
diff --git a/drivers/gpu/drm/i915/vgt/display.c b/drivers/gpu/drm/i915/vgt/display.c
new file mode 100644
index 0000000..0599c72
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/display.c
@@ -0,0 +1,1035 @@
+/*
+ * Display context switch
+ *
+ * Copyright 2008 (c) Intel Corporation
+ *   Jesse Barnes <jbarnes@virtuousgeek.org>
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/slab.h>
+#include <linux/delay.h>
+#include "vgt.h"
+
+static void vgt_restore_sreg(struct vgt_device *vgt,unsigned int reg)
+{
+	unsigned int real_reg;
+	if(vgt_map_plane_reg(vgt, reg, &real_reg))
+	{
+		VGT_MMIO_WRITE(vgt->pdev, real_reg, __sreg(vgt, (reg)));
+	}
+
+}
+
+static int vgt_restore_state(struct vgt_device *vgt, enum vgt_pipe pipe)
+{
+#if 0
+	unsigned int pipe_ctrl = VGT_MMIO_READ(vgt->pdev, VGT_PIPECONF(pipe));
+	if (pipe_ctrl & _REGBIT_PIPE_ENABLE) {
+#endif
+		vgt_dbg (VGT_DBG_DPY, "start to restore pipe %d.\n", pipe + 1);
+		vgt_restore_sreg(vgt, VGT_DSPCNTR(pipe));
+		vgt_restore_sreg(vgt, VGT_DSPSTRIDE(pipe));
+		vgt_restore_sreg(vgt, VGT_DSPSURF(pipe));
+		vgt_restore_sreg(vgt, VGT_DSPTILEOFF(pipe));
+		vgt_restore_sreg(vgt, VGT_DSPLINOFF(pipe));
+
+		vgt_restore_sreg(vgt, VGT_CURPOS(pipe));
+		vgt_restore_sreg(vgt, VGT_CURCNTR(pipe));
+		vgt_restore_sreg(vgt, VGT_CURBASE(pipe));
+		vgt_dbg (VGT_DBG_DPY, "finished pipe %d restore.\n", pipe + 1);
+#if 0
+	} else {
+		vgt_dbg (VGT_DBG_DPY, "pipe %d is not enabled.\n", pipe + 1);
+	}
+#endif
+	return 0;
+}
+
+static int wait_for_vblank_atomic(struct pgt_device *pdev, enum vgt_pipe pipe)
+{
+	int ret;
+	unsigned int frmcnt_mmio = VGT_PIPE_FRMCOUNT(pipe);
+	vgt_reg_t frmcnt = VGT_MMIO_READ(pdev, frmcnt_mmio);
+
+	ret = wait_for_atomic((VGT_MMIO_READ(pdev, frmcnt_mmio) != frmcnt),
+				VGT_VBLANK_TIMEOUT);
+	if (ret == -ETIMEDOUT) {
+		vgt_warn("pipe-%d: Timeout for waiting vblank!\n", pipe);
+	}
+	return ret;
+}
+
+static int wait_for_vblanks_atomic(struct pgt_device *pdev)
+{
+	int ret = 0;
+	enum vgt_pipe pipe;
+
+	for (pipe = PIPE_A; (pipe < I915_MAX_PIPES) && !ret; ++ pipe) {
+		vgt_reg_t pipeconf = VGT_MMIO_READ(pdev, VGT_PIPECONF(pipe));
+		if (pipeconf & _REGBIT_PIPE_ENABLE) {
+			ret = wait_for_vblank_atomic(pdev, pipe);
+		}
+	}
+	return ret;
+}
+
+int prepare_for_display_switch(struct pgt_device *pdev)
+{
+	int ret = 0;
+
+	if (!(idle_render_engine(pdev, RING_BUFFER_RCS) &&
+		idle_render_engine(pdev, RING_BUFFER_BCS))) {
+		vgt_warn("vGT: Ring RCS or Ring BCS is busy "
+			"in display switch!\n");
+		ret = -1;
+	}
+
+	if (!ret) {
+		ret = wait_for_vblanks_atomic(pdev);
+		if (ret)
+			vgt_warn("Failed to get vblank in display switch!\n");
+	}
+
+	return ret;
+}
+
+/*
+ * Do foreground vm switch.
+ */
+void do_vgt_fast_display_switch(struct pgt_device *pdev)
+{
+	struct vgt_device *to_vgt = pdev->next_foreground_vm;
+	enum vgt_pipe pipe;
+
+	vgt_dbg(VGT_DBG_DPY, "vGT: doing display switch: from %p to %p\n",
+			current_foreground_vm(pdev), to_vgt);
+
+	ASSERT(fastpath_dpy_switch);
+	ASSERT(spin_is_locked(&pdev->lock));
+
+	for (pipe = PIPE_A; pipe < I915_MAX_PIPES; ++ pipe) {
+		vgt_restore_state(to_vgt, pipe);
+		if (_PRI_PLANE_ENABLE & __vreg(to_vgt, VGT_DSPCNTR(pipe))) {
+			set_panel_fitting(to_vgt, pipe);
+		}
+	}
+
+	current_foreground_vm(pdev) = to_vgt;
+}
+
+static inline int get_event_and_edid_info(vgt_hotplug_cmd_t cmd,
+				enum vgt_event_type *pevent,
+				enum vgt_port_type *pedid_idx)
+{
+	int ret = 0;
+
+	switch(cmd.port_sel) {
+	case 0:
+		*pedid_idx = PORT_E;
+		*pevent = CRT_HOTPLUG;
+		break;
+	case 1:
+		*pedid_idx = I915_MAX_PORTS;
+		*pevent = EVENT_MAX;
+		printk("vGT: No support for hot plug type: DP_A!\n");
+		ret = -EINVAL;
+		break;
+	case 2:
+		*pedid_idx = PORT_B;
+		*pevent = DP_B_HOTPLUG;
+		break;
+	case 3:
+		*pedid_idx = PORT_C;
+		*pevent = DP_C_HOTPLUG;
+		break;
+	case 4:
+		*pedid_idx = PORT_D;
+		*pevent = DP_D_HOTPLUG;
+		break;
+	default:
+		*pedid_idx = I915_MAX_PORTS;
+		*pevent = EVENT_MAX;
+		printk("vGT: Not supported hot plug type: 0x%x!\n",
+			cmd.port_sel);
+		ret = -EINVAL;
+		break;
+	}
+	return ret;
+}
+
+void vgt_trigger_display_hot_plug(struct pgt_device *dev,
+		vgt_hotplug_cmd_t  hotplug_cmd)
+{
+	int i;
+	enum vgt_event_type event = EVENT_MAX;
+	enum vgt_port_type port_idx = VGT_PORT_MAX;
+	int cpu;
+
+	if (get_event_and_edid_info(hotplug_cmd, &event, &port_idx) < 0)
+		return;
+
+	vgt_lock_dev(dev, cpu);
+	for (i = 0; i < VGT_MAX_VMS; ++ i) {
+		struct vgt_device *vgt = dev->device[i];
+
+		if (!vgt)
+			continue;
+
+		if (hotplug_cmd.vmid != HOTPLUG_VMID_FOR_ALL_VMS) {
+			if (vgt != vmid_2_vgt_device(hotplug_cmd.vmid))
+				continue;
+		}
+
+		if (is_current_display_owner(vgt)) {
+			continue;
+		}
+
+		if (hotplug_cmd.action != 0x1) {
+			/* pull out */
+			vgt_clear_port(vgt, port_idx);
+		}
+
+		vgt_update_monitor_status(vgt);
+		vgt_trigger_virtual_event(vgt, event);
+	}
+
+	vgt_unlock_dev(dev, cpu);
+	return;
+}
+
+DECLARE_BITMAP(vgt_uevents_bitmap, UEVENT_MAX);
+extern struct kobject *vgt_ctrl_kobj;
+
+bool vgt_default_uevent_handler(struct vgt_uevent_info *uevent_entry, struct pgt_device *pdev)
+{
+	int retval;
+	retval = kobject_uevent_env(vgt_ctrl_kobj, uevent_entry->action, uevent_entry->env_var_table);
+	if (retval == 0)
+		return true;
+	else
+		return false;
+}
+
+bool vgt_hotplug_uevent_handler(enum vgt_uevent_type event,
+			struct vgt_uevent_info *uevent_entry,
+			struct pgt_device *pdev)
+{
+	return vgt_default_uevent_handler(uevent_entry, pdev);
+}
+
+static bool vgt_dpy_stat_uevent_handler(enum vgt_uevent_type event,
+			struct vgt_uevent_info *uevent_entry,
+			struct pgt_device *pdev)
+{
+	/* Add vmid */
+	int retval;
+	char vmid_str[20];
+	retval = snprintf(vmid_str, 20, "VMID=%d", uevent_entry->vm_id);
+	uevent_entry->env_var_table[1] = vmid_str;
+	return vgt_default_uevent_handler(uevent_entry, pdev);
+}
+
+static bool vgt_dpy_detect_uevent_handler(enum vgt_uevent_type event,
+			struct vgt_uevent_info *uevent_entry,
+			struct pgt_device *pdev)
+{
+	return vgt_default_uevent_handler(uevent_entry, pdev);
+}
+
+/*
+ When you add new uevents or add new environmental variable,
+ you should following rules:
+ Now you can at most define VGT_MAX_UEVENT_VARS environmental
+ variables with the form like "VAR=VALUE", all the
+ pointer of string are stored in env_var_table (below).
+struct vgt_uevent_info {
+	...
+	char *env_var_table[VGT_MAX_UEVENT_VARS];
+	...
+};
+ You should place a NULL as the termination of variable
+ definition, or function add_uevent_var() in line 219
+ of lib/kobject_uevent.c will fail.
+*/
+
+static struct vgt_uevent_info vgt_default_uevent_info_table[UEVENT_MAX] = {
+	{"CRT insert", -1, KOBJ_ADD, {"CRT_INSERT=1", NULL}, vgt_hotplug_uevent_handler},
+	{"CRT remove", -1, KOBJ_REMOVE, {"CRT_REMOVE=1", NULL}, vgt_hotplug_uevent_handler},
+	{"PORT A insert", -1, KOBJ_ADD, {"PORT_A_INSERT=1", NULL}, vgt_hotplug_uevent_handler},
+	{"PORT A remove", -1,KOBJ_REMOVE, {"PORT_A_REMOVE=1", NULL}, vgt_hotplug_uevent_handler},
+	{"PORT B insert", -1, KOBJ_ADD, {"PORT_B_INSERT=1", NULL}, vgt_hotplug_uevent_handler},
+	{"PORT B remove", -1, KOBJ_REMOVE, {"PORT_B_REMOVE=1", NULL}, vgt_hotplug_uevent_handler},
+	{"PORT C insert", -1, KOBJ_ADD, {"PORT_C_INSERT=1", NULL}, vgt_hotplug_uevent_handler},
+	{"PORT C remove", -1, KOBJ_REMOVE, {"PORT_C_REMOVE=1", NULL}, vgt_hotplug_uevent_handler},
+	{"PORT D insert", -1, KOBJ_ADD, {"PORT_D_INSERT=1", NULL}, vgt_hotplug_uevent_handler},
+	{"PORT D remove", -1, KOBJ_REMOVE, {"PORT_D_REMOVE=1", NULL}, vgt_hotplug_uevent_handler},
+	{"VGT enable VGA mode", -1, KOBJ_ADD, {"VGT_ENABLE_VGA=1", NULL, NULL}, vgt_dpy_stat_uevent_handler},
+	{"VGT disable VGA mode", -1, KOBJ_ADD, {"VGT_ENABLE_VGA=0", NULL, NULL}, vgt_dpy_stat_uevent_handler},
+	{"VGT display ready", -1, KOBJ_ADD, {"VGT_DISPLAY_READY=1", NULL, NULL}, vgt_dpy_stat_uevent_handler},
+	{"VGT display unready", -1, KOBJ_ADD, {"VGT_DISPLAY_READY=0", NULL, NULL}, vgt_dpy_stat_uevent_handler},
+	{"VGT detect PORT A", -1, KOBJ_ADD, {"VGT_DETECT_PORT_A=1", NULL, NULL}, vgt_dpy_detect_uevent_handler},
+	{"VGT detect PORT B", -1, KOBJ_ADD, {"VGT_DETECT_PORT_B=1", NULL, NULL}, vgt_dpy_detect_uevent_handler},
+	{"VGT detect PORT C", -1, KOBJ_ADD, {"VGT_DETECT_PORT_C=1", NULL, NULL}, vgt_dpy_detect_uevent_handler},
+	{"VGT detect PORT D", -1, KOBJ_ADD, {"VGT_DETECT_PORT_D=1", NULL, NULL}, vgt_dpy_detect_uevent_handler},
+	{"VGT detect PORT E", -1, KOBJ_ADD, {"VGT_DETECT_PORT_E=1", NULL, NULL}, vgt_dpy_detect_uevent_handler},
+};
+
+void vgt_set_uevent(struct vgt_device *vgt, enum vgt_uevent_type uevent)
+{
+	struct vgt_uevent_info *entry;
+
+	ASSERT(uevent < UEVENT_MAX);
+
+	entry = &vgt_default_uevent_info_table[uevent];
+	entry->vm_id = vgt->vm_id;
+
+	set_bit(uevent, vgt_uevents_bitmap);
+}
+
+void vgt_signal_uevent(struct pgt_device *pdev)
+{
+	struct vgt_uevent_info *info_entry;
+	bool rc;
+	int bit;
+
+	for_each_set_bit(bit, vgt_uevents_bitmap, UEVENT_MAX) {
+		clear_bit(bit, vgt_uevents_bitmap);
+
+		info_entry = &vgt_default_uevent_info_table[bit];
+
+		ASSERT(info_entry);
+		ASSERT(info_entry->vgt_uevent_handler);
+
+		rc = info_entry->vgt_uevent_handler(bit, info_entry, pdev);
+		if (rc == false)
+			printk("%s: %d: vGT: failed to send uevent [%s]!\n",
+					__func__, __LINE__, info_entry->uevent_name);
+	}
+}
+
+void vgt_hotplug_udev_notify_func(struct work_struct *work)
+{
+	struct hotplug_work *hpd_work = (struct hotplug_work *)work;
+	struct pgt_device *pdev = container_of(hpd_work, struct pgt_device, hpd_work);
+	int bit;
+
+	mutex_lock(&hpd_work->hpd_mutex);
+	for_each_set_bit(bit, hpd_work->hotplug_uevent, UEVENT_MAX) {
+		struct vgt_uevent_info *info_entry;
+		clear_bit(bit, hpd_work->hotplug_uevent);
+		info_entry = &vgt_default_uevent_info_table[bit];
+		vgt_default_uevent_handler(info_entry, pdev);
+	}
+	mutex_unlock(&hpd_work->hpd_mutex);
+}
+
+void vgt_update_monitor_status(struct vgt_device *vgt)
+{
+	if (is_current_display_owner(vgt))
+		return;
+
+	__vreg(vgt, _REG_SDEISR) &= ~(_REGBIT_DP_B_HOTPLUG |
+					_REGBIT_DP_C_HOTPLUG |
+					_REGBIT_DP_D_HOTPLUG);
+
+	if (dpy_has_monitor_on_port(vgt, PORT_B)) {
+		vgt_dbg(VGT_DBG_DPY, "enable B port monitor\n");
+		__vreg(vgt, _REG_SDEISR) |= _REGBIT_DP_B_HOTPLUG;
+	}
+	if (dpy_has_monitor_on_port(vgt, PORT_C)) {
+		vgt_dbg(VGT_DBG_DPY, "enable C port monitor\n");
+		__vreg(vgt, _REG_SDEISR) |= _REGBIT_DP_C_HOTPLUG;
+	}
+	if (dpy_has_monitor_on_port(vgt, PORT_D)) {
+		vgt_dbg(VGT_DBG_DPY, "enable D port monitor\n");
+		__vreg(vgt, _REG_SDEISR) |= _REGBIT_DP_D_HOTPLUG;
+	}
+	if (dpy_has_monitor_on_port(vgt, PORT_A))
+		__vreg(vgt, _REG_DDI_BUF_CTL_A) |= _DDI_BUFCTL_DETECT_MASK;
+}
+
+enum vgt_pipe get_edp_input(uint32_t wr_data)
+{
+	enum vgt_pipe pipe = I915_MAX_PIPES;
+
+	if ((_REGBIT_TRANS_DDI_FUNC_ENABLE & wr_data) == 0) {
+		return I915_MAX_PIPES;
+	}
+
+	switch (wr_data & _REGBIT_TRANS_DDI_EDP_INPUT_MASK) {
+		case _REGBIT_TRANS_DDI_EDP_INPUT_A_ON:
+		case _REGBIT_TRANS_DDI_EDP_INPUT_A_ONOFF:
+			pipe = PIPE_A;
+			break;
+		case _REGBIT_TRANS_DDI_EDP_INPUT_B_ONOFF:
+			pipe = PIPE_B;
+			break;
+		case _REGBIT_TRANS_DDI_EDP_INPUT_C_ONOFF:
+			pipe = PIPE_C;
+			break;
+		default:
+			pipe = I915_MAX_PIPES;
+	}
+	return pipe;
+}
+
+enum vgt_pipe get_pipe(unsigned int reg, uint32_t wr_data)
+{
+	enum vgt_pipe pipe = I915_MAX_PIPES;
+
+	if (reg == _REG_TRANS_DDI_FUNC_CTL_A) {
+		pipe = PIPE_A;
+	}
+	else if (reg == _REG_TRANS_DDI_FUNC_CTL_B) {
+		pipe = PIPE_B;
+	}
+	else if (reg == _REG_TRANS_DDI_FUNC_CTL_C) {
+		pipe = PIPE_C;
+	}else if (reg == _REG_TRANS_DDI_FUNC_CTL_EDP) {
+		pipe = get_edp_input (wr_data);
+	}
+
+	return pipe;
+}
+
+static void vgt_update_irq_reg(struct vgt_device *vgt)
+{
+	if (IS_PREBDW(vgt->pdev)) {
+		recalculate_and_update_ier(vgt->pdev, _REG_DEIER);
+		recalculate_and_update_imr(vgt->pdev, _REG_DEIMR);
+	} else {
+		recalculate_and_update_ier(vgt->pdev, _REG_DE_PIPE_IER(PIPE_A));
+		recalculate_and_update_ier(vgt->pdev, _REG_DE_PIPE_IER(PIPE_B));
+		recalculate_and_update_ier(vgt->pdev, _REG_DE_PIPE_IER(PIPE_C));
+
+		recalculate_and_update_imr(vgt->pdev, _REG_DE_PIPE_IMR(PIPE_A));
+		recalculate_and_update_imr(vgt->pdev, _REG_DE_PIPE_IMR(PIPE_B));
+		recalculate_and_update_imr(vgt->pdev, _REG_DE_PIPE_IMR(PIPE_C));
+	}
+
+	return;
+}
+
+bool rebuild_pipe_mapping(struct vgt_device *vgt, unsigned int reg, uint32_t new_data, uint32_t old_data)
+{
+	vgt_reg_t hw_value;
+	int i = 0;
+
+	enum vgt_pipe virtual_pipe = I915_MAX_PIPES;
+	enum vgt_pipe physical_pipe = I915_MAX_PIPES;
+
+	if (vgt->vm_id == 0) {
+		return true;
+	}
+
+	virtual_pipe = get_pipe(reg, new_data);
+
+	/*disable pipe case*/
+	if ((_REGBIT_TRANS_DDI_FUNC_ENABLE & new_data) == 0) {
+		if (reg == _REG_TRANS_DDI_FUNC_CTL_EDP) {
+			/*for disable case, we need to get edp input from old value
+			since the new data does not contain the edp input*/
+			virtual_pipe = get_edp_input(old_data);
+		}
+		if (virtual_pipe != I915_MAX_PIPES) {
+			vgt_set_pipe_mapping(vgt, virtual_pipe, I915_MAX_PIPES);
+			vgt_update_irq_reg(vgt);
+			vgt_dbg(VGT_DBG_DPY, "vGT: delete pipe mapping %x\n", virtual_pipe);
+			if (vgt_has_pipe_enabled(vgt, virtual_pipe))
+				vgt_update_frmcount(vgt, virtual_pipe);
+			vgt_calculate_frmcount_delta(vgt, virtual_pipe);
+		}
+		return true;
+	}
+
+	/*enable pipe case*/
+	ASSERT((reg == _REG_TRANS_DDI_FUNC_CTL_EDP) ||
+			(new_data & _REGBIT_TRANS_DDI_PORT_MASK));
+
+	if (reg == _REG_TRANS_DDI_FUNC_CTL_EDP) {
+		// In such case, it is virtual PORT_A mapping to physical PORT_A
+		hw_value = VGT_MMIO_READ(vgt->pdev, _REG_TRANS_DDI_FUNC_CTL_EDP);
+		if (_REGBIT_TRANS_DDI_FUNC_ENABLE & hw_value)
+			physical_pipe = get_edp_input(hw_value);
+	} else {
+		enum vgt_port vport, vport_override;
+		vport = (new_data & _REGBIT_TRANS_DDI_PORT_MASK) >> _TRANS_DDI_PORT_SHIFT;
+		vport_override = vgt->ports[vport].port_override;
+		if (vport_override == I915_MAX_PORTS) {
+			vgt_warn("Unexpected driver behavior to enable TRANS_DDI"
+					" for not ready port!!\n");
+			physical_pipe = I915_MAX_PIPES;
+		} else if (vport_override == PORT_A) {
+			hw_value = VGT_MMIO_READ(vgt->pdev, _REG_TRANS_DDI_FUNC_CTL_EDP);
+			if (_REGBIT_TRANS_DDI_FUNC_ENABLE & hw_value)
+				physical_pipe = get_edp_input(hw_value);
+								
+		} else {
+			for (i = 0; i <= TRANSCODER_C; i++) {
+				enum vgt_port pport;
+				hw_value = VGT_MMIO_READ(vgt->pdev, _VGT_TRANS_DDI_FUNC_CTL(i));
+				pport = (hw_value & _REGBIT_TRANS_DDI_PORT_MASK) >>
+						_TRANS_DDI_PORT_SHIFT;
+
+				printk("%s: Enable. pport = %d, vport = %d, "
+					"hw_value = 0x%08x, new_data = 0x%08x\n",
+			       		__FUNCTION__, pport, vport, hw_value, new_data);
+
+				if (!(_REGBIT_TRANS_DDI_FUNC_ENABLE & hw_value)) {
+					continue;
+				}
+
+				if (vport_override == pport) {
+					physical_pipe = i;
+					break;
+				}
+			}
+		}
+	}
+
+	ASSERT(virtual_pipe != I915_MAX_PIPES);
+	vgt_set_pipe_mapping(vgt, virtual_pipe, physical_pipe);
+	vgt_dbg(VGT_DBG_DPY, "vGT: add pipe mapping  %x - > %x \n", virtual_pipe, physical_pipe);
+	vgt_update_irq_reg(vgt);
+	if (vgt_has_pipe_enabled(vgt, virtual_pipe))
+		vgt_update_frmcount(vgt, virtual_pipe);
+	vgt_calculate_frmcount_delta(vgt, virtual_pipe);
+
+	if (current_foreground_vm(vgt->pdev) == vgt) {
+		vgt_restore_state(vgt, virtual_pipe);
+	}
+
+	return true;
+}
+
+bool update_pipe_mapping(struct vgt_device *vgt, unsigned int physical_reg, uint32_t physical_wr_data)
+{
+	int i = 0;
+	uint32_t virtual_wr_data;
+	enum vgt_pipe virtual_pipe = I915_MAX_PIPES;
+	enum vgt_pipe physical_pipe = I915_MAX_PIPES;
+	enum vgt_port pport;
+
+	physical_pipe = get_pipe(physical_reg, physical_wr_data);
+
+	/*disable pipe case*/
+	if ((_REGBIT_TRANS_DDI_FUNC_ENABLE & physical_wr_data) == 0) {
+		for (i = 0; i < I915_MAX_PIPES; i ++) {
+			if(vgt->pipe_mapping[i] == physical_pipe) {
+				vgt_set_pipe_mapping(vgt, i, I915_MAX_PIPES);
+				vgt_dbg(VGT_DBG_DPY, "vGT: Update mapping: delete pipe %x  \n", i);
+				if (vgt_has_pipe_enabled(vgt, i))
+					vgt_update_frmcount(vgt, i);
+				vgt_calculate_frmcount_delta(vgt, i);
+			}
+		}
+		vgt_update_irq_reg(vgt);
+		return true;
+	}
+
+	/*enable case*/
+	if (physical_reg == _REG_TRANS_DDI_FUNC_CTL_EDP) {
+		pport = PORT_A;
+		if (vgt->ports[PORT_A].port_override == PORT_A) {
+			virtual_pipe = get_edp_input(__vreg(vgt, _REG_TRANS_DDI_FUNC_CTL_EDP));
+		}
+	} else {
+		pport = (physical_wr_data & _REGBIT_TRANS_DDI_PORT_MASK) >> _TRANS_DDI_PORT_SHIFT;
+	}
+
+	for (i = 0; i <= TRANSCODER_C; i++) {
+		enum vgt_port vport, vport_override;
+		virtual_wr_data = __vreg(vgt, _VGT_TRANS_DDI_FUNC_CTL(i));
+		vport = (virtual_wr_data & _REGBIT_TRANS_DDI_PORT_MASK) >>
+				_TRANS_DDI_PORT_SHIFT;
+		vport_override = vgt->ports[vport].port_override;
+
+		printk("%s: Enable. pport = %d, vport = %d\n", __FUNCTION__, pport, vport);
+
+		if (!(_REGBIT_TRANS_DDI_FUNC_ENABLE & virtual_wr_data) ||
+			(vport_override == I915_MAX_PORTS)) {
+			continue;
+		}
+
+		if (vport_override == pport) {
+			virtual_pipe = i;
+			break;
+		}
+	}
+
+	if (virtual_pipe != I915_MAX_PIPES) {
+		vgt_set_pipe_mapping(vgt, virtual_pipe, physical_pipe);
+		vgt_dbg(VGT_DBG_DPY, "vGT: Update pipe mapping  %x - > %x \n", virtual_pipe, physical_pipe);
+		vgt_update_irq_reg(vgt);
+		if (vgt_has_pipe_enabled(vgt, virtual_pipe))
+			vgt_update_frmcount(vgt, virtual_pipe);
+		vgt_calculate_frmcount_delta(vgt, virtual_pipe);
+	}
+
+	if (current_foreground_vm(vgt->pdev) == vgt &&
+		virtual_pipe != I915_MAX_PIPES &&
+		(_PRI_PLANE_ENABLE & VGT_MMIO_READ(vgt->pdev, VGT_DSPCNTR(physical_pipe)))) {
+		vgt_restore_state(vgt, virtual_pipe);
+	}
+
+	return true;
+}
+
+/*
+TODO: 1, program watermark in vgt. 2, make sure dom0 set the max timing for
+each monitor in i915 driver
+*/
+
+bool set_panel_fitting(struct vgt_device *vgt, enum vgt_pipe pipe)
+{
+	unsigned int src_width, src_height;
+	unsigned int target_width, target_height;
+	unsigned int pf_ctl;
+	enum vgt_pipe real_pipe;
+	unsigned int h_total_reg;
+	unsigned int v_total_reg;
+	uint32_t edp_trans_code;
+	uint64_t  plane_wm;
+	uint64_t  sprite_wm;
+	uint64_t  cursor_wm;
+	unsigned int wm_reg;
+	unsigned int wm_value;
+
+	real_pipe = vgt->pipe_mapping[pipe];
+
+	if (!enable_panel_fitting) {
+		vgt_warn("panel fitting function is not enabled!\n");
+		return false;
+	}
+
+	if (real_pipe == I915_MAX_PIPES) {
+		vgt_dbg(VGT_DBG_DPY, "try to set panel fitting before pipe is mapped!\n");
+		return false;
+	}
+	if (((_PRI_PLANE_ENABLE & __vreg(vgt, VGT_DSPCNTR(pipe))) == 0) ||
+		(_PRI_PLANE_ENABLE & VGT_MMIO_READ(vgt->pdev, VGT_DSPCNTR(real_pipe))) == 0) {
+		return false;
+	}
+	src_width = (__vreg(vgt, VGT_PIPESRC(pipe)) & 0xffff0000) >> 16;
+	src_height = __vreg(vgt, VGT_PIPESRC(pipe)) & 0xffff;
+	ASSERT_VM(src_width != 0, vgt);
+	ASSERT_VM(src_height != 0, vgt);
+	src_width += 1;
+	src_height += 1;
+
+	h_total_reg = VGT_HTOTAL(real_pipe);
+	v_total_reg = VGT_VTOTAL(real_pipe);
+
+	edp_trans_code = VGT_MMIO_READ(vgt->pdev, _REG_TRANS_DDI_FUNC_CTL_EDP);
+	if ((_REGBIT_TRANS_DDI_FUNC_ENABLE & edp_trans_code)) {
+		if (real_pipe == get_edp_input(edp_trans_code)) {
+			h_total_reg = _REG_HTOTAL_EDP;
+			v_total_reg = _REG_VTOTAL_EDP;
+		}
+	}
+
+	target_width = VGT_MMIO_READ(vgt->pdev, h_total_reg) & 0xffff;
+	target_height = VGT_MMIO_READ(vgt->pdev, v_total_reg) & 0xffff;
+
+	ASSERT_VM(target_width != 0, vgt);
+	ASSERT_VM(target_height != 0, vgt);
+	target_width += 1;
+	target_height += 1;
+
+	/*fixed panel fitting mode to 3x3 mode, Restriction : A 3x3 capable filter must not be enabled
+		when the pipe horizontal source size is greater than 2048 pixels*/
+	if (IS_HSW(vgt->pdev))
+		pf_ctl =  _REGBIT_PF_FILTER_MED_3x3 | _REGBIT_PF_PIPE_SEL(real_pipe);
+	else /*after BDW the panel fitter is on the pipe, no need to assign.*/
+		pf_ctl =  _REGBIT_PF_FILTER_MED_3x3;
+
+	/*enable panel fitting only when the source mode does not eqaul to the target mode*/
+	if (src_width != target_width || src_height != target_height ) {
+		vgt_dbg(VGT_DBG_DPY, "enable panel fitting for VM %d, pipe %d, src_width:%d, src_height: %d, tgt_width:%d, tgt_height:%d!\n",
+			vgt->vm_id, real_pipe, src_width, src_height, target_width, target_height);
+		pf_ctl = pf_ctl | _REGBIT_PF_ENABLE;
+	} else {
+		vgt_dbg(VGT_DBG_DPY, "disable panel fitting for VM %d, for pipe %d!\n", vgt->vm_id, real_pipe);
+	}
+
+	/* we need to increase Water Mark in down scaling case */
+	if (src_width > target_width || src_height > target_height) {
+		wm_reg = real_pipe == PIPE_A ? _REG_WM0_PIPEA_ILK :
+			(real_pipe == PIPE_B ? _REG_WM0_PIPEB_ILK : _REG_WM0_PIPEC_IVB);
+		plane_wm = (__vreg(vgt_dom0, wm_reg) & _REGBIT_WM0_PIPE_PLANE_MASK)
+			>> _REGBIT_WM0_PIPE_PLANE_SHIFT;
+		sprite_wm = (__vreg(vgt_dom0, wm_reg) & _REGBIT_WM0_PIPE_SPRITE_MASK)
+			>> _REGBIT_WM0_PIPE_SPRITE_SHIFT;
+		cursor_wm = __vreg(vgt_dom0, wm_reg) & _REGBIT_WM0_PIPE_CURSOR_MASK;
+		plane_wm = plane_wm * src_width * src_height / (target_width * target_height);
+		sprite_wm = sprite_wm * src_width * src_height / (target_width * target_height);
+		cursor_wm = cursor_wm * src_width * src_height / (target_width * target_height);
+		plane_wm = plane_wm > DISPLAY_MAXWM ? DISPLAY_MAXWM : plane_wm;
+		sprite_wm = sprite_wm > DISPLAY_MAXWM ? DISPLAY_MAXWM : sprite_wm;
+		cursor_wm = cursor_wm > CURSOR_MAXWM ? CURSOR_MAXWM : cursor_wm;
+		wm_value = cursor_wm & _REGBIT_WM0_PIPE_CURSOR_MASK;
+		wm_value = wm_value | (sprite_wm  << _REGBIT_WM0_PIPE_SPRITE_SHIFT);
+		wm_value = wm_value | ((plane_wm << _REGBIT_WM0_PIPE_PLANE_SHIFT) &
+			_REGBIT_WM0_PIPE_PLANE_MASK);
+		VGT_MMIO_WRITE(vgt->pdev, wm_reg, wm_value);
+	}
+
+	VGT_MMIO_WRITE(vgt->pdev, VGT_PIPESRC(real_pipe),  ((src_width -1) << 16) | (src_height - 1));
+	VGT_MMIO_WRITE(vgt->pdev, VGT_PF_WIN_POS(real_pipe), 0);
+	VGT_MMIO_WRITE(vgt->pdev, VGT_PF_CTL(real_pipe), pf_ctl);
+	/* PF ctrl is a double buffered registers and gets updated when window
+	 size registered is updated*/
+	VGT_MMIO_WRITE(vgt->pdev, VGT_PF_WIN_SZ(real_pipe),  (target_width << 16) | target_height);
+	return true;
+}
+
+bool vgt_manage_emul_dpy_events(struct pgt_device *pdev)
+{
+	int i;
+	enum vgt_pipe pipe;
+	unsigned hw_enabled_pipes, hvm_required_pipes;
+	struct vgt_irq_host_state *hstate = pdev->irq_hstate;
+	bool hvm_no_pipe_mapping = false;
+
+
+	ASSERT(spin_is_locked(&pdev->lock));
+	hw_enabled_pipes = hvm_required_pipes = 0;
+
+	for (i = 0; i < VGT_MAX_VMS; i++) {
+		struct vgt_device *vgt = pdev->device[i];
+		vgt_reg_t pipeconf;
+
+		if (vgt == NULL)
+			continue;
+
+		for (pipe = PIPE_A; pipe < I915_MAX_PIPES; pipe ++) {
+			pipeconf = __vreg(vgt, VGT_PIPECONF(pipe));
+			if (pipeconf & _REGBIT_PIPE_ENABLE) {
+				if (is_current_display_owner(vgt))
+					hw_enabled_pipes |= (1 << pipe);
+				else {
+					enum vgt_pipe p_pipe;
+					p_pipe  = vgt->pipe_mapping[pipe];
+					if (p_pipe != I915_MAX_PIPES) {
+						hvm_required_pipes |=
+								(1 << pipe);
+					} else {
+						hvm_no_pipe_mapping = true;
+						break;
+					}
+				}
+			}
+		}
+
+		pipeconf = __vreg(vgt, _REG_PIPE_EDP_CONF);
+		if (pipeconf & _REGBIT_PIPE_ENABLE) {
+			pipe = get_edp_input(
+				__vreg(vgt, _REG_TRANS_DDI_FUNC_CTL_EDP));
+			if (pipe == I915_MAX_PIPES) {
+				vgt_err("vGT(%d): "
+					"Invalid input selection for eDP\n",
+					vgt->vgt_id);
+				return false;
+			}
+			if (is_current_display_owner(vgt))
+				hw_enabled_pipes |= (1 << pipe);
+			else {
+				enum vgt_pipe p_pipe = vgt->pipe_mapping[pipe];
+				if (p_pipe != I915_MAX_PIPES) {
+					hvm_required_pipes |= (1 << pipe);
+				} else {
+					hvm_no_pipe_mapping = true;
+					break;
+				}
+			}
+		}
+	}
+
+	hrtimer_cancel(&hstate->dpy_timer.timer);
+	if (hvm_no_pipe_mapping || (hvm_required_pipes & ~hw_enabled_pipes)) {
+		/*there is hvm enabled pipe which is not enabled on hardware */
+		hrtimer_start(&hstate->dpy_timer.timer,
+			ktime_add_ns(ktime_get(), hstate->dpy_timer.period),
+			HRTIMER_MODE_ABS);
+	}
+
+	return true;
+}
+
+void vgt_update_frmcount(struct vgt_device *vgt,
+	enum vgt_pipe pipe)
+{
+	uint32_t v_counter_addr, count, delta;
+	enum vgt_pipe phys_pipe;
+	v_counter_addr = VGT_PIPE_FRMCOUNT(pipe);
+	phys_pipe = vgt->pipe_mapping[pipe];
+	delta = vgt->frmcount_delta[pipe];
+	if (phys_pipe == I915_MAX_PIPES)
+		__vreg(vgt, v_counter_addr) = delta;
+	else {
+		uint32_t p_counter_addr = VGT_PIPE_FRMCOUNT(phys_pipe);
+		count = VGT_MMIO_READ(vgt->pdev, p_counter_addr);
+		if (count <= 0xffffffff - delta) {
+			__vreg(vgt, v_counter_addr) = count + delta;
+		} else { /* wrap it */
+			count = 0xffffffff - count;
+			__vreg(vgt, v_counter_addr) = delta - count - 1;
+		}
+	}
+}
+
+/* the calculation of delta may eliminate un-read frmcount in vreg.
+ * so if pipe is enabled, need to update frmcount first before
+ * calculating the delta
+ */
+void vgt_calculate_frmcount_delta(struct vgt_device *vgt,
+	enum vgt_pipe pipe)
+{
+	uint32_t delta;
+	uint32_t virt_counter = __vreg(vgt, VGT_PIPE_FRMCOUNT(pipe));
+	enum vgt_pipe phys_pipe = vgt->pipe_mapping[pipe];
+	uint32_t hw_counter;
+
+	/* if physical pipe is not enabled yet, Delta will be used
+	 * as the frmcount. When physical pipe is enabled, new delta
+	 * will be calculated based on the hw count value.
+	 */
+	if (phys_pipe == I915_MAX_PIPES) {
+		vgt->frmcount_delta[pipe] = virt_counter;
+	} else {
+		hw_counter = VGT_MMIO_READ(vgt->pdev,
+					VGT_PIPE_FRMCOUNT(pipe));
+		if (virt_counter >= hw_counter)
+			delta = virt_counter - hw_counter;
+		else {
+			delta = 0xffffffff - hw_counter;
+			delta += virt_counter + 1;
+		}
+		vgt->frmcount_delta[pipe] = delta;
+	}
+}
+
+void vgt_set_power_well(struct vgt_device *vgt, bool to_enable)
+{
+	bool is_enabled, enable_requested;
+	uint32_t tmp;
+
+	tmp = VGT_MMIO_READ(vgt->pdev, _REG_HSW_PWR_WELL_CTL2);
+	is_enabled = tmp & _REGBIT_HSW_PWR_WELL_STATE;
+	enable_requested = tmp & _REGBIT_HSW_PWR_WELL_ENABLE;
+
+	if (to_enable) {
+		if (!enable_requested)
+			VGT_MMIO_WRITE(vgt->pdev, _REG_HSW_PWR_WELL_CTL2, _REGBIT_HSW_PWR_WELL_ENABLE);
+
+		if (!is_enabled) {
+			if (wait_for_atomic((VGT_MMIO_READ(vgt->pdev, _REG_HSW_PWR_WELL_CTL2) &
+				      _REGBIT_HSW_PWR_WELL_STATE), 20))
+				vgt_err("Timeout enabling power well\n");
+		}
+	} else {
+		if (enable_requested) {
+			VGT_MMIO_WRITE(vgt->pdev, _REG_HSW_PWR_WELL_CTL2, 0);
+			tmp = VGT_MMIO_READ(vgt->pdev, _REG_HSW_PWR_WELL_CTL2);
+		}
+	}
+}
+
+#define DPCD_HEADER_SIZE	0xb
+
+u8 dpcd_fix_data[DPCD_HEADER_SIZE] = {
+	0x11, 0x0a, 0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00
+};
+
+static bool is_dp_port_type(enum vgt_port_type port_type)
+{
+	if (port_type == VGT_DP_A ||
+		port_type == VGT_DP_B ||
+		port_type == VGT_DP_C ||
+		port_type == VGT_DP_D) {
+		return true;
+	}
+	return false;
+}
+
+
+/* copy the cached value into corresponding port field. Meanwhile,
+ * Update system monitor state for EDID changes
+ */
+void vgt_flush_port_info(struct vgt_device *vgt, struct gt_port *port)
+{
+	int port_idx;
+	enum vgt_port_type legacy_porttype;
+	int i;
+	unsigned int reg_ddi[4] ={
+		_REG_TRANS_DDI_FUNC_CTL_A,
+		_REG_TRANS_DDI_FUNC_CTL_B,
+		_REG_TRANS_DDI_FUNC_CTL_C,
+		_REG_TRANS_DDI_FUNC_CTL_EDP,
+	};
+
+
+	if (!vgt || !port)
+		return;
+	if (!port->cache.valid) {
+		vgt_warn("port cache flush with invalid data. "
+				"Will be ignored!\n");
+		return;
+	}
+
+	port_idx = vgt_get_port(vgt, port);
+
+	if (port_idx == I915_MAX_PORTS) {
+		vgt_err ("VM-%d: port is not a valid pointer", vgt->vm_id);
+		goto finish_flush;
+	} 
+
+	legacy_porttype = port->cache.type;
+
+	if (legacy_porttype == VGT_PORT_MAX) {
+		if (port_idx == PORT_E)
+			legacy_porttype = VGT_CRT;
+		else if (port->dpcd && port->dpcd->data_valid)
+			legacy_porttype = VGT_DP_B + port_idx - 1;
+		else
+			legacy_porttype = VGT_HDMI_B + port_idx - 1;
+	}
+
+	if (port->edid == NULL) {
+		port->edid = kmalloc(sizeof(struct vgt_edid_data_t), GFP_ATOMIC);
+	}
+	if (port->edid == NULL) {
+		vgt_err("Memory allocation fail for EDID block!\n");
+		return;
+	}
+
+	if (!(port->cache.edid && port->cache.edid->data_valid)) {
+		port->edid->data_valid = false;
+		if (port->dpcd)
+			port->dpcd->data_valid = false;
+		port->type = VGT_PORT_MAX;
+		port->port_override = I915_MAX_PORTS;
+	} else {
+		memcpy(port->edid->edid_block,
+			port->cache.edid->edid_block, EDID_SIZE);
+		port->edid->data_valid = true;
+		port->type = legacy_porttype;
+		port->port_override = port->cache.port_override;
+		if (vgt_debug & VGT_DBG_DPY) {
+			vgt_info("Monitor detection:new monitor detected on %s\n", VGT_PORT_NAME(port->physcal_port));
+			vgt_print_edid(port->edid);
+		}
+
+		if (is_dp_port_type(port->type)) {
+			if (port->dpcd == NULL) {
+				port->dpcd = kmalloc(sizeof(struct vgt_dpcd_data),
+				GFP_ATOMIC);
+			}
+
+			if (port->dpcd == NULL) {
+				return;
+			}
+			memset(port->dpcd->data, 0, DPCD_SIZE);
+			memcpy(port->dpcd->data, dpcd_fix_data, DPCD_HEADER_SIZE);
+			port->dpcd->data_valid = true;
+			if (vgt_debug & VGT_DBG_DPY) {
+				vgt_info("Monitor detection:assign fixed dpcd to port %s\n", VGT_PORT_NAME(port->physcal_port));
+			}
+		}
+		
+		for (i = 0; i <= 3; i++) {
+			unsigned int ddi_value;
+			ddi_value = VGT_MMIO_READ(vgt->pdev, reg_ddi[i]);
+			if (_REGBIT_TRANS_DDI_FUNC_ENABLE & ddi_value) {
+				update_pipe_mapping(vgt, reg_ddi[i], ddi_value);
+			}
+		}
+	}
+	vgt_update_monitor_status(vgt);
+
+finish_flush:
+	port->cache.valid = false;
+	port->cache.type = VGT_PORT_MAX;
+}
+
+/*send uevent to user space to do display detection*/
+void vgt_detect_display(struct vgt_device *vgt, int index)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	int i;
+	enum vgt_uevent_type uevent;
+
+	if (index == -1) { /* -1 index means "ALL" */
+		for (i = 0; i < I915_MAX_PORTS; i++) {
+			vgt_detect_display(vgt, i);
+		}
+		return;
+	}
+
+	uevent = VGT_DETECT_PORT_A + index;
+
+	vgt_set_uevent(vgt, uevent);
+	vgt_raise_request(pdev, VGT_REQUEST_UEVENT);
+}
+
+/* Set the initial plane/pipe/port state to be disabled,
+ * letting gfx driver's mode setting to configure them late.
+ * Notice that display owner could access physical MMIO states. Here
+ * the setting only works for VMs who are not display owner.
+ */
+void vgt_dpy_init_modes(vgt_reg_t *mmio_array)
+{
+	enum vgt_port port;
+	enum vgt_pipe pipe;
+	unsigned int offset;
+
+	mmio_array[REG_INDEX(_REG_DDI_BUF_CTL_A)] &=
+				~_DDI_BUFCTL_DETECT_MASK;
+
+	for (port = PORT_A; port <= PORT_E; ++ port) {
+		offset = VGT_DDI_BUF_CTL(port);
+		mmio_array[REG_INDEX(offset)] &= ~_REGBIT_DDI_BUF_ENABLE;
+		offset = VGT_DP_TP_CTL(port);
+		mmio_array[REG_INDEX(offset)] &= ~_REGBIT_DP_TP_ENABLE;
+	}
+
+	for (pipe = PIPE_A; pipe <= PIPE_C; ++ pipe) {
+		offset = _VGT_TRANS_DDI_FUNC_CTL(pipe);
+		mmio_array[REG_INDEX(offset)] &= ~_REGBIT_TRANS_DDI_FUNC_ENABLE;
+		offset = VGT_PIPECONF(pipe);
+		mmio_array[REG_INDEX(offset)] &= ~_REGBIT_PIPE_ENABLE;
+		offset = VGT_TRANSCONF(pipe);
+		mmio_array[REG_INDEX(offset)] &= ~_REGBIT_TRANS_ENABLE;
+		offset = VGT_PF_CTL(pipe);
+		mmio_array[REG_INDEX(offset)] &= ~_REGBIT_PF_ENABLE;
+	}
+
+	mmio_array[REG_INDEX(_REG_TRANS_DDI_FUNC_CTL_EDP)] &=
+				~_REGBIT_TRANS_DDI_FUNC_ENABLE;
+	mmio_array[REG_INDEX(_REG_PIPE_EDP_CONF)] &=
+				~_REGBIT_PIPE_ENABLE;
+
+	mmio_array[REG_INDEX(_REG_SPLL_CTL)] &= ~_REGBIT_SPLL_CTL_ENABLE;
+	mmio_array[REG_INDEX(_REG_WRPLL_CTL1)] &= ~_REGBIT_WRPLL_ENABLE;
+	mmio_array[REG_INDEX(_REG_WRPLL_CTL2)] &= ~_REGBIT_WRPLL_ENABLE;
+}
diff --git a/drivers/gpu/drm/i915/vgt/edid.c b/drivers/gpu/drm/i915/vgt/edid.c
new file mode 100644
index 0000000..e3b1716
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/edid.c
@@ -0,0 +1,591 @@
+/*
+ * vGT EDID virtualization module
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/delay.h>
+#include <linux/slab.h>
+#include <drm/drmP.h>
+
+#include "vgt.h"
+
+static const u8 edid_header[] = {
+	0x00, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00
+};
+
+int vgt_edid_header_is_valid(const u8 *raw_edid)
+{
+	int i, score = 0;
+	for (i = 0; i < sizeof(edid_header); i++)
+		if (raw_edid[i] == edid_header[i])
+			score++;
+	return score;
+}
+
+bool vgt_is_edid_valid(u8 *raw_edid)
+{
+	bool is_valid = false;
+	int score, i;
+	u8 check_sum = 0;
+
+	score = vgt_edid_header_is_valid(raw_edid);
+
+	check_sum = 0;
+	for (i = 0; i < EDID_SIZE; ++i) {
+		check_sum += raw_edid[i];
+	}
+	if (check_sum) {
+		vgt_err("EDID check sum is invalid\n");
+	}
+
+	if ((score == 8) && (check_sum == 0)) {
+		is_valid = true;
+	}
+	return is_valid;
+}
+
+static inline void vgt_clear_edid(struct gt_port *port)
+{
+	if (port && port->edid && port->edid->data_valid) {
+		port->edid->data_valid = false;
+	}
+}
+
+static inline void vgt_clear_dpcd(struct gt_port *port)
+{
+	if (port && port->dpcd && port->dpcd->data_valid) {
+		port->dpcd->data_valid = false;
+	}
+}
+
+void vgt_clear_port(struct vgt_device *vgt, int index)
+{
+	struct gt_port *port;
+
+	if (!dpy_is_valid_port(index)) {
+		vgt_warn("Wrong port index input! Will do nothing!\n");
+		return;
+	}
+
+	port = &vgt->ports[index];
+	vgt_clear_edid(port);
+	vgt_clear_dpcd(port);
+	
+	port->type = VGT_PORT_MAX;
+}
+
+static unsigned char edid_get_byte(struct vgt_device *vgt)
+{
+	unsigned char chr = 0;
+	struct vgt_i2c_edid_t *edid = &vgt->vgt_i2c_edid;
+
+	if (edid->state == I2C_NOT_SPECIFIED || !edid->slave_selected) {
+		vgt_warn("Driver tries to read EDID without proper sequence!\n");
+		return 0;
+	}
+	if (edid->current_edid_read >= EDID_SIZE) {
+		vgt_warn("edid_get_byte() exceeds the size of EDID!\n");
+		return 0;
+	}
+
+	if (!edid->edid_available) {
+		vgt_warn("Reading EDID but EDID is not available!"
+			" Will return 0.\n");
+		return 0;
+	}
+
+	if (dpy_has_monitor_on_port(vgt, edid->port)) {
+		struct vgt_edid_data_t *edid_data = vgt->ports[edid->port].edid;
+		chr = edid_data->edid_block[edid->current_edid_read];
+		vgt_dbg(VGT_DBG_EDID,
+			"edid_get_byte with offset %d and value %d\n",
+			edid->current_edid_read, chr);
+		edid->current_edid_read ++;
+	} else {
+		vgt_warn("No EDID available during the reading?\n");
+	}
+
+	return chr;
+}
+
+/**************************************************************************
+ *
+ * GMBUS interface for I2C access
+ *
+ *************************************************************************/
+static inline enum vgt_port vgt_get_port_from_gmbus0(vgt_reg_t gmbus0)
+{
+	enum vgt_port port = I915_MAX_PORTS;
+	int port_select = gmbus0 & _GMBUS_PIN_SEL_MASK;
+
+	if (port_select == 2)
+		port = PORT_E;
+	else if (port_select == 4)
+		port = PORT_C;
+	else if (port_select == 5)
+		port = PORT_B;
+	else if (port_select == 6)
+		port = PORT_D;
+
+	return port;
+}
+
+/* GMBUS0 */
+static bool vgt_gmbus0_mmio_write(struct vgt_device *vgt,
+			unsigned int offset, void *p_data, unsigned int bytes)
+{
+	vgt_reg_t wvalue = *(vgt_reg_t *)p_data;
+	enum vgt_port port = I915_MAX_PORTS;
+	int pin_select = wvalue & _GMBUS_PIN_SEL_MASK;
+
+	vgt_init_i2c_edid(vgt);
+
+	if (pin_select == 0)
+		return true;
+
+	vgt->vgt_i2c_edid.state = I2C_GMBUS;
+	port = vgt_get_port_from_gmbus0(pin_select);
+	if (!dpy_is_valid_port(port)) {
+		vgt_dbg(VGT_DBG_EDID,
+			"VM(%d): Driver tries GMBUS write not on valid port!\n"
+			"gmbus write value is: 0x%x\n", vgt->vgt_id, wvalue);
+		return true;
+	}
+
+	vgt->vgt_i2c_edid.gmbus.phase = GMBUS_IDLE_PHASE;
+
+	/* FIXME: never clear _GMBUS_HW_WAIT */
+	__vreg(vgt, _REG_PCH_GMBUS2) &= ~ _GMBUS_ACTIVE;
+	__vreg(vgt, _REG_PCH_GMBUS2) |= _GMBUS_HW_RDY | _GMBUS_HW_WAIT;
+
+	if (dpy_has_monitor_on_port(vgt, port) && !dpy_port_is_dp(vgt, port)) {
+		vgt->vgt_i2c_edid.port = port;
+		vgt->vgt_i2c_edid.edid_available = true;
+		__vreg(vgt, _REG_PCH_GMBUS2) &= ~_GMBUS_NAK;
+	} else {
+		__vreg(vgt, _REG_PCH_GMBUS2) |= _GMBUS_NAK;
+	}
+
+	memcpy(p_data, (char *)vgt->state.vReg + offset, bytes);
+	return true;
+}
+
+/* TODO: */
+void vgt_reset_gmbus_controller(struct vgt_device *vgt)
+{
+	/* TODO: clear gmbus0 ? */
+	//__vreg(vgt, _REG_PCH_GMBUS0) = 0;
+	//__vreg(vgt, _REG_PCH_GMBUS1) = 0;
+	__vreg(vgt, _REG_PCH_GMBUS2) = _GMBUS_HW_RDY;
+	if (!vgt->vgt_i2c_edid.edid_available) {
+		__vreg(vgt, _REG_PCH_GMBUS2) |= _GMBUS_NAK;
+	}
+	//__vreg(vgt, _REG_PCH_GMBUS3) = 0;
+	//__vreg(vgt, _REG_PCH_GMBUS4) = 0;
+	//__vreg(vgt, _REG_PCH_GMBUS5) = 0;
+	vgt->vgt_i2c_edid.gmbus.phase = GMBUS_IDLE_PHASE;
+}
+
+
+static bool vgt_gmbus1_mmio_write(struct vgt_device *vgt, unsigned int offset,
+void *p_data, unsigned int bytes)
+{
+	u32 slave_addr;
+	struct vgt_i2c_edid_t *i2c_edid = &vgt->vgt_i2c_edid;
+
+	vgt_reg_t wvalue = *(vgt_reg_t *)p_data;
+	if (__vreg(vgt, offset) & _GMBUS_SW_CLR_INT) {
+		if (!(wvalue & _GMBUS_SW_CLR_INT)) {
+			__vreg(vgt, offset) &= ~_GMBUS_SW_CLR_INT;
+			vgt_reset_gmbus_controller(vgt);
+		}
+		/* TODO: "This bit is cleared to zero when an event
+		 * causes the HW_RDY bit transition to occur "*/
+	} else {
+		/* per bspec setting this bit can cause:
+		 1) INT status bit cleared
+		 2) HW_RDY bit asserted
+		 */
+		if (wvalue & _GMBUS_SW_CLR_INT) {
+			__vreg(vgt, _REG_PCH_GMBUS2) &= ~_GMBUS_INT_STAT;
+			__vreg(vgt, _REG_PCH_GMBUS2) |= _GMBUS_HW_RDY;
+		}
+
+		/* For virtualization, we suppose that HW is always ready,
+		 * so _GMBUS_SW_RDY should always be cleared
+		 */
+		if (wvalue & _GMBUS_SW_RDY)
+			wvalue &= ~_GMBUS_SW_RDY;
+
+		i2c_edid->gmbus.total_byte_count =
+			gmbus1_total_byte_count(wvalue);
+		slave_addr = gmbus1_slave_addr(wvalue);
+
+		/* vgt gmbus only support EDID */
+		if (slave_addr == EDID_ADDR) {
+			i2c_edid->slave_selected = true;
+		} else if (slave_addr != 0) {
+			vgt_dbg(VGT_DBG_DPY,
+				"vGT(%d): unsupported gmbus slave addr(0x%x)\n"
+				"	gmbus operations will be ignored.\n",
+					vgt->vgt_id, slave_addr);
+		}
+
+		if (wvalue & _GMBUS_CYCLE_INDEX) {
+			i2c_edid->current_edid_read = gmbus1_slave_index(wvalue);
+		}
+
+		i2c_edid->gmbus.cycle_type = gmbus1_bus_cycle(wvalue);
+		switch (gmbus1_bus_cycle(wvalue)) {
+			case GMBUS_NOCYCLE:
+				break;
+			case GMBUS_STOP:
+				/* From spec:
+				This can only cause a STOP to be generated
+				if a GMBUS cycle is generated, the GMBUS is
+				currently in a data/wait/idle phase, or it is in a
+				WAIT phase
+				 */
+				if (gmbus1_bus_cycle(__vreg(vgt, offset)) != GMBUS_NOCYCLE) {
+					vgt_init_i2c_edid(vgt);
+					/* After the 'stop' cycle, hw state would become
+					 * 'stop phase' and then 'idle phase' after a few
+					 * milliseconds. In emulation, we just set it as
+					 * 'idle phase' ('stop phase' is not
+					 * visible in gmbus interface)
+					 */
+					i2c_edid->gmbus.phase = GMBUS_IDLE_PHASE;
+					/*
+					FIXME: never clear _GMBUS_WAIT
+					__vreg(vgt, _REG_PCH_GMBUS2) &=
+						~(_GMBUS_ACTIVE | _GMBUS_HW_WAIT);
+					*/
+					__vreg(vgt, _REG_PCH_GMBUS2) &= ~_GMBUS_ACTIVE;
+				}
+				break;
+			case NIDX_NS_W:
+			case IDX_NS_W:
+			case NIDX_STOP:
+			case IDX_STOP:
+				/* From hw spec the GMBUS phase
+				 * transition like this:
+				 * START (-->INDEX) -->DATA
+				 */
+				i2c_edid->gmbus.phase = GMBUS_DATA_PHASE;
+				__vreg(vgt, _REG_PCH_GMBUS2) |= _GMBUS_ACTIVE;
+				/* FIXME: never clear _GMBUS_WAIT */
+				//__vreg(vgt, _REG_PCH_GMBUS2) &= ~_GMBUS_HW_WAIT;
+				break;
+			default:
+				vgt_err("Unknown/reserved GMBUS cycle detected!");
+				break;
+		}
+		/* From hw spec the WAIT state will be
+		 * cleared:
+		 * (1) in a new GMBUS cycle
+		 * (2) by generating a stop
+		 */
+		/* FIXME: never clear _GMBUS_WAIT
+		if (gmbus1_bus_cycle(wvalue) != GMBUS_NOCYCLE)
+			__vreg(vgt, _REG_PCH_GMBUS2) &= ~_GMBUS_HW_WAIT;
+		*/
+
+		__vreg(vgt, offset) = wvalue;
+	}
+	return true;
+}
+
+bool vgt_gmbus3_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	ASSERT_VM(0, vgt);
+	return true;
+}
+
+bool vgt_gmbus3_mmio_read(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	int i;
+	unsigned char byte_data;
+	struct vgt_i2c_edid_t *i2c_edid = &vgt->vgt_i2c_edid;
+	int byte_left = i2c_edid->gmbus.total_byte_count -
+				i2c_edid->current_edid_read;
+	int byte_count = byte_left;
+	vgt_reg_t reg_data = 0;
+
+	/* Data can only be recevied if previous settings correct */
+	if (__vreg(vgt, _REG_PCH_GMBUS1) & _GMBUS_SLAVE_READ) {
+		if (byte_left <= 0) {
+			memcpy((char *)p_data, (char *)vgt->state.vReg + offset, bytes);
+			return true;
+		}
+
+		if (byte_count > 4)
+			byte_count = 4;
+		for (i = 0; i< byte_count; i++) {
+			byte_data = edid_get_byte(vgt);
+			reg_data |= (byte_data << (i << 3));
+		}
+
+		memcpy((char *)p_data, (char *)&reg_data, byte_count);
+		memcpy((char *)vgt->state.vReg + offset, (char *)&reg_data, byte_count);
+
+		if (byte_left <= 4) {
+			switch (i2c_edid->gmbus.cycle_type) {
+				case NIDX_STOP:
+				case IDX_STOP:
+					i2c_edid->gmbus.phase = GMBUS_IDLE_PHASE;
+					break;
+				case NIDX_NS_W:
+				case IDX_NS_W:
+				default:
+					i2c_edid->gmbus.phase = GMBUS_WAIT_PHASE;
+					break;
+			}
+			//if (i2c_bus->gmbus.phase == GMBUS_WAIT_PHASE)
+			//__vreg(vgt, _REG_PCH_GMBUS2) |= _GMBUS_HW_WAIT;
+
+			vgt_init_i2c_edid(vgt);
+		}
+
+		/* Read GMBUS3 during send operation, return the latest written value */
+	} else {
+		memcpy((char *)p_data, (char *)vgt->state.vReg + offset, bytes);
+		printk("vGT(%d): warning: gmbus3 read with nothing retuned\n",
+				vgt->vgt_id);
+	}
+
+	return true;
+}
+
+static bool vgt_gmbus2_mmio_read(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	vgt_reg_t value = __vreg(vgt, offset);
+	if (!(__vreg(vgt, offset) & _GMBUS_IN_USE)) {
+		__vreg(vgt, offset) |= _GMBUS_IN_USE;
+	}
+
+	memcpy(p_data, (void *)&value, bytes);
+	return true;
+}
+
+bool vgt_gmbus2_mmio_write(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	vgt_reg_t wvalue = *(vgt_reg_t *)p_data;
+	if (wvalue & _GMBUS_IN_USE)
+		__vreg(vgt, offset) &= ~_GMBUS_IN_USE;
+	/* All other bits are read-only */
+	return true;
+}
+
+bool vgt_i2c_handle_gmbus_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	ASSERT(bytes <= 8 && !(offset & (bytes - 1)));
+	switch (offset) {
+		case _REG_PCH_GMBUS2:
+			return vgt_gmbus2_mmio_read(vgt, offset, p_data, bytes);
+		case _REG_PCH_GMBUS3:
+			return vgt_gmbus3_mmio_read(vgt, offset, p_data, bytes);
+		default:
+			memcpy(p_data, (char *)vgt->state.vReg + offset, bytes);
+	}
+	return true;
+}
+
+bool vgt_i2c_handle_gmbus_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	ASSERT(bytes <= 8 && !(offset & (bytes - 1)));
+	switch (offset) {
+		case _REG_PCH_GMBUS0:
+			return vgt_gmbus0_mmio_write(vgt, offset, p_data, bytes);
+		case _REG_PCH_GMBUS1:
+			return vgt_gmbus1_mmio_write(vgt, offset, p_data, bytes);
+		case _REG_PCH_GMBUS2:
+			return vgt_gmbus2_mmio_write(vgt, offset, p_data, bytes);
+		/* TODO: */
+		case _REG_PCH_GMBUS3:
+			BUG();
+			return false;
+		default:
+			memcpy((char *)vgt->state.vReg + offset, p_data, bytes);
+	}
+	return true;
+}
+
+
+/**************************************************************************
+ *
+ * Aux CH interface for I2C access
+ *
+ *************************************************************************/
+
+/* vgt_get_aux_ch_reg()
+ *
+ * return the AUX_CH register according to its lower 8 bits of the address
+ */
+static inline AUX_CH_REGISTERS vgt_get_aux_ch_reg(unsigned int offset)
+{
+	AUX_CH_REGISTERS reg;
+	switch (offset & 0xff) {
+	case 0x10:
+		reg = AUX_CH_CTL;
+		break;
+	case 0x14:
+		reg = AUX_CH_DATA1;
+		break;
+	case 0x18:
+		reg = AUX_CH_DATA2;
+		break;
+	case 0x1c:
+		reg = AUX_CH_DATA3;
+		break;
+	case 0x20:
+		reg = AUX_CH_DATA4;
+		break;
+	case 0x24:
+		reg = AUX_CH_DATA5;
+		break;
+	default:
+		reg = AUX_CH_INV;
+		break;
+	}
+	return reg;
+}
+
+#define AUX_CTL_MSG_LENGTH(reg) \
+	((reg & _DP_AUX_CH_CTL_MESSAGE_SIZE_MASK) >> \
+		_DP_AUX_CH_CTL_MESSAGE_SIZE_SHIFT)
+
+void vgt_i2c_handle_aux_ch_write(struct vgt_device *vgt,
+				enum vgt_port port_idx,
+				unsigned int offset,
+				void *p_data)
+{
+	struct vgt_i2c_edid_t *i2c_edid = &vgt->vgt_i2c_edid;
+	int msg_length, ret_msg_size;
+	int msg, addr, ctrl, op;
+	int value = *(int *)p_data;
+	int aux_data_for_write = 0;
+	AUX_CH_REGISTERS reg = vgt_get_aux_ch_reg(offset);
+
+	if (reg != AUX_CH_CTL) {
+		__vreg(vgt, offset) = value;
+		return;
+	}
+
+	msg_length = AUX_CTL_MSG_LENGTH(value);
+	// check the msg in DATA register.
+	msg = __vreg(vgt, offset + 4);
+	addr = (msg >> 8) & 0xffff;
+	ctrl = (msg >> 24)& 0xff;
+	op = ctrl >> 4;
+	if (!(value & _REGBIT_DP_AUX_CH_CTL_SEND_BUSY)) {
+		/* The ctl write to clear some states */
+		return;
+	}
+
+	/* Always set the wanted value for vms. */
+	ret_msg_size = (((op & 0x1) == VGT_AUX_I2C_READ) ? 2 : 1);
+	__vreg(vgt, offset) =
+		_REGBIT_DP_AUX_CH_CTL_DONE |
+		((ret_msg_size << _DP_AUX_CH_CTL_MESSAGE_SIZE_SHIFT) &
+		_DP_AUX_CH_CTL_MESSAGE_SIZE_MASK);
+
+	if (msg_length == 3) {
+		if (!(op & VGT_AUX_I2C_MOT)) {
+			/* stop */
+			vgt_dbg(VGT_DBG_EDID,
+				"AUX_CH: stop. reset I2C!\n");
+			vgt_init_i2c_edid(vgt);
+		} else {
+			/* start or restart */
+			vgt_dbg(VGT_DBG_EDID,
+				"AUX_CH: start or restart I2C!\n");
+			i2c_edid->aux_ch.i2c_over_aux_ch = true;
+			i2c_edid->aux_ch.aux_ch_mot = true;
+			if (addr == 0) {
+				/* reset the address */
+				vgt_dbg(VGT_DBG_EDID,
+					"AUX_CH: reset I2C!\n");
+				vgt_init_i2c_edid(vgt);
+			} else if (addr == EDID_ADDR) {
+				vgt_dbg(VGT_DBG_EDID,
+					"AUX_CH: setting EDID_ADDR!\n");
+				i2c_edid->state = I2C_AUX_CH;
+				i2c_edid->port = port_idx;
+				i2c_edid->slave_selected = true;
+				if (dpy_has_monitor_on_port(vgt, port_idx) &&
+					dpy_port_is_dp(vgt, port_idx))
+					i2c_edid->edid_available = true;
+			} else {
+				vgt_dbg(VGT_DBG_EDID,
+		"Not supported address access [0x%x]with I2C over AUX_CH!\n",
+				addr);
+			}
+		}
+	} else if ((op & 0x1) == VGT_AUX_I2C_WRITE) {
+		/* TODO
+		 * We only support EDID reading from I2C_over_AUX. And
+		 * we do not expect the index mode to be used. Right now
+		 * the WRITE operation is ignored. It is good enough to
+		 * support the gfx driver to do EDID access.
+		 */
+	} else {
+		ASSERT((op & 0x1) == VGT_AUX_I2C_READ);
+		ASSERT(msg_length == 4);
+		if (i2c_edid->edid_available && i2c_edid->slave_selected) {
+			unsigned char val = edid_get_byte(vgt);
+			aux_data_for_write = (val << 16);
+		}
+	}
+
+	/* write the return value in AUX_CH_DATA reg which includes:
+	 * ACK of I2C_WRITE
+	 * returned byte if it is READ
+	 */
+	aux_data_for_write |= (VGT_AUX_I2C_REPLY_ACK & 0xff) << 24;
+	__vreg(vgt, offset + 4) = aux_data_for_write;
+
+	return;
+}
+
+void vgt_init_i2c_edid(struct vgt_device *vgt)
+{
+	struct vgt_i2c_edid_t *edid = &vgt->vgt_i2c_edid;
+
+	edid->state = I2C_NOT_SPECIFIED;
+
+	edid->port = I915_MAX_PORTS;
+	edid->slave_selected = false;
+	edid->edid_available = false;
+	edid->current_edid_read = 0;
+	
+	memset(&edid->gmbus, 0, sizeof(struct vgt_i2c_gmbus_t));
+
+	edid->aux_ch.i2c_over_aux_ch = false;
+	edid->aux_ch.aux_ch_mot = false;
+}
diff --git a/drivers/gpu/drm/i915/vgt/edid.h b/drivers/gpu/drm/i915/vgt/edid.h
new file mode 100644
index 0000000..1cd2193
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/edid.h
@@ -0,0 +1,185 @@
+/*
+ * vGT header file for EDID virtualization
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+#ifndef _VGT_EDID_H_
+#define _VGT_EDID_H_
+
+#define EDID_SIZE		128
+#define EDID_ADDR		0x50 /* Linux hvm EDID addr */
+
+#define VGT_AUX_NATIVE_WRITE			0x8
+#define VGT_AUX_NATIVE_READ			0x9
+#define VGT_AUX_I2C_WRITE			0x0
+#define VGT_AUX_I2C_READ			0x1
+#define VGT_AUX_I2C_STATUS			0x2
+#define VGT_AUX_I2C_MOT				0x4
+#define VGT_AUX_I2C_REPLY_ACK			(0x0 << 6)
+
+#define _REGBIT_DP_AUX_CH_CTL_SEND_BUSY (1 << 31)
+#define _REGBIT_DP_AUX_CH_CTL_DONE (1 << 30)
+#define _DP_AUX_CH_CTL_MESSAGE_SIZE_SHIFT 	20
+#define _DP_AUX_CH_CTL_MESSAGE_SIZE_MASK 	(0x1f << 20)
+
+struct vgt_edid_data_t{
+	bool data_valid;
+	unsigned char edid_block[EDID_SIZE];
+};
+
+enum gmbus_cycle_type_t{
+	GMBUS_NOCYCLE	= 0x0,
+	NIDX_NS_W	= 0x1,
+	IDX_NS_W	= 0x3,
+	GMBUS_STOP	= 0x4,
+	NIDX_STOP	= 0x5,
+	IDX_STOP	= 0x7
+};
+
+/*
+ * States of GMBUS
+ *
+ * GMBUS0-3 could be related to the EDID virtualization. Another two GMBUS
+ * registers, GMBUS4 (interrupt mask) and GMBUS5 (2 byte indes register), are
+ * not considered here. Below describes the usage of GMBUS registers that are
+ * cared by the EDID virtualization
+ *
+ * GMBUS0:
+ * 	R/W
+ * 	port selection. value of bit0 - bit2 corresponds to the GPIO registers.
+ *
+ * GMBUS1:
+ * 	R/W Protect
+ * 	Command and Status.
+ * 	bit0 is the direction bit: 1 is read; 0 is write.
+ * 	bit1 - bit7 is slave 7-bit address.
+ * 	bit16 - bit24 total byte count (ignore?)
+ *
+ * GMBUS2:
+ * 	Most of bits are read only except bit 15 (IN_USE)
+ * 	Status register
+ * 	bit0 - bit8 current byte count
+ * 	bit 11: hardware ready;
+ *
+ * GMBUS3:
+ *	Read/Write
+ *	Data for transfer
+ */
+
+/* From hw specs, Other phases like START, ADDRESS, INDEX
+ * are invisible to GMBUS MMIO interface. So no definitions
+ * in below enum types
+ */
+enum vgt_gmbus_phase_t{
+	GMBUS_IDLE_PHASE = 0,
+	GMBUS_DATA_PHASE,
+	GMBUS_WAIT_PHASE,
+	//GMBUS_STOP_PHASE,
+	GMBUS_MAX_PHASE
+};
+
+struct vgt_i2c_gmbus_t {
+	unsigned total_byte_count; /* from GMBUS1 */
+	enum gmbus_cycle_type_t cycle_type;
+	enum vgt_gmbus_phase_t phase;
+};
+
+struct vgt_i2c_aux_ch_t{
+	bool i2c_over_aux_ch;
+	bool aux_ch_mot;
+};
+
+enum i2c_state_t {
+	I2C_NOT_SPECIFIED = 0,
+	I2C_GMBUS = 1,
+	I2C_AUX_CH = 2
+};
+
+/* I2C sequences cannot interleave.
+ * GMBUS and AUX_CH sequences cannot interleave.
+ */
+struct vgt_i2c_edid_t {
+	enum i2c_state_t state;
+
+	unsigned port;
+	bool slave_selected;
+	bool edid_available;
+	unsigned current_edid_read;
+
+	struct vgt_i2c_gmbus_t gmbus;
+	struct vgt_i2c_aux_ch_t aux_ch;
+};
+
+void vgt_init_i2c_edid(struct vgt_device *vgt);
+
+bool vgt_i2c_handle_gmbus_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes);
+
+bool vgt_i2c_handle_gmbus_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes);
+
+void vgt_i2c_handle_aux_ch_write(struct vgt_device *vgt,
+				enum vgt_port port_idx,
+				unsigned int offset,
+				void *p_data);
+
+bool vgt_is_edid_valid(u8 *raw_edid);
+
+#define AUX_REGISTER_NUM 6
+typedef enum {
+	AUX_CH_INV = -1,
+	AUX_CH_CTL = 0,
+	AUX_CH_DATA1,
+	AUX_CH_DATA2,
+	AUX_CH_DATA3,
+	AUX_CH_DATA4,
+	AUX_CH_DATA5
+}AUX_CH_REGISTERS;
+
+static inline enum vgt_port vgt_get_dp_port_idx(unsigned int offset)
+{
+	enum vgt_port port_idx;
+
+	if (offset >= _REG_DPA_AUX_CH_CTL
+		&& offset <= _REG_DPA_AUX_CH_CTL +
+				AUX_REGISTER_NUM * sizeof(vgt_reg_t)) {
+		return PORT_A;
+	}
+
+	switch (((offset & 0xff00) >> 8) - 0x41) {
+	case 0:
+		port_idx = PORT_B;
+		break;
+	case 1:
+		port_idx = PORT_C;
+		break;
+	case 2:
+		port_idx = PORT_D;
+		break;
+	default:
+		port_idx = I915_MAX_PORTS;
+		break;
+	}
+	return port_idx;
+}
+
+#endif /*_VGT_EDID_H_*/
diff --git a/drivers/gpu/drm/i915/vgt/execlists.c b/drivers/gpu/drm/i915/vgt/execlists.c
new file mode 100644
index 0000000..7e48327
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/execlists.c
@@ -0,0 +1,1821 @@
+/*
+ * BDW EXECLIST supports
+ *
+ * Copyright(c) 2011-2015 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "trace.h"
+#include "vgt.h"
+
+//#define EL_SLOW_DEBUG
+
+#define EXECLIST_CTX_PAGES(ring_id)	((ring_id) == RING_BUFFER_RCS ? 20 : 2)
+
+#define ROOT_POINTER_2_CTX_STATE(state, root, i)	\
+do{							\
+	state->pdp##i##_LDW.val = root[(i)<<1];		\
+	state->pdp##i##_UDW.val = root[((i) << 1) + 1];	\
+	vgt_dbg(VGT_DBG_EXECLIST, "New root[%d] in state is: 0x%x(high)-0x%x(low)\n",	\
+		i, root[((i) << 1) + 1], root[(i) << 1]);	\
+}while(0);
+
+#define CTX_STATE_2_ROOT_POINTER(root, state, i)	\
+do{							\
+	root[(i) << 1] = state->pdp##i##_LDW.val;	\
+	root[((i) << 1) + 1] = state->pdp##i##_UDW.val;	\
+}while(0);
+
+#define ROOTP_CTX_STATE_2_CTX_STATE(dst, src, i)	\
+do{							\
+	dst->pdp##i##_LDW.val = src->pdp##i##_LDW.val;	\
+	dst->pdp##i##_UDW.val = src->pdp##i##_UDW.val;	\
+}while(0);
+
+#define CTX_IS_SCHEDULED_OUT(ctx_status)		\
+((ctx_status)->preempted ||				\
+ (ctx_status)->element_switch ||			\
+ (ctx_status)->active_to_idle ||			\
+ (ctx_status)->context_complete ||			\
+ (ctx_status)->wait_on_sync_flip ||			\
+ (ctx_status)->wait_on_vblank ||			\
+ (ctx_status)->wait_on_semaphore ||			\
+ (ctx_status)->wait_on_scanline)
+
+#define WAIT_FOR_RING_DONE(ring_base, count)			\
+do {								\
+	int j = 0;						\
+	do {							\
+		vgt_reg_t val;					\
+		j ++;						\
+		val = VGT_MMIO_READ(vgt->pdev,			\
+			(ring_base) + 0x6c);			\
+		if ((val & 0xfffe) == 0xfffe)			\
+			break;					\
+		if (j == count) {				\
+			vgt_err("Did not get INSTDONE(0x%x)"	\
+			" done bit set! reg value: 0x%x.\n",	\
+			(ring_base) + 0x6c, val);		\
+			break;					\
+		}						\
+	} while(1);						\
+} while(0);
+
+/* trace the queue ops: 0 for enqueue, 1 for dequeue, 2 for delete */
+static void inline trace_el_queue_ops(struct vgt_device *vgt, int ring_id, int el_idx, int ops)
+{
+	int i;
+	char str[128];
+	int head = vgt_el_queue_head(vgt, ring_id);
+	int tail = vgt_el_queue_tail(vgt, ring_id);
+
+	/* if it was enqueue, the queue should not be empty now */
+	if (ops == 0)
+		ASSERT(head != tail);
+
+	for (i = 0; i < 2; ++ i) {
+		struct execlist_context *ctx;
+		uint32_t lrca;
+		ctx = vgt_el_queue_ctx(vgt, ring_id, el_idx, i);
+		if (!ctx)
+			continue;
+
+		lrca = ctx->guest_context.lrca;
+		snprintf(str, 128, "slot[%d] ctx[%d] %s "
+				"(queue head: %d; tail: %d)",
+			el_idx, i,
+			(ops == 0 ? "enqueue" : (ops == 1 ? "dequeue" : "delete")),
+			head, tail);
+		trace_ctx_lifecycle(vgt->vm_id, ring_id, lrca, str);
+	}
+}
+
+/* util functions */
+
+static inline enum vgt_ring_id el_mmio_to_ring_id(unsigned int reg)
+{
+	enum vgt_ring_id ring_id = MAX_ENGINES;
+	switch (reg) {
+	case _REG_RCS_CTX_SR_CTL:
+	case _REG_RCS_HEAD:
+	case _REG_RCS_TAIL:
+	case _REG_RCS_START:
+	case _REG_RCS_CTL:
+	case 0x2168:
+	case _REG_RCS_BB_ADDR:
+	case 0x2110:
+	case 0x211C:
+	case 0x2114:
+	case 0x2118:
+	case 0x21C0:
+	case 0x21C4:
+	case 0x21C8:
+		ring_id = RING_BUFFER_RCS;
+		break;
+	case _REG_VCS_CTX_SR_CTL:
+		ring_id = RING_BUFFER_VCS;
+		break;
+	case _REG_VECS_CTX_SR_CTL:
+		ring_id = RING_BUFFER_VECS;
+		break;
+	case _REG_VCS2_CTX_SR_CTL:
+		ring_id = RING_BUFFER_VCS2;
+		break;
+	case _REG_BCS_CTX_SR_CTL:
+		ring_id = RING_BUFFER_BCS;
+		break;
+	default:
+		break;
+	}
+
+	return ring_id;
+}
+
+static inline struct reg_state_ctx_header *
+vgt_get_reg_state_from_lrca(struct vgt_device *vgt, uint32_t lrca)
+{
+	struct reg_state_ctx_header *header;
+	uint32_t state_gma = (lrca + 1) << GTT_PAGE_SHIFT;
+
+	header = (struct reg_state_ctx_header *)
+			vgt_gma_to_va(vgt->gtt.ggtt_mm, state_gma);
+	return header;
+}
+
+static inline enum vgt_ring_id vgt_get_ringid_from_lrca(struct vgt_device *vgt,
+				unsigned int lrca)
+{
+	enum vgt_ring_id ring_id = MAX_ENGINES;
+	struct reg_state_ctx_header *reg_state;
+
+	reg_state = vgt_get_reg_state_from_lrca(vgt, lrca);
+
+	if (reg_state == NULL)
+		return ring_id;
+
+	ring_id = el_mmio_to_ring_id(reg_state->ctx_ctrl.addr);
+
+	return ring_id;
+}
+
+/* a queue implementation
+ *
+ * It is used to hold the submitted execlists through writing ELSP.
+ * In maximum VM can submit two execlists. The queue size
+ * is designed to be 3 to better recognize the queue full and empty. Queue
+ * tail points to the next slot to be written, whereas header points to the
+ * slot to be addressed. (header == tail) means the queue is full.
+ *
+ * The reason to use a queue is to keep the information of submission order.
+ */
+
+static bool vgt_el_slots_enqueue(struct vgt_device *vgt,
+			enum vgt_ring_id ring_id,
+			struct execlist_context *ctx0,
+			struct execlist_context *ctx1)
+{
+	struct vgt_exec_list *el_slot;
+	int tail = vgt_el_queue_tail(vgt, ring_id);
+	int new_tail = tail + 1;
+	if (new_tail == EL_QUEUE_SLOT_NUM)
+		new_tail = 0;
+
+	if (new_tail == vgt_el_queue_head(vgt, ring_id)) {
+		return false;
+	}
+	el_slot = &vgt_el_queue_slot(vgt, ring_id, tail);
+	el_slot->el_ctxs[0] = ctx0;
+	el_slot->el_ctxs[1] = ctx1;
+	el_slot->status = EL_PENDING;
+	vgt_el_queue_tail(vgt, ring_id) = new_tail;
+	trace_el_queue_ops(vgt, ring_id, tail, 0);
+	return true;
+}
+#if 0
+static int vgt_el_slots_dequeue(struct vgt_device *vgt, enum vgt_ring_id ring_id)
+{
+	int new_head;
+	int head = vgt_el_queue_head(vgt, ring_id);
+
+	if (head == vgt_el_queue_tail(vgt, ring_id)) {
+		// queue empty
+		return -1;
+	}
+
+	new_head = head + 1;
+	if (new_head == EL_QUEUE_SLOT_NUM)
+		new_head = 0;
+
+	vgt_el_queue_head(vgt, ring_id) = new_head;
+
+	trace_el_queue_ops(vgt, ring_id, head, 1);
+
+	return head;
+}
+#endif
+static void vgt_el_slots_delete(struct vgt_device *vgt,
+			enum vgt_ring_id ring_id, int idx)
+{
+	struct vgt_exec_list *el_slot;
+	int head = vgt_el_queue_head(vgt, ring_id);
+
+	if (idx == head) {
+		head ++;
+		if (head == EL_QUEUE_SLOT_NUM)
+			head = 0;
+		vgt_el_queue_head(vgt, ring_id) = head;
+	} else {
+		int idx_next = idx + 1;
+		if (idx_next == EL_QUEUE_SLOT_NUM)
+			idx_next = 0;
+		ASSERT(idx_next == vgt_el_queue_tail(vgt, ring_id));
+		vgt_el_queue_tail(vgt, ring_id) = idx;
+	}
+
+	trace_el_queue_ops(vgt, ring_id, idx, 2);
+
+	el_slot = &vgt_el_queue_slot(vgt, ring_id, idx);
+	el_slot->status = EL_EMPTY;
+	el_slot->el_ctxs[0] = NULL;
+	el_slot->el_ctxs[1] = NULL;
+}
+
+static void vgt_el_slots_find_submitted_ctx(bool forward_search, vgt_state_ring_t *ring_state,
+			uint32_t ctx_id, int *el_slot_idx, int *el_slot_ctx_idx)
+{
+	int head = ring_state->el_slots_head;
+	int tail = ring_state->el_slots_tail;
+
+	*el_slot_idx = -1;
+	*el_slot_ctx_idx = -1;
+
+	while ((head != tail) && (*el_slot_idx == -1)) {
+		int i;
+		struct vgt_exec_list *el_slot;
+
+		if (forward_search) {
+			el_slot = &ring_state->execlist_slots[head];
+		} else {
+			if (tail == 0)
+				tail = EL_QUEUE_SLOT_NUM;
+			tail --;
+			el_slot = &ring_state->execlist_slots[tail];
+		}
+
+		if (el_slot->status != EL_SUBMITTED)
+			continue;
+
+		for (i = 0; i < 2; ++ i) {
+			struct execlist_context *p = el_slot->el_ctxs[i];
+			if (p && p->guest_context.context_id == ctx_id) {
+				*el_slot_idx = forward_search ? head : tail;
+				*el_slot_ctx_idx = i;
+				break;
+			}
+		}
+
+		if (forward_search) {
+			head ++;
+			if (head == EL_QUEUE_SLOT_NUM)
+				head = 0;
+		}
+	}
+}
+
+static int vgt_el_slots_next_sched(vgt_state_ring_t *ring_state)
+{
+	int head = ring_state->el_slots_head;
+	int tail = ring_state->el_slots_tail;
+	if (head == tail) {
+		// queue empty
+		return -1;
+	} else {
+		while (ring_state->execlist_slots[head].status != EL_PENDING) {
+			head ++;
+			if (head == tail) {
+				head = -1;
+				break;
+			} else if (head == EL_QUEUE_SLOT_NUM) {
+				head = 0;
+			}
+		}
+		return head;
+	}
+}
+
+static int vgt_el_slots_number(vgt_state_ring_t *ring_state)
+{
+	int num;
+	int head = ring_state->el_slots_head;
+	int tail = ring_state->el_slots_tail;
+
+	if (tail >= head)
+		num = tail - head;
+	else
+		num = tail + EL_QUEUE_SLOT_NUM - head;
+
+	return num;
+}
+
+/* validation functions */
+
+static inline bool el_lrca_is_valid(struct vgt_device *vgt, uint32_t lrca)
+{
+	bool rc;
+	uint32_t gma;
+
+	gma = lrca << GTT_PAGE_SHIFT;
+	rc = g_gm_is_valid(vgt, gma);
+	if (!rc) {
+		/* it is a shadow context */
+		rc = g_gm_is_reserved(vgt, gma);
+	}
+
+	return rc;
+}
+
+static inline bool vgt_validate_elsp_descs(struct vgt_device *vgt,
+			struct ctx_desc_format *ctx0,
+			struct ctx_desc_format *ctx1)
+{
+	if (!ctx0->valid) {
+		vgt_err("Context[0] is invalid! Which is not expected\n");
+		return false;
+	}
+
+	if (!el_lrca_is_valid(vgt, ctx0->lrca)) {
+		vgt_err("The context[0] in ELSP does not have a valid lrca(0x%x)!",
+				ctx0->lrca);
+		return false;
+	}
+
+	if (ctx1->valid) {
+		if (!el_lrca_is_valid(vgt, ctx1->lrca)) {
+			vgt_err("The context[1] in ELSP does not have a "
+				"valid lrca(0x%x)!", ctx1->lrca);
+			return false;
+		}
+	}
+
+	return true;
+}
+
+static bool vgt_validate_elsp_submission(struct vgt_device *vgt,
+			struct vgt_elsp_store *elsp_store)
+{
+	struct ctx_desc_format *ctx0;
+	struct ctx_desc_format *ctx1;
+	ctx0 = (struct ctx_desc_format *)&elsp_store->element[2];
+	ctx1 = (struct ctx_desc_format *)&elsp_store->element[0];
+
+	return vgt_validate_elsp_descs(vgt, ctx0, ctx1);
+}
+
+static bool vgt_validate_status_entry(struct vgt_device *vgt,
+			enum vgt_ring_id ring_id,
+			struct context_status_format *status)
+{
+	int i;
+
+	struct vgt_device *v_try = NULL;
+	struct execlist_context *el_ctx;
+
+	/* FIXME
+	 * a hack here to treat context_id as lrca. That is the current usage
+	 * in both linux and windows gfx drivers, and it is currently the only
+	 * way to get lrca from status.
+	 * The value is used to check whether a given context status belongs to
+	 * vgt.
+	 *
+	 * When context_id does not equal to lrca some day, the check will not
+	 * be valid.
+	 */
+	uint32_t lrca = status->context_id;
+
+	if (lrca == 0) {
+		/* Always return true for lrca as 0. Context status buffer
+		 * contains valid entry with lrca/ctx_id as 0, especially
+		 * for the idle_to_active events.
+		 */
+		return true;
+	}
+	if (el_lrca_is_valid(vgt, lrca))
+		return true;
+
+	el_ctx = execlist_context_find(vgt, lrca);
+	if (el_ctx == NULL) {
+		vgt_err("VM-%d sees unknown contextID/lrca (0x%x) "
+			"in status buffer!\n", vgt->vm_id, lrca);
+		return false;
+	}
+
+	/* only report error once for one context */
+	if (el_ctx->error_reported == 0)
+		el_ctx->error_reported = 1;
+	else
+		return false;
+
+	/* try to figure out which VM the lrca belongs to.
+	 * It is doable only when shadow context is not used.
+	 */
+	if (shadow_execlist_context == PATCH_WITHOUT_SHADOW) {
+		for (i = 0; i < VGT_MAX_VMS; ++ i) {
+			struct vgt_device *v = vgt->pdev->device[i];
+			if (!v)
+				continue;
+			if (el_lrca_is_valid(v, lrca)) {
+				v_try = v;
+				break;
+			}
+		}
+
+		if (v_try) {
+			vgt_err("The context(lrca: 0x%x) is given to VM-%d "
+			"But it belongs to VM-%d actually!\n",
+			lrca, vgt->vm_id, v_try->vm_id);
+		} else {
+			vgt_err("The given context(lrca: 0x%x) in status "
+			"buffer does not belong to any VM!\n", lrca);
+		}
+	}
+
+	dump_ctx_status_buf(vgt, ring_id, true);
+	dump_el_context_information(vgt, el_ctx);
+
+	vgt_err("The context(lrca: 0x%x) in status buffer will be ignored.\n", lrca);
+	return false;
+}
+
+/* context shadow: write protection handler */
+
+static inline void vgt_set_wp_guest_ctx(struct vgt_device *vgt,
+			struct execlist_context *el_ctx, int idx)
+{
+	enum vgt_ring_id ring_id;
+
+	if (!wp_submitted_ctx &&
+		(shadow_execlist_context != NORMAL_CTX_SHADOW)) {
+		/* If option is set not to protect submitted_ctx, write
+		 * protection will be disabled, except that the shadow policy
+		 * is NORMAL_CTX_SHADOW. In normal shadowing case, the write
+		 * protection is from context creation to the context destroy.
+		 * It is needed for guest-shadow data sync-up, and cannot be
+		 * disabled.
+		 */
+		return;
+	}
+
+	ring_id = el_ctx->ring_id;
+	hypervisor_set_wp_pages(vgt,
+				&el_ctx->ctx_pages[idx].guest_page);
+	trace_ctx_protection(vgt->vm_id, ring_id, el_ctx->guest_context.lrca,
+				idx, el_ctx->ctx_pages[idx].guest_page.gfn,
+				"set_writeprotection");
+}
+
+static inline void vgt_clear_wp_guest_ctx(struct vgt_device *vgt,
+			struct execlist_context *el_ctx, int idx)
+{
+	enum vgt_ring_id ring_id;
+
+	if (!wp_submitted_ctx &&
+		(shadow_execlist_context != NORMAL_CTX_SHADOW)) {
+		return;
+	}
+
+	ring_id = el_ctx->ring_id;
+	hypervisor_unset_wp_pages(vgt,
+				&el_ctx->ctx_pages[idx].guest_page);
+	trace_ctx_protection(vgt->vm_id, ring_id, el_ctx->guest_context.lrca,
+				idx, el_ctx->ctx_pages[idx].guest_page.gfn,
+				"clear_writeprotection");
+}
+
+static bool sctx_mirror_state_wp_handler(void *gp, uint64_t pa, void *p_data, int bytes)
+{
+	guest_page_t *guest_page = (guest_page_t *)gp;
+	struct shadow_ctx_page *ctx_page = container_of(guest_page,
+					struct shadow_ctx_page, guest_page);
+	uint32_t offset = pa & (PAGE_SIZE - 1);
+
+	trace_ctx_write_trap(pa, bytes);
+	if (!guest_page->writeprotection) {
+		vgt_err("EXECLIST Ctx mirror wp handler is called without write protection! "
+			"addr <0x%llx>, bytes %i\n", pa, bytes);
+		return false;
+	}
+
+	if ((offset & (bytes -1)) != 0)
+		vgt_warn("Not aligned EXECLIST context update!");
+
+	memcpy(((unsigned char *)ctx_page->guest_page.vaddr) + offset,
+				p_data, bytes);
+	memcpy(((unsigned char *)ctx_page->shadow_page.vaddr) + offset,
+				p_data, bytes);
+
+	return true;
+}
+
+#define check_ldw_offset(offset, i, bytes)				\
+do{									\
+	int pdp_ldw;							\
+	pdp_ldw = offsetof(struct reg_state_ctx_header, pdp##i##_LDW);	\
+	if (((offset == pdp_ldw) && (bytes == 8)) ||			\
+		((offset == pdp_ldw + 4) && (bytes == 4)))		\
+		return i;						\
+}while(0);
+
+static int ctx_offset_2_rootp_idx(uint32_t offset, int bytes)
+{
+	check_ldw_offset(offset, 0, bytes);
+	check_ldw_offset(offset, 1, bytes);
+	check_ldw_offset(offset, 2, bytes);
+	check_ldw_offset(offset, 3, bytes);
+
+	return -1;
+}
+
+static bool sctx_reg_state_wp_handler(void *gp, uint64_t pa, void *p_data, int bytes)
+{
+	guest_page_t *guest_page = (guest_page_t *)gp;
+	struct execlist_context *el_ctx = (struct execlist_context *)guest_page->data;
+
+	uint32_t offset = pa & (PAGE_SIZE - 1);
+	int idx;
+	bool rc;
+
+	trace_ctx_write_trap(pa, bytes);
+	if (!guest_page->writeprotection) {
+		vgt_err("EXECLIST Ctx regstate wp handler is called without write protection! "
+			"addr <0x%llx>, bytes %i\n", pa, bytes);
+		return false;
+	}
+
+	rc = sctx_mirror_state_wp_handler(gp, pa, p_data, bytes);
+	if (!rc)
+		return rc;
+
+	if ((offset & (bytes -1)) != 0) {
+		vgt_warn("Not aligned write found in EXECLIST ctx wp handler. "
+			"addr <0x%llx>, bytes <%i>", pa, bytes);
+	}
+
+	if ((bytes != 4) && (bytes != 8)) {
+		/* FIXME Do not expect it is the chagne to root pointers.
+		 * So return directly here. Add more check in future.
+		 */
+		return true;
+	}
+
+	idx = ctx_offset_2_rootp_idx(offset, bytes);
+	if (idx != -1) {
+		vgt_dbg(VGT_DBG_EXECLIST, "wp handler: Emulate the rootp[%d] change\n", idx);
+		rc = vgt_handle_guest_write_rootp_in_context(el_ctx, idx);
+	}
+
+	return rc;
+}
+
+/* context shadow: context sync-up between guest/shadow */
+
+static inline bool ppgtt_update_shadow_ppgtt_for_ctx(struct vgt_device *vgt,
+				struct execlist_context *el_ctx)
+{
+	bool rc = true;
+	int i;
+
+	if (!vgt_require_shadow_context(vgt))
+		return rc;
+
+	for (i = 0; i < el_ctx->ppgtt_mm->page_table_entry_cnt; ++ i) {
+		vgt_dbg(VGT_DBG_EXECLIST, "Emulate the rootp[%d] change\n", i);
+		rc = vgt_handle_guest_write_rootp_in_context(el_ctx, i);
+		if (!rc)
+			break;
+	}
+	return rc;
+}
+
+/* not to copy PDP root pointers */
+static void memcpy_reg_state_page(void *dest_page, void *src_page)
+{
+	uint32_t pdp_backup[8];
+	struct reg_state_ctx_header *dest_ctx;
+	struct reg_state_ctx_header *src_ctx;
+	dest_ctx = (struct reg_state_ctx_header *)(dest_page);
+	src_ctx = (struct reg_state_ctx_header *)(src_page);
+
+	pdp_backup[0] = dest_ctx->pdp0_LDW.val;
+	pdp_backup[1] = dest_ctx->pdp0_UDW.val;
+	pdp_backup[2] = dest_ctx->pdp1_LDW.val;
+	pdp_backup[3] = dest_ctx->pdp1_UDW.val;
+	pdp_backup[4] = dest_ctx->pdp2_LDW.val;
+	pdp_backup[5] = dest_ctx->pdp2_UDW.val;
+	pdp_backup[6] = dest_ctx->pdp3_LDW.val;
+	pdp_backup[7] = dest_ctx->pdp3_UDW.val;
+
+	memcpy(dest_page, src_page, SIZE_PAGE);
+
+	dest_ctx->pdp0_LDW.val = pdp_backup[0];
+	dest_ctx->pdp0_UDW.val = pdp_backup[1];
+	dest_ctx->pdp1_LDW.val = pdp_backup[2];
+	dest_ctx->pdp1_UDW.val = pdp_backup[3];
+	dest_ctx->pdp2_LDW.val = pdp_backup[4];
+	dest_ctx->pdp2_UDW.val = pdp_backup[5];
+	dest_ctx->pdp3_LDW.val = pdp_backup[6];
+	dest_ctx->pdp3_UDW.val = pdp_backup[7];
+}
+
+static void vgt_update_shadow_ctx_from_guest(struct vgt_device *vgt,
+			struct execlist_context *el_ctx)
+{
+	if (!vgt_require_shadow_context(vgt))
+		return;
+}
+
+static void vgt_update_guest_ctx_from_shadow(struct vgt_device *vgt,
+			enum vgt_ring_id ring_id,
+			struct execlist_context *el_ctx)
+{
+	int ctx_pages = EXECLIST_CTX_PAGES(ring_id);
+
+	if (shadow_execlist_context == PATCH_WITHOUT_SHADOW) {
+		struct reg_state_ctx_header *reg_state;
+		uint32_t *g_rootp;
+		g_rootp = (uint32_t *)el_ctx->ppgtt_mm->virtual_page_table;
+		reg_state = (struct reg_state_ctx_header *)
+			el_ctx->ctx_pages[1].guest_page.vaddr;
+		ROOT_POINTER_2_CTX_STATE(reg_state, g_rootp, 0);
+		ROOT_POINTER_2_CTX_STATE(reg_state, g_rootp, 1);
+		ROOT_POINTER_2_CTX_STATE(reg_state, g_rootp, 2);
+		ROOT_POINTER_2_CTX_STATE(reg_state, g_rootp, 3);
+	} else {
+		int i;
+		for (i = 0; i < ctx_pages; ++ i) {
+			void *dst = el_ctx->ctx_pages[i].guest_page.vaddr;
+			void *src = el_ctx->ctx_pages[i].shadow_page.vaddr;
+
+			ASSERT(dst && src);
+			if (i == 1)
+				memcpy_reg_state_page(dst, src);
+			else
+				memcpy(dst, src, SIZE_PAGE);
+		}
+	}
+}
+
+static void vgt_patch_guest_context(struct execlist_context *el_ctx)
+{
+	struct reg_state_ctx_header *guest_state;
+	struct reg_state_ctx_header *shadow_state;
+
+	guest_state = (struct reg_state_ctx_header *)
+			el_ctx->ctx_pages[1].guest_page.vaddr;
+	shadow_state = (struct reg_state_ctx_header *)
+			el_ctx->ctx_pages[1].shadow_page.vaddr;
+
+	ROOTP_CTX_STATE_2_CTX_STATE(guest_state, shadow_state, 0);
+	ROOTP_CTX_STATE_2_CTX_STATE(guest_state, shadow_state, 1);
+	ROOTP_CTX_STATE_2_CTX_STATE(guest_state, shadow_state, 2);
+	ROOTP_CTX_STATE_2_CTX_STATE(guest_state, shadow_state, 3);
+}
+
+/* context shadow: context creation/destroy in execlist */
+
+static struct execlist_context *vgt_allocate_el_context(struct vgt_device *vgt,
+				struct ctx_desc_format *ctx_desc)
+{
+	uint32_t guest_lrca;
+	struct execlist_context *el_ctx;
+
+	el_ctx = kzalloc(sizeof(struct execlist_context), GFP_ATOMIC);
+	if (!el_ctx) {
+		vgt_err("Failed to allocate data structure for shadow context!\n");
+		return NULL;
+	}
+
+	memcpy(&el_ctx->guest_context, ctx_desc, sizeof(struct ctx_desc_format));
+
+	guest_lrca = el_ctx->guest_context.lrca;
+	hash_add(vgt->gtt.el_ctx_hash_table, &el_ctx->node, guest_lrca);
+
+	return el_ctx;
+}
+
+static void vgt_el_create_shadow_context(struct vgt_device *vgt,
+				enum vgt_ring_id ring_id,
+				struct execlist_context *el_ctx)
+{
+	struct vgt_gtt_pte_ops *ops = vgt->pdev->gtt.pte_ops;
+	uint32_t gfn;
+	uint32_t shadow_context_gma;
+	uint32_t guest_context_gma;
+	uint32_t sl, gl;
+	uint32_t rsvd_pages_idx;
+	uint32_t rsvd_aperture_gm;
+	int i;
+	int ctx_pages = EXECLIST_CTX_PAGES(ring_id);
+
+	guest_context_gma = el_ctx->guest_context.lrca << GTT_PAGE_SHIFT;
+
+	shadow_context_gma = aperture_2_gm(vgt->pdev,
+				rsvd_aperture_alloc(vgt->pdev,
+					(EXECLIST_CTX_PAGES(ring_id) << GTT_PAGE_SHIFT)));
+
+	ASSERT((shadow_context_gma & 0xfff) == 0);
+	el_ctx->shadow_lrca = shadow_context_gma >> GTT_PAGE_SHIFT;
+
+	rsvd_aperture_gm = aperture_2_gm(vgt->pdev, vgt->pdev->rsvd_aperture_base);
+	rsvd_pages_idx = el_ctx->shadow_lrca - (rsvd_aperture_gm >> GTT_PAGE_SHIFT);
+
+	vgt_dbg(VGT_DBG_EXECLIST, "Allocating aperture for shadow context "
+			"with idx: 0x%x and addr: 0x%x\n",
+			rsvd_pages_idx, shadow_context_gma);
+
+	/* per page copy from guest context to shadow context since its virtual
+	 * address may not be sequential.
+	 */
+	for (i = 0, sl = shadow_context_gma, gl = guest_context_gma; i < ctx_pages;
+			++ i, ++ rsvd_pages_idx, sl += SIZE_PAGE, gl += SIZE_PAGE) {
+		gtt_entry_t e;
+		e.pdev = vgt->pdev;
+		e.type = GTT_TYPE_GGTT_PTE;
+
+		ggtt_get_guest_entry(vgt->gtt.ggtt_mm, &e, gl >> GTT_PAGE_SHIFT);
+
+		gfn = ops->get_pfn(&e);
+		vgt_dbg(VGT_DBG_EXECLIST,
+			"pfn for context page %i (gma: 0x%x)is: 0x%x\n", i, gl, gfn);
+		if (i == 1) {
+			vgt_init_guest_page(vgt, &el_ctx->ctx_pages[i].guest_page,
+				gfn, sctx_reg_state_wp_handler, &el_ctx);
+		} else {
+			vgt_init_guest_page(vgt, &el_ctx->ctx_pages[i].guest_page,
+				gfn, sctx_mirror_state_wp_handler, &el_ctx);
+		}
+
+		/* backup the shadow context gtt entry */
+		el_ctx->shadow_entry_backup[i].pdev = vgt->pdev;
+		el_ctx->shadow_entry_backup[i].type = GTT_TYPE_GGTT_PTE;
+		ops->get_entry(NULL, &el_ctx->shadow_entry_backup[i],
+						sl >> GTT_PAGE_SHIFT, false, NULL);
+
+		{
+			el_ctx->ctx_pages[i].shadow_page.vaddr =
+				phys_aperture_vbase(vgt->pdev) + sl;
+			el_ctx->ctx_pages[i].shadow_page.page =
+				(*vgt->pdev->rsvd_aperture_pages)[rsvd_pages_idx];
+			ASSERT(el_ctx->ctx_pages[i].shadow_page.vaddr &&
+				el_ctx->ctx_pages[i].guest_page.vaddr);
+
+			vgt_dbg(VGT_DBG_EXECLIST, "memory copy for context page %d: dst addr: 0x%llx; "
+					"src addr: 0x%llx\n",
+				i, (u64)el_ctx->ctx_pages[i].shadow_page.vaddr,
+				(u64)el_ctx->ctx_pages[i].guest_page.vaddr);
+
+			if (shadow_execlist_context == NORMAL_CTX_SHADOW) {
+				memcpy(el_ctx->ctx_pages[i].shadow_page.vaddr,
+				el_ctx->ctx_pages[i].guest_page.vaddr, SIZE_PAGE);
+				vgt_set_wp_guest_ctx(vgt, el_ctx, i);
+			}
+		}
+		el_ctx->ctx_pages[i].vgt = vgt;
+	}
+}
+
+static bool vgt_el_create_shadow_ppgtt(struct vgt_device *vgt,
+				enum vgt_ring_id ring_id,
+				struct execlist_context *el_ctx)
+{
+	struct vgt_mm *mm;
+	u32 pdp[8];
+	uint32_t *s_rootp;
+
+	struct reg_state_ctx_header *reg_state;
+	struct ctx_desc_format *guest_ctx = &el_ctx->guest_context;
+	gtt_type_t root_entry_type;
+	int page_table_level;
+
+	if (guest_ctx->addressing_mode == 1) { /* legacy 32-bit */
+		page_table_level = 3;
+		root_entry_type = GTT_TYPE_PPGTT_ROOT_L3_ENTRY;
+	} else if (guest_ctx->addressing_mode == 3) { /* legacy 64 bit */
+		page_table_level = 4;
+		root_entry_type = GTT_TYPE_PPGTT_ROOT_L4_ENTRY;
+	} else {
+		page_table_level = 4;
+		root_entry_type = GTT_TYPE_PPGTT_ROOT_L4_ENTRY;
+		vgt_err("Advanced Context mode(SVM) is not supported!\n");
+	}
+
+	if (vgt_require_shadow_context(vgt)) {
+		reg_state = (struct reg_state_ctx_header *)
+				el_ctx->ctx_pages[1].guest_page.vaddr;
+	} else {
+		ASSERT(vgt->vm_id == 0);
+		reg_state = vgt_get_reg_state_from_lrca(vgt,
+				el_ctx->guest_context.lrca);
+	}
+
+	CTX_STATE_2_ROOT_POINTER(pdp, reg_state, 0);
+	if (page_table_level == 3) {
+		CTX_STATE_2_ROOT_POINTER(pdp, reg_state, 1);
+		CTX_STATE_2_ROOT_POINTER(pdp, reg_state, 2);
+		CTX_STATE_2_ROOT_POINTER(pdp, reg_state, 3);
+	}
+
+	mm = gen8_find_ppgtt_mm(vgt, page_table_level, pdp);
+	if (mm)
+		goto ppgtt_creation_done;
+
+	mm = vgt_create_mm(vgt, VGT_MM_PPGTT, root_entry_type,
+			pdp, page_table_level, 0);
+	if (!mm) {
+		vgt_err("fail to create mm object.\n");
+		return false;
+	}
+
+	vgt_warn("Given PPGTT in EL context for creation is not yet constructed! "
+		"It is not expected to happen! lrca = 0x%x\n",
+		el_ctx->guest_context.lrca);
+	dump_regstate_ctx_header(reg_state);
+
+ppgtt_creation_done:
+	el_ctx->ppgtt_mm = mm;
+
+	if (!mm->has_shadow_page_table)
+		goto finish;
+
+	/* update root pointers in context with shadow ones */
+	s_rootp = (uint32_t *)mm->shadow_page_table;
+	reg_state = (struct reg_state_ctx_header *)
+			el_ctx->ctx_pages[1].shadow_page.vaddr;
+
+	ROOT_POINTER_2_CTX_STATE(reg_state, s_rootp, 0);
+	if (page_table_level == 3) {
+		ROOT_POINTER_2_CTX_STATE(reg_state, s_rootp, 1);
+		ROOT_POINTER_2_CTX_STATE(reg_state, s_rootp, 2);
+		ROOT_POINTER_2_CTX_STATE(reg_state, s_rootp, 3);
+	}
+finish:
+	if (vgt_debug & VGT_DBG_EXECLIST) {
+		vgt_dbg(VGT_DBG_EXECLIST,
+			"VM-%d: The reg_state after shadow PPGTT creation:\n",
+			vgt->vm_id);
+		dump_el_context_information(vgt, el_ctx);
+	}
+	return true;
+}
+
+static struct execlist_context *vgt_create_execlist_context(struct vgt_device *vgt,
+				struct ctx_desc_format *ctx, enum vgt_ring_id ring_id)
+{
+	struct execlist_context *el_ctx;
+
+	vgt_dbg(VGT_DBG_EXECLIST, "creating new execlist context with desc below:\n");
+	if (vgt_debug & VGT_DBG_EXECLIST)
+		dump_ctx_desc(vgt, ctx);
+
+	ASSERT (execlist_context_find(vgt, ctx->lrca) == NULL);
+
+	if (ring_id == MAX_ENGINES) {
+		ring_id = vgt_get_ringid_from_lrca(vgt, ctx->lrca);
+		if (ring_id == MAX_ENGINES) {
+			vgt_warn("VM-%d: Invalid execlist context! "
+			"Ring info is not available in ring context.\n",
+					vgt->vm_id);
+			dump_ctx_desc(vgt, ctx);
+			return NULL;
+		}
+	}
+
+	el_ctx = vgt_allocate_el_context(vgt, ctx);
+	if (el_ctx == NULL)
+		return NULL;
+
+	el_ctx->ring_id = ring_id;
+
+	if (vgt_require_shadow_context(vgt))
+		vgt_el_create_shadow_context(vgt, ring_id, el_ctx);
+
+	vgt_el_create_shadow_ppgtt(vgt, ring_id, el_ctx);
+
+	trace_ctx_lifecycle(vgt->vm_id, ring_id,
+			el_ctx->guest_context.lrca, "create");
+	return el_ctx;
+}
+
+static void vgt_destroy_execlist_context(struct vgt_device *vgt,
+				struct execlist_context *el_ctx)
+{
+	int ctx_pages;
+	enum vgt_ring_id ring_id;
+	int i;
+
+	if (el_ctx == NULL)
+		return;
+
+	ring_id = el_ctx->ring_id;
+	if (ring_id == MAX_ENGINES) {
+		vgt_err("Invalid execlist context!\n");
+		ASSERT_VM(0, vgt);
+	}
+
+	trace_ctx_lifecycle(vgt->vm_id, ring_id,
+			el_ctx->guest_context.lrca, "destroy");
+
+	ctx_pages = EXECLIST_CTX_PAGES(ring_id);
+
+	for (i = 0; i < ctx_pages; ++ i) {
+		// remove the write protection;
+		if (shadow_execlist_context == NORMAL_CTX_SHADOW) {
+			hypervisor_unset_wp_pages(vgt,
+				&el_ctx->ctx_pages[i].guest_page);
+		}
+		vgt_clean_guest_page(vgt, &el_ctx->ctx_pages[i].guest_page);
+	}
+
+	// free the shadow context;
+	if (vgt_require_shadow_context(vgt)) {
+		unsigned long start;
+		unsigned int shadow_lrca = el_ctx->shadow_lrca;
+
+		ASSERT(hvm_render_owner || shadow_lrca);
+		if (!hvm_render_owner) {
+			start = phys_aperture_base(vgt->pdev) +
+					(shadow_lrca << GTT_PAGE_SHIFT);
+			rsvd_aperture_free(vgt->pdev, start,
+					ctx_pages << GTT_PAGE_SHIFT);
+		}
+	}
+
+	hash_del(&el_ctx->node);
+	kfree(el_ctx);
+}
+
+/* emulate the EXECLIST related MMIOs when vgt is not render owner,
+ * so that guest drivers treat the submission as a successful one.
+ * Currently we simply emulate the status register (234h) to reflect
+ * the active execlist, which is a must for the future other execlist
+ * submission. Others, like status buffer, keep unchanged. That gives
+ * guest driver the impression that submission has finished, but the
+ * contexts have not yet entered hardware.
+ *
+ * There is another option to emulate more here, for instance, to send
+ * virtual context switch interrupt of idle-to-active for the first
+ * execlist submission, and update virtual status buffer accordingly.
+ * But such emulation will bring complexity when the real ELSP write
+ * happens. We have to recognize the duplicated physical context switch
+ * interrupt and delete that one.
+ */
+
+static void vgt_emulate_submit_execlist(struct vgt_device *vgt, int ring_id,
+			struct execlist_context *ctx0,
+			struct execlist_context *ctx1)
+{
+	struct execlist_status_format status;
+	bool render_owner = is_current_render_owner(vgt);
+	uint32_t status_reg = el_ring_mmio(ring_id, _EL_OFFSET_STATUS);
+	uint32_t el_index;
+
+	if (!vgt_el_slots_enqueue(vgt, ring_id, ctx0, ctx1)) {
+		vgt_err("VM-%d: <ring-%d> EXECLIST slots are full while adding new contexts! "
+			"Contexts will be ignored:\n"
+			" -ctxid-0: 0x%x\n", vgt->vm_id, ring_id,
+				ctx0->guest_context.context_id);
+	}
+
+	if (render_owner)
+		return;
+
+	/* emulate status register below */
+	status.ldw = __vreg(vgt, status_reg);
+	status.udw = __vreg(vgt, status_reg + 4);
+	el_index = status.execlist_write_pointer;
+	if (status.execlist_queue_full) {
+		vgt_err("VM(%d): EXECLIST submission while the EL is full! "
+			"The submission will be ignored!\n", vgt->vm_id);
+		dump_execlist_info(vgt->pdev, ring_id);
+		return;
+	}
+
+	status.execlist_write_pointer = (el_index == 0 ? 1 : 0);
+
+	/* TODO
+	 * 1, Check whether we should set below two states. According to the observation
+	 * from dom0, when there is ELSP write, both active bit and valid bit will be
+	 * set.
+	 * 2, Consider the emulation of preemption and lite restore.
+	 * It is designed to be in context switch by adding corresponding status entries
+	 * into status buffer.
+	 */
+	if (el_index == 0) {
+		status.execlist_0_active = 1;
+		status.execlist_0_valid = 1;
+	} else {
+		status.execlist_1_active = 1;
+		status.execlist_1_valid = 1;
+	}
+
+	/* TODO emulate the status. Need double confirm
+	 *
+	 * Here non-render owner will not receive context switch interrupt
+	 * until it becomes a render owner. Meanwhile, the status register
+	 * is emulated to reflect the port submission operation.
+	 * It is noticed that the initial value of "current_execlist_pointer"
+	 * and "execlist_write_pointer" does not equal although the EXECLISTS
+	 * are all empty. It is then not appropriate to emulate "execlist_queue_full"
+	 * with the two bit value. Instead, the "execlist_queue_full" will be
+	 * set if valid bits of both "EXECLIST 0" and "EXECLIST 1" are set.
+	 * This needs the double confirm.
+	 */
+	if (status.execlist_0_valid && status.execlist_1_valid) {
+		status.execlist_queue_full = 1;
+		vgt_dbg(VGT_DBG_EXECLIST,"VM-%d: ring(%d) EXECLISTS becomes "
+			"full due to workload submission!\n",
+				vgt->vm_id, ring_id);
+		dump_execlist_status(&status, ring_id);
+	}
+
+	__vreg(vgt, status_reg) = status.ldw;
+	__vreg(vgt, status_reg + 4) = status.udw;
+}
+
+struct execlist_context * execlist_context_find(struct vgt_device *vgt,
+				uint32_t guest_lrca)
+{
+	struct execlist_context *el_ctx;
+	hash_for_each_possible(vgt->gtt.el_ctx_hash_table, el_ctx, node, guest_lrca) {
+		if (el_ctx->guest_context.lrca == guest_lrca)
+			return el_ctx;
+	}
+
+	return NULL;
+}
+
+/* guest context event emulation */
+
+static inline void vgt_add_ctx_switch_status(struct vgt_device *vgt, enum vgt_ring_id ring_id,
+			struct context_status_format *ctx_status)
+{
+	uint32_t ctx_status_reg;
+	uint32_t write_idx;
+	uint32_t offset;
+
+	ctx_status_reg = el_ring_mmio(ring_id, _EL_OFFSET_STATUS_BUF);
+
+	write_idx = vgt->rb[ring_id].csb_write_ptr;
+	if (write_idx == DEFAULT_INV_SR_PTR) {
+		write_idx = 0;
+	} else {
+		write_idx ++;
+		if (write_idx >= CTX_STATUS_BUF_NUM)
+			write_idx = 0;
+	}
+
+	offset = ctx_status_reg + write_idx * 8;
+	__vreg(vgt, offset) = ctx_status->ldw;
+	__vreg(vgt, offset + 4) = ctx_status->udw;
+
+	vgt->rb[ring_id].csb_write_ptr = write_idx;
+}
+
+static bool vgt_save_last_execlist_context(struct vgt_device *vgt,
+	enum vgt_ring_id ring_id, struct execlist_context *ctx)
+{
+	struct vgt_mm *mm = vgt->gtt.ggtt_mm;
+	void *dst = v_aperture(vgt->pdev, vgt->rb[ring_id].context_save_area);
+	u32 lrca = ctx->guest_context.lrca;
+	int nr_page = EXECLIST_CTX_PAGES(ring_id);
+	u32 *ring_context;
+	void *src;
+	int i;
+
+	if (ring_id != RING_BUFFER_RCS)
+		return true;
+
+	ring_context = vgt_gma_to_va(mm, (lrca + 1) << GTT_PAGE_SHIFT);
+	if (!ring_context) {
+		vgt_err("Fail to find ring_context, lrca %x.\n", lrca);
+		return false;
+	}
+
+	if (ring_context[1] != 0x1100101b) {
+		vgt_err("Not a valid guest context?!\n");
+		return false;
+	}
+
+	for (i = 1; i < nr_page; i++, dst += SIZE_PAGE) {
+		src = vgt_gma_to_va(mm, (lrca + i) << GTT_PAGE_SHIFT);
+		memcpy(dst, src, SIZE_PAGE);
+	}
+
+	vgt->has_context = 1;
+	return true;
+}
+
+static void vgt_emulate_context_status_change(struct vgt_device *vgt,
+				enum vgt_ring_id ring_id,
+				struct context_status_format *ctx_status)
+{
+	bool forward_search = true;
+	vgt_state_ring_t *ring_state;
+	uint32_t el_slot_ctx_idx = -1;
+	uint32_t el_slot_idx = -1;
+	struct vgt_exec_list *el_slot = NULL;
+	struct execlist_context *el_ctx = NULL;
+	uint32_t ctx_id = ctx_status->context_id;
+
+	ring_state = &vgt->rb[ring_id];
+	if (vgt_el_slots_number(ring_state) > 1) {
+		if (!ctx_status->preempted) {
+			/* TODO we may give warning here.
+			 * It is not expected but still work.
+			 */
+			forward_search = false;
+		}
+	}
+
+	vgt_el_slots_find_submitted_ctx(forward_search, ring_state, ctx_id,
+				&el_slot_idx, &el_slot_ctx_idx);
+	if (el_slot_idx == -1)
+		goto err_ctx_not_found;
+
+	el_slot = &vgt_el_queue_slot(vgt, ring_id, el_slot_idx);
+
+	ASSERT((el_slot_ctx_idx == 0) || (el_slot_ctx_idx == 1));
+	el_ctx = el_slot->el_ctxs[el_slot_ctx_idx];
+
+	if (CTX_IS_SCHEDULED_OUT(ctx_status)) {
+		char str[64];
+		snprintf(str, 64, "finish_running. status[0x%x]", ctx_status->ldw);
+		trace_ctx_lifecycle(vgt->vm_id, ring_id,
+			el_ctx->guest_context.lrca,
+			str);
+		if ((((el_slot_ctx_idx == 0) || (el_slot->el_ctxs[0] == NULL)) &&
+			((el_slot_ctx_idx == 1) || (el_slot->el_ctxs[1] == NULL))) ||
+			(ctx_status->preempted)) {
+			vgt_el_slots_delete(vgt, ring_id, el_slot_idx);
+		}
+		el_slot->el_ctxs[el_slot_ctx_idx] = NULL;
+	} else {
+		goto emulation_done;
+	}
+
+	if (ctx_status->context_complete)
+		vgt_save_last_execlist_context(vgt, ring_id, el_ctx);
+
+	if (!vgt_require_shadow_context(vgt))
+		goto emulation_done;
+
+	if (vgt_debug & VGT_DBG_EXECLIST)
+		dump_el_context_information(vgt, el_ctx);
+
+	if (ctx_status->context_complete)
+		vgt_update_guest_ctx_from_shadow(vgt, ring_id, el_ctx);
+
+emulation_done:
+	return;
+err_ctx_not_found:
+	{
+		static int warned_once = 0;
+		if ((ctx_id != 0) && !warned_once) {
+			warned_once = 1;
+			vgt_err("VM(%d) Ring(%d): Trying to emulate context status change"
+			" but did not find the shadow context in execlist!\n"
+			"\t\tContext ID: 0x%x; status: 0x%x\n", vgt->vm_id, ring_id,
+				ctx_id, ctx_status->ldw);
+		}
+	}
+	return;
+}
+
+static void vgt_emulate_csb_updates(struct vgt_device *vgt, enum vgt_ring_id ring_id)
+{
+	struct ctx_st_ptr_format ctx_ptr_val;
+	uint32_t ctx_ptr_reg;
+	uint32_t ctx_status_reg;
+
+	int read_idx;
+	int write_idx;
+
+	ctx_ptr_reg = el_ring_mmio(ring_id, _EL_OFFSET_STATUS_PTR);
+	ctx_ptr_val.dw = VGT_MMIO_READ(vgt->pdev, ctx_ptr_reg);
+
+	read_idx = el_read_ptr(vgt->pdev, ring_id);
+	ASSERT(read_idx == ctx_ptr_val.status_buf_read_ptr);
+
+	write_idx = el_write_ptr(vgt->pdev, ring_id);
+
+#ifdef EL_SLOW_DEBUG
+	if (vgt_debug & VGT_DBG_EXECLIST) {
+		vgt_dbg(VGT_DBG_EXECLIST, "Physical CTX Status buffer is below:\n");
+		dump_ctx_status_buf(vgt, ring_id, true);
+		vgt_dbg(VGT_DBG_EXECLIST, "Virtual CTX Status buffer before buffer "
+					"update is below:\n");
+		dump_ctx_status_buf(vgt, ring_id, false);
+	}
+#endif
+	if (write_idx == DEFAULT_INV_SR_PTR) {
+		vgt_err("No valid context switch status buffer in switch interrupts!\n");
+		return;
+	}
+
+	if (read_idx == write_idx) {
+		vgt_dbg(VGT_DBG_EXECLIST, "No status buffer update.\n");
+		return;
+	}
+
+	if (read_idx == DEFAULT_INV_SR_PTR) {
+		/* The first read of the status buffer will be from buffer entry 0 */
+		read_idx = -1;
+	}
+
+	if (read_idx > write_idx)
+		write_idx += CTX_STATUS_BUF_NUM;
+
+	ctx_status_reg = el_ring_mmio(ring_id, _EL_OFFSET_STATUS_BUF);
+	while (read_idx < write_idx) {
+		struct context_status_format ctx_status;
+		uint32_t offset;
+		read_idx ++;
+		offset = ctx_status_reg + (read_idx % CTX_STATUS_BUF_NUM) * 8;
+		READ_STATUS_MMIO(vgt->pdev, offset, ctx_status);
+
+		if (!vgt_validate_status_entry(vgt, ring_id, &ctx_status))
+			continue;
+
+		vgt_emulate_context_status_change(vgt, ring_id, &ctx_status);
+		vgt_add_ctx_switch_status(vgt, ring_id, &ctx_status);
+	}
+
+	read_idx = write_idx % CTX_STATUS_BUF_NUM;
+	el_read_ptr(vgt->pdev, ring_id) = read_idx;
+	ctx_ptr_val.status_buf_read_ptr = read_idx;
+	ctx_ptr_val.mask = _CTXBUF_READ_PTR_MASK;
+	VGT_MMIO_WRITE(vgt->pdev, ctx_ptr_reg, ctx_ptr_val.dw);
+
+#ifdef EL_SLOW_DEBUG
+	if (vgt_debug & VGT_DBG_EXECLIST) {
+		vgt_dbg(VGT_DBG_EXECLIST, "Virtual CTX Status buffer after buffer "
+					"update is below:\n");
+		dump_ctx_status_buf(vgt, ring_id, false);
+	}
+#endif
+}
+
+void vgt_emulate_context_switch_event(struct pgt_device *pdev,
+				enum vgt_ring_id ring_id)
+{
+	enum vgt_event_type event;
+	int cache_wptr;
+	struct vgt_irq_host_state *hstate = pdev->irq_hstate;
+	struct vgt_device *vgt = current_render_owner(pdev);
+
+	if (!vgt) {
+		vgt_err("Receiving context switch interrupt while there is "
+			"no render owner set in system!\n");
+		BUG();
+		return;
+	}
+
+	ASSERT(spin_is_locked(&pdev->lock));
+
+	cache_wptr = el_write_ptr(pdev, ring_id);
+	if ((cache_wptr == DEFAULT_INV_SR_PTR) ||
+			(cache_wptr == el_read_ptr(pdev, ring_id)))
+		return;
+
+	/* we cannot rely on hstate->pending_events that is set in irq handler
+	 * to tell us which ring is having context switch interrupts. The
+	 * reason is that irq physical handler could happen in parallel with this
+	 * function, and the bit can be cleared by the previous forwarding
+	 * function. It's a race condition.
+	 */
+	vgt_emulate_csb_updates(vgt, ring_id);
+	event = vgt_ring_id_to_ctx_event(ring_id);
+	set_bit(event, hstate->pending_events);
+	set_bit(VGT_REQUEST_IRQ, (void *)&pdev->request);
+}
+
+/* scheduling */
+#if 0
+static void vgt_emulate_el_preemption(struct vgt_device *vgt, enum vgt_ring_id ring_id)
+{
+	int el_slot_idx;
+	int num;
+	struct vgt_exec_list *el_slot;
+	struct execlist_context *el_ctx;
+	vgt_state_ring_t *ring_state;
+	struct context_status_format ctx_status;
+	enum vgt_event_type ctx_event;
+
+	ring_state = &vgt->rb[ring_id];
+	num = vgt_el_slots_number(ring_state);
+	if (num <= 1)
+		return;
+
+	ASSERT(num == 2);
+	el_slot_idx = vgt_el_slots_dequeue(vgt, ring_id);
+	el_slot = &vgt_el_queue_slot(vgt, ring_id, el_slot_idx);
+	ctx_event = vgt_ring_id_to_ctx_event(ring_id);
+
+	/* we do not need to care the second context in the preempted
+	 * execlist, even if it has.
+	 */
+	el_ctx = el_slot->el_ctxs[0];
+	ASSERT(el_ctx);
+
+	ctx_status.ldw = 0;
+	ctx_status.context_id = el_ctx->guest_context.context_id;
+	ctx_status.idle_to_active = 1;
+
+	vgt_add_ctx_switch_status(vgt, ring_id, &ctx_status);
+	vgt_trigger_virtual_event(vgt, ctx_event);
+
+	/* TODO
+	 * We could emulate lite restore here, but do not seem
+	 * to be a must right now.
+	 */
+	ctx_status.ldw = 0;
+	ctx_status.preempted = 1;
+
+	vgt_add_ctx_switch_status(vgt, ring_id, &ctx_status);
+	vgt_trigger_virtual_event(vgt, ctx_event);
+
+	trace_ctx_lifecycle(vgt->vm_id, ring_id,
+				el_ctx->guest_context.lrca,
+				"emulated_preemption");
+
+	el_slot->status = EL_EMPTY;
+	el_slot->el_ctxs[0] = NULL;
+	el_slot->el_ctxs[1] = NULL;
+}
+#endif
+static inline bool vgt_hw_ELSP_write(struct vgt_device *vgt,
+				unsigned int reg,
+				struct ctx_desc_format *ctx0,
+				struct ctx_desc_format *ctx1)
+{
+	int rc = true;
+
+	ASSERT(ctx0 && ctx1);
+
+	vgt_dbg(VGT_DBG_EXECLIST, "EXECLIST is submitted into hardware! "
+			"Writing 0x%x with: 0x%x; 0x%x; 0x%x; 0x%x\n",
+			reg,
+			ctx1->elm_high, ctx1->elm_low,
+			ctx0->elm_high, ctx0->elm_low);
+
+	vgt_force_wake_get();
+
+	VGT_MMIO_WRITE(vgt->pdev, reg, ctx1->elm_high);
+	VGT_MMIO_WRITE(vgt->pdev, reg, ctx1->elm_low);
+	VGT_MMIO_WRITE(vgt->pdev, reg, ctx0->elm_high);
+	VGT_MMIO_WRITE(vgt->pdev, reg, ctx0->elm_low);
+
+	vgt_force_wake_put();
+
+	return rc;
+}
+
+/* Below format is considered to be the buffer resubmission from preemption.
+ * <head> - <tail> - <last_tail>
+ */
+#define IS_PREEMPTION_RESUBMISSION(head, tail, last_tail)	\
+((((head) < (last_tail)) &&					\
+	((tail) < (last_tail)) &&				\
+	((tail) > (head))) ||					\
+ (((head) > (last_tail)) &&					\
+	!(((tail) >= (last_tail)) &&				\
+	  ((tail) <= (head)))))
+
+static void vgt_update_ring_info(struct vgt_device *vgt,
+			struct execlist_context *el_ctx)
+{
+	struct reg_state_ctx_header *guest_state;
+	vgt_ringbuffer_t	*vring;
+	enum vgt_ring_id ring_id = el_ctx->ring_id;
+
+	if (vgt_require_shadow_context(vgt)) {
+		guest_state = (struct reg_state_ctx_header *)
+			el_ctx->ctx_pages[1].guest_page.vaddr;
+	} else {
+		ASSERT(vgt->vm_id == 0);
+		guest_state = vgt_get_reg_state_from_lrca(vgt,
+					el_ctx->guest_context.lrca);
+	}
+
+	vring = &vgt->rb[ring_id].vring;
+
+	vring->tail = guest_state->ring_tail.val & RB_TAIL_OFF_MASK;
+	vring->head = guest_state->ring_header.val & RB_HEAD_OFF_MASK;
+	vring->start = guest_state->rb_start.val;
+	vring->ctl = guest_state->rb_ctrl.val;
+#if 0
+	if (vgt->rb[ring_id].active_ppgtt_mm) {
+		vgt_warn("vgt has ppgtt set for ring_id %d: 0x%llx\n",
+				ring_id,
+				(unsigned long long)vgt->rb[ring_id].active_ppgtt_mm);
+	}
+#endif
+	/* TODO
+	 * Will have better way to handle the per-rb value.
+	 * Right now we just leverage the cmd_scan/schedule code for ring buffer mode
+	 */
+	vgt->rb[ring_id].active_ppgtt_mm = el_ctx->ppgtt_mm;
+	vgt->rb[ring_id].has_ppgtt_mode_enabled = 1;
+	vgt->rb[ring_id].has_ppgtt_base_set = 1;
+	vgt->rb[ring_id].request_id = el_ctx->request_id;
+	vgt->rb[ring_id].last_scan_head = el_ctx->last_scan_head;
+	if (!IS_PREEMPTION_RESUBMISSION(vring->head, vring->tail, el_ctx->last_scan_head)) {
+		vgt->rb[ring_id].last_scan_head = el_ctx->last_scan_head;
+		vgt_scan_vring(vgt, ring_id);
+	}
+
+	/* the function is used to update ring/buffer only. No real submission inside */
+	vgt_submit_commands(vgt, ring_id);
+
+	el_ctx->request_id = vgt->rb[ring_id].request_id;
+	el_ctx->last_scan_head = vring->tail;
+	vgt->rb[ring_id].active_ppgtt_mm = NULL;
+}
+
+void vgt_kick_off_execlists(struct vgt_device *vgt)
+{
+	int i;
+	struct pgt_device *pdev = vgt->pdev;
+
+	for (i = 0; i < pdev->max_engines; i ++) {
+		int j;
+		int num = vgt_el_slots_number(&vgt->rb[i]);
+
+		vgt->rb[i].check_uninitialized_context = true;
+
+		if (num == 2)
+			vgt_dbg(VGT_DBG_EXECLIST,
+				"VM(%d) Ring-%d: Preemption is met while "
+				"kicking off execlists.\n", vgt->vm_id, i);
+		for (j = 0; j < num; ++ j)
+			vgt_submit_execlist(vgt, i);
+	}
+}
+
+bool vgt_idle_execlist(struct pgt_device *pdev, enum vgt_ring_id ring_id)
+{
+	uint32_t el_ring_base;
+	uint32_t el_status_reg;
+	struct execlist_status_format el_status;
+	uint32_t ctx_ptr_reg;
+	struct ctx_st_ptr_format ctx_st_ptr;
+
+	el_ring_base = vgt_ring_id_to_EL_base(ring_id);
+	el_status_reg = el_ring_base + _EL_OFFSET_STATUS;
+	el_status.ldw = VGT_MMIO_READ(pdev, el_status_reg);
+	if (el_status.execlist_0_valid || el_status.execlist_1_valid) {
+		vgt_info("EXECLIST still have valid items in context switch!\n");
+		return false;
+	}
+
+	ctx_ptr_reg = el_ring_mmio(ring_id, _EL_OFFSET_STATUS_PTR);
+	ctx_st_ptr.dw = VGT_MMIO_READ(pdev, ctx_ptr_reg);
+
+	if (ctx_st_ptr.status_buf_write_ptr == DEFAULT_INV_SR_PTR
+			|| ctx_st_ptr.status_buf_read_ptr == DEFAULT_INV_SR_PTR)
+		return true;
+
+	if (ctx_st_ptr.status_buf_read_ptr != ctx_st_ptr.status_buf_write_ptr)
+		return false;
+
+	return true;
+}
+
+static bool vgt_check_uninitialized_execlist_context(struct vgt_device *vgt,
+	enum vgt_ring_id ring_id, struct execlist_context *ctx)
+{
+	struct vgt_mm *mm = vgt->gtt.ggtt_mm;
+	void *src = v_aperture(vgt->pdev, vgt->rb[ring_id].context_save_area);
+	u32 lrca = ctx->guest_context.lrca;
+	int nr_page = EXECLIST_CTX_PAGES(ring_id);
+	u32 *ring_context;
+	u32 *ctx_sr_ctrl;
+	void *dst;
+	int i;
+
+	if (ring_id != RING_BUFFER_RCS
+			|| !vgt->rb[ring_id].check_uninitialized_context)
+		return true;
+
+	vgt->rb[ring_id].check_uninitialized_context = false;
+
+	if (!vgt->has_context)
+		return true;
+
+	ring_context = vgt_gma_to_va(mm, (lrca + 1) << GTT_PAGE_SHIFT);
+	if (!ring_context) {
+		vgt_err("Fail to find ring_context, lrca %x.\n", lrca);
+		return false;
+	}
+
+	if (ring_context[1] != 0x1100101b && ring_context[2] != 0x2244) {
+		vgt_err("Not a valid guest context?!\n");
+		return false;
+	}
+
+	ctx_sr_ctrl = ring_context + 3;
+	if ((*ctx_sr_ctrl & ((1 << 16) | (1 << 0))) != ((1 << 16) | (1 << 0)))
+		return false;
+
+	*ctx_sr_ctrl &= ~((1 << 16) | (1 << 0));
+
+	/* Fill the engine context in page 1. */
+	memcpy(ring_context + 0x50, src + 0x50 * 4, SIZE_PAGE - 0x50 * 4);
+
+	src += SIZE_PAGE;
+
+	for (i = 2; i < nr_page; i++, src += SIZE_PAGE) {
+		dst = vgt_gma_to_va(mm, (lrca + i) << GTT_PAGE_SHIFT);
+		memcpy(dst, src, SIZE_PAGE);
+	}
+
+	vgt_info("fill uninitialized guest context with last context, lrca: %x.\n", lrca);
+
+	return true;
+}
+
+void vgt_submit_execlist(struct vgt_device *vgt, enum vgt_ring_id ring_id)
+{
+	int i;
+	struct ctx_desc_format context_descs[2];
+	uint32_t elsp_reg;
+	int el_slot_idx;
+	vgt_state_ring_t *ring_state;
+	struct vgt_exec_list *execlist = NULL;
+	bool render_owner = is_current_render_owner(vgt);
+
+	if (!render_owner)
+		return;
+
+	ring_state = &vgt->rb[ring_id];
+	el_slot_idx = vgt_el_slots_next_sched(ring_state);
+	if (el_slot_idx == -1) {
+		return;
+	}
+	execlist = &vgt_el_queue_slot(vgt, ring_id, el_slot_idx);
+
+	if (execlist == NULL) {
+		/* no pending EL to submit */
+		return;
+	}
+
+	ASSERT (execlist->el_ctxs[0] != NULL);
+
+	for (i = 0; i < 2; ++ i) {
+		struct execlist_context *ctx = execlist->el_ctxs[i];
+		if (ctx == NULL) {
+			memset(&context_descs[i], 0,
+				sizeof(struct ctx_desc_format));
+			continue;
+		} else {
+			memcpy(&context_descs[i], &ctx->guest_context,
+				sizeof(struct ctx_desc_format));
+		}
+
+		ASSERT_VM(ring_id == ctx->ring_id, vgt);
+		vgt_update_shadow_ctx_from_guest(vgt, ctx);
+		vgt_update_ring_info(vgt, ctx);
+
+		trace_ctx_lifecycle(vgt->vm_id, ring_id,
+			ctx->guest_context.lrca, "schedule_to_run");
+
+		vgt_check_uninitialized_execlist_context(vgt, ring_id, ctx);
+
+		if (!vgt_require_shadow_context(vgt))
+			continue;
+
+		if (shadow_execlist_context == PATCH_WITHOUT_SHADOW)
+			vgt_patch_guest_context(ctx);
+		else
+			context_descs[i].lrca = ctx->shadow_lrca;
+
+#ifdef EL_SLOW_DEBUG
+		dump_el_context_information(vgt, ctx);
+#endif
+	}
+
+	elsp_reg = el_ring_mmio(ring_id, _EL_OFFSET_SUBMITPORT);
+	/* mark it submitted even if it failed the validation */
+	execlist->status = EL_SUBMITTED;
+
+	if (vgt_validate_elsp_descs(vgt, &context_descs[0], &context_descs[1])) {
+#ifdef EL_SLOW_DEBUG
+		struct execlist_status_format status;
+		uint32_t status_reg = el_ring_mmio(ring_id, _EL_OFFSET_STATUS);
+		READ_STATUS_MMIO(vgt->pdev, status_reg, status);
+		vgt_dbg(VGT_DBG_EXECLIST, "The EL status before ELSP submission!\n");
+		dump_execlist_status((struct execlist_status_format *)&status,
+					ring_id);
+#endif
+		vgt_hw_ELSP_write(vgt, elsp_reg, &context_descs[0],
+					&context_descs[1]);
+#ifdef EL_SLOW_DEBUG
+		READ_STATUS_MMIO(vgt->pdev, status_reg, status);
+		vgt_dbg(VGT_DBG_EXECLIST, "The EL status after ELSP submission:\n");
+		dump_execlist_status((struct execlist_status_format *)&status,
+					ring_id);
+#endif
+	}
+}
+
+bool vgt_batch_ELSP_write(struct vgt_device *vgt, int ring_id)
+{
+	struct vgt_elsp_store *elsp_store = &vgt->rb[ring_id].elsp_store;
+
+	struct execlist_context *el_ctxs[2];
+	struct ctx_desc_format *ctx_descs[2];
+	int i;
+
+	ASSERT(elsp_store->count == ELSP_BUNDLE_NUM);
+	if (!vgt_validate_elsp_submission(vgt, elsp_store)) {
+		vgt_err("VM(%d): Failed to submit an execution list!\n",
+						vgt->vm_id);
+		return false;
+	}
+
+	ctx_descs[0] = (struct ctx_desc_format *)&elsp_store->element[2];
+	ctx_descs[1] = (struct ctx_desc_format *)&elsp_store->element[0];
+
+	elsp_store->count = 0;
+	vgt_enable_ring(vgt, ring_id);
+
+	if (hvm_render_owner) {
+		uint32_t elsp_reg;
+		elsp_reg = el_ring_mmio(ring_id, _EL_OFFSET_SUBMITPORT);
+		if (!is_current_render_owner(vgt)) {
+			vgt_warn("VM-%d: ELSP submission but VM is not "
+			"render owner! But it will still be submitted.\n",
+				vgt->vm_id);
+		}
+		vgt_hw_ELSP_write(vgt, elsp_reg, ctx_descs[0], ctx_descs[1]);
+		return true;
+	}
+
+	for (i = 0; i < 2; ++ i) {
+		struct execlist_context *el_ctx;
+		if (!ctx_descs[i]->valid) {
+			vgt_dbg(VGT_DBG_EXECLIST, "ctx%d in SUBMISSION is invalid.\n", i);
+			el_ctxs[i] = NULL;
+			continue;
+		}
+
+		vgt_dbg(VGT_DBG_EXECLIST, "SUBMISSION: ctx%d guest lrca is: 0x%x\n",
+						i, ctx_descs[i]->lrca);
+		el_ctx = execlist_context_find(vgt, ctx_descs[i]->lrca);
+
+		if (el_ctx == NULL) {
+			vgt_warn("Given EXECLIST context is not yet constructed! "
+			"It is not expected to happen! lrca = 0x%x\n", ctx_descs[i]->lrca);
+			el_ctx = vgt_create_execlist_context(vgt,
+						ctx_descs[i], ring_id);
+			if (el_ctx == NULL) {
+				vgt_err("VM-%d: Failed to create execlist "
+				"context on ring %d in ELSP submission!\n",
+				vgt->vm_id, ring_id);
+				return false;
+			}
+		}
+
+		el_ctxs[i] = el_ctx;
+
+		vgt_dbg(VGT_DBG_EXECLIST, "SUBMISSION: ctx shadow lrca is: 0x%x\n",
+						el_ctx->shadow_lrca);
+	}
+
+	vgt_emulate_submit_execlist(vgt, ring_id, el_ctxs[0], el_ctxs[1]);
+
+	if (!ctx_switch_requested(vgt->pdev) && is_current_render_owner(vgt))
+		vgt_submit_execlist(vgt, ring_id);
+
+	return true;
+}
+
+/* init interface */
+
+void execlist_ctx_table_destroy(struct vgt_device *vgt)
+{
+	struct hlist_node *n;
+	struct execlist_context *el_ctx;
+	int i;
+
+	hash_for_each_safe(vgt->gtt.el_ctx_hash_table, i, n, el_ctx, node)
+		vgt_destroy_execlist_context(vgt, el_ctx);
+
+	return;
+}
+
+void vgt_clear_submitted_el_record(struct pgt_device *pdev, enum vgt_ring_id ring_id)
+{
+	int i;
+	for (i = 0; i < VGT_MAX_VMS; i++) {
+		struct vgt_device *vgt = pdev->device[i];
+		int idx;
+		if (!vgt)
+			continue;
+
+		for (idx = 0; idx < EL_QUEUE_SLOT_NUM; ++ idx) {
+			struct vgt_exec_list *execlist;
+			execlist = &vgt_el_queue_slot(vgt, ring_id, idx);
+			if (execlist->status == EL_SUBMITTED)
+				vgt_el_slots_delete(vgt, ring_id, idx);
+		}
+	}
+}
+
+/* pv interface */
+
+bool vgt_g2v_execlist_context_create(struct vgt_device *vgt)
+{
+	bool rc = true;
+	struct execlist_context *el_ctx;
+	struct ctx_desc_format ctx_desc;
+
+	ctx_desc.elm_high = __vreg(vgt, vgt_info_off(
+				execlist_context_descriptor_hi));
+	ctx_desc.elm_low = __vreg(vgt, vgt_info_off(
+				execlist_context_descriptor_lo));
+
+	vgt_dbg(VGT_DBG_EXECLIST, "VM-%d: Receive the el context creation "
+		"request for lrca 0x%x\n", vgt->vm_id, ctx_desc.lrca);
+
+	el_ctx = execlist_context_find(vgt, ctx_desc.lrca);
+	if (el_ctx) {
+		vgt_warn("VM-%d: A context creation request is received "
+			" but the context is already constructed!\n"
+			"The request will be ignored.\n", vgt->vm_id);
+		dump_ctx_desc(vgt, &ctx_desc);
+		return rc;
+	}
+
+	el_ctx = vgt_create_execlist_context(vgt, &ctx_desc, MAX_ENGINES);
+	if (el_ctx == NULL) {
+		/* The guest does not have ring state ready while
+		 * sending context creation notification to us. Such
+		 * notification will be ignored. And the context is
+		 * not expected to be used in ELSP submission.
+		 */
+		vgt_warn("VM-%d: Failed to create context with lrca 0x%x! "
+			"The request will be ignored.\n",
+			vgt->vm_id, ctx_desc.lrca);
+	} else if (hvm_render_owner) {
+		if (vgt_require_shadow_context(vgt)) {
+			vgt_patch_guest_context(el_ctx);
+		}
+	}
+
+	return rc;
+}
+
+bool vgt_g2v_execlist_context_destroy(struct vgt_device *vgt)
+{
+	bool rc = true;
+	struct execlist_context *el_ctx;
+	struct ctx_desc_format ctx_desc;
+
+	ctx_desc.elm_high = __vreg(vgt, vgt_info_off(
+				execlist_context_descriptor_hi));
+	ctx_desc.elm_low = __vreg(vgt, vgt_info_off(
+				execlist_context_descriptor_lo));
+
+	vgt_dbg(VGT_DBG_EXECLIST, "VM-%d: Receive the el context destroy "
+		"request for lrca 0x%x\n", vgt->vm_id, ctx_desc.lrca);
+
+	el_ctx = execlist_context_find(vgt, ctx_desc.lrca);
+	if (el_ctx == NULL) {
+		vgt_warn("VM-%d: A context destroy request is received "
+			" but the context is not found!\n"
+			"The request will be ignored.\n", vgt->vm_id);
+		dump_ctx_desc(vgt, &ctx_desc);
+		return rc;
+	}
+
+	vgt_destroy_execlist_context(vgt, el_ctx);
+	return rc;
+}
diff --git a/drivers/gpu/drm/i915/vgt/execlists.h b/drivers/gpu/drm/i915/vgt/execlists.h
new file mode 100644
index 0000000..dc0c809
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/execlists.h
@@ -0,0 +1,183 @@
+/*
+ * EXECLIST data structures
+ *
+ * Copyright(c) 2011-2015 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef _VGT_EXECLISTS_H_
+#define _VGT_EXECLISTS_H_
+
+#define vgt_require_shadow_context(vgt)	(!((vgt) && (vgt->vgt_id == 0)))
+
+#define MAX_EXECLIST_CTX_PAGES	20
+#define ELSP_BUNDLE_NUM		4
+#define EXECLIST_CTX_SIZE (SIZE_PAGE * MAX_EXECLIST_CTX_PAGES)
+
+#define CTX_OFFSET_PDP3		0x24
+#define CTX_OFFSET_PDP2		0x28
+#define CTX_OFFSET_PDP1		0x2c
+#define CTX_OFFSET_PDP0		0x30
+
+#define CTX_STATUS_BUF_NUM	6
+#define DEFAULT_INV_SR_PTR	7
+
+/* NORMAL_CTX_SHADOW will have guest contexts write protected
+ * after creating. Other policies only have write protection
+ * after the context submission.
+ */
+enum ctx_shadow_policy {
+	PATCH_WITHOUT_SHADOW	= 0,
+	NORMAL_CTX_SHADOW	= 1,
+};
+
+struct mmio_pair {
+	uint32_t addr;
+	uint32_t val;
+};
+
+/* The first 64 dwords in register state context */
+struct reg_state_ctx_header {
+	uint32_t nop1;
+	uint32_t lri_cmd_1;
+	struct mmio_pair ctx_ctrl;
+	struct mmio_pair ring_header;
+	struct mmio_pair ring_tail;
+	struct mmio_pair rb_start;
+	struct mmio_pair rb_ctrl;
+	struct mmio_pair bb_cur_head_UDW;
+	struct mmio_pair bb_cur_head_LDW;
+	struct mmio_pair bb_state;
+	struct mmio_pair second_bb_addr_UDW;
+	struct mmio_pair second_bb_addr_LDW;
+	struct mmio_pair second_bb_state;
+	struct mmio_pair bb_per_ctx_ptr;
+	struct mmio_pair rcs_indirect_ctx;
+	struct mmio_pair rcs_indirect_ctx_offset;
+	uint32_t nop2;
+	uint32_t nop3;
+	uint32_t nop4;
+	uint32_t lri_cmd_2;
+	struct mmio_pair ctx_timestamp;
+	struct mmio_pair pdp3_UDW;
+	struct mmio_pair pdp3_LDW;
+	struct mmio_pair pdp2_UDW;
+	struct mmio_pair pdp2_LDW;
+	struct mmio_pair pdp1_UDW;
+	struct mmio_pair pdp1_LDW;
+	struct mmio_pair pdp0_UDW;
+	struct mmio_pair pdp0_LDW;
+	uint32_t nops[12];
+};
+
+struct ctx_desc_format {
+	union {
+		uint32_t elm_high;
+		uint32_t context_id;
+	};
+	union {
+		uint32_t elm_low;
+		struct {
+			uint32_t valid			: 1;
+			uint32_t force_pd_restore	: 1;
+			uint32_t force_restore		: 1;
+			uint32_t addressing_mode	: 2;
+			uint32_t llc_coherency		: 1;
+			uint32_t fault_handling		: 2;
+			uint32_t privilege_access	: 1;
+			uint32_t reserved		: 3;
+			uint32_t lrca			: 20;
+		};
+	};
+};
+
+struct execlist_status_format {
+	union {
+		uint32_t ldw;
+		struct {
+			uint32_t current_execlist_pointer	:1;
+			uint32_t execlist_write_pointer		:1;
+			uint32_t execlist_queue_full		:1;
+			uint32_t execlist_1_valid		:1;
+			uint32_t execlist_0_valid		:1;
+			uint32_t last_ctx_switch_reason		:9;
+			uint32_t current_active_elm_status	:2;
+			uint32_t arbitration_enable		:1;
+			uint32_t execlist_1_active		:1;
+			uint32_t execlist_0_active		:1;
+			uint32_t reserved			:13;
+		};
+	};	
+	union {
+		uint32_t udw;
+		uint32_t context_id;
+	};
+};
+
+struct context_status_format {
+	union {
+		uint32_t ldw;
+		struct {
+			uint32_t idle_to_active		:1;
+			uint32_t preempted		:1;
+			uint32_t element_switch		:1;
+			uint32_t active_to_idle		:1;
+			uint32_t context_complete	:1;
+			uint32_t wait_on_sync_flip	:1;
+			uint32_t wait_on_vblank		:1;
+			uint32_t wait_on_semaphore	:1;
+			uint32_t wait_on_scanline	:1;
+			uint32_t reserved		:2;
+			uint32_t semaphore_wait_mode	:1;
+			uint32_t display_plane		:3;
+			uint32_t lite_restore		:1;
+			uint32_t reserved_2		:16;
+		};
+	};
+	union {
+		uint32_t udw;
+		uint32_t context_id;
+	};
+};
+
+struct ctx_st_ptr_format {
+	union {
+		uint32_t dw;
+		struct {
+			uint32_t status_buf_write_ptr	:3;
+			uint32_t reserved		:5;
+			uint32_t status_buf_read_ptr	:3;
+			uint32_t reserved2		:5;
+			uint32_t mask			:16;
+		};
+	};
+};
+
+/* read execlist status or ctx status which are 64-bit MMIO
+ * status can be different types but all with ldw/udw defined.
+ */
+#define READ_STATUS_MMIO(pdev, offset, status)		\
+do {							\
+	status.ldw = VGT_MMIO_READ(pdev, offset);	\
+	status.udw = VGT_MMIO_READ(pdev, offset + 4);	\
+} while(0);
+
+#endif /* _VGT_EXECLISTS_H_ */
diff --git a/drivers/gpu/drm/i915/vgt/fb_decoder.c b/drivers/gpu/drm/i915/vgt/fb_decoder.c
new file mode 100644
index 0000000..b3f305a
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/fb_decoder.c
@@ -0,0 +1,547 @@
+/*
+ * Decode framebuffer attributes from raw vMMIO
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/module.h>
+#include <linux/debugfs.h>
+#include <linux/connector.h>
+#include "vgt.h"
+#include "fb_decoder.h"
+#include <uapi/drm/drm_fourcc.h>
+#include <uapi/drm/i915_drm.h>
+
+#define FORMAT_NUM	16
+struct pixel_format {
+	int	drm_format;	/* Pixel format in DRM definition */
+	int	bpp;		/* Bits per pixel, 0 indicates invalid */
+	char	*desc;		/* The description */
+};
+
+/* non-supported format has bpp default to 0 */
+static struct pixel_format hsw_pixel_formats[FORMAT_NUM] = {
+	[0b0010]  = {DRM_FORMAT_C8, 8, "8-bit Indexed"},
+	[0b0101]  = {DRM_FORMAT_RGB565, 16, "16-bit BGRX (5:6:5 MSB-R:G:B)"},
+	[0b0110]  = {DRM_FORMAT_XRGB8888, 32, "32-bit BGRX (8:8:8:8 MSB-X:R:G:B)"},
+	[0b1000]  = {DRM_FORMAT_XBGR2101010, 32, "32-bit RGBX (2:10:10:10 MSB-X:B:G:R)"},
+	[0b1010] = {DRM_FORMAT_XRGB2101010, 32, "32-bit BGRX (2:10:10:10 MSB-X:R:G:B)"},
+	[0b1110] = {DRM_FORMAT_XBGR8888, 32, "32-bit RGBX (8:8:8:8 MSB-X:B:G:R)"},
+};
+
+int vgt_decode_primary_plane_format(struct vgt_device *vgt,
+	int pipe, struct vgt_primary_plane_format *plane)
+{
+	u32	val, fmt;
+
+	val = __vreg(vgt, VGT_DSPCNTR(pipe));
+	plane->enabled = !!(val & _PRI_PLANE_ENABLE);
+	if (!plane->enabled)
+		return 0;
+
+	plane->tiled = !!(val & _PRI_PLANE_TILE_MASK);
+
+	fmt = (val & _PRI_PLANE_FMT_MASK) >> _PRI_PLANE_FMT_SHIFT;
+	if (!hsw_pixel_formats[fmt].bpp) {
+		vgt_err("Non-supported pixel format (0x%x)\n", fmt);
+		return -EINVAL;
+	}
+	plane->hw_format = fmt;
+	plane->bpp = hsw_pixel_formats[fmt].bpp;
+	plane->drm_format = hsw_pixel_formats[fmt].drm_format;
+
+	plane->base = __vreg(vgt, VGT_DSPSURF(pipe)) & GTT_PAGE_MASK;
+	plane->stride = __vreg(vgt, VGT_DSPSTRIDE(pipe)) &
+				_PRI_PLANE_STRIDE_MASK;
+	plane->width = (__vreg(vgt, VGT_PIPESRC(pipe)) & _PIPE_H_SRCSZ_MASK) >>
+				_PIPE_H_SRCSZ_SHIFT;
+	plane->width += 1;
+	plane->height = (__vreg(vgt, VGT_PIPESRC(pipe)) &
+			 _PIPE_V_SRCSZ_MASK) >> _PIPE_V_SRCSZ_SHIFT;
+	plane->height += 1;	/* raw height is one minus the real value */
+
+	val = __vreg(vgt, VGT_DSPTILEOFF(pipe));
+	plane->x_offset = (val & _PRI_PLANE_X_OFF_MASK) >>
+			   _PRI_PLANE_X_OFF_SHIFT;
+	plane->y_offset = (val & _PRI_PLANE_Y_OFF_MASK) >>
+			   _PRI_PLANE_Y_OFF_SHIFT;
+	return 0;
+}
+
+#define CURSOR_MODE_NUM	(1 << 6)
+struct cursor_mode_format {
+	int	drm_format;	/* Pixel format in DRM definition */
+	u8	bpp;		/* Bits per pixel; 0 indicates invalid */
+	u32	width;		/* In pixel */
+	u32	height;		/* In lines */
+	char	*desc;		/* The description */
+};
+
+/* non-supported format has bpp default to 0 */
+static struct cursor_mode_format hsw_cursor_mode_formats[CURSOR_MODE_NUM] = {
+	[0b100010]  = {DRM_FORMAT_ARGB8888, 32, 128, 128,"128x128 32bpp ARGB"},
+	[0b100011]  = {DRM_FORMAT_ARGB8888, 32, 256, 256, "256x256 32bpp ARGB"},
+	[0b100111]  = {DRM_FORMAT_ARGB8888, 32, 64, 64, "64x64 32bpp ARGB"},
+	[0b000111]  = {DRM_FORMAT_ARGB8888, 32, 64, 64, "64x64 32bpp ARGB"},//actually inverted... figure this out later
+};
+int vgt_decode_cursor_plane_format(struct vgt_device *vgt,
+	int pipe, struct vgt_cursor_plane_format *plane)
+{
+	u32 val, mode;
+	u32 alpha_plane, alpha_force;
+
+	val = __vreg(vgt, VGT_CURCNTR(pipe));
+	mode = val & _CURSOR_MODE;
+	plane->enabled = (mode != _CURSOR_MODE_DISABLE);
+	if (!plane->enabled)
+		return 0;
+
+	if (!hsw_cursor_mode_formats[mode].bpp) {
+		vgt_err("Non-supported cursor mode (0x%x)\n", mode);
+		return -EINVAL;
+	}
+	plane->mode = mode;
+	plane->bpp = hsw_cursor_mode_formats[mode].bpp;
+	plane->drm_format = hsw_cursor_mode_formats[mode].drm_format;
+	plane->width = hsw_cursor_mode_formats[mode].width;
+	plane->height = hsw_cursor_mode_formats[mode].height;
+
+	alpha_plane = (val & _CURSOR_ALPHA_PLANE_MASK) >>
+				_CURSOR_ALPHA_PLANE_SHIFT;
+	alpha_force = (val & _CURSOR_ALPHA_FORCE_MASK) >>
+				_CURSOR_ALPHA_FORCE_SHIFT;
+	if (alpha_plane || alpha_force)
+		vgt_warn("alpha_plane=0x%x, alpha_force=0x%x\n",
+			alpha_plane, alpha_force);
+
+	plane->base = __vreg(vgt, VGT_CURBASE(pipe)) & GTT_PAGE_MASK;
+
+	val = __vreg(vgt, VGT_CURPOS(pipe));
+	plane->x_pos = (val & _CURSOR_POS_X_MASK) >> _CURSOR_POS_X_SHIFT;
+	plane->x_sign = (val & _CURSOR_SIGN_X_MASK) >> _CURSOR_SIGN_X_SHIFT;
+	plane->y_pos = (val & _CURSOR_POS_Y_MASK) >> _CURSOR_POS_Y_SHIFT;
+	plane->y_sign = (val & _CURSOR_SIGN_Y_MASK) >> _CURSOR_SIGN_Y_SHIFT;
+	plane->x_hot = __vreg(vgt, vgt_info_off(xhot));
+	plane->y_hot = __vreg(vgt, vgt_info_off(xhot));
+
+	return 0;
+}
+
+#define FORMAT_NUM_SRRITE	(1 << 3)
+
+/* The formats described in the sprite format field are the 1st level of
+ * cases RGB and YUV formats are further refined by the color_order and
+ * yuv_order fields to cover the full set of possible formats.
+ */
+
+static struct pixel_format hsw_pixel_formats_sprite[FORMAT_NUM_SRRITE] = {
+	[0b000]  = {DRM_FORMAT_YUV422, 16, "YUV 16-bit 4:2:2 packed"},
+	[0b001]  = {DRM_FORMAT_XRGB2101010, 32, "RGB 32-bit 2:10:10:10"},
+	[0b010]  = {DRM_FORMAT_XRGB8888, 32, "RGB 32-bit 8:8:8:8"},
+	[0b100] = {DRM_FORMAT_AYUV, 32, "YUV 32-bit 4:4:4 packed (8:8:8:8 MSB-X:Y:U:V)"},
+};
+
+/* Non-supported format has bpp default to 0 */
+int vgt_decode_sprite_plane_format(struct vgt_device *vgt,
+	int pipe, struct vgt_sprite_plane_format *plane)
+{
+	u32 val, fmt;
+	u32 width;
+	u32 color_order, yuv_order;
+	int drm_format;
+
+	val = __vreg(vgt, VGT_SPRCTL(pipe));
+	plane->enabled = !!(val & _SPRITE_ENABLE);
+	if (!plane->enabled)
+		return 0;
+
+	plane->tiled = !!(val & _SPRITE_TILED);
+	color_order = !!(val & _SPRITE_COLOR_ORDER_MASK);
+	yuv_order = (val & _SPRITE_YUV_ORDER_MASK) >>
+				_SPRITE_YUV_ORDER_SHIFT;
+
+	fmt = (val & _SPRITE_FMT_MASK) >> _SPRITE_FMT_SHIFT;
+	if (!hsw_pixel_formats_sprite[fmt].bpp) {
+		vgt_err("Non-supported pixel format (0x%x)\n", fmt);
+		return -EINVAL;
+	}
+	plane->hw_format = fmt;
+	plane->bpp = hsw_pixel_formats_sprite[fmt].bpp;
+	drm_format = hsw_pixel_formats_sprite[fmt].drm_format;
+
+	/* Order of RGB values in an RGBxxx buffer may be ordered RGB or
+	 * BGR depending on the state of the color_order field
+	 */
+	if (!color_order) {
+		if (drm_format == DRM_FORMAT_XRGB2101010)
+			drm_format = DRM_FORMAT_XBGR2101010;
+		else if (drm_format == DRM_FORMAT_XRGB8888)
+			drm_format = DRM_FORMAT_XBGR8888;
+	}
+
+	if (drm_format == DRM_FORMAT_YUV422) {
+		switch (yuv_order){
+		case	0:
+			drm_format = DRM_FORMAT_YUYV;
+			break;
+		case	1:
+			drm_format = DRM_FORMAT_UYVY;
+			break;
+		case	2:
+			drm_format = DRM_FORMAT_YVYU;
+			break;
+		case	3:
+			drm_format = DRM_FORMAT_VYUY;
+			break;
+		default:
+			/* yuv_order has only 2 bits */
+			BUG();
+			break;
+		}
+	}
+
+	plane->drm_format = drm_format;
+
+	plane->base = __vreg(vgt, VGT_SPRSURF(pipe)) & GTT_PAGE_MASK;
+	plane->width = __vreg(vgt, VGT_SPRSTRIDE(pipe)) &
+				_SPRITE_STRIDE_MASK;
+	plane->width /= plane->bpp / 8;	/* raw width in bytes */
+
+	val = __vreg(vgt, VGT_SPRSIZE(pipe));
+	plane->height = (val & _SPRITE_SIZE_HEIGHT_MASK) >>
+		_SPRITE_SIZE_HEIGHT_SHIFT;
+	width = (val & _SPRITE_SIZE_WIDTH_MASK) >> _SPRITE_SIZE_WIDTH_SHIFT;
+	plane->height += 1;	/* raw height is one minus the real value */
+	width += 1;		/* raw width is one minus the real value */
+	if (plane->width != width)
+		vgt_warn("sprite_plane: plane->width=%d, width=%d\n",
+			plane->width, width);
+
+	val = __vreg(vgt, VGT_SPRPOS(pipe));
+	plane->x_pos = (val & _SPRITE_POS_X_MASK) >> _SPRITE_POS_X_SHIFT;
+	plane->y_pos = (val & _SPRITE_POS_Y_MASK) >> _SPRITE_POS_Y_SHIFT;
+
+	val = __vreg(vgt, VGT_SPROFFSET(pipe));
+	plane->x_offset = (val & _SPRITE_OFFSET_START_X_MASK) >>
+			   _SPRITE_OFFSET_START_X_SHIFT;
+	plane->y_offset = (val & _SPRITE_OFFSET_START_Y_MASK) >>
+			   _SPRITE_OFFSET_START_Y_SHIFT;
+	return 0;
+}
+
+static void vgt_dump_primary_plane_format(struct dump_buffer *buf,
+	struct vgt_primary_plane_format *plane)
+{
+	dump_string(buf, "Primary Plane: [%s]\n",
+		plane->enabled ? "Enabled" : "Disabled");
+	if (!plane->enabled)
+		return;
+
+	dump_string(buf, "  tiled: %s\n", plane->tiled ? "yes" : "no");
+	if (!plane->bpp) {
+		dump_string(buf, "  BROKEN FORMAT (ZERO bpp)\n");
+		return;
+	}
+
+	dump_string(buf, "  bpp: %d\n", plane->bpp);
+	dump_string(buf, "  drm_format: 0x%08x: %s\n", plane->drm_format,
+		hsw_pixel_formats[plane->hw_format].desc);
+	dump_string(buf, "  base: 0x%x\n", plane->base);
+	dump_string(buf, "  x-off: %d\n", plane->x_offset);
+	dump_string(buf, "  y-off: %d\n", plane->y_offset);
+	dump_string(buf, "  width: %d\n", plane->width);
+	dump_string(buf, "  height: %d\n", plane->height);
+}
+
+static void vgt_dump_cursor_plane_format(struct dump_buffer *buf,
+	struct vgt_cursor_plane_format *plane)
+{
+	dump_string(buf, "Cursor Plane: [%s]\n",
+		plane->enabled ? "Enabled" : "Disabled");
+	if (!plane->enabled)
+		return;
+
+	if (!plane->bpp) {
+		dump_string(buf, "  BROKEN FORMAT (ZERO bpp)\n");
+		return;
+	}
+
+	dump_string(buf, "  bpp: %d\n", plane->bpp);
+	dump_string(buf, "  mode: 0x%08x: %s\n", plane->mode,
+		hsw_cursor_mode_formats[plane->mode].desc);
+	dump_string(buf, "  drm_format: 0x%08x\n", plane->drm_format);
+	dump_string(buf, "  base: 0x%x\n", plane->base);
+	dump_string(buf, "  x-pos: %d\n", plane->x_pos);
+	dump_string(buf, "  y-pos: %d\n", plane->y_pos);
+	dump_string(buf, "  x-sign: %d\n", plane->x_sign);
+	dump_string(buf, "  y-sign: %d\n", plane->y_sign);
+	dump_string(buf, "  width: %d\n", plane->width);
+	dump_string(buf, "  height: %d\n", plane->height);
+}
+
+static void vgt_dump_sprite_plane_format(struct dump_buffer *buf,
+	struct vgt_sprite_plane_format *plane)
+{
+	dump_string(buf, "Sprite Plane: [%s]\n",
+		plane->enabled ? "Enabled" : "Disabled");
+	if (!plane->enabled)
+		return;
+
+	dump_string(buf, "  tiled: %s\n", plane->tiled ? "yes" : "no");
+	if (!plane->bpp) {
+		dump_string(buf, "  BROKEN FORMAT (ZERO bpp)\n");
+		return;
+	}
+
+	dump_string(buf, "  bpp: %d\n", plane->bpp);
+	dump_string(buf, "  drm_format: 0x%08x: %s\n",
+		plane->drm_format,
+		hsw_pixel_formats_sprite[plane->hw_format].desc);
+	dump_string(buf, "  base: 0x%x\n", plane->base);
+	dump_string(buf, "  x-off: %d\n", plane->x_offset);
+	dump_string(buf, "  y-off: %d\n", plane->y_offset);
+	dump_string(buf, "  x-pos: %d\n", plane->x_pos);
+	dump_string(buf, "  y-pos: %d\n", plane->y_pos);
+	dump_string(buf, "  width: %d\n", plane->width);
+	dump_string(buf, "  height: %d\n", plane->height);
+}
+
+
+int vgt_dump_fb_format(struct dump_buffer *buf, struct vgt_fb_format *fb)
+{
+	int i;
+
+
+	for (i = 0; i < MAX_INTEL_PIPES; i++) {
+		struct vgt_pipe_format *pipe = &fb->pipes[i];
+		dump_string(buf, "[PIPE-%d]:\n", i);
+		vgt_dump_primary_plane_format(buf, &pipe->primary);
+		vgt_dump_cursor_plane_format(buf, &pipe->cursor);
+		vgt_dump_sprite_plane_format(buf, &pipe->sprite);
+		dump_string(buf, "Pipe remapping\n");
+		if (pipe->ddi_port == DDI_PORT_NONE) {
+			dump_string(buf, "  no mapping available for this pipe\n");
+		} else {
+			char port_id;
+			switch (pipe->ddi_port) {
+			case DDI_PORT_B:
+				port_id = 'B'; break;
+			case DDI_PORT_C:
+				port_id = 'C'; break;
+			case DDI_PORT_D:
+				port_id = 'D'; break;
+			case DDI_PORT_E:
+			default:
+				port_id = 'E'; break;
+			}
+			dump_string(buf, "  virtual pipe:%d -> DDI PORT:%c\n",
+				 i, port_id);
+		}
+	}
+	dump_string(buf, "\n");
+	return 0;
+}
+
+/* Debug facility */
+
+static void vgt_show_fb_format(int vmid, struct vgt_fb_format *fb)
+{
+	struct dump_buffer buf;
+	if (create_dump_buffer(&buf, 2048) < 0)
+		return;
+
+	vgt_dump_fb_format(&buf, fb);
+	printk("-----------FB format (VM-%d)--------\n", vmid);
+	printk("%s", buf.buffer);
+	destroy_dump_buffer(&buf);
+}
+
+/*
+ * Decode framebuffer information from raw vMMIO
+ *
+ * INPUT:
+ *   [domid] - specify the VM
+ * OUTPUT:
+ *   [format] - contain the decoded format info
+ *
+ * NOTE: The caller is expected to poll this interface, and reconstruct
+ * previous reference to the new format information
+ */
+
+int vgt_decode_fb_format(int vmid, struct vgt_fb_format *fb)
+{
+	int i;
+	struct vgt_device *vgt;
+	struct pgt_device *pdev = &default_device;
+	unsigned long flags;
+	int cpu;
+	int ret = 0;
+
+	if (!fb)
+		return -EINVAL;
+
+	if (!IS_HSW(pdev)) {
+		vgt_err("Only HSW is supported now\n");
+		return -EINVAL;
+	}
+
+	/* TODO: use fine-grained refcnt later */
+	vgt_lock_dev_flags(pdev, cpu, flags);
+
+	vgt = vmid_2_vgt_device(vmid);
+	if (!vgt) {
+		vgt_err("Invalid domain ID (%d)\n", vmid);
+		vgt_unlock_dev_flags(pdev, cpu, flags);
+		return -ENODEV;
+	}
+
+	for (i = 0; i < MAX_INTEL_PIPES; i++) {
+		struct vgt_pipe_format *pipe = &fb->pipes[i];
+		vgt_reg_t ddi_func_ctl = __vreg(vgt, _VGT_TRANS_DDI_FUNC_CTL(i));
+
+		if (!(ddi_func_ctl & _TRANS_DDI_PORT_SHIFT)) {
+			pipe->ddi_port = DDI_PORT_NONE;
+		} else {
+			vgt_reg_t port = (ddi_func_ctl & _REGBIT_TRANS_DDI_PORT_MASK) >>
+						_TRANS_DDI_PORT_SHIFT;
+			if (port <= DDI_PORT_E)
+				pipe->ddi_port = port;
+			else
+				pipe->ddi_port = DDI_PORT_NONE;
+		}
+
+		ret |= vgt_decode_primary_plane_format(vgt, i, &pipe->primary);
+		ret |= vgt_decode_sprite_plane_format(vgt, i, &pipe->sprite);
+		ret |= vgt_decode_cursor_plane_format(vgt, i, &pipe->cursor);
+
+		if (ret) {
+			vgt_err("Decode format error for pipe(%d)\n", i);
+			ret = -EINVAL;
+			break;
+		}
+	}
+
+	vgt_unlock_dev_flags(pdev, cpu, flags);
+
+	if(vgt_debug & VGT_DBG_GENERIC)
+	  vgt_show_fb_format(vmid, fb);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(vgt_decode_fb_format);
+
+static ATOMIC_NOTIFIER_HEAD(vgt_fb_notifier_list);
+
+int vgt_register_fb_notifier(struct notifier_block *nb)
+{
+	return atomic_notifier_chain_register(&vgt_fb_notifier_list, nb);
+}
+EXPORT_SYMBOL_GPL(vgt_register_fb_notifier);
+
+int vgt_unregister_fb_notifier(struct notifier_block *nb)
+{
+	return atomic_notifier_chain_unregister(&vgt_fb_notifier_list, nb);
+}
+EXPORT_SYMBOL_GPL(vgt_unregister_fb_notifier);
+
+int vgt_fb_notifier_call_chain(unsigned long val, void *data)
+{
+	return atomic_notifier_call_chain(&vgt_fb_notifier_list, val, data);
+}
+EXPORT_SYMBOL_GPL(vgt_fb_notifier_call_chain);
+
+static int vgt_plane_to_i915_plane(unsigned vgt_plane)
+{
+	int ret = -ENOENT;
+	switch (vgt_plane) {
+		case PRIMARY_PLANE:
+			ret = I915_VGT_PLANE_PRIMARY;
+			break;
+		case CURSOR_PLANE:
+			ret = I915_VGT_PLANE_CURSOR;
+			break;
+		case SPRITE_PLANE:
+			ret = I915_VGT_PLANE_SPRITE;
+			break;
+		default:
+			vgt_err("invalid plane: %d\n", vgt_plane);
+			break;
+	}
+	return (ret);
+}
+
+/*
+ * A notifier API for userspace processes
+ * By opening a netlink socket of id CN_IDX_VGT
+ * userspace may get notifications of framebuffer events
+ */
+static int vgt_fb_event(struct notifier_block *nb,
+			unsigned long val, void *data)
+{
+	int ret;
+	static int seq = 0;
+	struct fb_notify_msg *msg = data;
+	struct cn_msg *m;
+	int data_sz;
+	struct vgt_device *vgt;
+
+	/* Don't notify for dom0 */
+	if (msg->vm_id == 0)
+		return (0);
+
+	vgt = vmid_2_vgt_device(msg->vm_id);
+	if (!vgt)
+		return (-EINVAL);
+
+	data_sz = sizeof(*msg);
+	m = kzalloc(sizeof(*m) + data_sz, GFP_ATOMIC);
+	if (!m)
+		return (-ENOMEM);
+
+	m->id.idx = CN_IDX_VGT;
+	m->id.val = msg->pipe_id;
+
+	/*
+	 * vgt plane ids are not exposed to userspace.
+	 * Swap it out for drm's concept before sending it along.
+	 * A plane without a mapping (MAX_PLANE) is not interesting, so
+	 * drop it.
+	 */
+	msg->plane_id = vgt_plane_to_i915_plane(msg->plane_id);
+
+	m->seq = seq++;
+	m->len = data_sz;
+	memcpy(m + 1, msg, data_sz);
+
+	ret = cn_netlink_send(m, 0, CN_IDX_VGT, GFP_ATOMIC);
+
+	kfree(m);
+	return (ret);
+}
+
+static struct notifier_block vgt_fb_notifier = {
+	.notifier_call = vgt_fb_event,
+};
+
+void vgt_init_fb_notify(void)
+{
+	vgt_register_fb_notifier(&vgt_fb_notifier);
+}
diff --git a/drivers/gpu/drm/i915/vgt/fb_decoder.h b/drivers/gpu/drm/i915/vgt/fb_decoder.h
new file mode 100644
index 0000000..cb640ab
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/fb_decoder.h
@@ -0,0 +1,139 @@
+#ifndef __FB_DECODER_H__
+#define __FB_DECODER_H__
+/*
+ * Decode framebuffer attributes from raw vMMIO
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/* color space conversion and gamma correction are not included */
+struct vgt_primary_plane_format {
+	u8	enabled;	/* plane is enabled */
+	u8	tiled;		/* X-tiled */
+	u8	bpp;		/* bits per pixel */
+	u32	hw_format;	/* format field in the PRI_CTL register */
+	u32	drm_format;	/* format in DRM definition */
+	u32	base;		/* framebuffer base in graphics memory */
+	u32	x_offset;	/* in pixels */
+	u32	y_offset;	/* in lines */
+	u32	width;		/* in pixels */
+	u32	height;		/* in lines */
+	u32	stride;		/* in bytes */
+};
+
+struct vgt_sprite_plane_format {
+	u8	enabled;	/* plane is enabled */
+	u8	tiled;		/* X-tiled */
+	u8	bpp;		/* bits per pixel */
+	u32	hw_format;	/* format field in the SPR_CTL register */
+	u32	drm_format;	/* format in DRM definition */
+	u32	base;		/* sprite base in graphics memory */
+	u32	x_pos;		/* in pixels */
+	u32	y_pos;		/* in lines */
+	u32	x_offset;	/* in pixels */
+	u32	y_offset;	/* in lines */
+	u32	width;		/* in pixels */
+	u32	height;		/* in lines */
+};
+
+struct vgt_cursor_plane_format {
+	u8	enabled;
+	u8	mode;		/* cursor mode select */
+	u8	bpp;		/* bits per pixel */
+	u32	drm_format;	/* format in DRM definition */
+	u32	base;		/* cursor base in graphics memory */
+	u32	x_pos;		/* in pixels */
+	u32	y_pos;		/* in lines */
+	u8	x_sign;		/* X Position Sign */
+	u8	y_sign;		/* Y Position Sign */
+	u32	width;		/* in pixels */
+	u32	height;		/* in lines */
+	u32	x_hot;		/* in pixels */
+	u32	y_hot;		/* in pixels */
+};
+
+/* The virtual DDI port type definition.
+ *
+ * DDI port A for eDP is not supported.
+ * DDI port E is for CRT.
+ * DDI_PORT_NONE means no valid port information available. When getting
+ * this return value from vgt_pipe_format, caller should stop using the
+ * virtual pipe and retry later.
+ */
+typedef enum {
+	DDI_PORT_NONE	= 0,
+	DDI_PORT_B	= 1,
+	DDI_PORT_C	= 2,
+	DDI_PORT_D	= 3,
+	DDI_PORT_E	= 4
+} ddi_port_t;
+
+struct vgt_pipe_format {
+	struct vgt_primary_plane_format	primary;
+	struct vgt_sprite_plane_format	sprite;
+	struct vgt_cursor_plane_format	cursor;
+	ddi_port_t ddi_port;  /* the DDI port that the pipe is connected to */
+};
+
+#define MAX_INTEL_PIPES	3
+struct vgt_fb_format{
+	struct vgt_pipe_format	pipes[MAX_INTEL_PIPES];
+};
+
+typedef enum {
+	FB_MODE_SET_START = 1,
+	FB_MODE_SET_END,
+	FB_DISPLAY_FLIP,
+}fb_event_t;
+
+struct fb_notify_msg {
+	unsigned vm_id;
+	unsigned pipe_id; /* id starting from 0 */
+	unsigned plane_id; /* primary, cursor, or sprite */
+};
+
+/*
+ * Decode framebuffer information from raw vMMIO
+ *
+ * INPUT:
+ *   [domid] - specify the VM
+ * OUTPUT:
+ *   [format] - contain the decoded format info
+ *
+ */
+int vgt_decode_fb_format(int vmid, struct vgt_fb_format *fb);
+
+/*
+ * Register callback to get notification of frame buffer changes
+ * "struct fb_notify_msg" will be the argument to the call back
+ * function, from which user could get the changed frame buffer
+ * information.
+ *
+ */
+int vgt_register_fb_notifier(struct notifier_block *nb);
+
+/*
+ * Unregister the callback for notification
+ */
+int vgt_unregister_fb_notifier(struct notifier_block *nb);
+
+#endif
diff --git a/drivers/gpu/drm/i915/vgt/gtt.c b/drivers/gpu/drm/i915/vgt/gtt.c
new file mode 100644
index 0000000..9d6d6d0
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/gtt.c
@@ -0,0 +1,1923 @@
+/*
+ * GTT virtualization
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/highmem.h>
+#include "vgt.h"
+#include "trace.h"
+
+/*
+ * Mappings between GTT_TYPE* enumerations.
+ * Following informations can be found according to the given type:
+ * - type of next level page table
+ * - type of entry inside this level page table
+ * - type of entry with PSE set
+ *
+ * If the given type doesn't have such a kind of information,
+ * e.g. give a l4 root entry type, then request to get its PSE type,
+ * give a PTE page table type, then request to get its next level page
+ * table type, as we know l4 root entry doesn't have a PSE bit,
+ * and a PTE page table doesn't have a next level page table type,
+ * GTT_TYPE_INVALID will be returned. This is useful when traversing a
+ * page table.
+ */
+
+struct gtt_type_table_entry {
+	gtt_type_t entry_type;
+	gtt_type_t next_pt_type;
+	gtt_type_t pse_entry_type;
+};
+
+#define GTT_TYPE_TABLE_ENTRY(type, e_type, npt_type, pse_type) \
+	[type] = { \
+		.entry_type = e_type, \
+		.next_pt_type = npt_type, \
+		.pse_entry_type = pse_type, \
+	}
+
+static struct gtt_type_table_entry gtt_type_table[] = {
+	GTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_ROOT_L4_ENTRY,
+			GTT_TYPE_PPGTT_ROOT_L4_ENTRY,
+			GTT_TYPE_PPGTT_PML4_PT,
+			GTT_TYPE_INVALID),
+	GTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_PML4_PT,
+			GTT_TYPE_PPGTT_PML4_ENTRY,
+			GTT_TYPE_PPGTT_PDP_PT,
+			GTT_TYPE_INVALID),
+	GTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_PML4_ENTRY,
+			GTT_TYPE_PPGTT_PML4_ENTRY,
+			GTT_TYPE_PPGTT_PDP_PT,
+			GTT_TYPE_INVALID),
+	GTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_PDP_PT,
+			GTT_TYPE_PPGTT_PDP_ENTRY,
+			GTT_TYPE_PPGTT_PDE_PT,
+			GTT_TYPE_PPGTT_PTE_1G_ENTRY),
+	GTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_ROOT_L3_ENTRY,
+			GTT_TYPE_PPGTT_ROOT_L3_ENTRY,
+			GTT_TYPE_PPGTT_PDE_PT,
+			GTT_TYPE_PPGTT_PTE_1G_ENTRY),
+	GTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_PDP_ENTRY,
+			GTT_TYPE_PPGTT_PDP_ENTRY,
+			GTT_TYPE_PPGTT_PDE_PT,
+			GTT_TYPE_PPGTT_PTE_1G_ENTRY),
+	GTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_PDE_PT,
+			GTT_TYPE_PPGTT_PDE_ENTRY,
+			GTT_TYPE_PPGTT_PTE_PT,
+			GTT_TYPE_PPGTT_PTE_2M_ENTRY),
+	GTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_PDE_ENTRY,
+			GTT_TYPE_PPGTT_PDE_ENTRY,
+			GTT_TYPE_PPGTT_PTE_PT,
+			GTT_TYPE_PPGTT_PTE_2M_ENTRY),
+	GTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_PTE_PT,
+			GTT_TYPE_PPGTT_PTE_4K_ENTRY,
+			GTT_TYPE_INVALID,
+			GTT_TYPE_INVALID),
+	GTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_PTE_4K_ENTRY,
+			GTT_TYPE_PPGTT_PTE_4K_ENTRY,
+			GTT_TYPE_INVALID,
+			GTT_TYPE_INVALID),
+	GTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_PTE_2M_ENTRY,
+			GTT_TYPE_PPGTT_PDE_ENTRY,
+			GTT_TYPE_INVALID,
+			GTT_TYPE_PPGTT_PTE_2M_ENTRY),
+	GTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_PTE_1G_ENTRY,
+			GTT_TYPE_PPGTT_PDP_ENTRY,
+			GTT_TYPE_INVALID,
+			GTT_TYPE_PPGTT_PTE_1G_ENTRY),
+	GTT_TYPE_TABLE_ENTRY(GTT_TYPE_GGTT_PTE,
+			GTT_TYPE_GGTT_PTE,
+			GTT_TYPE_INVALID,
+			GTT_TYPE_INVALID),
+};
+
+static inline gtt_type_t get_next_pt_type(gtt_type_t type) {
+	return gtt_type_table[type].next_pt_type;
+}
+
+static inline gtt_type_t get_entry_type(gtt_type_t type) {
+	return gtt_type_table[type].entry_type;
+}
+
+static inline gtt_type_t get_pse_type(gtt_type_t type) {
+	return gtt_type_table[type].pse_entry_type;
+}
+
+/*
+ * Per-platform GTT entry routines.
+ */
+static gtt_entry_t *gtt_get_entry32(void *pt, gtt_entry_t *e,
+		unsigned long index, bool hypervisor_access,
+		struct vgt_device *vgt)
+{
+	struct vgt_device_info *info = &e->pdev->device_info;
+
+	ASSERT(info->gtt_entry_size == 4);
+
+	if (!pt) {
+		e->val32[0] = vgt_read_gtt(e->pdev, index);
+		e->val32[1] = 0;
+	} else {
+		if (!hypervisor_access) {
+			e->val32[0] = *((u32 *)pt + index);
+			e->val32[1] = 0;
+		} else {
+			hypervisor_read_va(vgt, (u32 *)pt + index, &e->val32[0], 4, 1);
+			e->val32[1] = 0;
+		}
+	}
+	return e;
+}
+
+static gtt_entry_t *gtt_set_entry32(void *pt, gtt_entry_t *e,
+		unsigned long index, bool hypervisor_access,
+		struct vgt_device *vgt)
+{
+	struct vgt_device_info *info = &e->pdev->device_info;
+
+	ASSERT(info->gtt_entry_size == 4);
+
+	if (!pt)
+		vgt_write_gtt(e->pdev, index, e->val32[0]);
+	else {
+		if (!hypervisor_access)
+			*((u32 *)pt + index) = e->val32[0];
+		else
+			hypervisor_write_va(vgt, (u32 *)pt + index, &e->val32[0], 4, 1);
+	}
+	return e;
+}
+
+static inline gtt_entry_t *gtt_get_entry64(void *pt, gtt_entry_t *e,
+		unsigned long index, bool hypervisor_access,
+		struct vgt_device *vgt)
+{
+	struct vgt_device_info *info = &e->pdev->device_info;
+
+	ASSERT(info->gtt_entry_size == 8);
+
+	if (!pt)
+		e->val64 = vgt_read_gtt64(e->pdev, index);
+	else {
+		if (!hypervisor_access)
+			e->val64 = *((u64 *)pt + index);
+		else
+			hypervisor_read_va(vgt, (u64 *)pt + index, &e->val64, 8, 1);
+	}
+	return e;
+}
+
+static inline gtt_entry_t *gtt_set_entry64(void *pt, gtt_entry_t *e,
+		unsigned long index, bool hypervisor_access,
+		struct vgt_device *vgt)
+{
+	struct vgt_device_info *info = &e->pdev->device_info;
+
+	ASSERT(info->gtt_entry_size == 8);
+
+	if (!pt)
+		vgt_write_gtt64(e->pdev, index, e->val64);
+	else {
+		if (!hypervisor_access)
+			*((u64 *)pt + index) = e->val64;
+		else
+			hypervisor_write_va(vgt, (u64 *)pt + index, &e->val64, 8, 1);
+	}
+	return e;
+}
+
+static unsigned long gen7_gtt_get_pfn(gtt_entry_t *e)
+{
+	u32 pte = e->val32[0];
+	u64 addr = 0;
+
+	if (IS_SNB(e->pdev) || IS_IVB(e->pdev))
+		addr = (((u64)pte & 0xff0) << 28) | (u64)(pte & 0xfffff000);
+	else if (IS_HSW(e->pdev))
+		addr = (((u64)pte & 0x7f0) << 28) | (u64)(pte & 0xfffff000);
+
+	return (addr >> GTT_PAGE_SHIFT);
+}
+
+static void gen7_gtt_set_pfn(gtt_entry_t *e, unsigned long pfn)
+{
+	u64 addr = pfn << GTT_PAGE_SHIFT;
+	u32 addr_mask = 0, ctl_mask = 0;
+	u32 old_pte = e->val32[0];
+
+	if (IS_SNB(e->pdev) || IS_IVB(e->pdev)) {
+		addr_mask = 0xff0;
+		ctl_mask = _REGBIT_PTE_CTL_MASK_GEN7;
+	} else if (IS_HSW(e->pdev)) {
+		addr_mask = 0x7f0;
+		ctl_mask = _REGBIT_PTE_CTL_MASK_GEN7_5;
+	}
+
+	e->val32[0] = (addr & ~0xfff) | ((addr >> 28) & addr_mask);
+	e->val32[0] |= (old_pte & ctl_mask);
+	e->val32[0] |= _REGBIT_PTE_VALID;
+
+	return;
+}
+
+static bool gen7_gtt_test_present(gtt_entry_t *e)
+{
+	return (e->val32[0] & _REGBIT_PTE_VALID);
+}
+
+static bool gen7_gtt_test_pse(gtt_entry_t *e)
+{
+	return false;
+}
+
+static unsigned long gen8_gtt_get_pfn(gtt_entry_t *e)
+{
+	if (e->type == GTT_TYPE_PPGTT_PTE_1G_ENTRY)
+		return (e->val64 & (0x1ff << 30)) >> 12;
+	else if (e->type == GTT_TYPE_PPGTT_PTE_2M_ENTRY)
+		return (e->val64 & (0x3ffff << 21)) >> 12;
+	else
+		return (e->val64 >> 12) & 0x7ffffff;
+}
+
+static void gen8_gtt_set_pfn(gtt_entry_t *e, unsigned long pfn)
+{
+	if (e->type == GTT_TYPE_PPGTT_PTE_1G_ENTRY) {
+		e->val64 &= ~(0x1ff << 30);
+		pfn &= ((0x1ff << 30) >> 12);
+	} else if (e->type == GTT_TYPE_PPGTT_PTE_2M_ENTRY) {
+		e->val64 &= ~(0x3ffff << 21);
+		pfn &= ((0x3ffff << 21) >> 12);
+	} else {
+		e->val64 &= ~(0x7ffffff << 12);
+		pfn &= 0x7ffffff;
+	}
+
+	e->val64 |= (pfn << 12);
+}
+
+static bool gen8_gtt_test_pse(gtt_entry_t *e)
+{
+	/* Entry doesn't have PSE bit. */
+	if (get_pse_type(e->type) == GTT_TYPE_INVALID)
+		return false;
+
+	e->type = get_entry_type(e->type);
+	if (!(e->val64 & (1 << 7)))
+		return false;
+
+	e->type = get_pse_type(e->type);
+	return true;
+}
+
+static bool gen8_gtt_test_present(gtt_entry_t *e)
+{
+	/*
+	 * i915 writes PDP root pointer registers without present bit,
+	 * it also works, so we need to treat root pointer entry
+	 * specifically.
+	 */
+	if (e->type == GTT_TYPE_PPGTT_ROOT_L3_ENTRY
+			|| e->type == GTT_TYPE_PPGTT_ROOT_L4_ENTRY)
+		return (e->val64 != 0);
+	else
+		return (e->val32[0] & _REGBIT_PTE_VALID);
+}
+
+static void gtt_entry_clear_present(gtt_entry_t *e)
+{
+	e->val32[0] &= ~_REGBIT_PTE_VALID;
+}
+
+/*
+ * Per-platform GMA routines.
+ */
+static unsigned long gma_to_ggtt_pte_index(unsigned long gma)
+{
+	unsigned long x = (gma >> GTT_PAGE_SHIFT);
+	trace_gma_index(__func__, gma, x);
+	return x;
+}
+
+#define DEFINE_PPGTT_GMA_TO_INDEX(prefix, ename, exp) \
+	static unsigned long prefix##_gma_to_##ename##_index(unsigned long gma) { \
+		unsigned long x = (exp); \
+		trace_gma_index(__func__, gma, x); \
+		return x; \
+	}
+
+DEFINE_PPGTT_GMA_TO_INDEX(gen7, pte, (gma >> 12 & 0x3ff));
+DEFINE_PPGTT_GMA_TO_INDEX(gen7, pde, (gma >> 22 & 0x1ff));
+
+DEFINE_PPGTT_GMA_TO_INDEX(gen8, pte, (gma >> 12 & 0x1ff));
+DEFINE_PPGTT_GMA_TO_INDEX(gen8, pde, (gma >> 21 & 0x1ff));
+DEFINE_PPGTT_GMA_TO_INDEX(gen8, l3_pdp, (gma >> 30 & 0x3));
+DEFINE_PPGTT_GMA_TO_INDEX(gen8, l4_pdp, (gma >> 30 & 0x1ff));
+DEFINE_PPGTT_GMA_TO_INDEX(gen8, pml4, (gma >> 39 & 0x1ff));
+
+struct vgt_gtt_pte_ops gen7_gtt_pte_ops = {
+	.get_entry = gtt_get_entry32,
+	.set_entry = gtt_set_entry32,
+	.clear_present = gtt_entry_clear_present,
+	.test_present = gen7_gtt_test_present,
+	.test_pse = gen7_gtt_test_pse,
+	.get_pfn = gen7_gtt_get_pfn,
+	.set_pfn = gen7_gtt_set_pfn,
+};
+
+struct vgt_gtt_gma_ops gen7_gtt_gma_ops = {
+	.gma_to_ggtt_pte_index = gma_to_ggtt_pte_index,
+	.gma_to_pte_index = gen7_gma_to_pte_index,
+	.gma_to_pde_index = gen7_gma_to_pde_index,
+};
+
+struct vgt_gtt_pte_ops gen8_gtt_pte_ops = {
+	.get_entry = gtt_get_entry64,
+	.set_entry = gtt_set_entry64,
+	.clear_present = gtt_entry_clear_present,
+	.test_present = gen8_gtt_test_present,
+	.test_pse = gen8_gtt_test_pse,
+	.get_pfn = gen8_gtt_get_pfn,
+	.set_pfn = gen8_gtt_set_pfn,
+};
+
+struct vgt_gtt_gma_ops gen8_gtt_gma_ops = {
+	.gma_to_ggtt_pte_index = gma_to_ggtt_pte_index,
+	.gma_to_pte_index = gen8_gma_to_pte_index,
+	.gma_to_pde_index = gen8_gma_to_pde_index,
+	.gma_to_l3_pdp_index = gen8_gma_to_l3_pdp_index,
+	.gma_to_l4_pdp_index = gen8_gma_to_l4_pdp_index,
+	.gma_to_pml4_index = gen8_gma_to_pml4_index,
+};
+
+static bool gtt_entry_p2m(struct vgt_device *vgt, gtt_entry_t *p, gtt_entry_t *m)
+{
+        struct vgt_gtt_pte_ops *ops = vgt->pdev->gtt.pte_ops;
+        unsigned long gfn, mfn;
+
+        *m = *p;
+
+        if (!ops->test_present(p))
+                return true;
+
+        gfn = ops->get_pfn(p);
+
+        mfn = hypervisor_g2m_pfn(vgt, gfn);
+        if (mfn == INVALID_MFN) {
+                vgt_err("fail to translate gfn: 0x%lx\n", gfn);
+                return false;
+        }
+
+        ops->set_pfn(m, mfn);
+
+        return true;
+}
+
+/*
+ * MM helpers.
+ */
+gtt_entry_t *vgt_mm_get_entry(struct vgt_mm *mm,
+		void *page_table, gtt_entry_t *e,
+		unsigned long index)
+{
+	struct pgt_device *pdev = mm->vgt->pdev;
+	struct vgt_gtt_pte_ops *ops = pdev->gtt.pte_ops;
+
+	e->pdev = pdev;
+	e->type = mm->page_table_entry_type;
+
+	/*
+	 * If this read goes to HW, we translate
+	 * the relative index to absolute index
+	 * for pre-bdw platform.
+	 */
+	if (IS_PREBDW(pdev)) {
+		if (mm->type == VGT_MM_PPGTT && !page_table)
+			index += mm->pde_base_index;
+	}
+
+	ops->get_entry(page_table, e, index, false, NULL);
+	ops->test_pse(e);
+
+	return e;
+}
+
+gtt_entry_t *vgt_mm_set_entry(struct vgt_mm *mm,
+		void *page_table, gtt_entry_t *e,
+		unsigned long index)
+{
+	struct pgt_device *pdev = mm->vgt->pdev;
+	struct vgt_gtt_pte_ops *ops = pdev->gtt.pte_ops;
+
+	e->pdev = pdev;
+
+	/*
+	 * If this write goes to HW, we translate
+	 * the relative index to absolute index
+	 * for pre-bdw platform.
+	 */
+	if (IS_PREBDW(pdev)) {
+		if (mm->type == VGT_MM_PPGTT && !page_table)
+			index += mm->pde_base_index;
+	}
+
+	return ops->set_entry(page_table, e, index, false, NULL);
+}
+
+/*
+ * PPGTT shadow page table helpers.
+ */
+static inline gtt_entry_t *ppgtt_spt_get_entry(ppgtt_spt_t *spt,
+		void *page_table, gtt_type_t type,
+		gtt_entry_t *e, unsigned long index,
+		bool guest)
+{
+	struct pgt_device *pdev = spt->vgt->pdev;
+	struct vgt_gtt_pte_ops *ops = pdev->gtt.pte_ops;
+
+	e->pdev = pdev;
+	e->type = get_entry_type(type);
+
+	ASSERT(gtt_type_is_entry(e->type));
+
+	ops->get_entry(page_table, e, index, guest, spt->vgt);
+	ops->test_pse(e);
+
+	return e;
+}
+
+static inline gtt_entry_t *ppgtt_spt_set_entry(ppgtt_spt_t *spt,
+		void *page_table, gtt_type_t type,
+		gtt_entry_t *e, unsigned long index,
+		bool guest)
+{
+	struct pgt_device *pdev = spt->vgt->pdev;
+	struct vgt_gtt_pte_ops *ops = pdev->gtt.pte_ops;
+
+	e->pdev = pdev;
+
+	ASSERT(gtt_type_is_entry(e->type));
+
+	return ops->set_entry(page_table, e, index, guest, spt->vgt);
+}
+
+#define ppgtt_get_guest_entry(spt, e, index) \
+	ppgtt_spt_get_entry(spt, spt->guest_page.vaddr, \
+		spt->guest_page_type, e, index, true)
+
+#define ppgtt_set_guest_entry(spt, e, index) \
+	ppgtt_spt_set_entry(spt, spt->guest_page.vaddr, \
+		spt->guest_page_type, e, index, true)
+
+#define ppgtt_get_shadow_entry(spt, e, index) \
+	ppgtt_spt_get_entry(spt, spt->shadow_page.vaddr, \
+		spt->shadow_page.type, e, index, false)
+
+#define ppgtt_set_shadow_entry(spt, e, index) \
+	ppgtt_spt_set_entry(spt, spt->shadow_page.vaddr, \
+		spt->shadow_page.type, e, index, false)
+
+
+bool vgt_init_guest_page(struct vgt_device *vgt, guest_page_t *guest_page,
+		unsigned long gfn, guest_page_handler_t handler, void *data)
+{
+	INIT_HLIST_NODE(&guest_page->node);
+
+	guest_page->vaddr = hypervisor_gpa_to_va(vgt, gfn << GTT_PAGE_SHIFT);
+	if (!guest_page->vaddr)
+		return false;
+
+	guest_page->writeprotection = false;
+	guest_page->gfn = gfn;
+	guest_page->handler = handler;
+	guest_page->data = data;
+
+	hash_add(vgt->gtt.guest_page_hash_table, &guest_page->node, guest_page->gfn);
+
+	return true;
+}
+
+void vgt_clean_guest_page(struct vgt_device *vgt, guest_page_t *guest_page)
+{
+	if(!hlist_unhashed(&guest_page->node))
+		hash_del(&guest_page->node);
+
+	if (guest_page->writeprotection)
+		hypervisor_unset_wp_pages(vgt, guest_page);
+}
+
+guest_page_t *vgt_find_guest_page(struct vgt_device *vgt, unsigned long gfn)
+{
+	guest_page_t *guest_page;
+
+	hash_for_each_possible(vgt->gtt.guest_page_hash_table, guest_page, node, gfn)
+		if (guest_page->gfn == gfn)
+			return guest_page;
+
+	return NULL;
+}
+
+/*
+ * Shadow page manipulation routines.
+ */
+static inline bool vgt_init_shadow_page(struct vgt_device *vgt,
+		shadow_page_t *sp, gtt_type_t type)
+{
+	sp->vaddr = page_address(sp->page);
+	sp->type = type;
+	memset(sp->vaddr, 0, PAGE_SIZE);
+
+	INIT_HLIST_NODE(&sp->node);
+	sp->mfn = hypervisor_virt_to_mfn(sp->vaddr);
+	hash_add(vgt->gtt.shadow_page_hash_table, &sp->node, sp->mfn);
+
+	return true;
+}
+
+static inline void vgt_clean_shadow_page(shadow_page_t *sp)
+{
+	if(!hlist_unhashed(&sp->node))
+		hash_del(&sp->node);
+}
+
+static inline shadow_page_t *vgt_find_shadow_page(struct vgt_device *vgt,
+		unsigned long mfn)
+{
+	shadow_page_t *shadow_page;
+
+	hash_for_each_possible(vgt->gtt.shadow_page_hash_table, shadow_page, node, mfn) {
+		if (shadow_page->mfn == mfn)
+			return shadow_page;
+	}
+
+	return NULL;
+}
+
+#define guest_page_to_ppgtt_spt(ptr) \
+	container_of(ptr, ppgtt_spt_t, guest_page)
+
+#define shadow_page_to_ppgtt_spt(ptr) \
+	container_of(ptr, ppgtt_spt_t, shadow_page)
+
+static void ppgtt_free_shadow_page(ppgtt_spt_t *spt)
+{
+	trace_spt_free(spt->vgt->vm_id, spt, spt->shadow_page.type);
+
+	vgt_clean_shadow_page(&spt->shadow_page);
+	vgt_clean_guest_page(spt->vgt, &spt->guest_page);
+
+	mempool_free(spt, spt->vgt->gtt.mempool);
+}
+
+static void ppgtt_free_all_shadow_page(struct vgt_device *vgt)
+{
+	struct hlist_node *n;
+	shadow_page_t *sp;
+	int i;
+
+	hash_for_each_safe(vgt->gtt.shadow_page_hash_table, i, n, sp, node)
+		ppgtt_free_shadow_page(shadow_page_to_ppgtt_spt(sp));
+
+	return;
+}
+
+static bool ppgtt_handle_guest_write_page_table(guest_page_t *gpt, gtt_entry_t *we,
+		unsigned long index);
+
+static bool ppgtt_write_protection_handler(void *gp, uint64_t pa, void *p_data, int bytes)
+{
+	guest_page_t *gpt = (guest_page_t *)gp;
+	ppgtt_spt_t *spt = guest_page_to_ppgtt_spt(gpt);
+	struct vgt_device *vgt = spt->vgt;
+	struct vgt_device_info *info = &vgt->pdev->device_info;
+	struct vgt_gtt_pte_ops *ops = vgt->pdev->gtt.pte_ops;
+	gtt_type_t type = get_entry_type(spt->guest_page_type);
+	unsigned long index;
+	gtt_entry_t e;
+
+	if (bytes != 4 && bytes != 8)
+		return false;
+
+	if (!gpt->writeprotection)
+		return false;
+
+	e.val64 = 0;
+
+	if (info->gtt_entry_size == 4) {
+		gtt_init_entry(&e, type, vgt->pdev, *(u32 *)p_data);
+	} else if (info->gtt_entry_size == 8) {
+		ASSERT_VM(bytes == 8, vgt);
+		gtt_init_entry(&e, type, vgt->pdev, *(u64 *)p_data);
+	}
+
+	ops->test_pse(&e);
+
+	index = (pa & (PAGE_SIZE - 1)) >> info->gtt_entry_size_shift;
+
+	return ppgtt_handle_guest_write_page_table(gpt, &e, index);
+}
+
+static ppgtt_spt_t *ppgtt_alloc_shadow_page(struct vgt_device *vgt,
+		gtt_type_t type, unsigned long gpt_gfn)
+{
+	ppgtt_spt_t *spt = NULL;
+
+	spt = mempool_alloc(vgt->gtt.mempool, GFP_ATOMIC);
+	if (!spt) {
+		vgt_err("fail to allocate ppgtt shadow page.\n");
+		return NULL;
+	}
+
+	spt->guest_page_type = type;
+	atomic_set(&spt->refcount, 1);
+
+	/*
+	 * TODO: Guest page may be different with shadow page type,
+	 *	 if we support PSE page in future.
+	 */
+	if (!vgt_init_shadow_page(vgt, &spt->shadow_page, type)) {
+		vgt_err("fail to initialize shadow_page_t for spt.\n");
+		goto err;
+	}
+
+	if (!vgt_init_guest_page(vgt, &spt->guest_page,
+				gpt_gfn, ppgtt_write_protection_handler, NULL)) {
+		vgt_err("fail to initialize shadow_page_t for spt.\n");
+		goto err;
+	}
+
+	trace_spt_alloc(vgt->vm_id, spt, type, spt->shadow_page.mfn, gpt_gfn);
+
+	return spt;
+err:
+	ppgtt_free_shadow_page(spt);
+	return NULL;
+}
+
+static ppgtt_spt_t *ppgtt_find_shadow_page(struct vgt_device *vgt, unsigned long mfn)
+{
+	shadow_page_t *sp = vgt_find_shadow_page(vgt, mfn);
+
+	if (sp)
+		return shadow_page_to_ppgtt_spt(sp);
+
+	vgt_err("VM %d fail to find ppgtt shadow page: 0x%lx.\n",
+			vgt->vm_id, mfn);
+
+	return NULL;
+}
+
+#define pt_entry_size_shift(spt) \
+	((spt)->vgt->pdev->device_info.gtt_entry_size_shift)
+
+#define pt_entries(spt) \
+	(PAGE_SIZE >> pt_entry_size_shift(spt))
+
+#define for_each_present_guest_entry(spt, e, i) \
+	for (i = 0; i < pt_entries(spt); i++) \
+	if (spt->vgt->pdev->gtt.pte_ops->test_present(ppgtt_get_guest_entry(spt, e, i)))
+
+#define for_each_present_shadow_entry(spt, e, i) \
+	for (i = 0; i < pt_entries(spt); i++) \
+	if (spt->vgt->pdev->gtt.pte_ops->test_present(ppgtt_get_shadow_entry(spt, e, i)))
+
+static void ppgtt_get_shadow_page(ppgtt_spt_t *spt)
+{
+	int v = atomic_read(&spt->refcount);
+
+	trace_spt_refcount(spt->vgt->vm_id, "inc", spt, v, (v + 1));
+
+	atomic_inc(&spt->refcount);
+}
+
+static bool ppgtt_invalidate_shadow_page(ppgtt_spt_t *spt);
+
+static bool ppgtt_invalidate_shadow_page_by_shadow_entry(struct vgt_device *vgt,
+		gtt_entry_t *e)
+{
+	struct vgt_gtt_pte_ops *ops = vgt->pdev->gtt.pte_ops;
+	ppgtt_spt_t *s;
+
+	if (!gtt_type_is_pt(get_next_pt_type(e->type)))
+		return false;
+
+	s = ppgtt_find_shadow_page(vgt, ops->get_pfn(e));
+	if (!s) {
+		vgt_err("VM %d fail to find shadow page: mfn: 0x%lx.\n",
+				vgt->vm_id, ops->get_pfn(e));
+		return false;
+	}
+
+	return ppgtt_invalidate_shadow_page(s);
+}
+
+static bool ppgtt_invalidate_shadow_page(ppgtt_spt_t *spt)
+{
+	gtt_entry_t e;
+	unsigned long index;
+	int v = atomic_read(&spt->refcount);
+
+	trace_spt_change(spt->vgt->vm_id, "die", spt,
+			spt->guest_page.gfn, spt->shadow_page.type);
+
+	trace_spt_refcount(spt->vgt->vm_id, "dec", spt, v, (v - 1));
+
+	if (atomic_dec_return(&spt->refcount) > 0)
+		return true;
+
+	if (gtt_type_is_pte_pt(spt->shadow_page.type))
+		goto release;
+
+	for_each_present_shadow_entry(spt, &e, index) {
+		if (!gtt_type_is_pt(get_next_pt_type(e.type))) {
+			vgt_err("VGT doesn't support pse bit now.\n");
+			return false;
+		}
+		if (!ppgtt_invalidate_shadow_page_by_shadow_entry(spt->vgt, &e))
+			goto fail;
+	}
+
+release:
+	trace_spt_change(spt->vgt->vm_id, "release", spt,
+			spt->guest_page.gfn, spt->shadow_page.type);
+	ppgtt_free_shadow_page(spt);
+	return true;
+fail:
+	vgt_err("fail: shadow page %p shadow entry 0x%llx type %d.\n",
+			spt, e.val64, e.type);
+	return false;
+}
+
+static bool ppgtt_populate_shadow_page(ppgtt_spt_t *spt);
+
+static ppgtt_spt_t *ppgtt_populate_shadow_page_by_guest_entry(struct vgt_device *vgt,
+		gtt_entry_t *we)
+{
+	struct vgt_gtt_pte_ops *ops = vgt->pdev->gtt.pte_ops;
+	ppgtt_spt_t *s = NULL;
+	guest_page_t *g;
+
+	if (!gtt_type_is_pt(get_next_pt_type(we->type)))
+		goto fail;
+
+	g = vgt_find_guest_page(vgt, ops->get_pfn(we));
+	if (g) {
+		s = guest_page_to_ppgtt_spt(g);
+		ppgtt_get_shadow_page(s);
+	} else {
+		gtt_type_t type = get_next_pt_type(we->type);
+		s = ppgtt_alloc_shadow_page(vgt, type, ops->get_pfn(we));
+		if (!s)
+			goto fail;
+
+		if (!hypervisor_set_wp_pages(vgt, &s->guest_page))
+			goto fail;
+
+		if (!ppgtt_populate_shadow_page(s))
+			goto fail;
+
+		trace_spt_change(vgt->vm_id, "new", s, s->guest_page.gfn, s->shadow_page.type);
+	}
+	return s;
+fail:
+	vgt_err("fail: shadow page %p guest entry 0x%llx type %d.\n",
+			s, we->val64, we->type);
+	return NULL;
+}
+
+static inline void ppgtt_generate_shadow_entry(gtt_entry_t *se,
+		ppgtt_spt_t *s, gtt_entry_t *ge)
+{
+	struct vgt_gtt_pte_ops *ops = s->vgt->pdev->gtt.pte_ops;
+
+	se->type = ge->type;
+	se->val64 = ge->val64;
+	se->pdev = ge->pdev;
+
+	ops->set_pfn(se, s->shadow_page.mfn);
+}
+
+static bool ppgtt_populate_shadow_page(ppgtt_spt_t *spt)
+{
+	struct vgt_device *vgt = spt->vgt;
+	ppgtt_spt_t *s;
+	gtt_entry_t se, ge;
+	unsigned long i;
+
+	trace_spt_change(spt->vgt->vm_id, "born", spt,
+			spt->guest_page.gfn, spt->shadow_page.type);
+
+	if (gtt_type_is_pte_pt(spt->shadow_page.type)) {
+		for_each_present_guest_entry(spt, &ge, i) {
+			if (!gtt_entry_p2m(vgt, &ge, &se))
+				goto fail;
+			ppgtt_set_shadow_entry(spt, &se, i);
+		}
+		return true;
+	}
+
+	for_each_present_guest_entry(spt, &ge, i) {
+		if (!gtt_type_is_pt(get_next_pt_type(ge.type))) {
+			vgt_err("VGT doesn't support pse bit now.\n");
+			goto fail;
+		}
+
+		s = ppgtt_populate_shadow_page_by_guest_entry(vgt, &ge);
+		if (!s)
+			goto fail;
+		ppgtt_get_shadow_entry(spt, &se, i);
+		ppgtt_generate_shadow_entry(&se, s, &ge);
+		ppgtt_set_shadow_entry(spt, &se, i);
+	}
+	return true;
+fail:
+	vgt_err("fail: shadow page %p guest entry 0x%llx type %d.\n",
+			spt, ge.val64, ge.type);
+	return false;
+}
+
+static bool ppgtt_handle_guest_entry_removal(guest_page_t *gpt,
+		gtt_entry_t *we, unsigned long index)
+{
+	ppgtt_spt_t *spt = guest_page_to_ppgtt_spt(gpt);
+	shadow_page_t *sp = &spt->shadow_page;
+	struct vgt_device *vgt = spt->vgt;
+	struct vgt_gtt_pte_ops *ops = vgt->pdev->gtt.pte_ops;
+	gtt_entry_t e;
+
+	trace_guest_pt_change(spt->vgt->vm_id, "remove", spt, sp->type, we->val64, index);
+
+	if (gtt_type_is_pt(get_next_pt_type(we->type))) {
+		guest_page_t *g = vgt_find_guest_page(vgt, ops->get_pfn(we));
+		if (!g) {
+			vgt_err("fail to find guest page.\n");
+			goto fail;
+		}
+		if (!ppgtt_invalidate_shadow_page(guest_page_to_ppgtt_spt(g)))
+			goto fail;
+	}
+	ppgtt_get_shadow_entry(spt, &e, index);
+	e.val64 = 0;
+	ppgtt_set_shadow_entry(spt, &e, index);
+	return true;
+fail:
+	vgt_err("fail: shadow page %p guest entry 0x%llx type %d.\n",
+			spt, we->val64, we->type);
+	return false;
+}
+
+static bool ppgtt_handle_guest_entry_add(guest_page_t *gpt,
+		gtt_entry_t *we, unsigned long index)
+{
+	ppgtt_spt_t *spt = guest_page_to_ppgtt_spt(gpt);
+	shadow_page_t *sp = &spt->shadow_page;
+	struct vgt_device *vgt = spt->vgt;
+	gtt_entry_t m;
+	ppgtt_spt_t *s;
+
+	trace_guest_pt_change(spt->vgt->vm_id, "add", spt, sp->type, we->val64, index);
+
+	if (gtt_type_is_pt(get_next_pt_type(we->type))) {
+		s = ppgtt_populate_shadow_page_by_guest_entry(vgt, we);
+		if (!s)
+			goto fail;
+		ppgtt_get_shadow_entry(spt, &m, index);
+		ppgtt_generate_shadow_entry(&m, s, we);
+		ppgtt_set_shadow_entry(spt, &m, index);
+	} else {
+		if (!gtt_entry_p2m(vgt, we, &m))
+			goto fail;
+		ppgtt_set_shadow_entry(spt, &m, index);
+	}
+
+	return true;
+
+fail:
+	vgt_err("fail: spt %p guest entry 0x%llx type %d.\n", spt, we->val64, we->type);
+	return false;
+}
+
+/*
+ * The heart of PPGTT shadow page table.
+ */
+static bool ppgtt_handle_guest_write_page_table(guest_page_t *gpt, gtt_entry_t *we,
+		unsigned long index)
+{
+	ppgtt_spt_t *spt = guest_page_to_ppgtt_spt(gpt);
+	struct vgt_device *vgt = spt->vgt;
+	struct vgt_gtt_pte_ops *ops = vgt->pdev->gtt.pte_ops;
+	gtt_entry_t ge;
+
+	int old_present, new_present;
+
+	ppgtt_get_guest_entry(spt, &ge, index);
+
+	old_present = ops->test_present(&ge);
+	new_present = ops->test_present(we);
+
+	ppgtt_set_guest_entry(spt, we, index);
+
+	if (old_present && new_present) {
+		if (!ppgtt_handle_guest_entry_removal(gpt, &ge, index)
+		|| !ppgtt_handle_guest_entry_add(gpt, we, index))
+			goto fail;
+	} else if (!old_present && new_present) {
+		if (!ppgtt_handle_guest_entry_add(gpt, we, index))
+			goto fail;
+	} else if (old_present && !new_present) {
+		if (!ppgtt_handle_guest_entry_removal(gpt, &ge, index))
+			goto fail;
+	}
+	return true;
+fail:
+	vgt_err("fail: shadow page %p guest entry 0x%llx type %d.\n",
+			spt, we->val64, we->type);
+	return false;
+}
+
+bool ppgtt_handle_guest_write_root_pointer(struct vgt_mm *mm,
+		gtt_entry_t *we, unsigned long index)
+{
+	struct vgt_device *vgt = mm->vgt;
+	struct vgt_gtt_pte_ops *ops = vgt->pdev->gtt.pte_ops;
+	ppgtt_spt_t *spt = NULL;
+	gtt_entry_t e;
+
+	if (mm->type != VGT_MM_PPGTT || !mm->shadowed)
+		return false;
+
+	trace_guest_pt_change(vgt->vm_id, __func__, NULL,
+			we->type, we->val64, index);
+
+	ppgtt_get_guest_root_entry(mm, &e, index);
+
+	if (ops->test_present(&e)) {
+		ppgtt_get_shadow_root_entry(mm, &e, index);
+
+		trace_guest_pt_change(vgt->vm_id, "destroy old root pointer",
+				spt, e.type, e.val64, index);
+
+		if (gtt_type_is_pt(get_next_pt_type(e.type))) {
+			if (!ppgtt_invalidate_shadow_page_by_shadow_entry(vgt, &e))
+				goto fail;
+		} else {
+			vgt_err("VGT doesn't support pse bit now.\n");
+			goto fail;
+		}
+		e.val64 = 0;
+		ppgtt_set_shadow_root_entry(mm, &e, index);
+	}
+
+	if (ops->test_present(we)) {
+		if (gtt_type_is_pt(get_next_pt_type(we->type))) {
+			spt = ppgtt_populate_shadow_page_by_guest_entry(vgt, we);
+			if (!spt) {
+				vgt_err("fail to populate root pointer.\n");
+				goto fail;
+			}
+			ppgtt_generate_shadow_entry(&e, spt, we);
+			ppgtt_set_shadow_root_entry(mm, &e, index);
+		} else {
+			vgt_err("VGT doesn't support pse bit now.\n");
+			goto fail;
+		}
+		trace_guest_pt_change(vgt->vm_id, "populate root pointer",
+				spt, e.type, e.val64, index);
+	}
+	return true;
+fail:
+	vgt_err("fail: shadow page %p guest entry 0x%llx type %d.\n",
+			spt, we->val64, we->type);
+	return false;
+}
+
+/*
+ * mm page table allocation policy for pre-bdw:
+ *  - for ggtt, a virtual page table will be allocated.
+ *  - for ppgtt, the virtual page table(root entry) will use a part of
+ *	virtual page table from ggtt.
+ */
+bool gen7_mm_alloc_page_table(struct vgt_mm *mm)
+{
+	struct vgt_device *vgt = mm->vgt;
+	struct pgt_device *pdev = vgt->pdev;
+	struct vgt_vgtt_info *gtt = &vgt->gtt;
+	struct vgt_device_info *info = &pdev->device_info;
+	void *mem;
+
+	if (mm->type == VGT_MM_PPGTT) {
+		struct vgt_mm *ggtt_mm = gtt->ggtt_mm;
+		if (!ggtt_mm) {
+			vgt_err("ggtt mm hasn't been created.\n");
+			return false;
+		}
+		mm->page_table_entry_cnt = 512;
+		mm->page_table_entry_size = mm->page_table_entry_cnt *
+			info->gtt_entry_size;
+		mm->virtual_page_table = ggtt_mm->virtual_page_table +
+			(mm->pde_base_index << info->gtt_entry_size_shift);
+		/* shadow page table resides in the hw mmio entries. */
+	} else if (mm->type == VGT_MM_GGTT) {
+		mm->page_table_entry_cnt = (gm_sz(pdev) >> GTT_PAGE_SHIFT);
+		mm->page_table_entry_size = mm->page_table_entry_cnt *
+			info->gtt_entry_size;
+		mem = vzalloc(mm->page_table_entry_size);
+		if (!mem) {
+			vgt_err("fail to allocate memory.\n");
+			return false;
+		}
+		mm->virtual_page_table = mem;
+	}
+	return true;
+}
+
+void gen7_mm_free_page_table(struct vgt_mm *mm)
+{
+	if (mm->type == VGT_MM_GGTT) {
+		if (mm->virtual_page_table)
+			vfree(mm->virtual_page_table);
+	}
+	mm->virtual_page_table = mm->shadow_page_table = NULL;
+}
+
+/*
+ * mm page table allocation policy for bdw+
+ *  - for ggtt, only virtual page table will be allocated.
+ *  - for ppgtt, dedicated virtual/shadow page table will be allocated.
+ */
+bool gen8_mm_alloc_page_table(struct vgt_mm *mm)
+{
+	struct vgt_device *vgt = mm->vgt;
+	struct pgt_device *pdev = vgt->pdev;
+	struct vgt_device_info *info = &pdev->device_info;
+	void *mem;
+
+	if (mm->type == VGT_MM_PPGTT) {
+		mm->page_table_entry_cnt = 4;
+		mm->page_table_entry_size = mm->page_table_entry_cnt *
+			info->gtt_entry_size;
+		mem = kzalloc(mm->has_shadow_page_table ?
+			mm->page_table_entry_size * 2 : mm->page_table_entry_size,
+			GFP_ATOMIC);
+		if (!mem) {
+			vgt_err("fail to allocate memory.\n");
+			return false;
+		}
+		mm->virtual_page_table = mem;
+		if (!mm->has_shadow_page_table)
+			return true;
+		mm->shadow_page_table = mem + mm->page_table_entry_size;
+	} else if (mm->type == VGT_MM_GGTT) {
+		mm->page_table_entry_cnt = (gm_sz(pdev) >> GTT_PAGE_SHIFT);
+		mm->page_table_entry_size = mm->page_table_entry_cnt *
+			info->gtt_entry_size;
+		mem = vzalloc(mm->page_table_entry_size);
+		if (!mem) {
+			vgt_err("fail to allocate memory.\n");
+			return false;
+		}
+		mm->virtual_page_table = mem;
+	}
+	return true;
+}
+
+void gen8_mm_free_page_table(struct vgt_mm *mm)
+{
+	if (mm->type == VGT_MM_PPGTT) {
+		if (mm->virtual_page_table)
+			kfree(mm->virtual_page_table);
+	} else if (mm->type == VGT_MM_GGTT) {
+		if (mm->virtual_page_table)
+			vfree(mm->virtual_page_table);
+	}
+	mm->virtual_page_table = mm->shadow_page_table = NULL;
+}
+
+void vgt_destroy_mm(struct vgt_mm *mm)
+{
+	struct vgt_device *vgt = mm->vgt;
+	struct pgt_device *pdev = vgt->pdev;
+	struct vgt_gtt_info *gtt = &pdev->gtt;
+	struct vgt_gtt_pte_ops *ops = gtt->pte_ops;
+	gtt_entry_t se;
+	int i;
+
+	if (!mm->initialized)
+		goto out;
+
+	if (atomic_dec_return(&mm->refcount) > 0)
+		return;
+
+	list_del(&mm->list);
+
+	if (mm->has_shadow_page_table && mm->shadowed) {
+		for (i = 0; i < mm->page_table_entry_cnt; i++) {
+			ppgtt_get_shadow_root_entry(mm, &se, i);
+			if (!ops->test_present(&se))
+				continue;
+			ppgtt_invalidate_shadow_page_by_shadow_entry(vgt, &se);
+			se.val64 = 0;
+			ppgtt_set_shadow_root_entry(mm, &se, i);
+
+			trace_guest_pt_change(vgt->vm_id, "destroy root pointer",
+					NULL, se.type, se.val64, i);
+		}
+	}
+	gtt->mm_free_page_table(mm);
+out:
+	kfree(mm);
+}
+
+struct vgt_mm *vgt_create_mm(struct vgt_device *vgt,
+		vgt_mm_type_t mm_type, gtt_type_t page_table_entry_type,
+		void *virtual_page_table, int page_table_level,
+		u32 pde_base_index)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	struct vgt_gtt_info *gtt = &pdev->gtt;
+	struct vgt_gtt_pte_ops *ops = gtt->pte_ops;
+	struct vgt_mm *mm;
+	ppgtt_spt_t *spt;
+	gtt_entry_t ge, se;
+	int i;
+
+	mm = kzalloc(sizeof(*mm), GFP_ATOMIC);
+	if (!mm) {
+		vgt_err("fail to allocate memory for mm.\n");
+		goto fail;
+	}
+
+	mm->type = mm_type;
+	mm->page_table_entry_type = page_table_entry_type;
+	mm->page_table_level = page_table_level;
+	mm->pde_base_index = pde_base_index;
+
+	mm->vgt = vgt;
+	mm->has_shadow_page_table = (vgt->vm_id != 0 && mm_type == VGT_MM_PPGTT);
+
+	atomic_set(&mm->refcount, 1);
+	INIT_LIST_HEAD(&mm->list);
+	list_add_tail(&mm->list, &vgt->gtt.mm_list_head);
+
+	if (!gtt->mm_alloc_page_table(mm)) {
+		vgt_err("fail to allocate page table for mm.\n");
+		goto fail;
+	}
+
+	mm->initialized = true;
+
+	if (virtual_page_table)
+		memcpy(mm->virtual_page_table, virtual_page_table,
+				mm->page_table_entry_size);
+
+	if (mm->has_shadow_page_table) {
+		for (i = 0; i < mm->page_table_entry_cnt; i++) {
+			ppgtt_get_guest_root_entry(mm, &ge, i);
+			if (!ops->test_present(&ge))
+				continue;
+
+			trace_guest_pt_change(vgt->vm_id, __func__, NULL,
+					ge.type, ge.val64, i);
+
+			spt = ppgtt_populate_shadow_page_by_guest_entry(vgt, &ge);
+			if (!spt) {
+				vgt_err("fail to populate guest root pointer.\n");
+				goto fail;
+			}
+			ppgtt_generate_shadow_entry(&se, spt, &ge);
+			ppgtt_set_shadow_root_entry(mm, &se, i);
+
+			trace_guest_pt_change(vgt->vm_id, "populate root pointer",
+					NULL, se.type, se.val64, i);
+		}
+		mm->shadowed = true;
+	}
+	return mm;
+fail:
+	vgt_err("fail to create mm.\n");
+	if (mm)
+		vgt_destroy_mm(mm);
+	return NULL;
+}
+
+/*
+ * GMA translation APIs.
+ */
+static inline bool ppgtt_get_next_level_entry(struct vgt_mm *mm,
+		gtt_entry_t *e, unsigned long index, bool guest)
+{
+	struct vgt_device *vgt = mm->vgt;
+	struct vgt_gtt_pte_ops *ops = vgt->pdev->gtt.pte_ops;
+	ppgtt_spt_t *s;
+	void *pt;
+
+	if (mm->has_shadow_page_table) {
+		if (!(s = ppgtt_find_shadow_page(vgt, ops->get_pfn(e))))
+			return false;
+		if (!guest)
+			ppgtt_get_shadow_entry(s, e, index);
+		else
+			ppgtt_get_guest_entry(s, e, index);
+	} else {
+		pt = hypervisor_mfn_to_virt(ops->get_pfn(e));
+		ops->get_entry(pt, e, index, false, NULL);
+		e->type = get_entry_type(get_next_pt_type(e->type));
+	}
+	return true;
+}
+
+static inline unsigned long vgt_gma_to_gpa(struct vgt_mm *mm, unsigned long gma)
+{
+	struct vgt_device *vgt = mm->vgt;
+	struct pgt_device *pdev = vgt->pdev;
+	struct vgt_gtt_pte_ops *pte_ops = pdev->gtt.pte_ops;
+	struct vgt_gtt_gma_ops *gma_ops = pdev->gtt.gma_ops;
+
+	unsigned long gpa = INVALID_ADDR;
+	unsigned long gma_index[4];
+	gtt_entry_t e;
+	int i, index;
+
+	if (mm->type != VGT_MM_GGTT && mm->type != VGT_MM_PPGTT)
+		return INVALID_ADDR;
+
+	if (mm->type == VGT_MM_GGTT) {
+		if (!g_gm_is_valid(vgt, gma))
+			goto err;
+
+		ggtt_get_guest_entry(mm, &e, gma_ops->gma_to_ggtt_pte_index(gma));
+		gpa = (pte_ops->get_pfn(&e) << GTT_PAGE_SHIFT) + (gma & ~GTT_PAGE_MASK);
+
+		trace_gma_translate(vgt->vm_id, "ggtt", 0, 0, gma, gpa);
+
+		return gpa;
+	}
+
+	switch (mm->page_table_level) {
+		case 4:
+			ppgtt_get_shadow_root_entry(mm, &e, 0);
+			gma_index[0] = gma_ops->gma_to_pml4_index(gma);
+			gma_index[1] = gma_ops->gma_to_l4_pdp_index(gma);
+			gma_index[2] = gma_ops->gma_to_pde_index(gma);
+			gma_index[3] = gma_ops->gma_to_pte_index(gma);
+			index = 4;
+			break;
+		case 3:
+			ppgtt_get_shadow_root_entry(mm, &e, gma_ops->gma_to_l3_pdp_index(gma));
+			gma_index[0] = gma_ops->gma_to_pde_index(gma);
+			gma_index[1] = gma_ops->gma_to_pte_index(gma);
+			index = 2;
+			break;
+		case 2:
+			ppgtt_get_shadow_root_entry(mm, &e, gma_ops->gma_to_pde_index(gma));
+			gma_index[0] = gma_ops->gma_to_pte_index(gma);
+			index = 1;
+			break;
+		default:
+			BUG();
+	}
+	/* walk into the last level shadow page table and get gpa from guest entry */
+	for (i = 0; i < index; i++)
+		if (!ppgtt_get_next_level_entry(mm, &e, gma_index[i],
+			(i == index - 1)))
+			goto err;
+
+	gpa = (pte_ops->get_pfn(&e) << GTT_PAGE_SHIFT) + (gma & ~GTT_PAGE_MASK);
+
+	trace_gma_translate(vgt->vm_id, "ppgtt", 0, mm->page_table_level, gma, gpa);
+
+	return gpa;
+err:
+	vgt_err("invalid mm type: %d, gma %lx\n", mm->type, gma);
+	return INVALID_ADDR;
+}
+
+void *vgt_gma_to_va(struct vgt_mm *mm, unsigned long gma)
+{
+	struct vgt_device *vgt = mm->vgt;
+	unsigned long gpa;
+
+	gpa = vgt_gma_to_gpa(mm, gma);
+	if (gpa == INVALID_ADDR) {
+		vgt_warn("invalid gpa! gma 0x%lx, mm type %d\n", gma, mm->type);
+		return NULL;
+	}
+
+	return hypervisor_gpa_to_va(vgt, gpa);
+}
+
+/*
+ * GTT MMIO emulation.
+ */
+bool gtt_mmio_read(struct vgt_device *vgt,
+	unsigned int off, void *p_data, unsigned int bytes)
+{
+	struct vgt_mm *ggtt_mm = vgt->gtt.ggtt_mm;
+	struct vgt_device_info *info = &vgt->pdev->device_info;
+	unsigned long index = off >> info->gtt_entry_size_shift;
+	gtt_entry_t e;
+
+	if (bytes != 4 && bytes != 8)
+		return false;
+
+	ggtt_get_guest_entry(ggtt_mm, &e, index);
+
+	if (bytes == 4 && info->gtt_entry_size == 4)
+		*(u32 *)p_data = e.val32[0];
+	else if (info->gtt_entry_size == 8)
+		memcpy(p_data, &e.val64 + (off & 0x7), bytes);
+
+	return true;
+}
+
+bool gtt_emulate_read(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes)
+{
+	struct vgt_device_info *info = &vgt->pdev->device_info;
+	int ret;
+	cycles_t t0, t1;
+	struct vgt_statistics *stat = &vgt->stat;
+
+	if (bytes != 4 && bytes != 8)
+		return false;
+
+	t0 = get_cycles();
+	stat->gtt_mmio_rcnt++;
+
+	off -= info->gtt_start_offset;
+
+	if (IS_PREBDW(vgt->pdev)) {
+		ret = gtt_mmio_read(vgt, off, p_data, 4);
+		if (ret && bytes == 8)
+			ret = gtt_mmio_read(vgt, off + 4, (char*)p_data + 4, 4);
+	} else {
+		ret = gtt_mmio_read(vgt, off, p_data, bytes);
+	}
+
+	t1 = get_cycles();
+	stat->gtt_mmio_rcycles += (u64) (t1 - t0);
+	return ret;
+}
+
+static bool process_ppgtt_root_pointer(struct vgt_device *vgt,
+		gtt_entry_t *ge, unsigned int gtt_index)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	struct vgt_mm *mm = NULL;
+	gtt_entry_t e;
+	int index;
+	struct list_head *pos;
+
+	if (!IS_PREBDW(pdev))
+		return true;
+
+	list_for_each(pos, &vgt->gtt.mm_list_head) {
+		struct vgt_mm *ppgtt_mm = container_of(pos, struct vgt_mm, list);
+		if (ppgtt_mm->type != VGT_MM_PPGTT || !ppgtt_mm->initialized)
+			continue;
+
+		if (gtt_index >= ppgtt_mm->pde_base_index
+			&& gtt_index < ppgtt_mm->pde_base_index +
+			ppgtt_mm->page_table_entry_cnt) {
+			mm = ppgtt_mm;
+			break;
+		}
+	}
+
+	if (!mm || !mm->has_shadow_page_table)
+		return true;
+
+	gtt_init_entry(&e, GTT_TYPE_PPGTT_PDE_ENTRY, pdev, ge->val64);
+
+	index = gtt_index - mm->pde_base_index;
+
+	ppgtt_set_guest_root_entry(mm, &e, index);
+
+	if (!ppgtt_handle_guest_write_root_pointer(mm, &e, index))
+		return false;
+
+	ge->type = GTT_TYPE_PPGTT_PDE_ENTRY;
+
+	return true;
+}
+
+bool gtt_mmio_write(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	struct vgt_device_info *info = &pdev->device_info;
+	struct vgt_mm *ggtt_mm = vgt->gtt.ggtt_mm;
+	unsigned long g_gtt_index = off >> info->gtt_entry_size_shift;
+	unsigned long gma;
+	gtt_entry_t e, m;
+	int rc;
+
+	if (bytes != 4 && bytes != 8)
+		return false;
+
+	gma = g_gtt_index << GTT_PAGE_SHIFT;
+	/* the VM may configure the whole GM space when ballooning is used */
+	if (!g_gm_is_valid(vgt, gma)) {
+		static int count = 0;
+
+		/* print info every 32MB */
+		if (!(count % 8192))
+			vgt_dbg(VGT_DBG_MEM, "vGT(%d): capture ballooned write for %d times (%x)\n",
+				vgt->vgt_id, count, off);
+
+		count++;
+		/* in this case still return true since the impact is on vgtt only */
+		goto out;
+	}
+
+	if (bytes == 4 && info->gtt_entry_size == 4)
+		e.val32[0] = *(u32 *)p_data;
+	else if (info->gtt_entry_size == 8)
+		memcpy(&e.val64 + (off & 7), p_data, bytes);
+
+	gtt_init_entry(&e, GTT_TYPE_GGTT_PTE, vgt->pdev, e.val64);
+
+	if (!process_ppgtt_root_pointer(vgt, &e, g_gtt_index))
+		return false;
+
+	if (e.type != GTT_TYPE_GGTT_PTE)
+		return true;
+
+	ggtt_set_guest_entry(ggtt_mm, &e, g_gtt_index);
+
+	rc = gtt_entry_p2m(vgt, &e, &m);
+	if (!rc) {
+		vgt_err("VM %d: failed to translate guest gtt entry\n", vgt->vm_id);
+		return false;
+	}
+
+	ggtt_set_shadow_entry(ggtt_mm, &m, g_gtt_index);
+out:
+	return true;
+}
+
+bool gtt_emulate_write(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes)
+{
+	struct vgt_device_info *info = &vgt->pdev->device_info;
+	int ret;
+	cycles_t t0, t1;
+	struct vgt_statistics *stat = &vgt->stat;
+
+	if (bytes != 4 && bytes != 8)
+		return false;
+
+	t0 = get_cycles();
+	stat->gtt_mmio_wcnt++;
+
+	off -= info->gtt_start_offset;
+
+	if (IS_PREBDW(vgt->pdev)) {
+		ret = gtt_mmio_write(vgt, off, p_data, 4);
+		if (ret && bytes == 8)
+			ret = gtt_mmio_write(vgt, off + 4, (char*)p_data + 4, 4);
+	} else {
+		ret = gtt_mmio_write(vgt, off, p_data, bytes);
+	}
+
+	t1 = get_cycles();
+	stat->gtt_mmio_wcycles += (u64) (t1 - t0);
+	return ret;
+}
+
+#define ring_id_to_pp_dclv(pdev, ring_id) \
+	(RB_TAIL(pdev, ring_id) - 0x30 + 0x220)
+
+#define ring_id_to_pp_dir_base(pdev, ring_id) \
+	(RB_TAIL(pdev, ring_id) - 0x30 + 0x228)
+
+static void gen7_ppgtt_mm_switch(struct vgt_mm *mm, int ring_id)
+{
+	struct pgt_device *pdev = mm->vgt->pdev;
+	u32 base = mm->pde_base_index << GTT_PAGE_SHIFT;
+
+	VGT_MMIO_WRITE(pdev, ring_id_to_pp_dclv(pdev, ring_id), 0xffffffff);
+	VGT_MMIO_WRITE(pdev, ring_id_to_pp_dir_base(pdev, ring_id), base);
+
+	return;
+}
+
+struct vgt_mm *gen7_find_ppgtt_mm(struct vgt_device *vgt,
+		u32 pde_base_index)
+{
+	struct list_head *pos;
+	struct vgt_mm *mm;
+
+	list_for_each(pos, &vgt->gtt.mm_list_head) {
+		mm = container_of(pos, struct vgt_mm, list);
+		if (mm->type != VGT_MM_PPGTT)
+			continue;
+
+		if (mm->pde_base_index == pde_base_index)
+			return mm;
+	}
+
+	return NULL;
+}
+
+bool gen7_ppgtt_mm_setup(struct vgt_device *vgt, int ring_id)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	vgt_state_ring_t *rb = &vgt->rb[ring_id];
+	struct vgt_mm *mm = rb->active_ppgtt_mm;
+	u32 pde_base_index = rb->sring_ppgtt_info.base >> GTT_PAGE_SHIFT;
+
+	if (!IS_PREBDW(pdev))
+		return false;
+
+	if (!rb->has_ppgtt_base_set
+		|| !rb->has_ppgtt_mode_enabled)
+		return true;
+
+	if (mm)
+		vgt_destroy_mm(mm);
+
+	mm = gen7_find_ppgtt_mm(vgt, pde_base_index);
+	if (mm) {
+		atomic_inc(&mm->refcount);
+	} else {
+		mm = vgt_create_mm(vgt, VGT_MM_PPGTT, rb->ppgtt_root_pointer_type,
+				NULL, rb->ppgtt_page_table_level, pde_base_index);
+		if (!mm)
+			return false;
+	}
+
+	rb->active_ppgtt_mm = mm;
+
+	set_bit(ring_id, &vgt->gtt.active_ppgtt_mm_bitmap);
+
+	if (current_render_owner(pdev) == vgt)
+		gen7_ppgtt_mm_switch(mm, ring_id);
+
+	return true;
+}
+
+void vgt_ppgtt_switch(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	struct vgt_vgtt_info *gtt = &vgt->gtt;
+	struct vgt_mm *mm;
+	int bit;
+
+	if (!IS_PREBDW(pdev))
+		return;
+
+	if (current_render_owner(pdev) != vgt)
+		return;
+
+	for_each_set_bit(bit, &gtt->active_ppgtt_mm_bitmap, vgt->pdev->max_engines) {
+		mm = vgt->rb[bit].active_ppgtt_mm;
+		gen7_ppgtt_mm_switch(mm, bit);
+	}
+}
+
+bool vgt_expand_shadow_page_mempool(struct vgt_device *vgt)
+{
+	mempool_t *mempool = vgt->gtt.mempool;
+	int new_min_nr;
+
+	if (mempool->curr_nr >= preallocated_shadow_pages / 3)
+		return true;
+
+	/*
+	 * Have to do this to let the pool expand directly.
+	 */
+	new_min_nr = preallocated_shadow_pages - 1;
+	if (mempool_resize(mempool, new_min_nr, GFP_KERNEL)) {
+		vgt_err("fail to resize the mempool.\n");
+		return false;
+	}
+
+	new_min_nr = preallocated_shadow_pages;
+	if (mempool_resize(mempool, new_min_nr, GFP_KERNEL)) {
+		vgt_err("fail to resize the mempool.\n");
+		return false;
+	}
+
+	return true;
+}
+
+static void *mempool_alloc_spt(gfp_t gfp_mask, void *pool_data)
+{
+	struct vgt_device *vgt = pool_data;
+	ppgtt_spt_t *spt;
+
+	spt = kzalloc(sizeof(*spt), gfp_mask);
+	if (!spt)
+		return NULL;
+
+	spt->shadow_page.page = alloc_page(gfp_mask);
+	if (!spt->shadow_page.page) {
+		kfree(spt);
+		return NULL;
+	}
+	spt->vgt = vgt;
+	return spt;
+}
+
+static void mempool_free_spt(void *element, void *pool_data)
+{
+	ppgtt_spt_t *spt = element;
+
+	__free_page(spt->shadow_page.page);
+	kfree(spt);
+}
+
+bool vgt_init_vgtt(struct vgt_device *vgt)
+{
+	struct vgt_vgtt_info *gtt = &vgt->gtt;
+	struct vgt_mm *ggtt_mm;
+
+	hash_init(gtt->guest_page_hash_table);
+	hash_init(gtt->shadow_page_hash_table);
+	hash_init(gtt->el_ctx_hash_table);
+
+	INIT_LIST_HEAD(&gtt->mm_list_head);
+
+	ggtt_mm = vgt_create_mm(vgt, VGT_MM_GGTT,
+			GTT_TYPE_GGTT_PTE, NULL, 1, 0);
+	if (!ggtt_mm) {
+		vgt_err("fail to create mm for ggtt.\n");
+		return false;
+	}
+
+	gtt->ggtt_mm = ggtt_mm;
+
+	if (!vgt->vm_id)
+		return true;
+
+	gtt->mempool = mempool_create(preallocated_shadow_pages,
+		mempool_alloc_spt, mempool_free_spt, vgt);
+	if (!gtt->mempool) {
+		vgt_err("fail to create mempool.\n");
+		return false;
+	}
+
+	return true;
+}
+
+void vgt_clean_vgtt(struct vgt_device *vgt)
+{
+	struct list_head *pos, *n;
+	struct vgt_mm *mm;
+
+	ppgtt_free_all_shadow_page(vgt);
+
+	if (vgt->gtt.mempool)
+		mempool_destroy(vgt->gtt.mempool);
+
+	list_for_each_safe(pos, n, &vgt->gtt.mm_list_head) {
+		mm = container_of(pos, struct vgt_mm, list);
+		vgt->pdev->gtt.mm_free_page_table(mm);
+		kfree(mm);
+	}
+
+	execlist_ctx_table_destroy(vgt);
+
+	return;
+}
+
+int ring_ppgtt_mode(struct vgt_device *vgt, int ring_id, u32 off, u32 mode)
+{
+	vgt_state_ring_t *rb = &vgt->rb[ring_id];
+	vgt_ring_ppgtt_t *v_info = &rb->vring_ppgtt_info;
+	vgt_ring_ppgtt_t *s_info = &rb->sring_ppgtt_info;
+
+	v_info->mode = mode;
+	s_info->mode = mode;
+
+	__sreg(vgt, off) = mode;
+	__vreg(vgt, off) = mode;
+
+	if (reg_hw_access(vgt, off)) {
+		vgt_dbg(VGT_DBG_MEM, "RING mode: offset 0x%x write 0x%x\n", off, s_info->mode);
+		VGT_MMIO_WRITE(vgt->pdev, off, s_info->mode);
+	}
+
+	/* sanity check */
+	if ((mode & _REGBIT_PPGTT_ENABLE) && (mode & (_REGBIT_PPGTT_ENABLE << 16))) {
+		/* XXX the order of mode enable for PPGTT and PPGTT dir base
+		 * setting is not strictly defined, e.g linux driver first
+		 * enables PPGTT bit in mode reg, then write PP dir base...
+		 */
+		vgt->rb[ring_id].has_ppgtt_mode_enabled = 1;
+		if (IS_PREBDW(vgt->pdev)) {
+			rb->ppgtt_page_table_level = 2;
+			rb->ppgtt_root_pointer_type = GTT_TYPE_PPGTT_PDE_ENTRY;
+		} else {
+			rb->ppgtt_page_table_level = 3;
+			rb->ppgtt_root_pointer_type = GTT_TYPE_PPGTT_ROOT_L3_ENTRY;
+
+			if ((mode & _REGBIT_PPGTT64_ENABLE)
+					&& (mode & (_REGBIT_PPGTT64_ENABLE << 16))) {
+				printk("PPGTT 64 bit VA enabling on ring %d\n", ring_id);
+				rb->ppgtt_page_table_level = 4;
+				rb->ppgtt_root_pointer_type = GTT_TYPE_PPGTT_ROOT_L4_ENTRY;
+			}
+		}
+
+		printk("PPGTT enabling on ring %d page table level %d type %d\n",
+				ring_id, rb->ppgtt_page_table_level,
+				rb->ppgtt_root_pointer_type);
+	}
+
+	return 0;
+}
+
+struct vgt_mm *gen8_find_ppgtt_mm(struct vgt_device *vgt,
+		int page_table_level, void *root_entry)
+{
+	struct list_head *pos;
+	struct vgt_mm *mm;
+	u64 *src, *dst;
+
+	list_for_each(pos, &vgt->gtt.mm_list_head) {
+		mm = container_of(pos, struct vgt_mm, list);
+		if (mm->type != VGT_MM_PPGTT)
+			continue;
+
+		if (mm->page_table_level != page_table_level)
+			continue;
+
+		src = root_entry;
+		dst = mm->virtual_page_table;
+
+		if (page_table_level == 3) {
+			if (src[0] == dst[0]
+					&& src[1] == dst[1]
+					&& src[2] == dst[2]
+					&& src[3] == dst[3])
+				return mm;
+		} else {
+			if (src[0] == dst[0])
+				return mm;
+		}
+	}
+
+	return NULL;
+}
+
+bool vgt_g2v_create_ppgtt_mm(struct vgt_device *vgt, int page_table_level)
+{
+	u64 *pdp = (u64 *)&__vreg64(vgt, vgt_info_off(pdp0_lo));
+	gtt_type_t root_entry_type = page_table_level == 4 ?
+		GTT_TYPE_PPGTT_ROOT_L4_ENTRY : GTT_TYPE_PPGTT_ROOT_L3_ENTRY;
+
+	struct vgt_mm *mm;
+
+	ASSERT(page_table_level == 4 || page_table_level == 3);
+
+	mm = gen8_find_ppgtt_mm(vgt, page_table_level, pdp);
+	if (mm) {
+		atomic_inc(&mm->refcount);
+	} else {
+		mm = vgt_create_mm(vgt, VGT_MM_PPGTT, root_entry_type,
+				pdp, page_table_level, 0);
+		if (!mm)
+			return false;
+	}
+
+	return true;
+}
+
+bool vgt_g2v_destroy_ppgtt_mm(struct vgt_device *vgt, int page_table_level)
+{
+	u64 *pdp = (u64 *)&__vreg64(vgt, vgt_info_off(pdp0_lo));
+	struct vgt_mm *mm;
+
+	ASSERT(page_table_level == 4 || page_table_level == 3);
+
+	mm = gen8_find_ppgtt_mm(vgt, page_table_level, pdp);
+	if (!mm) {
+		vgt_err("fail to find ppgtt instance.\n");
+		return false;
+	}
+
+	vgt_destroy_mm(mm);
+
+	return true;
+}
+
+#define get_pdp_from_context(status, pdp_udw, pdp_ldw, idx)	\
+do{								\
+	switch(idx) {						\
+		case 0:						\
+			pdp_udw = &status->pdp0_UDW;		\
+			pdp_ldw = &status->pdp0_LDW;		\
+			break;					\
+		case 1:						\
+			pdp_udw = &status->pdp1_UDW;		\
+			pdp_ldw = &status->pdp1_LDW;		\
+			break;					\
+		case 2:						\
+			pdp_udw = &status->pdp2_UDW;		\
+			pdp_ldw = &status->pdp2_LDW;		\
+			break;					\
+		case 3:						\
+			pdp_udw = &status->pdp3_UDW;		\
+			pdp_ldw = &status->pdp3_LDW;		\
+			break;					\
+		default:					\
+			BUG();					\
+	}							\
+}while(0);
+
+static inline bool ppgtt_get_rootp_from_ctx(
+			struct reg_state_ctx_header *state,
+			gtt_entry_t *e, int idx)
+{
+	struct mmio_pair *pdp_udw;
+	struct mmio_pair *pdp_ldw;
+
+	get_pdp_from_context(state, pdp_udw, pdp_ldw, idx);
+
+	e->val32[0] = pdp_ldw->val;
+	e->val32[1] = pdp_udw->val;
+
+	e->type= GTT_TYPE_INVALID;
+	e->pdev = NULL;
+	return true;
+}
+
+static bool ppgtt_set_rootp_to_ctx(
+			struct reg_state_ctx_header *state,
+			gtt_entry_t *e, int idx)
+{
+	struct mmio_pair *pdp_udw;
+	struct mmio_pair *pdp_ldw;
+
+	get_pdp_from_context(state, pdp_udw, pdp_ldw, idx);
+
+	pdp_udw->val = e->val32[1];
+	pdp_ldw->val = e->val32[0];
+
+	return true;
+}
+
+bool vgt_handle_guest_write_rootp_in_context(struct execlist_context *el_ctx, int idx)
+{
+	struct reg_state_ctx_header *guest_ctx_state;
+	struct reg_state_ctx_header *shadow_ctx_state;
+	gtt_entry_t guest_rootp;
+	gtt_entry_t shadow_rootp;
+	gtt_entry_t ctx_g_rootp;
+	struct vgt_mm *mm = el_ctx->ppgtt_mm;
+	bool rc = true;
+
+	guest_ctx_state = (struct reg_state_ctx_header *)
+					el_ctx->ctx_pages[1].guest_page.vaddr;
+	shadow_ctx_state = (struct reg_state_ctx_header *)
+					el_ctx->ctx_pages[1].shadow_page.vaddr;
+
+	ppgtt_get_guest_root_entry(mm, &guest_rootp, idx);
+	ppgtt_get_rootp_from_ctx(guest_ctx_state, &ctx_g_rootp, idx);
+
+	vgt_dbg(VGT_DBG_EXECLIST, "Guest root pointer in context is: 0x%llx\n",
+					ctx_g_rootp.val64);
+	vgt_dbg(VGT_DBG_EXECLIST, "Guest root pointer in root table is: 0x%llx\n",
+					guest_rootp.val64);
+
+	if (ctx_g_rootp.val64 == guest_rootp.val64)
+		return rc;
+
+	ctx_g_rootp.type = guest_rootp.type;
+	ctx_g_rootp.pdev = guest_rootp.pdev;
+
+	rc = ppgtt_handle_guest_write_root_pointer(mm, &ctx_g_rootp, idx);
+	if (!rc)
+		return rc;
+
+	ppgtt_set_guest_root_entry(mm, &ctx_g_rootp, idx);
+
+	ppgtt_get_shadow_root_entry(mm, &shadow_rootp, idx);
+	vgt_dbg(VGT_DBG_EXECLIST, "Shadow root pointer for guest rootp is: 0x%llx\n",
+					shadow_rootp.val64);
+	ppgtt_set_rootp_to_ctx(shadow_ctx_state, &shadow_rootp, idx);
+
+	return rc;
+}
diff --git a/drivers/gpu/drm/i915/vgt/handlers.c b/drivers/gpu/drm/i915/vgt/handlers.c
new file mode 100644
index 0000000..cd95e75
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/handlers.c
@@ -0,0 +1,3846 @@
+/*
+ * MMIO virtualization handlers
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/delay.h>
+#include <linux/acpi.h>
+
+#include "fb_decoder.h"
+#include "vgt.h"
+
+
+static bool vgt_error_handler(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	printk("vGT: reg (%x) needs special handler\n", offset);
+	ASSERT(0);
+	return true;
+}
+
+static bool vgt_not_allowed_mmio_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	vgt_err("VM(%d): MMIO reading of reg 0x%x is not allowed. "
+			"0 will be returned!\n", vgt->vm_id, offset);
+	*(vgt_reg_t *)p_data = 0;
+	return true;
+}
+
+static bool vgt_not_allowed_mmio_write(struct vgt_device *vgt,
+	unsigned int offset, void *p_data, unsigned int bytes)
+{
+	vgt_err("VM(%d): MMIO write of reg 0x%x with 0x%x (%d)bytes is not allowed. ",
+			vgt->vm_id, offset, *(vgt_reg_t *)p_data, bytes);
+	return true;
+}
+
+static bool gmbus_mmio_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	if (reg_hw_access(vgt, offset))
+		return default_mmio_read(vgt, offset, p_data, bytes);
+	else
+		return vgt_i2c_handle_gmbus_read(vgt, offset, p_data, bytes);
+}
+
+static bool gmbus_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	if (reg_hw_access(vgt, offset))
+		return default_mmio_write(vgt, offset, p_data, bytes);
+	else
+		return vgt_i2c_handle_gmbus_write(vgt, offset, p_data, bytes);
+}
+
+static bool fence_mmio_read(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes)
+{
+	int id;
+	ASSERT(bytes <= 8 && !(off & (bytes - 1)));
+	id = (off - _REG_FENCE_0_LOW) >> 3;
+
+	if (id >= vgt->fence_sz) {
+		printk("vGT(%d) , read fence register %x,"
+			" %x out of assignment %x.\n", vgt->vgt_id,
+			off, id, vgt->fence_sz);
+	}
+	memcpy (p_data, (char *)vgt->state.vReg + off, bytes);
+	return true;
+}
+
+static bool fence_mmio_write(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes)
+{
+	int id;
+	ASSERT(bytes <= 8 && !(off & (bytes - 1)));
+	id = (off - _REG_FENCE_0_LOW) >> 3;
+
+	if (id >= vgt->fence_sz) {
+		printk("vGT (%d) , write fence register %x,"
+			" %x out of assignment %x.\n", vgt->vgt_id,
+			off, id, vgt->fence_sz);
+	}
+	else {
+		memcpy ((char *)vgt->state.vReg + off, p_data, bytes);
+		memcpy ((char *)vgt->state.sReg + off, p_data, bytes);
+		/* TODO: Check address space */
+
+		/* FENCE registers are physically assigned, update! */
+		if (bytes < 8)
+			VGT_MMIO_WRITE(vgt->pdev, off + vgt->fence_base * 8,
+				__sreg(vgt, off));
+		else
+			VGT_MMIO_WRITE_BYTES(vgt->pdev, off + vgt->fence_base * 8,
+				__sreg64(vgt, off), 8);
+	}
+	return true;
+}
+
+static inline void set_vRC(struct vgt_device *vgt, int c)
+{
+	__vreg(vgt, _REG_GT_CORE_STATUS) = c;
+	__vreg(vgt, _REG_GT_THREAD_STATUS) = c;
+}
+
+static void set_vRC_to_C6(struct vgt_device *vgt)
+{
+	vgt_dbg(VGT_DBG_GENERIC, "Virtual Render C state set to C6\n");
+	set_vRC(vgt, 3);
+}
+
+static void set_vRC_to_C0(struct vgt_device *vgt)
+{
+	vgt_dbg(VGT_DBG_GENERIC, "Virtual Render C state set to C0\n");
+	set_vRC(vgt, 0);
+}
+
+static void v_force_wake_get(struct vgt_device *vgt)
+{
+	/* ignore hvm guest's forcewake req */
+	if (vgt->vm_id != 0)
+		return;
+
+	WARN(1, "Host driver should take care forcewake itself!\n");
+}
+
+static void v_force_wake_put(struct vgt_device *vgt)
+{
+	/* ignore hvm guest's forcewake req */
+	if (vgt->vm_id != 0)
+		return;
+
+	WARN(1, "Host driver should take care forcewake itself!\n");
+}
+
+static bool force_wake_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	uint32_t data;
+
+	data = (*(uint32_t*) p_data) & 1 ;
+
+	vgt_dbg(VGT_DBG_GENERIC, "VM%d write register FORCE_WAKE with %x\n", vgt->vm_id, data);
+
+	if (IS_HSW(vgt->pdev)) {
+		__vreg(vgt, _REG_FORCEWAKE_ACK_HSW) = data;
+	} else {
+		__vreg(vgt, _REG_FORCEWAKE_ACK) = data;
+	}
+
+	__vreg(vgt, _REG_FORCEWAKE) = data;
+	if (data == 1){
+		set_vRC_to_C0(vgt);
+		v_force_wake_get(vgt);
+	}
+	else{
+		set_vRC_to_C6(vgt);
+		v_force_wake_put(vgt);
+	}
+
+	return true;
+}
+
+static bool mul_force_wake_ack_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	*(u32 *)p_data = __vreg(vgt, offset);
+	return true;
+}
+
+static bool mul_force_wake_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	uint32_t data, mask, wake, old_wake, new_wake;
+
+	data = *(uint32_t*) p_data;
+
+	vgt_dbg(VGT_DBG_GENERIC, "VM%d write register FORCE_WAKE_MT with %x\n", vgt->vm_id, data);
+
+	if (!IS_BDWPLUS(vgt->pdev) && !(__vreg(vgt, _REG_ECOBUS) & ECOBUS_FORCEWAKE_MT_ENABLE)) {
+		__vreg(vgt, _REG_MUL_FORCEWAKE) = data;
+		return true;
+	}
+
+	/* bit 16-31: mask
+	   bit 0-15: force wake
+	   forcewake bit apply only if its mask bit is 1
+	 */
+	mask = data >> 16;
+	wake = data & 0xFFFF;
+	old_wake = __vreg(vgt, _REG_MUL_FORCEWAKE) & 0xFFFF;
+
+	new_wake = (old_wake & ~mask) + (wake & mask);
+	__vreg(vgt, _REG_MUL_FORCEWAKE) = (data & 0xFFFF0000) + new_wake;
+
+	if (IS_HSW(vgt->pdev) || IS_BDWPLUS(vgt->pdev)) {
+		__vreg(vgt, _REG_FORCEWAKE_ACK_HSW) = new_wake;
+	} else {
+		/* IVB */
+		__vreg(vgt, _REG_MUL_FORCEWAKE_ACK) = new_wake;
+	}
+
+	if (new_wake){
+		v_force_wake_get(vgt);
+		set_vRC_to_C0(vgt);
+	}else{
+		v_force_wake_put(vgt);
+		set_vRC_to_C6(vgt);
+	}
+
+	return true;
+}
+
+static bool rc_state_ctrl_1_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	uint32_t data;
+
+	data = *(uint32_t*)p_data;
+	printk("VM%d write register RC_STATE_CTRL_1 with 0x%x\n", vgt->vm_id, data);
+
+	if ( (data & _REGBIT_RC_HW_CTRL_ENABLE) && (data & (_REGBIT_RC_RC6_ENABLE
+					| _REGBIT_RC_DEEPEST_RC6_ENABLE	| _REGBIT_RC_DEEP_RC6_ENABLE) ) )
+		set_vRC_to_C6(vgt);
+	else
+		set_vRC_to_C0(vgt);
+
+	return default_mmio_write(vgt, offset, p_data, bytes);
+
+}
+
+static bool handle_device_reset(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes, unsigned long ring_bitmap)
+{
+	int bit;
+
+	vgt_info("VM %d is trying to reset device: %s.\n", vgt->vm_id,
+		ring_bitmap == 0xff ? "full reset" : "per-engine reset");
+
+	show_debug(vgt->pdev);
+
+	/* after this point, driver should re-initialize the device */
+	vgt->warn_untrack = 1;
+	set_bit(RESET_INPROGRESS, &vgt->reset_flags);
+
+	clear_bit(WAIT_RESET, &vgt->reset_flags);
+
+	vgt_reset_virtual_states(vgt, ring_bitmap);
+
+	if (ring_bitmap != 0xff && vgt->vm_id && vgt->enabled_rings_before_reset) {
+		vgt->enabled_rings_before_reset &= ~ring_bitmap;
+
+		for_each_set_bit(bit, &vgt->enabled_rings_before_reset,
+				sizeof(vgt->enabled_rings_before_reset)) {
+			vgt_info("VM %d: re-enable ring %d after per-engine reset.\n",
+					vgt->vm_id, bit);
+			vgt_enable_ring(vgt, bit);
+		}
+
+		vgt->enabled_rings_before_reset = 0;
+	}
+
+	vgt->last_reset_time = get_seconds();
+
+	if (vgt->vm_id == 0) {
+		if (device_is_reseting(vgt->pdev))
+			return default_mmio_write(vgt, offset, p_data, bytes);
+	} else {
+		if (current_render_owner(vgt->pdev) == vgt)
+			vgt_request_force_removal(vgt);
+	}
+
+	return true;
+}
+
+static bool gen6_gdrst_mmio_write(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	uint32_t data = 0;
+	unsigned long ring_bitmap = 0;
+
+	memcpy(&data, p_data, bytes);
+
+	if (data & _REGBIT_GEN6_GRDOM_FULL) {
+		vgt_info("VM %d request Full GPU Reset\n", vgt->vm_id);
+		ring_bitmap = 0xff;
+	}
+
+	if (data & _REGBIT_GEN6_GRDOM_RENDER) {
+		vgt_info("VM %d request GPU Render Reset\n", vgt->vm_id);
+		ring_bitmap |= (1 << RING_BUFFER_RCS);
+	}
+
+	if (data & _REGBIT_GEN6_GRDOM_MEDIA) {
+		vgt_info("VM %d request GPU Media Reset\n", vgt->vm_id);
+		ring_bitmap |= (1 << RING_BUFFER_VCS);
+	}
+
+	if (data & _REGBIT_GEN6_GRDOM_BLT) {
+		vgt_info("VM %d request GPU BLT Reset\n", vgt->vm_id);
+		ring_bitmap |= (1 << RING_BUFFER_BCS);
+	}
+
+	if (IS_HSW(vgt->pdev) && (data & (1 << 4))) {
+		vgt_info("VM %d request GPU VECS Reset\n", vgt->vm_id);
+		ring_bitmap |= (1 << RING_BUFFER_VECS);
+	}
+
+	return handle_device_reset(vgt, offset, p_data, bytes, ring_bitmap);
+}
+
+
+static bool gen6_gdrst_mmio_read(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	vgt_reg_t v;
+
+	*(u32 *)p_data = 0;
+
+	if (device_is_reseting(vgt->pdev) && vgt->vm_id == 0) {
+		v = VGT_MMIO_READ(vgt->pdev, offset);
+
+		memcpy(p_data, &v, bytes);
+
+		if (v) {
+			vgt_info("device is still reseting...\n");
+		} else {
+			vgt_info("device is idle.\n");
+
+			show_interrupt_regs(vgt->pdev, NULL);
+		}
+	}
+
+	return true;
+}
+
+static bool rrmr_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	uint32_t old_rrmr, new_rrmr, new_physical_rrmr;
+	struct pgt_device *pdev = vgt->pdev;
+
+	old_rrmr = __vreg(vgt, offset);
+	new_physical_rrmr = new_rrmr = *(u32 *)p_data;
+
+	__vreg(vgt, offset) = new_rrmr;
+
+	if (old_rrmr != new_rrmr) {
+		new_physical_rrmr = vgt_recalculate_mask_bits(pdev, offset);
+		VGT_MMIO_WRITE(pdev, offset, new_physical_rrmr);
+	}
+
+	vgt_info("RRMR: VM%d: old (%x), new (%x), new_physical (%x)\n",
+		vgt->vm_id, old_rrmr, new_rrmr, new_physical_rrmr);
+	return true;
+}
+
+static bool pch_pp_control_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	uint32_t data;
+	uint32_t reg;
+	union PCH_PP_CONTROL pp_control;
+	union PCH_PP_STAUTS pp_status;
+
+	reg = offset & ~(bytes - 1);
+	if (reg_hw_access(vgt, reg)){
+		return default_mmio_write(vgt, offset, p_data, bytes);
+	}
+
+	data = *(uint32_t*)p_data;
+
+	__vreg(vgt, _REG_PCH_PP_CONTROL) = data;
+
+	pp_control.data = data;
+	pp_status.data = __vreg(vgt, _REG_PCH_PP_STATUS);
+	if (pp_control.power_state_target == 1){
+		/* power on panel */
+		pp_status.panel_powere_on_statue = 1;
+		pp_status.power_sequence_progress = 0;
+		pp_status.power_cycle_delay_active = 0;
+	} else {
+		/* power down panel */
+		pp_status.panel_powere_on_statue = 0;
+		pp_status.power_sequence_progress = 0;
+		pp_status.power_cycle_delay_active = 0;
+	}
+	__vreg(vgt, _REG_PCH_PP_STATUS) = pp_status.data;
+
+	return true;
+}
+
+static bool transaconf_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	uint32_t reg;
+	union _TRANS_CONFIG config;
+
+	reg = offset & ~(bytes - 1);
+	if (reg_hw_access(vgt, reg)){
+		return default_mmio_write(vgt, offset, p_data, bytes);
+	}
+
+	config.data = *(uint32_t*)p_data;
+	/* transcoder state should synced with enable */
+	config.transcoder_state = config.transcoder_enable;
+
+	__vreg(vgt, reg) = config.data;
+
+	return true;
+}
+
+static bool shotplug_ctl_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	vgt_reg_t val = *(vgt_reg_t *)p_data;
+	vgt_reg_t sticky_mask = _REGBIT_DP_B_STATUS |
+				_REGBIT_DP_C_STATUS |
+				_REGBIT_DP_D_STATUS;
+
+	__vreg(vgt, offset) = (val & ~sticky_mask) |
+				(__vreg(vgt, offset) & sticky_mask);
+	__vreg(vgt, offset) &= ~(val & sticky_mask);
+
+	__sreg(vgt, offset) = val;
+
+	if (reg_hw_access(vgt, offset)) {
+		vgt_reg_t enable_mask = _REGBIT_DP_B_ENABLE |
+					_REGBIT_DP_C_ENABLE |
+					_REGBIT_DP_D_ENABLE;
+
+		if (~(val & enable_mask) & enable_mask) {
+			vgt_warn("vGT(%d): Is trying to disable HOTPLUG"
+			" with writing 0x%x to SHOTPLUG_CTL!\n",
+			vgt->vgt_id, val);
+		}
+		/* do not let display owner clear the status bits.
+		 * vgt driver will do so in interrupt handling.
+		 */
+		val &= ~sticky_mask;
+		VGT_MMIO_WRITE(vgt->pdev, offset, val);
+	}
+
+	return true;
+}
+static bool lcpll_ctl_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	unsigned int reg;
+	vgt_reg_t vreg_data;
+	bool rc;
+
+	reg = offset & ~(bytes - 1);
+	vreg_data = *(vgt_reg_t *)p_data;
+
+	rc = default_mmio_write(vgt, offset, &vreg_data, bytes);
+
+	if (!reg_hw_access(vgt, reg)) {
+		vreg_data = __vreg(vgt, offset);
+
+		if (vreg_data & _REGBIT_LCPLL_PLL_DISABLE)
+			vreg_data &= ~_REGBIT_LCPLL_PLL_LOCK;
+		else
+			vreg_data |= _REGBIT_LCPLL_PLL_LOCK;
+
+		if (vreg_data & _REGBIT_LCPLL_CD_SOURCE_FCLK)
+			vreg_data |= _REGBIT_LCPLL_CD_SOURCE_FCLK_DONE;
+		else
+			vreg_data &= ~_REGBIT_LCPLL_CD_SOURCE_FCLK_DONE;
+
+		__vreg(vgt, offset) = vreg_data;
+	}
+
+	return rc;
+
+}
+
+/* Pipe Frame Count */
+static bool pipe_frmcount_mmio_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	enum vgt_pipe pipe;
+
+	/* TODO
+	 *
+	 * If we can switch display owner, the frmcount should be handled specially
+	 * also so that hvm(including dom0) could have monotinic view of frmcount
+	 * during the owner ship switching. But Right now we do not allow the
+	 * display owner switch, so it is OK.
+	 */
+	if (is_current_display_owner(vgt))
+		return default_passthrough_mmio_read(vgt, offset,
+				p_data, bytes);
+
+	pipe = VGT_FRMCOUNTPIPE(offset);
+	ASSERT(pipe >= PIPE_A && pipe < I915_MAX_PIPES);
+
+	if (vgt_has_pipe_enabled(vgt, pipe))
+		vgt_update_frmcount(vgt, pipe);
+
+	*(vgt_reg_t *)p_data = __vreg(vgt, offset);
+
+	return true;
+}
+
+/* Pipe Display Scan Line*/
+static bool pipe_dsl_mmio_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	return default_passthrough_mmio_read(vgt, offset, p_data, bytes);
+}
+
+static bool dpy_reg_mmio_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	*(uint32_t*)p_data = (1<<17);
+
+	return true;
+}
+
+static bool dpy_reg_mmio_read_2(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	*(uint32_t*)p_data = 3;
+
+	return true;
+}
+
+static bool dpy_reg_mmio_read_3(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	*(uint32_t*)p_data = (0x2F << 16);
+
+	return true;
+}
+
+static int mmio_to_ring_id(unsigned int reg)
+{
+	int ring_id;
+
+	switch (reg) {
+	case _REG_RCS_PP_DIR_BASE_IVB:
+	case _REG_RCS_GFX_MODE_IVB:
+	case _REG_RCS_EXECLIST_SUBMITPORT:
+	case _REG_RCS_EXECLIST_STATUS:
+		ring_id = RING_BUFFER_RCS;
+		break;
+	case _REG_BCS_PP_DIR_BASE:
+	case _REG_BCS_BLT_MODE_IVB:
+	case _REG_BCS_EXECLIST_SUBMITPORT:
+	case _REG_BCS_EXECLIST_STATUS:
+		ring_id = RING_BUFFER_BCS;
+		break;
+	case _REG_VCS_PP_DIR_BASE:
+	case _REG_VCS_MFX_MODE_IVB:
+	case _REG_VCS_EXECLIST_SUBMITPORT:
+	case _REG_VCS_EXECLIST_STATUS:
+		ring_id = RING_BUFFER_VCS;
+		break;
+	case _REG_VECS_PP_DIR_BASE:
+	case _REG_VEBOX_MODE:
+	case _REG_VECS_EXECLIST_SUBMITPORT:
+	case _REG_VECS_EXECLIST_STATUS:
+		ring_id = RING_BUFFER_VECS;
+		break;
+	case _REG_VCS2_MFX_MODE_BDW:
+	case _REG_VCS2_EXECLIST_SUBMITPORT:
+	case _REG_VCS2_EXECLIST_STATUS:
+		ring_id = RING_BUFFER_VCS2;
+		break;
+	default:
+		ring_id = -1;
+		break;
+	}
+
+	ASSERT(ring_id != -1);
+	return ring_id;
+}
+
+static bool pp_dir_base_read(struct vgt_device *vgt, unsigned int off,
+			void *p_data, unsigned int bytes)
+{
+	int ring_id = mmio_to_ring_id(off);
+	vgt_ring_ppgtt_t *v_info = &vgt->rb[ring_id].vring_ppgtt_info;
+
+	*(u32 *)p_data = v_info->base;
+
+	vgt_dbg(VGT_DBG_RENDER, "<ring-%d>PP_DIR_BASE read: 0x%x\n", ring_id, v_info->base);
+	return true;
+}
+
+static bool pp_dir_base_write(struct vgt_device *vgt, unsigned int off,
+			void *p_data, unsigned int bytes)
+{
+	u32 base = *(u32 *)p_data;
+	int ring_id = mmio_to_ring_id(off);
+	vgt_ring_ppgtt_t *v_info = &vgt->rb[ring_id].vring_ppgtt_info;
+	vgt_ring_ppgtt_t *s_info = &vgt->rb[ring_id].sring_ppgtt_info;
+
+	vgt_dbg(VGT_DBG_RENDER, "<ring-%d> PP_DIR_BASE write: 0x%x\n", ring_id, base);
+
+	/* convert base which is in form of bit 31-16 in 64bytes cachelines,
+	 * it turns out to be ((((base >> 16) * 64) >> 2) << PAGE_SHIFT), which
+	 * is just base. */
+	v_info->base = base;
+	s_info->base = mmio_g2h_gmadr(vgt, off, v_info->base);
+	__vreg(vgt, off) = base;
+	__sreg(vgt, off) = s_info->base;
+
+	vgt->rb[ring_id].has_ppgtt_base_set = 1;
+
+	return gen7_ppgtt_mm_setup(vgt, ring_id);
+}
+
+static bool pp_dclv_read(struct vgt_device *vgt, unsigned int off,
+			void *p_data, unsigned int bytes)
+{
+	*(u32 *)p_data = 0xFFFFFFFF;
+	return true;
+}
+
+static bool pp_dclv_write(struct vgt_device *vgt, unsigned int off,
+			void *p_data, unsigned int bytes)
+{
+	u32 dclv = *(u32 *)p_data;
+	__vreg(vgt, off) = dclv;
+	__sreg(vgt, off) = dclv;
+
+	/* TODO: forward to pReg? */
+	vgt_dbg(VGT_DBG_RENDER, "PP_DCLV write: 0x%x\n", dclv);
+	return true;
+}
+
+/* TODO: there are other mode control bits in the registers */
+static bool ring_pp_mode_read(struct vgt_device *vgt, unsigned int off,
+			void *p_data, unsigned int bytes)
+{
+	int ring_id = mmio_to_ring_id(off);
+	vgt_ring_ppgtt_t *v_info = &vgt->rb[ring_id].vring_ppgtt_info;
+
+	*(u32 *)p_data = v_info->mode;
+	vgt_dbg(VGT_DBG_RENDER, "<ring-%d>GFX_MODE read: 0x%x\n", ring_id, v_info->mode);
+	return true;
+}
+
+static bool ring_pp_mode_write(struct vgt_device *vgt, unsigned int off,
+			void *p_data, unsigned int bytes)
+{
+	u32 mode = *(u32 *)p_data;
+	int ring_id = mmio_to_ring_id(off);
+
+	vgt_dbg(VGT_DBG_RENDER, "<ring-%d>GFX_MODE write: 0x%x\n", ring_id, mode);
+
+	if (ring_id == RING_BUFFER_VECS)
+		vgt->vebox_support = 1;
+
+	/* check for execlist */
+	if (GFX_MODE_BIT_SET_IN_MASK(mode, _REGBIT_EXECLIST_ENABLE)) {
+		bool ring_execlist = !!(mode & _REGBIT_EXECLIST_ENABLE);
+
+		/* execlist mode is enabled if anyone wants execlist mode*/
+		if (ring_execlist)
+			vgt->pdev->enable_execlist = true;
+
+		vgt->rb[ring_id].has_execlist_enabled = ring_execlist;
+		vgt_info("EXECLIST %s on ring %d.\n",
+			(ring_execlist ? "enabling" : "disabling"), ring_id);
+	}
+
+	ring_ppgtt_mode(vgt, ring_id, off, mode);
+	return true;
+}
+
+static bool dpy_trans_ddi_ctl_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	uint32_t new_data;
+	uint32_t old_data;
+	int i;
+
+	/* force to use panel fitting path for eDP for HSW,
+	 * it's no need for BDW as the panel fitter for pipe A
+	 * is now also in the always-on power well.
+	 */
+	if (IS_HSW(vgt->pdev) &&
+		enable_panel_fitting &&
+		is_current_display_owner(vgt) &&
+		offset == _REG_TRANS_DDI_FUNC_CTL_EDP &&
+		PIPE_A  == get_edp_input(*((uint32_t *)p_data))) {
+		*((uint32_t *)p_data) |= _REGBIT_TRANS_DDI_EDP_INPUT_A_ONOFF;
+		vgt_set_power_well(vgt, true);
+	}
+
+	old_data = __vreg(vgt, offset);
+	default_mmio_write(vgt, offset, p_data, bytes);
+
+	new_data = *((uint32_t *)p_data);
+
+	/* if it is to enable this pipe, then rebuild the mapping for this pipe*/
+	if (is_current_display_owner(vgt)) {
+		/*when dom0 change the physical pipe/port connection,
+		we need to rebuild pipe mapping for the vgt device.*/
+		for (i = 0; i < VGT_MAX_VMS; ++ i) {
+			struct vgt_device *vgt_virtual = vgt->pdev->device[i];
+			if (!vgt_virtual || vgt_virtual->vm_id == 0)
+				continue;
+			update_pipe_mapping(vgt_virtual, offset, new_data);
+		}
+
+	} else {
+		rebuild_pipe_mapping(vgt,  offset, new_data, old_data);
+	}
+
+	return true;
+}
+
+extern int vgt_decode_primary_plane_format(struct vgt_device *vgt,
+	int pipe, struct vgt_primary_plane_format *plane);
+extern int vgt_decode_cursor_plane_format(struct vgt_device *vgt,
+	int pipe, struct vgt_cursor_plane_format *plane);
+extern int vgt_decode_sprite_plane_format(struct vgt_device *vgt,
+	int pipe, struct vgt_sprite_plane_format *plane);
+
+vgt_reg_t vgt_surf_base_range_check (struct vgt_device *vgt,
+	enum vgt_pipe pipe, enum vgt_plane_type plane)
+{
+	uint32_t  reg = _REG_INVALID;
+	vgt_reg_t surf_base = 0;
+	uint32_t  range;
+	struct vgt_primary_plane_format primary_plane;
+	struct vgt_sprite_plane_format  sprite_plane;
+	struct vgt_cursor_plane_format  cursor_plane;
+
+	if (!vgt_has_pipe_enabled(vgt, pipe)) {
+		return 0;
+	}
+
+	switch (plane)
+	{
+	case PRIMARY_PLANE:
+		vgt_decode_primary_plane_format(vgt, pipe, &primary_plane);
+		if (primary_plane.enabled){
+			reg = VGT_DSPSURF(pipe);
+			range = primary_plane.stride * primary_plane.height;
+		}
+		break;
+
+	case SPRITE_PLANE:
+		vgt_decode_sprite_plane_format(vgt, pipe, &sprite_plane);
+		if (sprite_plane.enabled){
+			reg = VGT_SPRSURF(pipe);
+			range = sprite_plane.width* sprite_plane.height*
+					(sprite_plane.bpp / 8);
+		}
+		break;
+
+	case CURSOR_PLANE:
+		vgt_decode_cursor_plane_format(vgt, pipe, &cursor_plane);
+		if (cursor_plane.enabled) {
+			reg = VGT_CURBASE(pipe);
+			range = cursor_plane.width * cursor_plane.height *
+					(cursor_plane.bpp / 8);
+		}
+		break;
+
+	default:
+		break;
+	}
+
+	if (reg != _REG_INVALID){
+		reg_aux_addr_size(vgt->pdev, reg) = range;
+		surf_base = mmio_g2h_gmadr (vgt, reg, __vreg(vgt, reg));
+	}
+
+	return surf_base;
+}
+
+static bool pipe_conf_mmio_write(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	bool rc, orig_pipe_enabled, curr_pipe_enabled;
+	unsigned int reg;
+	enum vgt_pipe pipe;
+	enum vgt_plane_type plane;
+	uint32_t wr_data;
+
+	reg = offset & ~(bytes - 1);
+
+	wr_data = *((uint32_t *)p_data);
+	/* vreg status will be updated when when read hardware status */
+	if (!reg_hw_access(vgt, reg)) {
+		if (wr_data & _REGBIT_PIPE_ENABLE)
+			wr_data |= _REGBIT_PIPE_STAT_ENABLED;
+		else if (!(wr_data & _REGBIT_PIPE_ENABLE))
+			wr_data &= ~_REGBIT_PIPE_STAT_ENABLED;
+	}
+
+	if (offset == _REG_PIPE_EDP_CONF) {
+		vgt_reg_t ctl_edp;
+		ctl_edp = __vreg(vgt, _REG_TRANS_DDI_FUNC_CTL_EDP);
+		pipe = get_edp_input(ctl_edp);
+	} else {
+		pipe = VGT_PIPECONFPIPE(offset);
+	}
+	orig_pipe_enabled = vgt_has_pipe_enabled(vgt, pipe);
+	rc = default_mmio_write(vgt, offset, &wr_data, bytes);
+	curr_pipe_enabled = vgt_has_pipe_enabled(vgt, pipe);
+
+	if (offset == _REG_PIPE_EDP_CONF) {
+		if (!curr_pipe_enabled)
+			pipe = I915_MAX_PIPES;
+	}
+
+	if (orig_pipe_enabled && !curr_pipe_enabled) {
+		if (pipe != I915_MAX_PIPES) {
+			vgt_update_frmcount(vgt, pipe);
+		} else {
+			vgt_update_frmcount(vgt, PIPE_A);
+			vgt_update_frmcount(vgt, PIPE_B);
+			vgt_update_frmcount(vgt, PIPE_C);
+		}
+	}
+
+	if (!orig_pipe_enabled && curr_pipe_enabled) {
+		if (pipe == I915_MAX_PIPES) {
+			vgt_err("VM(%d): eDP pipe does not have corresponding"
+				"mapped pipe while it is enabled!\n", vgt->vm_id);
+			return false;
+		}
+		vgt_calculate_frmcount_delta(vgt, pipe);
+
+		for (plane = PRIMARY_PLANE; plane < MAX_PLANE; plane++) {
+			vgt_surf_base_range_check(vgt, pipe, plane);
+		}
+	}
+
+	if (rc)
+		rc = vgt_manage_emul_dpy_events(vgt->pdev);
+
+	return rc;
+}
+
+static bool ddi_buf_ctl_mmio_write(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	bool rc;
+	vgt_reg_t reg_val;
+
+	reg_val = *(vgt_reg_t *)p_data;
+
+	// set the fully virtualized RO bit with its original value
+	reg_val = (reg_val & ~_DDI_BUFCTL_DETECT_MASK)
+		| (__vreg(vgt, offset) & _DDI_BUFCTL_DETECT_MASK);
+
+	rc = default_mmio_write(vgt, offset, &reg_val, bytes);
+
+	//update idle status when enable/disable DDI buf
+	if (!reg_hw_access(vgt, offset)) {
+		reg_val = __vreg(vgt, offset);
+
+		if (reg_val & _REGBIT_DDI_BUF_ENABLE)
+			reg_val &= ~_REGBIT_DDI_BUF_IS_IDLE;
+		else
+			reg_val |= _REGBIT_DDI_BUF_IS_IDLE;
+
+		__vreg(vgt, offset) = reg_val;
+	}
+
+	// clear the auto_training done bit
+	if ((offset == _REG_DDI_BUF_CTL_E) &&
+		(!(reg_val & _REGBIT_DDI_BUF_ENABLE))) {
+		if (!reg_hw_access(vgt, offset)) {
+			__vreg(vgt, _REG_DP_TP_STATUS_E) &=
+				~_REGBIT_DP_TP_STATUS_AUTOTRAIN_DONE;
+		}
+	}
+
+	return rc;
+}
+
+static bool fdi_rx_iir_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	unsigned int reg;
+	vgt_reg_t wr_data, old_iir;
+	bool rc;
+
+	reg = offset & ~(bytes -1);
+
+	wr_data = *(vgt_reg_t *)p_data;
+	old_iir = __vreg(vgt, reg);
+
+	rc = default_mmio_write(vgt, offset, p_data, bytes);
+
+	/* FIXME: sreg will be updated only when reading hardware status happened,
+	 * so when dumping sreg space, the "hardware status" related bits may not
+	 * be trusted */
+	if (!reg_hw_access(vgt, reg))
+		__vreg(vgt, reg) = old_iir ^ wr_data;
+
+	return rc;
+}
+
+
+
+#define FDI_LINK_TRAIN_PATTERN_1	0
+#define FDI_LINK_TRAIN_PATTERN_2	1
+
+static bool fdi_auto_training_started(struct vgt_device *vgt)
+{
+	bool rc = false;
+	vgt_reg_t ddi_buf_ctl = __vreg(vgt, _REG_DDI_BUF_CTL_E);
+	vgt_reg_t rx_ctl = __vreg(vgt, _REG_FDI_RXA_CTL);
+	vgt_reg_t tx_ctl = __vreg(vgt, _REG_DP_TP_CTL_E);
+
+	if ((ddi_buf_ctl & _REGBIT_DDI_BUF_ENABLE) &&
+		(rx_ctl & _REGBIT_FDI_RX_ENABLE) &&
+		(rx_ctl & _REGBIT_FDI_RX_FDI_AUTO_TRAIN_ENABLE) &&
+		(tx_ctl & _REGBIT_DP_TP_ENABLE) &&
+		(tx_ctl & _REGBIT_DP_TP_FDI_AUTO_TRAIN_ENABLE)) {
+			rc = true;
+	}
+
+	return rc;
+}
+
+/* FIXME: this function is highly platform-dependent (SNB + CPT) */
+static bool check_fdi_rx_train_status(struct vgt_device *vgt,
+		enum vgt_pipe pipe, unsigned int train_pattern)
+{
+	unsigned int fdi_rx_imr, fdi_tx_ctl, fdi_rx_ctl;
+	unsigned int fdi_rx_check_bits, fdi_tx_check_bits;
+	unsigned int fdi_rx_train_bits, fdi_tx_train_bits;
+	unsigned int fdi_iir_check_bits;
+
+	fdi_rx_imr = VGT_FDI_RX_IMR(pipe);
+	fdi_tx_ctl = VGT_FDI_TX_CTL(pipe);
+	fdi_rx_ctl = VGT_FDI_RX_CTL(pipe);
+
+	if (train_pattern == FDI_LINK_TRAIN_PATTERN_1) {
+		fdi_rx_train_bits =_REGBIT_FDI_LINK_TRAIN_PATTERN_1_CPT;
+		fdi_tx_train_bits = _REGBIT_FDI_LINK_TRAIN_PATTERN_1;
+		fdi_iir_check_bits = _REGBIT_FDI_RX_BIT_LOCK;
+	} else if (train_pattern == FDI_LINK_TRAIN_PATTERN_2) {
+		fdi_rx_train_bits = _REGBIT_FDI_LINK_TRAIN_PATTERN_2_CPT;
+		fdi_tx_train_bits = _REGBIT_FDI_LINK_TRAIN_PATTERN_2;
+		fdi_iir_check_bits = _REGBIT_FDI_RX_SYMBOL_LOCK;
+	} else {
+		BUG();
+	}
+
+	fdi_rx_check_bits = _REGBIT_FDI_RX_ENABLE | fdi_rx_train_bits;
+	fdi_tx_check_bits = _REGBIT_FDI_TX_ENABLE | fdi_tx_train_bits;
+
+	/* If imr bit not been masked */
+	if (((__vreg(vgt, fdi_rx_imr) & fdi_iir_check_bits) == 0)
+		&& ((__vreg(vgt, fdi_tx_ctl)
+			& fdi_tx_check_bits) == fdi_tx_check_bits)
+		&& ((__vreg(vgt, fdi_rx_ctl)
+			& fdi_rx_check_bits) == fdi_rx_check_bits))
+		return true;
+	else
+		return false;
+}
+
+static bool update_fdi_rx_iir_status(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	enum vgt_pipe pipe;
+	unsigned int reg, fdi_rx_iir;
+	bool rc;
+
+	reg = offset & ~(bytes - 1);
+
+	switch (offset) {
+		case _REG_FDI_RXA_CTL:
+		case _REG_FDI_TXA_CTL:
+		case _REG_FDI_RXA_IMR:
+			pipe = PIPE_A;
+			break;
+
+		case _REG_FDI_RXB_CTL:
+		case _REG_FDI_TXB_CTL:
+		case _REG_FDI_RXB_IMR:
+			pipe = PIPE_B;
+			break;
+
+		case _REG_FDI_RXC_CTL:
+		case _REG_FDI_TXC_CTL:
+		case _REG_FDI_RXC_IMR:
+			pipe = PIPE_C;
+			break;
+
+		default:
+			BUG();
+	}
+
+	fdi_rx_iir = VGT_FDI_RX_IIR(pipe);
+
+	rc = default_mmio_write(vgt, offset, p_data, bytes);
+	if (!reg_hw_access(vgt, reg)) {
+		if (check_fdi_rx_train_status(vgt, pipe, FDI_LINK_TRAIN_PATTERN_1))
+			__vreg(vgt, fdi_rx_iir) |= _REGBIT_FDI_RX_BIT_LOCK;
+		if (check_fdi_rx_train_status(vgt, pipe, FDI_LINK_TRAIN_PATTERN_2))
+			__vreg(vgt, fdi_rx_iir) |= _REGBIT_FDI_RX_SYMBOL_LOCK;
+		if (offset == _REG_FDI_RXA_CTL) {
+			if (fdi_auto_training_started(vgt))
+				__vreg(vgt, _REG_DP_TP_STATUS_E) |=
+					_REGBIT_DP_TP_STATUS_AUTOTRAIN_DONE;
+		}
+	}
+	return rc;
+}
+
+#define DP_TP_CTL_10_8_MASK	0x00000700
+#define DP_TP_CTL_8_SHIFT	0x8
+#define DP_TP_STATUS_25_SHIFT	25
+
+static bool dp_tp_ctl_mmio_write(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	enum vgt_port port;
+	unsigned int dp_tp_status_reg, val;
+	vgt_reg_t ctl_val;
+	bool rc;
+
+	rc = default_mmio_write(vgt, offset, p_data, bytes);
+
+	if (!reg_hw_access(vgt, offset)) {
+		port = VGT_DP_TP_CTL_PORT(offset);
+		ctl_val = __vreg(vgt, offset);
+		val = (ctl_val & DP_TP_CTL_10_8_MASK) >> DP_TP_CTL_8_SHIFT;
+
+		if (val == 0x2) {
+			dp_tp_status_reg = VGT_DP_TP_STATUS(port);
+			__vreg(vgt, dp_tp_status_reg) |= (1 << DP_TP_STATUS_25_SHIFT);
+			__sreg(vgt, dp_tp_status_reg) = __vreg(vgt, dp_tp_status_reg);
+		}
+	}
+
+	return rc;
+}
+
+#define BIT_27		27
+#define BIT_26		26
+#define BIT_24		24
+
+static bool dp_tp_status_mmio_write(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	bool rc = true;
+	vgt_reg_t reg_val;
+	vgt_reg_t sticky_mask;
+
+	reg_val = *((vgt_reg_t *)p_data);
+	sticky_mask = (1 << BIT_27) | (1 << BIT_26) | (1 << BIT_24);
+
+	__vreg(vgt, offset) = (reg_val & ~sticky_mask) |
+				(__vreg(vgt, offset) & sticky_mask);
+	__vreg(vgt, offset) &= ~(reg_val & sticky_mask);
+
+	__sreg(vgt, offset) = reg_val;
+
+	if (reg_hw_access(vgt, offset)) {
+		VGT_MMIO_WRITE(vgt->pdev, offset, reg_val);
+	}
+
+	return rc;
+}
+
+static bool pch_adpa_mmio_write(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	vgt_reg_t old, new;
+
+	new = *(vgt_reg_t *)p_data;
+	old = __vreg(vgt, offset);
+
+	/* Clear the bits of 'force hotplug trigger' and status because they
+	 * will be fully virtualized. Other bits will be written to hardware.
+	 */
+	default_mmio_write(vgt, offset, p_data, bytes);
+
+	if (reg_hw_access(vgt, offset))
+		return true;
+
+	if (new & _REGBIT_ADPA_CRT_HOTPLUG_FORCE_TRIGGER) {
+
+		if ((new & _REGBIT_ADPA_DAC_ENABLE)) {
+			vgt_warn("HOTPLUG_FORCE_TRIGGER is set while VGA is enabled!\n");
+		}
+
+		/* emulate the status based on monitor connection information */
+		new &= ~_REGBIT_ADPA_CRT_HOTPLUG_FORCE_TRIGGER;
+
+		if (dpy_has_monitor_on_port(vgt, PORT_E))
+			new |= _REGBIT_ADPA_CRT_HOTPLUG_MONITOR_MASK;
+		else
+			new &= ~_REGBIT_ADPA_CRT_HOTPLUG_MONITOR_MASK;
+	} else {
+		/* ignore the status bits in new value
+		 * since they are read only actually
+		 */
+		new = (new & ~_REGBIT_ADPA_CRT_HOTPLUG_MONITOR_MASK) |
+			(old & _REGBIT_ADPA_CRT_HOTPLUG_MONITOR_MASK);
+	}
+
+	__vreg(vgt, offset) = __sreg(vgt, offset) = new;
+
+	return true;
+}
+
+static bool dp_ctl_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	vgt_reg_t vreg_data;
+	bool rc;
+
+	vreg_data = *(vgt_reg_t *)p_data;
+
+	// Keep the fully virtualized RO bit with its original value
+	vreg_data = (vreg_data & ~_REGBIT_DP_PORT_DETECTED)
+			| (__vreg(vgt, offset) & _REGBIT_DP_PORT_DETECTED);
+
+	rc = default_mmio_write(vgt, offset, p_data, bytes);
+
+	return rc;
+}
+
+static bool hdmi_ctl_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	vgt_reg_t vreg_data;
+	bool rc;
+
+	vreg_data = *(vgt_reg_t *)p_data;
+
+	// Keep the fully virtualized RO bit with its original value
+	vreg_data = (vreg_data & ~_REGBIT_HDMI_PORT_DETECTED)
+			| (__vreg(vgt, offset) & _REGBIT_HDMI_PORT_DETECTED);
+
+	rc = default_mmio_write(vgt, offset, p_data, bytes);
+
+	return rc;
+}
+
+bool vgt_map_plane_reg(struct vgt_device *vgt, unsigned int reg, unsigned int *p_real_reg)
+{
+	enum vgt_pipe virtual_pipe;
+	enum vgt_pipe real_pipe ;
+
+	switch (reg)
+	{
+	case _REG_CURABASE:
+	case _REG_CURACNTR:
+	case _REG_CURAPOS:
+	case _REG_DSPACNTR:
+	case _REG_DSPASURF:
+	case _REG_DSPASURFLIVE:
+	case _REG_DSPALINOFF:
+	case _REG_DSPASTRIDE:
+	case _REG_DSPAPOS:
+	case _REG_DSPASIZE:
+	case _REG_DSPATILEOFF:
+	case _REG_SPRASURF:
+	case _REG_SPRA_CTL:
+	case _REG_PIPEASRC:
+		real_pipe = vgt->pipe_mapping[0];
+		virtual_pipe = PIPE_A;
+		break;
+
+	case _REG_CURBBASE_SNB:
+	case _REG_CURBCNTR_SNB:
+	case _REG_CURBPOS_SNB:
+	case _REG_CURBBASE:
+	case _REG_CURBCNTR:
+	case _REG_CURBPOS:
+	case _REG_DSPBCNTR:
+	case _REG_DSPBSURF:
+	case _REG_DSPBSURFLIVE:
+	case _REG_DSPBLINOFF:
+	case _REG_DSPBSTRIDE:
+	case _REG_DSPBPOS:
+	case _REG_DSPBSIZE:
+	case _REG_DSPBTILEOFF:
+	case _REG_SPRBSURF:
+	case _REG_SPRB_CTL:
+	case _REG_PIPEBSRC:
+		real_pipe = vgt->pipe_mapping[1];
+		virtual_pipe = PIPE_B;
+		break;
+
+	case _REG_CURCBASE:
+	case _REG_CURCCNTR:
+	case _REG_CURCPOS:
+	case _REG_DSPCCNTR:
+	case _REG_DSPCSURF:
+	case _REG_DSPCSURFLIVE:
+	case _REG_DSPCLINOFF:
+	case _REG_DSPCSTRIDE:
+	case _REG_DSPCPOS:
+	case _REG_DSPCSIZE:
+	case _REG_DSPCTILEOFF:
+	case _REG_SPRCSURF:
+	case _REG_SPRC_CTL:
+	case _REG_PIPECSRC:
+		real_pipe = vgt->pipe_mapping[2];
+		virtual_pipe = PIPE_C;
+		break;
+
+	default:
+		vgt_warn("try to map mmio that is not plane related! reg = %x\n", reg);
+		ASSERT(0);
+	}
+
+	if(real_pipe == I915_MAX_PIPES)
+	{
+		vgt_dbg(VGT_DBG_DPY, "the mapping for pipe %d is not ready or created!\n", virtual_pipe);
+		return false;
+	}
+
+	*p_real_reg = reg + 0x1000 * real_pipe - 0x1000 * virtual_pipe;
+
+	return true;
+
+}
+
+static bool dpy_plane_mmio_read(struct vgt_device *vgt, unsigned int offset,
+			void *p_data, unsigned int bytes)
+{
+
+	*(vgt_reg_t *)p_data = __vreg(vgt, offset);
+
+	return true;
+}
+
+static bool dpy_plane_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	unsigned int real_offset;
+
+	memcpy ((char *)vgt->state.vReg + offset, p_data, bytes);
+	memcpy ((char *)vgt->state.sReg + offset, p_data, bytes);
+	if (current_foreground_vm(vgt->pdev) == vgt &&
+		vgt_map_plane_reg(vgt, offset, &real_offset)) {
+		VGT_MMIO_WRITE(vgt->pdev, real_offset, __sreg(vgt, offset));
+	}
+	return true;
+}
+
+static bool dpy_plane_ctl_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	enum vgt_pipe pipe = PIPE_A;
+	enum vgt_pipe p_pipe = I915_MAX_PIPES;
+	enum vgt_pipe v_pipe = I915_MAX_PIPES;
+	vgt_reg_t new_plane_ctl;
+	bool enable_plane = false;
+	struct vgt_device *foreground_vgt;
+	int i;
+
+	new_plane_ctl = *(vgt_reg_t *)p_data;
+	pipe = VGT_DSPCNTRPIPE(offset);
+	if ( (_PRI_PLANE_ENABLE & new_plane_ctl) &&  (_PRI_PLANE_ENABLE & __vreg(vgt, offset)) == 0) {
+		enable_plane = true;
+	}
+
+	dpy_plane_mmio_write(vgt, offset, p_data, bytes);
+	if (enable_plane) {
+		if (current_foreground_vm(vgt->pdev) == vgt) {
+			set_panel_fitting(vgt, pipe);
+		} else if (is_current_display_owner(vgt)) {
+			p_pipe = vgt->pipe_mapping[pipe];
+			foreground_vgt = current_foreground_vm(vgt->pdev);
+			for (i = 0; i < I915_MAX_PIPES; i++) {
+				if (foreground_vgt->pipe_mapping[i] == p_pipe) {
+					v_pipe = i;
+					break;
+				}
+			}
+			if (p_pipe != I915_MAX_PIPES && v_pipe != I915_MAX_PIPES) {
+				set_panel_fitting(foreground_vgt, v_pipe);
+			}
+		}
+		vgt_surf_base_range_check(vgt, pipe, PRIMARY_PLANE);
+	}
+
+	return true;
+}
+
+
+static bool pri_surf_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	struct fb_notify_msg msg;
+	enum vgt_pipe pipe = VGT_DSPSURFPIPE(offset);
+	unsigned int real_offset;
+	vgt_reg_t ret_val;
+
+	__vreg(vgt, offset) = *(vgt_reg_t*)p_data;
+	ret_val = vgt_surf_base_range_check(vgt, pipe, PRIMARY_PLANE);
+	__sreg(vgt, offset) = ret_val ? ret_val : __vreg(vgt, offset);
+
+	__vreg(vgt, VGT_PIPE_FLIPCOUNT(pipe))++;
+
+	if (current_foreground_vm(vgt->pdev) == vgt &&
+		vgt_map_plane_reg(vgt, offset, &real_offset)) {
+		VGT_MMIO_WRITE(vgt->pdev, real_offset, __sreg(vgt, offset));
+	}
+
+	msg.vm_id = vgt->vm_id;
+	msg.plane_id = PRIMARY_PLANE;
+	msg.pipe_id = VGT_DSPSURFPIPE(offset);
+	vgt_fb_notifier_call_chain(FB_DISPLAY_FLIP, &msg);
+
+	vgt_inject_flip_done(vgt, VGT_DSPSURFPIPE(offset));
+
+	return true;
+}
+
+static bool sprite_plane_ctl_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	enum vgt_pipe pipe = VGT_SPRCNTRPIPE(offset);
+
+	dpy_plane_mmio_write(vgt, offset, p_data, bytes);
+	vgt_surf_base_range_check(vgt, pipe, SPRITE_PLANE);
+
+	return true;
+}
+
+static bool spr_surf_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	struct fb_notify_msg msg;
+	enum vgt_pipe pipe = VGT_SPRSURFPIPE(offset);
+	unsigned int real_offset;
+	vgt_reg_t ret_val;
+
+	__vreg(vgt, offset) = *(vgt_reg_t*)p_data;
+	ret_val = vgt_surf_base_range_check(vgt, pipe, SPRITE_PLANE);
+	__sreg(vgt, offset) = ret_val ? ret_val : __vreg(vgt, offset);
+
+	if (current_foreground_vm(vgt->pdev) == vgt &&
+		vgt_map_plane_reg(vgt, offset, &real_offset)) {
+		VGT_MMIO_WRITE(vgt->pdev, real_offset, __sreg(vgt, offset));
+	}
+
+	msg.vm_id = vgt->vm_id;
+	msg.plane_id = SPRITE_PLANE;
+	msg.pipe_id = VGT_SPRSURFPIPE(offset);
+	vgt_fb_notifier_call_chain(FB_DISPLAY_FLIP, &msg);
+
+	return true;
+}
+
+static bool cur_plane_ctl_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	enum vgt_pipe pipe = VGT_CURCNTRPIPE(offset);
+
+	dpy_plane_mmio_write(vgt,offset, p_data, bytes);
+	vgt_surf_base_range_check(vgt, pipe, CURSOR_PLANE);
+
+	return true;
+}
+
+static bool cur_surf_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	enum vgt_pipe pipe = VGT_CURSURFPIPE(offset);
+	unsigned int real_offset;
+	vgt_reg_t ret_val;
+
+	__vreg(vgt, offset) = *(vgt_reg_t*)p_data;
+	ret_val = vgt_surf_base_range_check(vgt, pipe, CURSOR_PLANE);
+	__sreg(vgt, offset) = ret_val ? ret_val : __vreg(vgt, offset);
+
+	if (current_foreground_vm(vgt->pdev) == vgt &&
+		vgt_map_plane_reg(vgt, offset, &real_offset)) {
+		VGT_MMIO_WRITE(vgt->pdev, real_offset, __sreg(vgt, offset));
+	}
+
+	return true;
+}
+
+static bool dpy_modeset_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	bool rc;
+
+	rc = default_mmio_write(vgt, offset, p_data, bytes);
+
+	if (!reg_hw_access(vgt, offset) &&
+		(*(vgt_reg_t *)p_data != __vreg(vgt, offset))) {
+
+		vgt_warn("modeset mmio[0x%x] change value from 0x%x to 0x%x\n"
+			 "\twhich is not supported. MMIO write is ignored!\n",
+						offset,
+						__vreg(vgt, offset),
+						*(vgt_reg_t *)p_data);
+	}
+
+	return true;
+}
+
+static bool south_chicken2_write(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	if (!default_mmio_write(vgt, offset, p_data, bytes))
+		return false;
+
+	if (!reg_hw_access(vgt, offset)) {
+		if (__vreg(vgt, offset) & _REGBIT_MPHY_IOSFSB_RESET_CTL)
+			__vreg(vgt, offset) |= _REGBIT_FDI_MPHY_IOSFSB_RESET_STATUS;
+		else
+			__vreg(vgt, offset) &= ~_REGBIT_FDI_MPHY_IOSFSB_RESET_STATUS;
+
+		__sreg(vgt, offset) = __vreg(vgt, offset);
+	}
+
+	return true;
+}
+
+static bool surflive_mmio_read(struct vgt_device *vgt, unsigned int offset,
+			void *p_data, unsigned int bytes, enum vgt_plane_type plane)
+{
+	vgt_reg_t surflive_val;
+	unsigned int surf_reg = 0;
+	enum vgt_pipe pipe;
+
+	if (plane == PRIMARY_PLANE) {
+		pipe = VGT_DSPSURFLIVEPIPE(offset);
+		surf_reg = VGT_DSPSURF(pipe);
+	} else if (plane == CURSOR_PLANE) {
+		if (offset == _REG_CURBSURFLIVE_SNB) {
+			surf_reg = _REG_CURBBASE_SNB;
+		} else {
+			pipe = VGT_CURSURFPIPE(offset);
+			surf_reg = VGT_CURSURF(pipe);
+		}
+	} else if (plane == SPRITE_PLANE) {
+		pipe = VGT_SPRSURFPIPE(offset);
+		surf_reg = VGT_SPRSURF(pipe);
+	} else {
+		BUG();
+	}
+
+	surflive_val = __vreg(vgt, surf_reg);
+	__vreg(vgt, offset) = __sreg(vgt, offset) = surflive_val;
+	*(vgt_reg_t *)p_data = surflive_val;
+
+	return true;
+}
+
+static bool pri_surflive_mmio_read(struct vgt_device *vgt, unsigned int offset,
+			void *p_data, unsigned int bytes)
+{
+	return surflive_mmio_read(vgt, offset, p_data, bytes, PRIMARY_PLANE);
+}
+
+static bool cur_surflive_mmio_read(struct vgt_device *vgt, unsigned int offset,
+			void *p_data, unsigned int bytes)
+{
+	return surflive_mmio_read(vgt, offset, p_data, bytes, CURSOR_PLANE);
+}
+
+static bool spr_surflive_mmio_read(struct vgt_device *vgt, unsigned int offset,
+			void *p_data, unsigned int bytes)
+{
+	return surflive_mmio_read(vgt, offset, p_data, bytes, SPRITE_PLANE);
+}
+
+static bool surflive_mmio_write (struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	/* surflive is readonly registers. ignore the write from driver*/
+	return true;
+}
+
+static void dp_aux_ch_trigger_interrupt_on_done(struct vgt_device *vgt, vgt_reg_t value,
+	 unsigned int reg)
+{
+	enum vgt_event_type event = EVENT_MAX;
+
+	if (reg == _REG_DPA_AUX_CH_CTL) {
+		event = AUX_CHANNEL_A;
+	} else if (reg == _REG_PCH_DPB_AUX_CH_CTL) {
+		event = AUX_CHENNEL_B;
+	} else if (reg == _REG_PCH_DPC_AUX_CH_CTL) {
+		event = AUX_CHENNEL_C;
+	} else if (reg == _REG_PCH_DPD_AUX_CH_CTL) {
+		event = AUX_CHENNEL_D;
+	}
+
+	if (event != EVENT_MAX && (_REGBIT_DP_AUX_CH_CTL_INTERRUPT & value)) {
+		vgt_trigger_virtual_event(vgt, event);
+	}
+}
+
+static void dp_aux_ch_ctl_trans_done(struct vgt_device *vgt, vgt_reg_t value,
+	 unsigned int reg, int len, bool data_valid)
+{
+	/* mark transaction done */
+	value |= _REGBIT_DP_AUX_CH_CTL_DONE;
+	value &= ~_REGBIT_DP_AUX_CH_CTL_SEND_BUSY;
+	value &= ~_REGBIT_DP_AUX_CH_CTL_RECV_ERR;
+
+	if (data_valid) {
+		value &= ~_REGBIT_DP_AUX_CH_CTL_TIME_OUT_ERR;
+	} else {
+		value |= _REGBIT_DP_AUX_CH_CTL_TIME_OUT_ERR;
+	}
+
+	/* message size */
+	value &= ~(0xf << 20);
+	value |= (len << 20);
+	__vreg(vgt, reg) = value;
+
+	dp_aux_ch_trigger_interrupt_on_done(vgt, value, reg);
+}
+
+static void dp_aux_ch_ctl_link_training(struct vgt_dpcd_data *dpcd, uint8_t t)
+{
+	if ((t & DPCD_TRAINING_PATTERN_SET_MASK) == DPCD_TRAINING_PATTERN_1) {
+
+		/* training pattern 1 for CR */
+		/* set LANE0_CR_DONE, LANE1_CR_DONE */
+		dpcd->data[DPCD_LANE0_1_STATUS] |= DPCD_LANES_CR_DONE;
+		/* set LANE2_CR_DONE, LANE3_CR_DONE */
+		dpcd->data[DPCD_LANE2_3_STATUS] |= DPCD_LANES_CR_DONE;
+
+	} else if ((t & DPCD_TRAINING_PATTERN_SET_MASK) ==
+		DPCD_TRAINING_PATTERN_2) {
+
+		/* training pattern 2 for EQ */
+
+		/* Set CHANNEL_EQ_DONE and  SYMBOL_LOCKED for Lane0_1 */
+		dpcd->data[DPCD_LANE0_1_STATUS] |= DPCD_LANES_EQ_DONE;
+		dpcd->data[DPCD_LANE0_1_STATUS] |= DPCD_SYMBOL_LOCKED;
+
+		/* Set CHANNEL_EQ_DONE and  SYMBOL_LOCKED for Lane2_3 */
+		dpcd->data[DPCD_LANE2_3_STATUS] |= DPCD_LANES_EQ_DONE;
+		dpcd->data[DPCD_LANE2_3_STATUS] |= DPCD_SYMBOL_LOCKED;
+		/* set INTERLANE_ALIGN_DONE */
+		dpcd->data[DPCD_LANE_ALIGN_STATUS_UPDATED] |=
+			DPCD_INTERLANE_ALIGN_DONE;
+
+	} else if ((t & DPCD_TRAINING_PATTERN_SET_MASK) ==
+		DPCD_LINK_TRAINING_DISABLED) {
+
+		/* finish link training */
+		/* set sink status as synchronized */
+		dpcd->data[DPCD_SINK_STATUS] = DPCD_SINK_IN_SYNC;
+	}
+
+}
+
+static bool dp_aux_ch_ctl_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	unsigned int reg = 0;
+	vgt_reg_t value = *(vgt_reg_t *)p_data;
+	int msg, addr, ctrl, op, len;
+	struct vgt_dpcd_data *dpcd = NULL;
+	enum vgt_port port_idx = vgt_get_dp_port_idx(offset);
+	struct gt_port *port = NULL;
+
+	ASSERT(bytes == 4);
+	ASSERT((offset & (bytes - 1)) == 0);
+
+	reg = offset & ~(bytes - 1);
+
+	default_mmio_write(vgt, offset, p_data, bytes);
+
+	/* HW access had been handled by default_mmio_write() */
+	if (reg_hw_access(vgt, reg))
+		return true;
+
+	if (reg != _REG_DPA_AUX_CH_CTL &&
+	    reg != _REG_PCH_DPB_AUX_CH_CTL &&
+	    reg != _REG_PCH_DPC_AUX_CH_CTL &&
+	    reg != _REG_PCH_DPD_AUX_CH_CTL) {
+		/* write to the data registers */
+		return true;
+	}
+
+	if (!(value & _REGBIT_DP_AUX_CH_CTL_SEND_BUSY)) {
+		/* just want to clear the sticky bits */
+		__vreg(vgt, reg) = 0;
+		return true;
+	}
+
+	if (!dpy_is_valid_port(port_idx)) {
+		vgt_warn("vGT(%d): Unsupported DP port access!\n",
+				vgt->vgt_id);
+		return true;
+	}
+
+	port = &vgt->ports[port_idx];
+
+	if (port) {
+		dpcd = port->dpcd;
+	}
+
+	/* read out message from DATA1 register */
+	msg = __vreg(vgt, reg + 4);
+	addr = (msg >> 8) & 0xffff;
+	ctrl = (msg >> 24) & 0xff;
+	len = msg & 0xff;
+	op = ctrl >> 4;
+
+	if (op == VGT_AUX_NATIVE_WRITE) {
+		int t;
+		uint8_t buf[16];
+
+		if ((addr + len + 1) >= DPCD_SIZE) {
+			/*
+			 * Write request exceeds what we supported,
+			 * DCPD spec: When a Source Device is writing a DPCD
+			 * address not supported by the Sink Device, the Sink
+			 * Device shall reply with AUX NACK and M equal to zero.
+			 */
+
+			/* NAK the write */
+			__vreg(vgt, reg + 4) = AUX_NATIVE_REPLY_NAK;
+
+			dp_aux_ch_ctl_trans_done(vgt, value, reg, 2, true);
+
+			return true;
+		}
+
+		/*
+		 * Write request format: (command + address) occupies
+		 * 3 bytes, followed by (len + 1) bytes of data.
+		 */
+		ASSERT((len + 4) <= AUX_BURST_SIZE);
+
+		/* unpack data from vreg to buf */
+		for (t = 0; t < 4; t ++) {
+			vgt_reg_t r = __vreg(vgt, reg + 8 + t*4);
+
+			buf[t*4] = (r >> 24) & 0xff;
+			buf[t*4 + 1] = (r >> 16) & 0xff;
+			buf[t*4 + 2] = (r >> 8) & 0xff;
+			buf[t*4 + 3] = r & 0xff;
+		}
+
+		/* write to virtual DPCD */
+		if (dpcd && dpcd->data_valid) {
+			for (t = 0; t <= len; t ++) {
+				int p = addr + t;
+
+				dpcd->data[p] = buf[t];
+
+				/* check for link training */
+				if (p == DPCD_TRAINING_PATTERN_SET)
+					dp_aux_ch_ctl_link_training(dpcd, buf[t]);
+			}
+		}
+
+		/* ACK the write */
+		__vreg(vgt, reg + 4) = 0;
+
+		dp_aux_ch_ctl_trans_done(vgt, value, reg, 1, dpcd && dpcd->data_valid);
+
+		return true;
+	}
+
+	if (op == VGT_AUX_NATIVE_READ) {
+		int idx, i, ret = 0;
+
+		if ((addr + len + 1) >= DPCD_SIZE) {
+			/*
+			 * read request exceeds what we supported
+			 * DPCD spec: A Sink Device receiving a Native AUX CH
+			 * read request for an unsupported DPCD address must
+			 * reply with an AUX ACK and read data set equal to
+			 * zero instead of replying with AUX NACK.
+			 */
+
+			/* ACK the READ*/
+			__vreg(vgt, reg + 4) = 0;
+			__vreg(vgt, reg + 8) = 0;
+			__vreg(vgt, reg + 12) = 0;
+			__vreg(vgt, reg + 16) = 0;
+			__vreg(vgt, reg + 20) = 0;
+
+			dp_aux_ch_ctl_trans_done(vgt ,value, reg, len + 2, true);
+
+			return true;
+		}
+
+		for (idx = 1; idx <= 5; idx ++) {
+			/* clear the data registers */
+			__vreg(vgt, reg + 4 * idx) = 0;
+		}
+
+		/*
+		 * Read reply format: ACK (1 byte) plus (len + 1) bytes of data.
+		 */
+		ASSERT((len + 2) <= AUX_BURST_SIZE);
+
+		/* read from virtual DPCD to vreg */
+		/* first 4 bytes: [ACK][addr][addr+1][addr+2] */
+		if (dpcd && dpcd->data_valid) {
+			for (i = 1; i <= (len + 1); i ++) {
+				int t;
+
+				t = dpcd->data[addr + i - 1];
+				t <<= (24 - 8*(i%4));
+				ret |= t;
+
+				if ((i%4 == 3) || (i == (len + 1))) {
+					__vreg(vgt, reg + (i/4 + 1)*4) = ret;
+					ret = 0;
+				}
+			}
+		}
+
+		dp_aux_ch_ctl_trans_done(vgt, value, reg, len + 2, dpcd && dpcd->data_valid);
+
+		return true;
+	}
+
+	/* i2c transaction starts */
+	vgt_i2c_handle_aux_ch_write(vgt, port_idx, offset, p_data);
+
+	dp_aux_ch_trigger_interrupt_on_done(vgt, value, reg);
+	return true;
+}
+
+static void vgt_dpy_stat_notify(struct vgt_device *vgt,
+	enum vgt_uevent_type event)
+{
+	struct pgt_device *pdev = vgt->pdev;
+
+	ASSERT(event >= VGT_ENABLE_VGA && event <= VGT_DISPLAY_UNREADY);
+
+	vgt_set_uevent(vgt, event);
+	vgt_raise_request(pdev, VGT_REQUEST_UEVENT);
+}
+
+static bool vga_control_r(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	return default_mmio_read(vgt, offset, p_data, bytes);
+}
+
+static bool vga_control_w (struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	enum vgt_uevent_type event;
+	bool vga_disable;
+
+	default_mmio_write(vgt, offset, p_data, bytes);
+
+	vga_disable = __vreg(vgt, offset) & _REGBIT_VGA_DISPLAY_DISABLE;
+
+	vgt_info("VM(%d): %s VGA mode %x\n", vgt->vgt_id,
+		vga_disable ? "Disable" : "Enable",
+		(unsigned int)__vreg(vgt, offset));
+
+	event = vga_disable ? VGT_DISABLE_VGA : VGT_ENABLE_VGA;
+
+	vgt_dpy_stat_notify(vgt, event);
+
+	return true;
+}
+
+static bool err_int_r(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	bool rc = default_mmio_read(vgt, offset, p_data, bytes);
+	return rc;
+}
+
+static bool err_int_w(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	bool rc = default_mmio_write(vgt, offset, p_data, bytes);
+	return rc;
+}
+
+static vgt_reg_t get_sbi_reg_cached_value(struct vgt_device *vgt,
+	unsigned int sbi_offset)
+{
+	int i;
+	int num = vgt->sbi_regs.number;
+	vgt_reg_t value = 0;
+
+	for (i = 0; i < num; ++ i) {
+		if (vgt->sbi_regs.registers[i].offset == sbi_offset)
+			break;
+	}
+
+	if (i < num) {
+		value = vgt->sbi_regs.registers[i].value;
+	} else {
+		vgt_warn("vGT(%d): SBI reading did not find the cached value"
+			" for offset 0x%x. 0 will be returned!\n",
+			vgt->vgt_id, sbi_offset);
+	}
+
+	return value;
+}
+
+static void cache_sbi_reg_value(struct vgt_device *vgt, unsigned int sbi_offset,
+	vgt_reg_t value)
+{
+	int i;
+	int num = vgt->sbi_regs.number;
+
+	for (i = 0; i < num; ++ i) {
+		if (vgt->sbi_regs.registers[i].offset == sbi_offset)
+			break;
+	}
+
+	if (i == num) {
+		if (num < SBI_REG_MAX) {
+			vgt->sbi_regs.number ++;
+		} else {
+			vgt_warn("vGT(%d): SBI caching meets maximum limits!\n",
+				vgt->vgt_id);
+			return;
+		}
+	}
+
+	vgt->sbi_regs.registers[i].offset = sbi_offset;
+	vgt->sbi_regs.registers[i].value = value;
+}
+
+static bool sbi_mmio_data_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	bool rc;
+
+	rc = default_mmio_read(vgt, offset, p_data, bytes);
+
+	if (!reg_hw_access(vgt, offset)) {
+		if (((__vreg(vgt, _REG_SBI_CTL_STAT) & _SBI_OPCODE_MASK) >>
+			_SBI_OPCODE_SHIFT) == _SBI_CMD_CRRD) {
+			unsigned int sbi_offset = (__vreg(vgt, _REG_SBI_ADDR) &
+				_SBI_ADDR_OFFSET_MASK) >> _SBI_ADDR_OFFSET_SHIFT;
+			vgt_reg_t val = get_sbi_reg_cached_value(vgt, sbi_offset);
+			*(vgt_reg_t *)p_data = val;
+		}
+	}
+
+	return rc;
+}
+
+static bool sbi_mmio_ctl_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	bool rc;
+
+	rc = default_mmio_write(vgt, offset, p_data, bytes);
+
+	if (!reg_hw_access(vgt, offset)) {
+		vgt_reg_t data = __vreg(vgt, offset);
+
+		data &= ~(_SBI_STAT_MASK << _SBI_STAT_SHIFT);
+		data |= _SBI_READY;
+
+		data &= ~(_SBI_RESPONSE_MASK << _SBI_RESPONSE_SHIFT);
+		data |= _SBI_RESPONSE_SUCCESS;
+
+		__vreg(vgt, offset) = data;
+
+		if (((__vreg(vgt, _REG_SBI_CTL_STAT) & _SBI_OPCODE_MASK) >>
+			_SBI_OPCODE_SHIFT) == _SBI_CMD_CRWR) {
+			unsigned int sbi_offset = (__vreg(vgt, _REG_SBI_ADDR) &
+				_SBI_ADDR_OFFSET_MASK) >> _SBI_ADDR_OFFSET_SHIFT;
+			vgt_reg_t val = __vreg(vgt, _REG_SBI_DATA);
+			cache_sbi_reg_value(vgt, sbi_offset, val);
+		}
+	}
+
+	return rc;
+}
+
+static bool pvinfo_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	bool rc = default_mmio_read(vgt, offset, p_data, bytes);
+	bool invalid_read = false;
+
+	switch (offset) {
+		case vgt_info_off(magic) ... vgt_info_off(vgt_id):
+			if (offset + bytes > vgt_info_off(vgt_id) + 4)
+				invalid_read = true;
+			break;
+
+		case vgt_info_off(avail_rs.low_gmadr.my_base) ...
+			vgt_info_off(avail_rs.fence_num):
+			if (offset + bytes >
+				vgt_info_off(avail_rs.fence_num) + 4)
+				invalid_read = true;
+			break;
+
+		case vgt_info_off(drv_version_major) ...
+			vgt_info_off(min_fence_num):
+			if (offset + bytes > vgt_info_off(min_fence_num) + 4)
+				invalid_read = true;
+			break;
+		case vgt_info_off(v2g_notify):
+			/* set cursor setting here.  For example:
+			 *   *((unsigned int *)p_data)) = VGT_V2G_SET_SW_CURSOR;
+			 */
+			break;
+		default:
+			invalid_read = true;
+			break;
+	}
+
+	if (invalid_read)
+		vgt_warn("invalid pvinfo read: [%x:%x] = %x!!!\n",
+			offset, bytes, *(vgt_reg_t *)p_data);
+
+	return rc;
+}
+
+static void fb_notify_all_mapped_pipes(struct vgt_device *vgt,
+	enum vgt_plane_type planeid)
+{
+	unsigned i;
+
+	for (i = 0; i < I915_MAX_PIPES; i++) {
+		if (vgt->pipe_mapping[i] != I915_MAX_PIPES) {
+			struct fb_notify_msg msg;
+
+			msg.vm_id = vgt->vm_id;
+			msg.plane_id = planeid;
+			msg.pipe_id = i;
+
+			vgt_fb_notifier_call_chain(FB_DISPLAY_FLIP, &msg);
+		}
+	}
+}
+
+static bool pvinfo_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	vgt_reg_t val = *(vgt_reg_t *)p_data;
+	vgt_reg_t min;
+	bool rc = true;
+	enum vgt_uevent_type event;
+
+	switch (offset) {
+		case vgt_info_off(min_low_gmadr):
+			min = val;
+			if (vgt->aperture_sz < min) {
+				vgt_err("VM(%d): aperture size(%llx) is less than"
+					"its driver's minimum requirement(%x)!\n",
+					vgt->vm_id, vgt->aperture_sz, min);
+				rc = false;
+			}
+			break;
+		case vgt_info_off(min_high_gmadr):
+			min = val;
+			if (vgt->gm_sz - vgt->aperture_sz < min) {
+				vgt_err("VM(%d): hiden gm size(%llx) is less than"
+					"its driver's minimum requirement(%x)!\n",
+					vgt->vm_id, vgt->gm_sz - vgt->aperture_sz,
+				        min);
+				rc = false;
+			}
+			break;
+		case vgt_info_off(min_fence_num):
+			min = val;
+			if (vgt->fence_sz < min) {
+				vgt_err("VM(%d): fence size(%x) is less than"
+					"its drivers minimum requirement(%x)!\n",
+					vgt->vm_id, vgt->fence_sz, min);
+				rc = false;
+			}
+			break;
+		case vgt_info_off(display_ready):
+			switch (val) {
+			case 0:
+				event = VGT_DISPLAY_UNREADY;
+				break;
+			case 1:
+				event = VGT_DISPLAY_READY;
+				break;
+			case 2:
+				event = VGT_ENABLE_VGA;
+				break;
+			default:
+				event = UEVENT_MAX;
+				vgt_warn("invalid display event: %d\n", val);
+				break;
+			}
+
+			if (event != UEVENT_MAX)
+				 vgt_dpy_stat_notify(vgt, event);
+
+			if (vgt->vm_id && event == VGT_DISPLAY_READY
+				&& hvm_boot_foreground == true
+				&& !vgt->hvm_boot_foreground_visible) {
+				/*
+				 * Guest had a vaild surface to show.
+				 */
+				vgt->hvm_boot_foreground_visible = 1;
+				vgt->pdev->next_foreground_vm = vgt;
+				vgt_raise_request(vgt->pdev, VGT_REQUEST_DPY_SWITCH);
+			}
+			break;
+		case vgt_info_off(g2v_notify):
+			if (val == VGT_G2V_DISPLAY_REFRESH) {
+				fb_notify_all_mapped_pipes(vgt, PRIMARY_PLANE);
+			} else if (val == VGT_G2V_SET_POINTER_SHAPE) {
+				struct fb_notify_msg msg;
+				msg.vm_id = vgt->vm_id;
+				msg.plane_id = CURSOR_PLANE;
+				msg.pipe_id = 0;
+				vgt_fb_notifier_call_chain(FB_DISPLAY_FLIP, &msg);
+			} else if (val == VGT_G2V_PPGTT_L3_PAGE_TABLE_CREATE) {
+				rc = vgt_g2v_create_ppgtt_mm(vgt, 3);
+			} else if (val == VGT_G2V_PPGTT_L3_PAGE_TABLE_DESTROY) {
+				rc = vgt_g2v_destroy_ppgtt_mm(vgt, 3);
+			} else if (val == VGT_G2V_PPGTT_L4_PAGE_TABLE_CREATE) {
+				rc = vgt_g2v_create_ppgtt_mm(vgt, 4);
+			} else if (val == VGT_G2V_PPGTT_L4_PAGE_TABLE_DESTROY) {
+				rc = vgt_g2v_destroy_ppgtt_mm(vgt, 4);
+			} else if (val == VGT_G2V_EXECLIST_CONTEXT_ELEMENT_CREATE) {
+				rc = vgt_g2v_execlist_context_create(vgt);
+			} else if (val == VGT_G2V_EXECLIST_CONTEXT_ELEMENT_DESTROY) {
+				rc = vgt_g2v_execlist_context_destroy(vgt);
+			} else {
+				vgt_warn("Invalid PV notification. %x\n", val);
+			}
+			break;
+		case vgt_info_off(xhot):
+		case vgt_info_off(yhot):
+			{
+				struct fb_notify_msg msg;
+				msg.vm_id = vgt->vm_id;
+				msg.plane_id = CURSOR_PLANE;
+				msg.pipe_id = 0;
+				vgt_fb_notifier_call_chain(FB_DISPLAY_FLIP, &msg);
+			}
+			break;
+
+                case vgt_info_off(pdp0_lo):
+                case vgt_info_off(pdp0_hi):
+                case vgt_info_off(pdp1_lo):
+                case vgt_info_off(pdp1_hi):
+                case vgt_info_off(pdp2_lo):
+                case vgt_info_off(pdp2_hi):
+                case vgt_info_off(pdp3_lo):
+                case vgt_info_off(pdp3_hi):
+                case vgt_info_off(execlist_context_descriptor_lo):
+                case vgt_info_off(execlist_context_descriptor_hi):
+                        break;
+
+		default:
+			/* keep rc's default value: true.
+			 * NOTE: returning false will crash the VM.
+			 */
+			vgt_warn("invalid pvinfo write: [%x:%x] = %x!!!\n",
+				offset, bytes, val);
+			break;
+	}
+
+	if (rc == true)
+		 rc = default_mmio_write(vgt, offset, p_data, bytes);
+
+	return rc;
+}
+
+static bool pf_read(struct vgt_device *vgt, unsigned int offset,
+			void *p_data, unsigned int bytes)
+{
+	if (enable_panel_fitting) {
+		*(vgt_reg_t *)p_data = __vreg(vgt, offset);
+	} else {
+		default_mmio_read(vgt, offset, p_data, bytes);
+	}
+
+	return true;
+}
+
+static bool pf_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+
+	if (enable_panel_fitting) {
+		memcpy ((char *)vgt->state.vReg + offset, p_data, bytes);
+	} else {
+		default_mmio_write(vgt, offset, p_data, bytes);
+	}
+
+	return true;
+}
+
+static bool power_well_ctl_read(struct vgt_device *vgt, unsigned int offset,
+			void *p_data, unsigned int bytes)
+{
+	bool rc = true;
+	vgt_reg_t data;
+	if (is_current_display_owner(vgt)) {
+		data = VGT_MMIO_READ(vgt->pdev, offset);
+	} else {
+		data = __vreg(vgt, offset);
+	}
+
+	if (IS_HSW(vgt->pdev) && enable_panel_fitting && offset == _REG_HSW_PWR_WELL_CTL2) {
+		data = __vreg(vgt, offset);
+	}
+
+	*(vgt_reg_t *)p_data = data;
+	return rc;
+}
+
+static bool power_well_ctl_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	bool rc = true;
+	vgt_reg_t value = *(vgt_reg_t *)p_data;
+
+	memcpy ((char *)vgt->state.vReg + offset, p_data, bytes);
+
+	if (value & _REGBIT_HSW_PWR_WELL_ENABLE) {
+		__vreg(vgt, offset) |= _REGBIT_HSW_PWR_WELL_STATE;
+	} else {
+		__vreg(vgt, offset) &= ~_REGBIT_HSW_PWR_WELL_STATE;
+	}
+
+	if (is_current_display_owner(vgt)) {
+		/* force to enable power well physically */
+		if (IS_HSW(vgt->pdev) && enable_panel_fitting && offset == _REG_HSW_PWR_WELL_CTL2) {
+			value |= _REGBIT_HSW_PWR_WELL_ENABLE;
+		}
+		VGT_MMIO_WRITE(vgt->pdev, offset, value);
+	}
+
+	return rc;
+}
+
+static bool ring_mmio_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	/* TODO
+	 * We do not support the mix usage of RB mode and EXECLIST from
+	 * different VMs. If that happens, VM with RB mode cannot have
+	 * workload being submitted/executed correctly.
+	 */
+	if (vgt->pdev->enable_execlist)
+		return default_mmio_read(vgt, offset, p_data, bytes);
+	else
+		return ring_mmio_read_in_rb_mode(vgt, offset, p_data, bytes);
+}
+
+static bool ring_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	if (vgt->pdev->enable_execlist) {
+		int ring_id = tail_to_ring_id(vgt->pdev, _tail_reg_(offset));
+		if (!vgt->rb[ring_id].has_execlist_enabled) {
+			vgt_err("VM(%d): Workload submission with ringbuffer "
+			"mode is not allowed since system is in execlist mode. "
+			"VM will be killed!\n", vgt->vm_id);
+			ASSERT_VM(0, vgt);
+		}
+		return default_mmio_write(vgt, offset, p_data, bytes);
+	} else {
+		return ring_mmio_write_in_rb_mode(vgt, offset, p_data, bytes);
+	}
+}
+
+static bool ring_uhptr_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	/* TODO
+	 * Same as ring_mmio_read/write
+	 */
+	if (vgt->pdev->enable_execlist)
+		return default_mmio_write(vgt, offset, p_data, bytes);
+	else
+		return ring_uhptr_write_in_rb_mode(vgt, offset, p_data, bytes);
+}
+
+static bool instpm_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	vgt_reg_t val = *(vgt_reg_t *)p_data;
+	uint16_t val_low = val & 0xFFFF;
+	uint16_t val_high = val >> 16;
+	uint16_t old_val_low = __vreg16(vgt, offset);
+	unsigned int bit;
+	bool hw_access = reg_hw_access(vgt, offset);
+	bool sync_flush = false;
+	bool tlb_invd = false;
+	bool warn_msg = false;
+
+	__vreg16(vgt, offset) =  (old_val_low & ~val_high) |
+		(val_low & val_high);
+
+	for_each_set_bit(bit, (unsigned  long *)&val_high, 16) {
+		bool enable = !!test_bit(bit, (void *)&val_low);
+
+		switch (1 << bit)  {
+		case  _REGBIT_INSTPM_SYNC_FLUSH:
+			sync_flush = enable;
+			break;
+
+		case _REGBIT_INSTPM_FORCE_ORDERING:
+			if (enable && offset != _REG_RCS_INSTPM)
+				warn_msg = true;
+			break;
+
+		case  _REGBIT_INSTPM_TLB_INVALIDATE:
+			if (!enable)
+				break;
+			if (!sync_flush) {
+				warn_msg = true;
+				break;
+			}
+			tlb_invd = true;
+			break;
+		default:
+			if (enable && !hw_access)
+				warn_msg = true;
+			break;
+		}
+	}
+
+	if (warn_msg)
+		vgt_warn("unknown INSTPM write: VM%d: off=0x%x, val=0x%x\n",
+			vgt->vm_id, offset, val);
+
+	if (hw_access || tlb_invd) {
+		if (!hw_access && tlb_invd)
+			__vreg(vgt, offset) = _MASKED_BIT_ENABLE(
+				_REGBIT_INSTPM_TLB_INVALIDATE |
+				_REGBIT_INSTPM_SYNC_FLUSH);
+
+		VGT_MMIO_WRITE(pdev, offset, __vreg(vgt, offset));
+
+		if (tlb_invd) {
+			/*
+			 * The time is usually 0.2ms for 3.11.6 Ubuntu guest.
+			 * 3.8 Linux and Win don't use this to flush GPU tlb.
+			 */
+			if (wait_for_atomic((VGT_MMIO_READ(pdev, offset) &
+				_REGBIT_INSTPM_SYNC_FLUSH) == 0, 1))
+				vgt_warn("INSTPM_TLB_INVALIDATE timed out!\n");
+			__vreg16(vgt, offset) &= ~_REGBIT_INSTPM_SYNC_FLUSH;
+		}
+
+	}
+
+	__sreg(vgt, offset) = __vreg(vgt, offset);
+
+	return true;
+}
+
+bool fpga_dbg_mmio_read(struct vgt_device *vgt, unsigned int reg,
+        void *p_data, unsigned int bytes)
+{
+	bool rc;
+
+	rc = default_mmio_read(vgt, reg, p_data, bytes);
+	if (!rc)
+		return false;
+
+	if (vgt->vm_id == 0 && (__vreg(vgt, reg) & _REGBIT_FPGA_DBG_RM_NOCLAIM)) {
+		VGT_MMIO_WRITE(vgt->pdev, reg, _REGBIT_FPGA_DBG_RM_NOCLAIM);
+
+		__vreg(vgt, reg) &= ~_REGBIT_FPGA_DBG_RM_NOCLAIM;
+		__sreg(vgt, reg) = __vreg(vgt, reg);
+
+		*(vgt_reg_t *)p_data &= ~_REGBIT_FPGA_DBG_RM_NOCLAIM;
+	}
+
+	return true;
+}
+
+bool fpga_dbg_mmio_write(struct vgt_device *vgt, unsigned int reg,
+	void *p_data, unsigned int bytes)
+{
+	vgt_reg_t v = *(vgt_reg_t *)p_data;
+
+	vgt_warn("VM %d writes FPGA_DBG register: %x.\n", vgt->vm_id, v);
+
+	if (vgt->vm_id == 0)
+		return default_mmio_write(vgt, reg, p_data, bytes);
+	else {
+		__vreg(vgt, reg) &= ~v;
+		__sreg(vgt, reg) = __vreg(vgt, reg);
+	}
+
+	return true;
+}
+
+static bool sfuse_strap_mmio_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	bool rc = default_mmio_read(vgt, offset, p_data, bytes);
+	/*
+	 * VM guest driver using _REG_SFUSE_STRAP to detect PORT_B/C/D,
+	 * for indirect mode, we provide full PORT B,C,D capability to VM
+	 */
+	if (!propagate_monitor_to_guest && !is_current_display_owner(vgt)) {
+		*(vgt_reg_t*)p_data |=  (_REGBIT_SFUSE_STRAP_B_PRESENTED
+			| _REGBIT_SFUSE_STRAP_C_PRESENTED | _REGBIT_SFUSE_STRAP_D_PRESENTED);
+	}
+	return rc;
+}
+
+static bool vgt_write_submitport(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	bool rc = true;
+	int ring_id = mmio_to_ring_id(offset);
+	struct vgt_elsp_store *elsp_store = &vgt->rb[ring_id].elsp_store;
+
+	ASSERT((bytes == 4) && ((offset & 3) == 0));
+	ASSERT(elsp_store->count < ELSP_BUNDLE_NUM);
+
+	elsp_store->element[elsp_store->count] = *(vgt_reg_t *)p_data;
+	elsp_store->count ++;
+	vgt_dbg(VGT_DBG_EXECLIST,
+		"VM(%d): MMIO write to virtual submitPort 0x%x with 0x%x\n",
+			vgt->vm_id, offset, *(vgt_reg_t *)p_data);
+	if (elsp_store->count == ELSP_BUNDLE_NUM) {
+		rc = vgt_batch_ELSP_write(vgt, ring_id);
+	}
+
+	return rc;
+}
+
+/*
+ * Track policies of all captured registers
+ *
+ * The registers are organized in blocks according to their locations
+ * on the spec:
+ *	- render
+ *	- display
+ *	- others (pm, workaround, etc.)
+ *      - un-categorized
+ *
+ * The poclies within the same block can vary:
+ *      - [F_VIRT]: default virtualization policy
+ *          * all VMs access vReg
+ *      - [F_RDR]/[F_DPY]: ownership based virtualization
+ *          * owner accesses pReg
+ *          * non-owner accesses vReg
+ *          * vReg<->pReg at ownership switch time
+ *      - [F_DOM0]: uniquely owned by Dom0
+ *          * dom0 accesses pReg
+ *          * other VMs accesses vReg
+ *      - [F_PT]: passthrough policy with HIGH RISK
+ *          * all VMs access pReg!!!
+ *          * temp solution. must be removed in the end
+ *
+ * There are some ancillary attributes, which can be linked together
+ *      - [ADRFIX]: require address check
+ *      - [HWSTS]: need sync with pReg for status bit change
+ *      - [MODE]: higher 16bits are mask bits
+ *
+ * When there are handlers registered, handlers can supersede all
+ * above policies.
+ */
+reg_attr_t vgt_base_reg_info[] = {
+
+/* Interrupt registers - GT */
+{_REG_GTIMR, 4, F_VIRT, 0, D_PRE_BDW, NULL, vgt_reg_imr_handler},
+{_REG_GTIER, 4, F_VIRT, 0, D_PRE_BDW, NULL, vgt_reg_ier_handler},
+{_REG_GTIIR, 4, F_VIRT, 0, D_PRE_BDW, NULL, vgt_reg_iir_handler},
+{_REG_GTISR, 4, F_VIRT, 0, D_PRE_BDW, NULL, NULL},
+{_REG_RCS_IMR, 4, F_RDR, 0, D_HSW_PLUS, NULL, vgt_reg_imr_handler},
+{_REG_BCS_IMR, 4, F_RDR, 0, D_HSW_PLUS, NULL, vgt_reg_imr_handler},
+{_REG_VCS_IMR, 4, F_RDR, 0, D_HSW_PLUS, NULL, vgt_reg_imr_handler},
+{_REG_VCS2_IMR, 4, F_RDR, 0, D_BDW_PLUS, NULL, vgt_reg_imr_handler},
+{_REG_VECS_IMR, 4, F_RDR, 0, D_HSW_PLUS, NULL, vgt_reg_imr_handler},
+
+/* Interrupt registers - Display */
+{_REG_DEIMR, 4, F_VIRT, 0, D_PRE_BDW, NULL, vgt_reg_imr_handler},
+{_REG_DEIER, 4, F_VIRT, 0, D_PRE_BDW, NULL, vgt_reg_ier_handler},
+{_REG_DEIIR, 4, F_VIRT, 0, D_PRE_BDW, NULL, vgt_reg_iir_handler},
+{_REG_DEISR, 4, F_VIRT, 0, D_PRE_BDW, NULL, NULL},
+
+/* Interrupt registers - PM */
+{_REG_PMIMR, 4, F_VIRT, 0, D_PRE_BDW, NULL, vgt_reg_imr_handler},
+{_REG_PMIER, 4, F_VIRT, 0, D_PRE_BDW, NULL, vgt_reg_ier_handler},
+{_REG_PMIIR, 4, F_VIRT, 0, D_PRE_BDW, NULL, vgt_reg_iir_handler},
+{_REG_PMISR, 4, F_VIRT, 0, D_PRE_BDW, NULL, NULL},
+
+/* Interrupt registers - PCH */
+{_REG_SDEIMR, 4, F_VIRT, 0, D_ALL, NULL, vgt_reg_imr_handler},
+{_REG_SDEIER, 4, F_VIRT, 0, D_ALL, NULL, vgt_reg_ier_handler},
+{_REG_SDEIIR, 4, F_VIRT, 0, D_ALL, NULL, vgt_reg_iir_handler},
+{_REG_SDEISR, 4, F_VIRT, 0, D_ALL, vgt_reg_isr_read, vgt_reg_isr_write},
+
+/* Interrupt registers - BDW */
+{_REG_GT_IMR(0), 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_imr_handler},
+{_REG_GT_IER(0), 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_ier_handler},
+{_REG_GT_IIR(0), 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_iir_handler},
+{_REG_GT_ISR(0), 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_GT_IMR(1), 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_imr_handler},
+{_REG_GT_IER(1), 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_ier_handler},
+{_REG_GT_IIR(1), 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_iir_handler},
+{_REG_GT_ISR(1), 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_GT_IMR(2), 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_imr_handler},
+{_REG_GT_IER(2), 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_ier_handler},
+{_REG_GT_IIR(2), 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_iir_handler},
+{_REG_GT_ISR(2), 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_GT_IMR(3), 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_imr_handler},
+{_REG_GT_IER(3), 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_ier_handler},
+{_REG_GT_IIR(3), 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_iir_handler},
+{_REG_GT_ISR(3), 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_DE_PIPE_IMR(PIPE_A), 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_imr_handler},
+{_REG_DE_PIPE_IER(PIPE_A), 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_ier_handler},
+{_REG_DE_PIPE_IIR(PIPE_A), 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_iir_handler},
+{_REG_DE_PIPE_ISR(PIPE_A), 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_DE_PIPE_IMR(PIPE_B), 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_imr_handler},
+{_REG_DE_PIPE_IER(PIPE_B), 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_ier_handler},
+{_REG_DE_PIPE_IIR(PIPE_B), 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_iir_handler},
+{_REG_DE_PIPE_ISR(PIPE_B), 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_DE_PIPE_IMR(PIPE_C), 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_imr_handler},
+{_REG_DE_PIPE_IER(PIPE_C), 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_ier_handler},
+{_REG_DE_PIPE_IIR(PIPE_C), 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_iir_handler},
+{_REG_DE_PIPE_ISR(PIPE_C), 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_DE_PORT_IMR, 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_imr_handler},
+{_REG_DE_PORT_IER, 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_ier_handler},
+{_REG_DE_PORT_IIR, 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_iir_handler},
+{_REG_DE_PORT_ISR, 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_DE_MISC_IMR, 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_imr_handler},
+{_REG_DE_MISC_IER, 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_ier_handler},
+{_REG_DE_MISC_IIR, 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_iir_handler},
+{_REG_DE_MISC_ISR, 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_PCU_IMR, 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_imr_handler},
+{_REG_PCU_IER, 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_ier_handler},
+{_REG_PCU_IIR, 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_iir_handler},
+{_REG_PCU_ISR, 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_MASTER_IRQ, 4, F_VIRT, 0, D_BDW_PLUS, NULL, vgt_reg_master_irq_handler},
+
+/* -------render regs---------- */
+{_REG_RCS_HWSTAM, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_VCS_HWSTAM, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_VCS2_HWSTAM, 4, F_RDR, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_BCS_HWSTAM, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_VECS_HWSTAM, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_RCS_HWS_PGA, 4, F_RDR_ADRFIX, 0xFFFFF000, D_ALL, NULL, NULL},
+{_REG_VCS_HWS_PGA, 4, F_RDR_ADRFIX, 0xFFFFF000, D_ALL, NULL, NULL},
+{_REG_BCS_HWS_PGA, 4, F_RDR_ADRFIX, 0xFFFFF000, D_SNB, NULL, NULL},
+{_REG_BCS_HWS_PGA_GEN7, 4, F_RDR_ADRFIX, 0xFFFFF000, D_GEN7PLUS, NULL, NULL},
+{_REG_VEBOX_HWS_PGA_GEN7, 4, F_RDR_ADRFIX, 0xFFFFF000, D_GEN7PLUS, NULL, NULL},
+{_REG_VECS_HWS_PGA, 4, F_RDR_ADRFIX, 0xFFFFF000, D_HSW, NULL, NULL},
+
+/* maybe an error in Linux driver. meant for VCS_HWS_PGA */
+{0x14080, 4, F_VIRT, 0, D_SNB, NULL, NULL},
+{_REG_RCS_EXCC, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_VCS_EXCC, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_BCS_EXCC, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_VECS_EXCC, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{_REG_RCS_UHPTR, 4, F_RDR_HWSTS, 0, D_ALL, NULL, ring_uhptr_write},
+{_REG_VCS_UHPTR, 4, F_RDR_HWSTS, 0, D_ALL, NULL, ring_uhptr_write},
+{_REG_VCS2_UHPTR, 4, F_RDR_HWSTS, 0, D_BDW_PLUS, NULL, ring_uhptr_write},
+{_REG_BCS_UHPTR, 4, F_RDR_HWSTS, 0, D_ALL, NULL, ring_uhptr_write},
+{_REG_VECS_UHPTR, 4, F_RDR_HWSTS, 0, D_HSW_PLUS, NULL, ring_uhptr_write},
+{_REG_RCS_BB_PREEMPT_ADDR, 4, F_RDR_ADRFIX, 0xFFFFF000, D_ALL, NULL, NULL},
+{_REG_CCID, 4, F_RDR_ADRFIX, 0xFFFFF000, D_ALL, NULL, NULL},
+{0x12198, 4, F_RDR_ADRFIX, 0xFFFFF000, D_ALL, NULL, NULL},
+
+{_REG_CXT_SIZE, 4, F_PT, 0, D_SNB, NULL, NULL},
+{_REG_GEN7_CXT_SIZE, 4, F_PT, 0, D_ALL, NULL, NULL},
+
+{_REG_RCS_TAIL, 4, F_RDR, 0, D_ALL, ring_mmio_read, ring_mmio_write},
+{_REG_RCS_HEAD, 4, F_RDR, 0, D_ALL, ring_mmio_read, ring_mmio_write},
+{_REG_RCS_START, 4, F_RDR_ADRFIX, 0xFFFFF000, D_ALL,
+	ring_mmio_read, ring_mmio_write},
+{_REG_RCS_CTL, 4, F_RDR, 0, D_ALL, ring_mmio_read, ring_mmio_write},
+{_REG_VCS_TAIL, 4, F_RDR, 0, D_ALL, ring_mmio_read, ring_mmio_write},
+{_REG_VCS_HEAD, 4, F_RDR, 0, D_ALL, ring_mmio_read, ring_mmio_write},
+{_REG_VCS_START, 4, F_RDR_ADRFIX, 0xFFFFF000, D_ALL,
+	ring_mmio_read, ring_mmio_write},
+{_REG_VCS_CTL, 4, F_RDR, 0, D_ALL, ring_mmio_read, ring_mmio_write},
+{_REG_BCS_TAIL, 4, F_RDR, 0, D_ALL, ring_mmio_read, ring_mmio_write},
+{_REG_BCS_HEAD, 4, F_RDR, 0, D_ALL, ring_mmio_read, ring_mmio_write},
+{_REG_BCS_START, 4, F_RDR_ADRFIX, 0xFFFFF000, D_ALL,
+	ring_mmio_read, ring_mmio_write},
+{_REG_BCS_CTL, 4, F_RDR, 0, D_ALL, ring_mmio_read, ring_mmio_write},
+
+{_REG_VECS_TAIL, 4, F_RDR, 0, D_HSW_PLUS, ring_mmio_read, ring_mmio_write},
+{_REG_VECS_HEAD, 4, F_RDR, 0, D_HSW_PLUS, ring_mmio_read, ring_mmio_write},
+{_REG_VECS_START, 4, F_RDR_ADRFIX, 0xFFFFF000, D_HSW_PLUS, ring_mmio_read, ring_mmio_write},
+{_REG_VECS_CTL, 4, F_RDR, 0, D_HSW_PLUS, ring_mmio_read, ring_mmio_write},//for TLB
+
+{_REG_VCS2_TAIL, 4, F_RDR, 0, D_BDW_PLUS, ring_mmio_read, ring_mmio_write},
+{_REG_VCS2_HEAD, 4, F_RDR, 0, D_BDW_PLUS, ring_mmio_read, ring_mmio_write},
+{_REG_VCS2_START, 4, F_RDR_ADRFIX, 0xFFFFF000, D_BDW_PLUS,
+	ring_mmio_read, ring_mmio_write},
+{_REG_VCS2_CTL, 4, F_RDR, 0, D_BDW_PLUS, ring_mmio_read, ring_mmio_write},
+
+{_REG_RCS_ACTHD, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_BCS_ACTHD, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_VCS_ACTHD, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_VCS2_ACTHD, 4, F_RDR, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_VECS_ACTHD, 4, F_RDR, 0, D_HSW_PLUS, NULL, NULL},
+
+{_REG_RCS_ACTHD_UDW, 4, F_RDR, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_BCS_ACTHD_UDW, 4, F_RDR, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_VCS_ACTHD_UDW, 4, F_RDR, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_VCS2_ACTHD_UDW, 4, F_RDR, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_VECS_ACTHD_UDW, 4, F_RDR, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_GFX_MODE, 4, F_RDR_MODE, 0, D_SNB, NULL, NULL},
+{_REG_RCS_GFX_MODE_IVB, 4, F_RDR_MODE, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_VCS_MFX_MODE_IVB, 4, F_RDR_MODE, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_VCS2_MFX_MODE_BDW, 4, F_RDR_MODE, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_BCS_BLT_MODE_IVB, 4, F_RDR_MODE, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_VEBOX_MODE, 4, F_RDR_MODE, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_ARB_MODE, 4, F_RDR_MODE, 0, D_ALL, NULL, NULL},
+
+{_REG_RCS_MI_MODE, 4, F_RDR_MODE, 0, D_ALL, NULL, NULL},
+{_REG_VCS_MI_MODE, 4, F_RDR_MODE, 0, D_ALL, NULL, NULL},
+{_REG_VCS2_MI_MODE, 4, F_RDR_MODE, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_BCS_MI_MODE, 4, F_RDR_MODE, 0, D_ALL, NULL, NULL},
+{_REG_VECS_MI_MODE, 4, F_RDR_MODE, 0, D_HSW_PLUS, NULL, NULL},
+
+{_REG_RCS_INSTPM, 4, F_RDR_MODE, 0, D_ALL, NULL, instpm_write},
+{_REG_VCS_INSTPM, 4, F_RDR_MODE, 0, D_ALL, NULL, instpm_write},
+{_REG_VCS2_INSTPM, 4, F_RDR_MODE, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_BCS_INSTPM, 4, F_RDR_MODE, 0, D_ALL, NULL, instpm_write},
+{_REG_VECS_INSTPM, 4, F_RDR_MODE, 0, D_HSW_PLUS, NULL, instpm_write},
+
+{_REG_GT_MODE, 4, F_RDR_MODE, 0, D_SNB, NULL, NULL},
+{_REG_GT_MODE_IVB, 4, F_RDR_MODE, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_CACHE_MODE_0, 4, F_RDR_MODE, 0, D_ALL, NULL, NULL},
+{_REG_CACHE_MODE_1, 4, F_RDR_MODE, 0, D_ALL, NULL, NULL},
+{_REG_CACHE_MODE_0_IVB, 4, F_RDR_MODE, 0, D_ALL, NULL, NULL},
+{_REG_CACHE_MODE_1_IVB, 4, F_RDR_MODE, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_RCS_BB_ADDR, 4, F_RDR_ADRFIX, 0xFFFFF000, D_ALL, NULL, NULL},
+{_REG_VCS_BB_ADDR, 4, F_RDR_ADRFIX, 0xFFFFF000, D_ALL, NULL, NULL},
+{_REG_BCS_BB_ADDR, 4, F_RDR_ADRFIX, 0xFFFFF000, D_ALL, NULL, NULL},
+{_REG_VECS_BB_ADDR, 4, F_RDR_ADRFIX, 0xFFFFF000, D_HSW_PLUS, NULL, NULL},
+/* TODO: need a handler */
+{_REG_RCS_PP_DIR_BASE_READ, 4, F_RDR_ADRFIX, 0xFFFFF000, D_SNB, NULL, NULL},
+{_REG_RCS_PP_DIR_BASE_IVB, 4, F_RDR_ADRFIX, 0xFFFFF000, D_PRE_BDW, NULL, NULL},
+{_REG_VCS_PP_DIR_BASE, 4, F_RDR_ADRFIX, 0xFFFFF000, D_PRE_BDW, NULL, NULL},
+{_REG_BCS_PP_DIR_BASE, 4, F_RDR_ADRFIX, 0xFFFFF000, D_PRE_BDW, NULL, NULL},
+{_REG_VECS_PP_DIR_BASE, 4, F_RDR_ADRFIX, 0xFFFFF000, D_HSW, NULL, NULL},
+{_REG_RCS_PP_DCLV, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_VCS_PP_DCLV, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_BCS_PP_DCLV, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_VECS_PP_DCLV, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{_REG_RBSYNC, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_RVSYNC, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_RVESYNC, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{_REG_BRSYNC, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_BVSYNC, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_BVESYNC, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{_REG_VBSYNC, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_VRSYNC, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_VVESYNC, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{_REG_VEBSYNC, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{_REG_VERSYNC, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{_REG_VEVSYNC, 4, F_RDR, 0, D_HSW, NULL, NULL},
+
+{0x2450, 8, F_RDR, 0, D_HSW, NULL, NULL},
+{0x91b8, 4, F_VIRT, 0, D_HSW, NULL, NULL},
+{0x91c0, 4, F_VIRT, 0, D_HSW, NULL, NULL},
+{0x9150, 4, F_VIRT, 0, D_HSW, NULL, NULL},
+{0x9154, 4, F_VIRT, 0, D_HSW, NULL, NULL},
+{0x9160, 4, F_VIRT, 0, D_HSW, NULL, NULL},
+{0x9164, 4, F_VIRT, 0, D_HSW, NULL, NULL},
+
+{0x4040, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{0xb010, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{0xb020, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{0xb024, 4, F_RDR, 0, D_HSW, NULL, NULL},
+
+{0x2050, 4, F_PT, 0, D_ALL, NULL, NULL},
+{0x12050, 4, F_PT, 0, D_ALL, NULL, NULL},
+{0x1c050, 4, F_PT, 0, D_BDW_PLUS, NULL, NULL},
+{0x22050, 4, F_PT, 0, D_ALL, NULL, NULL},
+{0x1A050, 4, F_PT, 0, D_HSW_PLUS, NULL, NULL},
+
+{0x20dc, 4, F_RDR_MODE, 0, D_ALL, NULL, NULL},
+{_REG_3D_CHICKEN3, 4, F_RDR_MODE, 0, D_ALL, NULL, NULL},
+{0x2088, 4, F_RDR_MODE, 0, D_ALL, NULL, NULL},
+{0x20e4, 4, F_RDR_MODE, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_VFSKPD, 4, F_RDR_MODE, 0, D_ALL, NULL, NULL},
+{_REG_ECOCHK, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_GEN7_COMMON_SLICE_CHICKEN1, 4, F_RDR, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_GEN7_COMMON_SLICE_CHICKEN2, 4, F_RDR, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_GEN7_L3CNTLREG1, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{_REG_GEN7_L3_CHICKEN_MODE_REGISTER, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{_REG_GEN7_SQ_CHICKEN_MBCUNIT_CONFIG, 4, F_RDR, 0, D_HSW_PLUS, NULL, NULL},
+{0x20a0, 4, F_RDR, 0, D_IVB_PLUS, NULL, NULL},
+{0x20e8, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{_REG_RCS_TIMESTAMP, 8, F_PT, 0, D_ALL, NULL, NULL},
+{_REG_VCS_TIMESTAMP, 8, F_PT, 0, D_ALL, NULL, NULL},
+{_REG_VCS2_TIMESTAMP, 8, F_PT, 0, D_BDW_PLUS, NULL, NULL},
+{0x1a358, 8, F_PT, 0, D_ALL, NULL, NULL},
+{_REG_BCS_TIMESTAMP, 8, F_PT, 0, D_ALL, NULL, NULL},
+{0xb008, 4, F_VIRT, 0, D_HSW, NULL, NULL},
+{0xb208, 4, F_VIRT, 0, D_HSW, NULL, NULL},
+{0x2420, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{0x2430, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{0x2434, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{0x2438, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{0x243c, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{0x7018, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{0xe184, 4, F_RDR, 0, D_ALL, NULL, NULL},
+
+{_REG_RCS_EXECLIST_SUBMITPORT, 4, F_VIRT, 0, D_BDW_PLUS,
+			vgt_not_allowed_mmio_read, vgt_write_submitport},
+{_REG_VCS_EXECLIST_SUBMITPORT, 4, F_VIRT, 0, D_BDW_PLUS,
+			vgt_not_allowed_mmio_read, vgt_write_submitport},
+{_REG_VECS_EXECLIST_SUBMITPORT, 4, F_VIRT, 0, D_BDW_PLUS,
+			vgt_not_allowed_mmio_read, vgt_write_submitport},
+{_REG_VCS2_EXECLIST_SUBMITPORT, 4, F_VIRT, 0, D_BDW_PLUS,
+			vgt_not_allowed_mmio_read, vgt_write_submitport},
+{_REG_BCS_EXECLIST_SUBMITPORT, 4, F_VIRT, 0, D_BDW_PLUS,
+			vgt_not_allowed_mmio_read, vgt_write_submitport},
+
+{_REG_RCS_EXECLIST_STATUS, 8, F_RDR, 0, D_BDW_PLUS, NULL,
+					vgt_not_allowed_mmio_write},
+{_REG_VCS_EXECLIST_STATUS, 8, F_RDR, 0, D_BDW_PLUS, NULL,
+					vgt_not_allowed_mmio_write},
+{_REG_VECS_EXECLIST_STATUS, 8, F_RDR, 0, D_BDW_PLUS, NULL,
+					vgt_not_allowed_mmio_write},
+{_REG_VCS2_EXECLIST_STATUS, 8, F_RDR, 0, D_BDW_PLUS, NULL,
+					vgt_not_allowed_mmio_write},
+{_REG_BCS_EXECLIST_STATUS, 8, F_RDR, 0, D_BDW_PLUS, NULL,
+					vgt_not_allowed_mmio_write},
+
+{_REG_RCS_CTX_SR_CTL, 4, F_RDR, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_VCS_CTX_SR_CTL, 4, F_RDR, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_VECS_CTX_SR_CTL, 4, F_RDR, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_VCS2_CTX_SR_CTL, 4, F_RDR, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_BCS_CTX_SR_CTL, 4, F_RDR, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_RCS_CTX_STATUS_BUF, 48, F_VIRT, 0, D_BDW_PLUS, NULL,
+					vgt_not_allowed_mmio_write},
+{_REG_VCS_CTX_STATUS_BUF, 48, F_VIRT, 0, D_BDW_PLUS, NULL,
+					vgt_not_allowed_mmio_write},
+{_REG_VECS_CTX_STATUS_BUF, 48, F_VIRT, 0, D_BDW_PLUS, NULL,
+					vgt_not_allowed_mmio_write},
+{_REG_VCS2_CTX_STATUS_BUF, 48, F_VIRT, 0, D_BDW_PLUS, NULL,
+					vgt_not_allowed_mmio_write},
+{_REG_BCS_CTX_STATUS_BUF, 48, F_VIRT, 0, D_BDW_PLUS, NULL,
+					vgt_not_allowed_mmio_write},
+
+{_REG_RCS_CTX_STATUS_PTR, 4, F_VIRT | VGT_REG_MODE_CTL, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_VCS_CTX_STATUS_PTR, 4, F_VIRT | VGT_REG_MODE_CTL, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_VECS_CTX_STATUS_PTR, 4, F_VIRT | VGT_REG_MODE_CTL, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_VCS2_CTX_STATUS_PTR, 4, F_VIRT | VGT_REG_MODE_CTL, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_BCS_CTX_STATUS_PTR, 4, F_VIRT | VGT_REG_MODE_CTL, 0, D_BDW_PLUS, NULL, NULL},
+
+	/* -------display regs---------- */
+
+{0x60220, 0x20, F_DPY, 0, D_ALL, NULL, NULL},
+{0x602a0, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{0x65050, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{0x650b4, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_VGA_CR_INDEX_MDA, 1, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_VGA_ST01_MDA, 1, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_VGA_AR_INDEX, 1, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_VGA_DACMASK, 1, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_VGA_MSR_READ, 1, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_VGA0, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_VGA1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_VGA_PD, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{0x42080, 4, F_DOM0, 0, D_HSW_PLUS, NULL, NULL},
+{0xc4040, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+
+{_REG_DE_RRMR, 4, F_VIRT, 0, D_ALL, NULL, rrmr_mmio_write},
+
+{_REG_PIPEADSL, 4, F_DPY, 0, D_ALL, pipe_dsl_mmio_read, NULL},
+{_REG_PIPEACONF, 4, F_DPY, 0, D_ALL, NULL, pipe_conf_mmio_write},
+{_REG_PIPEASTAT, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PIPEA_FRMCOUNT, 4, F_DPY, 0, D_ALL, pipe_frmcount_mmio_read, NULL},
+{_REG_PIPEA_FLIPCOUNT, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_PIPE_MISC_A, 4, F_DPY, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_PIPEBDSL, 4, F_DPY, 0, D_ALL, pipe_dsl_mmio_read, NULL},
+{_REG_PIPEBCONF, 4, F_DPY, 0, D_ALL, NULL, pipe_conf_mmio_write},
+{_REG_PIPEBSTAT, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PIPEB_FRMCOUNT, 4, F_DPY, 0, D_ALL, pipe_frmcount_mmio_read, NULL},
+{_REG_PIPEB_FLIPCOUNT, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_PIPE_MISC_B, 4, F_DPY, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_PIPECDSL, 4, F_DPY, 0, D_HSW_PLUS, pipe_dsl_mmio_read, NULL},
+{_REG_PIPECCONF, 4, F_DPY, 0, D_HSW_PLUS, NULL, pipe_conf_mmio_write},
+{_REG_PIPECSTAT, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_PIPEC_FRMCOUNT, 4, F_DPY, 0, D_HSW_PLUS, pipe_frmcount_mmio_read, NULL},
+{_REG_PIPEC_FLIPCOUNT, 4, F_VIRT, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_PIPE_MISC_C, 4, F_DPY, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_PIPE_EDP_CONF, 4, F_DPY, 0, D_HSW_PLUS, NULL, pipe_conf_mmio_write},
+
+{_REG_CURABASE, 4, F_DPY_ADRFIX, 0xFFFFF000, D_ALL, dpy_plane_mmio_read,
+						cur_surf_mmio_write},
+{_REG_CURACNTR, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read, cur_plane_ctl_write},
+{_REG_CURAPOS, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read, dpy_plane_mmio_write},
+{_REG_CURASURFLIVE, 4, F_DPY_HWSTS_ADRFIX, 0xFFFFF000, D_ALL, cur_surflive_mmio_read,
+					surflive_mmio_write},
+
+{_REG_CURAPALET_0, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_CURAPALET_1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_CURAPALET_2, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_CURAPALET_3, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_CURBBASE_SNB, 4, F_DPY_ADRFIX, 0xFFFFF000, D_SNB, dpy_plane_mmio_read,
+						dpy_plane_mmio_write},
+{_REG_CURBCNTR_SNB, 4, F_DPY, 0, D_SNB, dpy_plane_mmio_read,
+						dpy_plane_mmio_write},
+{_REG_CURBPOS_SNB, 4, F_DPY, 0, D_SNB, dpy_plane_mmio_read,
+						dpy_plane_mmio_write},
+{_REG_CURBSURFLIVE_SNB, 4, F_DPY_HWSTS_ADRFIX, 0xFFFFF000, D_SNB, cur_surflive_mmio_read,
+					surflive_mmio_write},
+
+{_REG_CURBBASE, 4, F_DPY_ADRFIX, 0xFFFFF000, D_GEN7PLUS, dpy_plane_mmio_read,
+						cur_surf_mmio_write},
+{_REG_CURBCNTR, 4, F_DPY, 0, D_GEN7PLUS, dpy_plane_mmio_read,
+						cur_plane_ctl_write},
+{_REG_CURBPOS, 4, F_DPY, 0, D_GEN7PLUS, dpy_plane_mmio_read,
+						dpy_plane_mmio_write},
+{_REG_CURBSURFLIVE, 4, F_DPY_HWSTS_ADRFIX, 0xFFFFF000, D_GEN7PLUS, cur_surflive_mmio_read,
+					surflive_mmio_write},
+{_REG_CURCBASE, 4, F_DPY_ADRFIX, 0xFFFFF000, D_GEN7PLUS, dpy_plane_mmio_read,
+						cur_surf_mmio_write},
+
+{_REG_CURCCNTR, 4, F_DPY, 0, D_GEN7PLUS, dpy_plane_mmio_read,
+						cur_plane_ctl_write},
+{_REG_CURCPOS, 4, F_DPY, 0, D_GEN7PLUS, dpy_plane_mmio_read,
+						dpy_plane_mmio_write},
+{_REG_CURCSURFLIVE, 4, F_DPY_HWSTS_ADRFIX, 0xFFFFF000, D_GEN7PLUS, cur_surflive_mmio_read,
+					surflive_mmio_write},
+
+{0x7008C, 4, F_DPY, 0, D_ALL, NULL, vgt_error_handler},
+
+{0x700D0, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{0x700D4, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{0x700D8, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{0x700DC, 4, F_DPY, 0, D_SNB, NULL, NULL},
+
+{0x701b0, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+
+{_REG_DSPACNTR, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read,
+							dpy_plane_ctl_write},
+{_REG_DSPASURF, 4, F_DPY_ADRFIX, 0xFFFFF000, D_ALL, dpy_plane_mmio_read,
+							pri_surf_mmio_write},
+{_REG_DSPASURFLIVE, 4, F_DPY_HWSTS_ADRFIX, 0xFFFFF000, D_ALL, pri_surflive_mmio_read,
+							surflive_mmio_write},
+{_REG_DSPALINOFF, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+{_REG_DSPASTRIDE, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+{_REG_DSPAPOS, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+{_REG_DSPASIZE, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+{_REG_DSPATILEOFF, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+
+{_REG_DSPBCNTR, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read,
+							dpy_plane_ctl_write},
+{_REG_DSPBSURF, 4, F_DPY_ADRFIX, 0xFFFFF000, D_ALL, dpy_plane_mmio_read,
+							pri_surf_mmio_write},
+{_REG_DSPBSURFLIVE, 4, F_DPY_HWSTS_ADRFIX, 0xFFFFF000, D_ALL, pri_surflive_mmio_read,
+							surflive_mmio_write},
+{_REG_DSPBLINOFF, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+{_REG_DSPBSTRIDE, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+{_REG_DSPBPOS, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+{_REG_DSPBSIZE, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+{_REG_DSPBTILEOFF, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+
+{_REG_DSPCCNTR, 4, F_DPY, 0, D_HSW_PLUS, dpy_plane_mmio_read,
+							dpy_plane_ctl_write},
+{_REG_DSPCSURF, 4, F_DPY_ADRFIX, 0xFFFFF000, D_HSW_PLUS, dpy_plane_mmio_read,
+							pri_surf_mmio_write},
+{_REG_DSPCSURFLIVE, 4, F_DPY_HWSTS_ADRFIX, 0xFFFFF000, D_HSW_PLUS, pri_surflive_mmio_read,
+							surflive_mmio_write},
+{_REG_DSPCLINOFF, 4, F_DPY, 0, D_HSW, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+{_REG_DSPCSTRIDE, 4, F_DPY, 0, D_HSW_PLUS, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+{_REG_DSPCPOS, 4, F_DPY, 0, D_HSW, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+{_REG_DSPCSIZE, 4, F_DPY, 0, D_HSW, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+{_REG_DSPCTILEOFF, 4, F_DPY, 0, D_HSW_PLUS, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+
+{_REG_DVSACNTR, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSASURF, 4, F_DPY_ADRFIX, 0xFFFFF000, D_SNB, NULL, NULL},
+{_REG_DVSASURFLIVE, 4, F_DPY_HWSTS_ADRFIX, 0xFFFFF000, D_SNB, NULL, NULL},
+{_REG_DVSALINOFF, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSAPOS, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSASIZE, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSATILEOFF, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSAKEYVAL, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSAKEYMSK, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSAKEYMAXVAL, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSASCALE, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSBCNTR, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSBSURF, 4, F_DPY_ADRFIX, 0xFFFFF000, D_ALL, NULL, NULL},
+{_REG_DVSBSURFLIVE, 4, F_DPY_HWSTS_ADRFIX, 0xFFFFF000, D_SNB, NULL, NULL},
+{_REG_DVSBLINOFF, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSBPOS, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSBSIZE, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSBTILEOFF, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSBKEYVAL, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSBKEYMSK, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSBKEYMAXVAL, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSBSCALE, 4, F_DPY, 0, D_SNB, NULL, NULL},
+
+{_REG_SPRASURF, 4, F_DPY_ADRFIX, 0xFFFFF000, D_HSW_PLUS,
+			dpy_plane_mmio_read, spr_surf_mmio_write},
+{_REG_SPRASURFLIVE, 4, F_DPY_HWSTS_ADRFIX, 0xFFFFF000, D_HSW_PLUS,
+			spr_surflive_mmio_read, surflive_mmio_write},
+
+{_REG_SPRBSURF, 4, F_DPY_ADRFIX, 0xFFFFF000, D_HSW_PLUS,
+			dpy_plane_mmio_read, spr_surf_mmio_write},
+{_REG_SPRBSURFLIVE, 4, F_DPY_HWSTS_ADRFIX, 0xFFFFF000, D_HSW_PLUS,
+			spr_surflive_mmio_read, surflive_mmio_write},
+
+{_REG_SPRCSURF, 4, F_DPY_ADRFIX, 0xFFFFF000, D_HSW_PLUS,
+			dpy_plane_mmio_read, spr_surf_mmio_write},
+{_REG_SPRCSURFLIVE, 4, F_DPY_HWSTS_ADRFIX, 0xFFFFF000, D_HSW_PLUS,
+			spr_surflive_mmio_read, surflive_mmio_write},
+
+{_REG_SPRA_CTL, 4, F_DPY, 0, D_HSW_PLUS, NULL, sprite_plane_ctl_write},
+{_REG_SPRA_SCALE, 4, F_DPY, 0, D_HSW, NULL, NULL},
+
+{_REG_SPRB_CTL, 4, F_DPY, 0, D_HSW_PLUS, NULL, sprite_plane_ctl_write},
+{_REG_SPRB_SCALE, 4, F_DPY, 0, D_HSW, NULL, NULL},
+
+{_REG_SPRC_CTL, 4, F_DPY, 0, D_HSW_PLUS, NULL, sprite_plane_ctl_write},
+
+{_REG_SPRC_SCALE, 4, F_DPY, 0, D_HSW, NULL, NULL},
+
+
+{_REG_LGC_PALETTE_A, 4*256, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_LGC_PALETTE_B, 4*256, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_LGC_PALETTE_C, 4*256, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+
+{_REG_HTOTAL_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_HBLANK_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_HSYNC_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_VTOTAL_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_VBLANK_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_VSYNC_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_PIPEASRC, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read, dpy_plane_mmio_write},
+{_REG_BCLRPAT_A, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_VSYNCSHIFT_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+
+{_REG_HTOTAL_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_HBLANK_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_HSYNC_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_VTOTAL_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_VBLANK_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_VSYNC_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_PIPEBSRC, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read, dpy_plane_mmio_write},
+{_REG_BCLRPAT_B, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_VSYNCSHIFT_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+
+{_REG_HTOTAL_C, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_HBLANK_C, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_HSYNC_C, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_VTOTAL_C, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_VBLANK_C, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_VSYNC_C, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_PIPECSRC, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read, dpy_plane_mmio_write},
+{_REG_BCLRPAT_C, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_VSYNCSHIFT_C, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+
+{0x6F000, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{0x6F004, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{0x6F008, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{0x6F00C, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{0x6F010, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{0x6F014, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{0x6F028, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{0x6F030, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{0x6F034, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{0x6F040, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{0x6F044, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_PIPEA_DATA_M1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PIPEA_DATA_N1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PIPEA_LINK_M1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PIPEA_LINK_N1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_PIPEA_DATA_M2, 4, F_DPY, 0, D_IVB, NULL, NULL},
+{_REG_PIPEA_DATA_N2, 4, F_DPY, 0, D_IVB, NULL, NULL},
+{_REG_PIPEA_LINK_M2, 4, F_DPY, 0, D_IVB, NULL, NULL},
+{_REG_PIPEA_LINK_N2, 4, F_DPY, 0, D_IVB, NULL, NULL},
+
+{_REG_PIPEB_DATA_M1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PIPEB_DATA_N1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PIPEB_LINK_M1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PIPEB_LINK_N1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_PIPEB_DATA_M2, 4, F_DPY, 0, D_IVB, NULL, NULL},
+{_REG_PIPEB_DATA_N2, 4, F_DPY, 0, D_IVB, NULL, NULL},
+{_REG_PIPEB_LINK_M2, 4, F_DPY, 0, D_IVB, NULL, NULL},
+{_REG_PIPEB_LINK_N2, 4, F_DPY, 0, D_IVB, NULL, NULL},
+
+{_REG_PIPEC_DATA_M1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PIPEC_DATA_N1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PIPEC_LINK_M1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PIPEC_LINK_N1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_PIPEC_DATA_M2, 4, F_DPY, 0, D_IVB, NULL, NULL},
+{_REG_PIPEC_DATA_N2, 4, F_DPY, 0, D_IVB, NULL, NULL},
+{_REG_PIPEC_LINK_M2, 4, F_DPY, 0, D_IVB, NULL, NULL},
+{_REG_PIPEC_LINK_N2, 4, F_DPY, 0, D_IVB, NULL, NULL},
+
+{_REG_PF_CTL_0, 4, F_DPY, 0, D_ALL, pf_read, pf_write},
+{_REG_PF_WIN_SZ_0, 4, F_DPY, 0, D_ALL, pf_read, pf_write},
+{_REG_PF_WIN_POS_0, 4, F_DPY, 0, D_ALL, pf_read, pf_write},
+{_REG_PF_CTL_1, 4, F_DPY, 0, D_ALL, pf_read, pf_write},
+{_REG_PF_WIN_SZ_1, 4, F_DPY, 0, D_ALL, pf_read, pf_write},
+{_REG_PF_WIN_POS_1, 4, F_DPY, 0, D_ALL, pf_read, pf_write},
+{_REG_PF_CTL_2, 4, F_DPY, 0, D_GEN7PLUS, pf_read, pf_write},
+{_REG_PF_WIN_SZ_2, 4, F_DPY, 0, D_GEN7PLUS, pf_read, pf_write},
+{_REG_PF_WIN_POS_2, 4, F_DPY, 0, D_GEN7PLUS, pf_read, pf_write},
+
+{_REG_WM0_PIPEA_ILK, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_WM0_PIPEB_ILK, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_WM0_PIPEC_IVB, 4, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_WM1_LP_ILK, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_WM2_LP_ILK, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_WM3_LP_ILK, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_WM1S_LP_ILK, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_WM2S_LP_IVB, 4, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_WM3S_LP_IVB, 4, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+
+{_REG_HISTOGRAM_THRSH, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_BLC_PWM_CPU_CTL2, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_BLC_PWM_CPU_CTL, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_BLC_PWM_PCH_CTL1, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_BLC_PWM_PCH_CTL2, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+
+{_REG_PCH_GMBUS0, 4*4, F_DPY, 0, D_ALL, gmbus_mmio_read, gmbus_mmio_write},
+{_REG_PCH_GPIOA, 6*4, F_VIRT, 0, D_ALL, NULL, NULL},
+
+{_REG_DP_BUFTRANS, 0x28, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_PCH_DPB_AUX_CH_CTL, 6*4, F_DPY, 0, D_ALL, NULL, dp_aux_ch_ctl_mmio_write},
+{_REG_PCH_DPC_AUX_CH_CTL, 6*4, F_DPY, 0, D_ALL, NULL, dp_aux_ch_ctl_mmio_write},
+{_REG_PCH_DPD_AUX_CH_CTL, 6*4, F_DPY, 0, D_ALL, NULL, dp_aux_ch_ctl_mmio_write},
+
+{_REG_PCH_ADPA, 4, F_DPY, 0, D_ALL, NULL, pch_adpa_mmio_write},
+{_REG_DP_B_CTL, 4, F_DPY, 0, D_SNB|D_IVB, NULL, dp_ctl_mmio_write},
+{_REG_DP_C_CTL, 4, F_DPY, 0, D_SNB|D_IVB, NULL, dp_ctl_mmio_write},
+{_REG_DP_D_CTL, 4, F_DPY, 0, D_SNB|D_IVB, NULL, dp_ctl_mmio_write},
+{_REG_HDMI_B_CTL, 4, F_DPY, 0, D_SNB|D_IVB, NULL, hdmi_ctl_mmio_write},
+{_REG_HDMI_C_CTL, 4, F_DPY, 0, D_SNB|D_IVB, NULL, hdmi_ctl_mmio_write},
+{_REG_HDMI_D_CTL, 4, F_DPY, 0, D_SNB|D_IVB, NULL, hdmi_ctl_mmio_write},
+{_REG_TRANSACONF, 4, F_DPY, 0, D_ALL, NULL, transaconf_mmio_write},
+{_REG_TRANSBCONF, 4, F_DPY, 0, D_ALL, NULL, transaconf_mmio_write},
+{_REG_FDI_RXA_IIR, 4, F_DPY, 0, D_ALL, NULL, fdi_rx_iir_mmio_write},
+{_REG_FDI_RXB_IIR, 4, F_DPY, 0, D_ALL, NULL, fdi_rx_iir_mmio_write},
+{_REG_FDI_RXC_IIR, 4, F_DPY, 0, D_GEN7PLUS, NULL, fdi_rx_iir_mmio_write},
+{_REG_FDI_RXA_CTL, 4, F_DPY, 0, D_ALL, NULL, update_fdi_rx_iir_status},
+{_REG_FDI_RXB_CTL, 4, F_DPY, 0, D_ALL, NULL, update_fdi_rx_iir_status},
+{_REG_FDI_RXC_CTL, 4, F_DPY, 0, D_GEN7PLUS, NULL, update_fdi_rx_iir_status},
+{_REG_FDI_TXA_CTL, 4, F_DPY, 0, D_ALL, NULL, update_fdi_rx_iir_status},
+{_REG_FDI_TXB_CTL, 4, F_DPY, 0, D_ALL, NULL, update_fdi_rx_iir_status},
+{_REG_FDI_TXC_CTL, 4, F_DPY, 0, D_GEN7PLUS, NULL, update_fdi_rx_iir_status},
+{_REG_FDI_RXA_IMR, 4, F_DPY, 0, D_ALL, NULL, update_fdi_rx_iir_status},
+{_REG_FDI_RXB_IMR, 4, F_DPY, 0, D_ALL, NULL, update_fdi_rx_iir_status},
+{_REG_FDI_RXC_IMR, 4, F_DPY, 0, D_GEN7PLUS, NULL, update_fdi_rx_iir_status},
+
+{_REG_TRANS_HTOTAL_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_TRANS_HBLANK_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_TRANS_HSYNC_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_TRANS_VTOTAL_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_TRANS_VBLANK_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_TRANS_VSYNC_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_TRANS_VSYNCSHIFT_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+
+{_REG_TRANS_HTOTAL_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_TRANS_HBLANK_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_TRANS_HSYNC_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_TRANS_VTOTAL_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_TRANS_VBLANK_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_TRANS_VSYNC_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_TRANS_VSYNCSHIFT_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+
+{_REG_TRANSA_DATA_M1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSA_DATA_N1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSA_DATA_M2, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSA_DATA_N2, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSA_DP_LINK_M1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSA_DP_LINK_N1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSA_DP_LINK_M2, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSA_DP_LINK_N2, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_TRANSA_VIDEO_DIP_CTL, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSA_VIDEO_DIP_DATA, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSA_VIDEO_DIP_GCP, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSA_DP_CTL, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSB_VIDEO_DIP_CTL, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSB_VIDEO_DIP_DATA, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSB_VIDEO_DIP_GCP, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSB_DP_CTL, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSC_VIDEO_DIP_CTL, 4, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_TRANSC_VIDEO_DIP_DATA, 4, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_TRANSC_VIDEO_DIP_GCP, 4, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_TRANSC_DP_CTL, 4, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+
+{_REG_FDI_RXA_MISC, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_FDI_RXB_MISC, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_FDI_RXA_TUSIZE1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_FDI_RXA_TUSIZE2, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_FDI_RXB_TUSIZE1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_FDI_RXB_TUSIZE2, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_PCH_PP_CONTROL, 4, F_DPY, 0, D_ALL, NULL, pch_pp_control_mmio_write},
+{_REG_PCH_PP_DIVISOR, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_PP_STATUS, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_LVDS, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_DPLL_A, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_DPLL_B, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_FPA0, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_FPA1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_FPB0, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_FPB1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_DREF_CONTROL, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_RAWCLK_FREQ, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_DPLL_SEL, 4, F_DPY, 0, D_ALL, NULL, NULL},
+	/* Linux defines as PP_ON_DEPLAY/PP_OFF_DELAY. Not in spec */
+{0x61208, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{0x6120c, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_PP_ON_DELAYS, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_PP_OFF_DELAYS, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{0xE651C, 4, F_DPY, 0, D_ALL, dpy_reg_mmio_read, NULL},
+{0xE661C, 4, F_DPY, 0, D_ALL, dpy_reg_mmio_read, NULL},
+{0xE671C, 4, F_DPY, 0, D_ALL, dpy_reg_mmio_read, NULL},
+{0xE681C, 4, F_DPY, 0, D_ALL, dpy_reg_mmio_read, NULL},
+{0xE6C04, 4, F_DPY, 0, D_ALL,
+	dpy_reg_mmio_read_2, NULL},
+{0xE6E1C, 4, F_DPY, 0, D_ALL,
+	dpy_reg_mmio_read_3, NULL},
+{_REG_SHOTPLUG_CTL, 4, F_VIRT, 0, D_ALL, NULL, shotplug_ctl_mmio_write},
+{_REG_LCPLL_CTL, 4, F_DPY, 0, D_HSW_PLUS, NULL, lcpll_ctl_mmio_write},
+{_REG_HSW_FUSE_STRAP, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_DP_A_HOTPLUG_CNTL, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+
+{_REG_DISP_ARB_CTL, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_DISP_ARB_CTL2, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_DISPLAY_CHICKEN_BITS_1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_DISPLAY_CHICKEN_BITS_2, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_DSPCLK_GATE_D, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_SOUTH_CHICKEN1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_SOUTH_CHICKEN2, 4, F_DPY, 0, D_ALL, NULL, south_chicken2_write},
+{_REG_TRANSA_CHICKEN1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSB_CHICKEN1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_SOUTH_DSPCLK_GATE_D, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSA_CHICKEN2, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSB_CHICKEN2, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+/*
+ * framebuffer compression is disabled for now
+ * until it's handled at display context switch
+ * and we figure out how stolen memory should be virtualized (FBC needs use
+ * stolen memory).
+ */
+{_REG_DPFC_CB_BASE, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_DPFC_CONTROL, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_DPFC_RECOMP_CTL, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_DPFC_CPU_FENCE_OFFSET, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_DPFC_CONTROL_SA, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_DPFC_CPU_FENCE_OFFSET_SA, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+
+{_REG_CSC_A_COEFFICIENTS, 4*6, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_CSC_A_MODE, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PRECSC_A_HIGH_COLOR_CHANNEL_OFFSET, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PRECSC_A_MEDIUM_COLOR_CHANNEL_OFFSET, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PRECSC_A_LOW_COLOR_CHANNEL_OFFSET, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_CSC_B_COEFFICIENTS, 4*6, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_CSC_B_MODE, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PRECSC_B_HIGH_COLOR_CHANNEL_OFFSET, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PRECSC_B_MEDIUM_COLOR_CHANNEL_OFFSET, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PRECSC_B_LOW_COLOR_CHANNEL_OFFSET, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_CSC_C_COEFFICIENTS, 4*6, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_CSC_C_MODE, 4, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_PRECSC_C_HIGH_COLOR_CHANNEL_OFFSET, 4, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_PRECSC_C_MEDIUM_COLOR_CHANNEL_OFFSET, 4, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_PRECSC_C_LOW_COLOR_CHANNEL_OFFSET, 4, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+
+{0x60110, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{0x61110, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{0x70400, 0x40, F_DPY, 0, D_ALL, NULL, NULL},
+{0x71400, 0x40, F_DPY, 0, D_ALL, NULL, NULL},
+{0x72400, 0x40, F_DPY, 0, D_ALL, NULL, NULL},
+
+{0x70440, 0xc, F_DPY, 0, D_ALL, NULL, NULL},
+{0x71440, 0xc, F_DPY, 0, D_ALL, NULL, NULL},
+{0x72440, 0xc, F_DPY, 0, D_ALL, NULL, NULL},
+
+{0x7044c, 0xc, F_DPY, 0, D_ALL, NULL, NULL},
+{0x7144c, 0xc, F_DPY, 0, D_ALL, NULL, NULL},
+{0x7244c, 0xc, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_WM_DBG, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_PIPE_WM_LINETIME_A, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_PIPE_WM_LINETIME_B, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_PIPE_WM_LINETIME_C, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_SPLL_CTL, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_WRPLL_CTL1, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_WRPLL_CTL2, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_PORT_CLK_SEL_DDIA, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_PORT_CLK_SEL_DDIB, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_PORT_CLK_SEL_DDIC, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_PORT_CLK_SEL_DDID, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_PORT_CLK_SEL_DDIE, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_TRANS_CLK_SEL_A, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_TRANS_CLK_SEL_B, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_TRANS_CLK_SEL_C, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0x46408, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0x46508, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0x49040, 0xc, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0x49140, 0xc, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0x49240, 0xc, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0x49080, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0x49090, 0x14, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0x49180, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0x49190, 0x14, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0x49280, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0x49290, 0x14, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0x4A400, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0x4A480, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0x4AC00, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0x4AC80, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0x4B400, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0x4B480, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+
+{0x6002C, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+
+{_REG_HSW_VIDEO_DIP_CTL_A, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_HSW_VIDEO_DIP_CTL_B, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_HSW_VIDEO_DIP_CTL_C, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_HSW_VIDEO_DIP_CTL_EDP, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+
+{_REG_SFUSE_STRAP, 4, F_DPY, 0, D_HSW_PLUS, sfuse_strap_mmio_read, NULL},
+{_REG_SBI_ADDR, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_SBI_DATA, 4, F_DPY, 0, D_HSW_PLUS, sbi_mmio_data_read, NULL},
+{_REG_SBI_CTL_STAT, 4, F_DPY, 0, D_HSW_PLUS, NULL, sbi_mmio_ctl_write},
+{_REG_PIXCLK_GATE, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0xF200C, 4, F_DPY, 0, D_SNB, NULL, NULL},
+
+{_REG_DPA_AUX_CH_CTL, 6*4, F_DPY, 0, D_HSW_PLUS, NULL, dp_aux_ch_ctl_mmio_write},
+
+{_REG_DDI_BUF_CTL_A, 4, F_DPY, 0, D_HSW_PLUS, NULL, ddi_buf_ctl_mmio_write},
+{_REG_DDI_BUF_CTL_B, 4, F_DPY, 0, D_HSW_PLUS, NULL, ddi_buf_ctl_mmio_write},
+{_REG_DDI_BUF_CTL_C, 4, F_DPY, 0, D_HSW_PLUS, NULL, ddi_buf_ctl_mmio_write},
+{_REG_DDI_BUF_CTL_D, 4, F_DPY, 0, D_HSW_PLUS, NULL, ddi_buf_ctl_mmio_write},
+{_REG_DDI_BUF_CTL_E, 4, F_DPY, 0, D_HSW_PLUS, NULL, ddi_buf_ctl_mmio_write},
+
+{_REG_DP_TP_CTL_A, 4, F_DPY, 0, D_HSW_PLUS, NULL, dp_tp_ctl_mmio_write},
+{_REG_DP_TP_CTL_B, 4, F_DPY, 0, D_HSW_PLUS, NULL, dp_tp_ctl_mmio_write},
+{_REG_DP_TP_CTL_C, 4, F_DPY, 0, D_HSW_PLUS, NULL, dp_tp_ctl_mmio_write},
+{_REG_DP_TP_CTL_D, 4, F_DPY, 0, D_HSW_PLUS, NULL, dp_tp_ctl_mmio_write},
+{_REG_DP_TP_CTL_E, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+
+{_REG_DP_TP_STATUS_A, 4, F_DPY, 0, D_HSW_PLUS, NULL, dp_tp_status_mmio_write},
+{_REG_DP_TP_STATUS_B, 4, F_DPY, 0, D_HSW_PLUS, NULL, dp_tp_status_mmio_write},
+{_REG_DP_TP_STATUS_C, 4, F_DPY, 0, D_HSW_PLUS, NULL, dp_tp_status_mmio_write},
+{_REG_DP_TP_STATUS_D, 4, F_DPY, 0, D_HSW_PLUS, NULL, dp_tp_status_mmio_write},
+{_REG_DP_TP_STATUS_E, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_DDI_BUF_TRANS_A, 0x50, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0x64E60, 0x50, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0x64Ec0, 0x50, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0x64F20, 0x50, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0x64F80, 0x50, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_HSW_AUD_CONFIG_A, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0x650C0, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0x6661c, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{0x66C00, 8, F_DPY, 0, D_HSW, NULL, NULL},
+
+{_REG_TRANS_DDI_FUNC_CTL_A, 4, F_DPY, 0, D_HSW_PLUS, NULL, dpy_trans_ddi_ctl_write},
+{_REG_TRANS_DDI_FUNC_CTL_B, 4, F_DPY, 0, D_HSW_PLUS, NULL, dpy_trans_ddi_ctl_write},
+{_REG_TRANS_DDI_FUNC_CTL_C, 4, F_DPY, 0, D_HSW_PLUS, NULL, dpy_trans_ddi_ctl_write},
+{_REG_TRANS_DDI_FUNC_CTL_EDP, 4, F_DPY, 0, D_HSW_PLUS, NULL, dpy_trans_ddi_ctl_write},
+
+{_REG_TRANS_MSA_MISC_A, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_TRANS_MSA_MISC_B, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_TRANS_MSA_MISC_C, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+{0x6F410, 4, F_DPY, 0, D_HSW_PLUS, NULL, NULL},
+
+	/* -------others---------- */
+{_REG_FORCEWAKE, 4, F_VIRT, 0, D_ALL, NULL, force_wake_write},
+{_REG_FORCEWAKE_ACK, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_GT_CORE_STATUS, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_GT_THREAD_STATUS, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_GTFIFODBG, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_GTFIFO_FREE_ENTRIES, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_MUL_FORCEWAKE, 4, F_VIRT, 0, D_ALL, NULL, mul_force_wake_write},
+{_REG_MUL_FORCEWAKE_ACK, 4, F_VIRT, 0, D_SNB|D_IVB, mul_force_wake_ack_read, NULL},
+{_REG_FORCEWAKE_ACK_HSW, 4, F_VIRT, 0, D_HSW_PLUS, mul_force_wake_ack_read, NULL},
+{_REG_ECOBUS, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RC_CONTROL, 4, F_DOM0, 0, D_ALL, NULL, rc_state_ctrl_1_mmio_write},
+{_REG_RC_STATE, 4, F_DOM0, 0, D_ALL, NULL, rc_state_ctrl_1_mmio_write},
+{_REG_RPNSWREQ, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RC_VIDEO_FREQ, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_DOWN_TIMEOUT, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_INTERRUPT_LIMITS, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RPSTAT1, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_CONTROL, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_UP_THRESHOLD, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_DOWN_THRESHOLD, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_CUR_UP_EI, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_CUR_UP, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_PREV_UP, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_CUR_DOWN_EI, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_CUR_DOWN, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_PREV_DOWN, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_UP_EI, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_DOWN_EI, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_IDLE_HYSTERSIS, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RC1_WAKE_RATE_LIMIT, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RC6_WAKE_RATE_LIMIT, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RC6pp_WAKE_RATE_LIMIT, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RC_EVALUATION_INTERVAL, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RC_IDLE_HYSTERSIS, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RC_SLEEP, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RC1e_THRESHOLD, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RC6_THRESHOLD, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RC6p_THRESHOLD, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RC6pp_THRESHOLD, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_PMINTRMSK, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_HSW_PWR_WELL_CTL1, 4, F_DOM0, 0, D_HSW_PLUS, power_well_ctl_read, power_well_ctl_write},
+{_REG_HSW_PWR_WELL_CTL2, 4, F_DOM0, 0, D_HSW_PLUS, power_well_ctl_read, power_well_ctl_write},
+{_REG_HSW_PWR_WELL_CTL3, 4, F_DOM0, 0, D_HSW_PLUS, power_well_ctl_read, power_well_ctl_write},
+{_REG_HSW_PWR_WELL_CTL4, 4, F_DOM0, 0, D_HSW_PLUS, power_well_ctl_read, power_well_ctl_write},
+{_REG_HSW_PWR_WELL_CTL5, 4, F_DOM0, 0, D_HSW_PLUS, power_well_ctl_read, power_well_ctl_write},
+{_REG_HSW_PWR_WELL_CTL6, 4, F_DOM0, 0, D_HSW_PLUS, power_well_ctl_read, power_well_ctl_write},
+
+{_REG_RSTDBYCTL, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+
+{_REG_GEN6_GDRST, 4, F_DOM0, 0, D_ALL, gen6_gdrst_mmio_read, gen6_gdrst_mmio_write},
+{_REG_FENCE_0_LOW, 0x80, F_VIRT, 0, D_ALL, fence_mmio_read, fence_mmio_write},
+{VGT_PVINFO_PAGE, VGT_PVINFO_SIZE, F_VIRT, 0, D_ALL, pvinfo_read, pvinfo_write},
+{_REG_CPU_VGACNTRL, 4, F_DOM0, 0, D_ALL, vga_control_r, vga_control_w},
+
+/* TODO: MCHBAR, suppose read-only */
+{_REG_MCHBAR_MIRROR, 0x40000, F_VIRT, 0, D_ALL, NULL, NULL},
+
+{_REG_TILECTL, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+
+{_REG_UCG_CTL1, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_UCG_CTL2, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+
+{_REG_SWF, 0x110, F_VIRT, 0, D_SNB, NULL, NULL},
+{_REG_SWF, 0x90, F_VIRT, 0, D_GEN7PLUS, NULL, NULL},
+
+{_REG_GTDRIVER_MAILBOX_INTERFACE, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_GTDRIVER_MAILBOX_DATA0, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{0x13812c, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_GTT_FAULT_STATUS, 4, F_VIRT, 0, D_ALL, err_int_r, err_int_w},
+{0x120010, 4, F_VIRT, 0, D_HSW_PLUS, NULL, NULL},
+{0x9008, 4, F_DOM0, 0, D_HSW_PLUS, NULL, NULL},
+{_REG_GFX_FLSH_CNT, 4, F_PT, 0, D_ALL, NULL, NULL},
+{0x320f0, 8, F_DOM0, 0, D_HSW, NULL, NULL},
+{0x320fc, 4, F_DOM0, 0, D_HSW, NULL, NULL},
+{0x32230, 4, F_DOM0, 0, D_HSW, NULL, NULL},
+{0x44084, 4, F_DOM0, 0, D_HSW, NULL, NULL},
+{0x4408c, 4, F_DOM0, 0, D_HSW, NULL, NULL},
+{0x1082c0, 4, F_DOM0, 0, D_HSW, NULL, NULL},
+
+	/* -------un-categorized regs--------- */
+
+{0x3c, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{0x860, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+/* no definition on this. from Linux */
+{_REG_GEN3_MI_ARB_STATE, 4, F_PT, 0, D_SNB, NULL, NULL},
+{_REG_RCS_ECOSKPD, 4, F_PT, 0, D_ALL, NULL, NULL},
+{0x121d0, 4, F_PT, 0, D_ALL, NULL, NULL},
+{0x1c1d0, 4, F_PT, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_BCS_ECOSKPD, 4, F_PT, 0, D_ALL, NULL, NULL},
+{0x41d0, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_GAC_ECOCHK, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_2D_CG_DIS, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_3D_CG_DIS, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_3D_CG_DIS2, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0x7004, 4, F_VIRT, 0, D_SNB, NULL, NULL},
+{0x7118, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0x7180, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0x7408, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0x7c00, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_SNPCR, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_MBCTL, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0x911c, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0x9120, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+
+{_REG_GAB_CTL, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0x48800, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+
+{0xce044, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xe6500, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xe6504, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xe6600, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xe6604, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xe6700, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xe6704, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xe6800, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xe6804, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+/* FIXME: now looks gmbus handler can't cover 4/5 ports */
+{_REG_PCH_GMBUS4, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_GMBUS5, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_SUPER_QUEUE_CONFIG, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_MISC_CLOCK_GATING, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec008, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec00c, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec008+0x18, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec00c+0x18, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec008+0x18*2, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec00c+0x18*2, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec008+0x18*3, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec00c+0x18*3, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec408, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec40c, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec408+0x18, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec40c+0x18, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec408+0x18*2, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec40c+0x18*2, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec408+0x18*3, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec40c+0x18*3, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfc810, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfc81c, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfc828, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfc834, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfcc00, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfcc0c, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfcc18, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfcc24, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfd000, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfd00c, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfd018, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfd024, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfd034, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+
+/* HSW */
+{0x2214, 4, F_PT, 0, D_HSW, NULL, NULL},
+
+{0x8000, 4, F_PT, 0, D_HSW, NULL, NULL},
+{0x8008, 4, F_PT, 0, D_HSW, NULL, NULL},
+
+{0x45260, 4, F_PT, 0, D_HSW, NULL, NULL},
+{0x13005c, 4, F_PT, 0, D_HSW, NULL, NULL},
+{_REG_FPGA_DBG, 4, F_DOM0, 0, D_HSW_PLUS, fpga_dbg_mmio_read, fpga_dbg_mmio_write},
+
+/* DOM0 PM owns these registers. */
+{_REG_SCRATCH1, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{_REG_ROW_CHICKEN3, 4, F_DOM0, 0, D_HSW, NULL, NULL},
+/* MAXCNT means max idle count */
+
+{_REG_RC_PWRCTX_MAXCNT, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{0x12054, 4, F_DOM0, 0, D_HSW_PLUS, NULL, NULL},
+{0x1C054, 4, F_DOM0, 0, D_BDW_PLUS, NULL, NULL},
+{0x22054, 4, F_DOM0, 0, D_HSW_PLUS, NULL, NULL},
+{0x1A054, 4, F_DOM0, 0, D_HSW_PLUS, NULL, NULL},
+
+{0x44070, 4, F_DOM0, 0, D_HSW_PLUS, NULL, NULL},
+
+/*command accessed registers, supplement for reg audit in cmd parser*/
+{_REG_GEN7_L3SQCREG4, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{0x2178, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{0x217c, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{0x12178, 4, F_RDR, 0, D_HSW_PLUS, NULL, NULL},
+{0x1217c, 4, F_RDR, 0, D_HSW_PLUS, NULL, NULL},
+{0x12400, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{0x12468, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{0x124a0, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{0x124a4, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{0x124b4, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{0x124b8, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{0x124bc, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{0x124d0, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{_REG_BCS_SWCTRL, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_HS_INVOCATION_COUNT, 8, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_DS_INVOCATION_COUNT, 8, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_IA_VERTICES_COUNT  , 8, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_IA_PRIMITIVES_COUNT, 8, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_VS_INVOCATION_COUNT, 8, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_GS_INVOCATION_COUNT, 8, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_GS_PRIMITIVES_COUNT, 8, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_CL_INVOCATION_COUNT, 8, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_CL_PRIMITIVES_COUNT, 8, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_PS_INVOCATION_COUNT, 8, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_PS_DEPTH_COUNT	 , 8, F_RDR, 0, D_ALL, NULL, NULL},
+
+
+/* BDW */
+{_REG_GEN8_PRIVATE_PAT, 4, F_RDR, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_GEN8_PRIVATE_PAT + 4, 4, F_RDR, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_GAMTARBMODE, 4, F_DOM0, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_RCS_PDP_UDW(0) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_RCS_PDP_LDW(0) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_RCS_PDP_UDW(1) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_RCS_PDP_LDW(1) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_RCS_PDP_UDW(2) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_RCS_PDP_LDW(2) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_RCS_PDP_UDW(3) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_RCS_PDP_LDW(3) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_VCS_PDP_UDW(0) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_VCS_PDP_LDW(0) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_VCS_PDP_UDW(1) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_VCS_PDP_LDW(1) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_VCS_PDP_UDW(2) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_VCS_PDP_LDW(2) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_VCS_PDP_UDW(3) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_VCS_PDP_LDW(3) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_VECS_PDP_UDW(0) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_VECS_PDP_LDW(0) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_VECS_PDP_UDW(1) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_VECS_PDP_LDW(1) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_VECS_PDP_UDW(2) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_VECS_PDP_LDW(2) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_VECS_PDP_UDW(3) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_VECS_PDP_LDW(3) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_VCS2_PDP_UDW(0) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_VCS2_PDP_LDW(0) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_VCS2_PDP_UDW(1) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_VCS2_PDP_LDW(1) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_VCS2_PDP_UDW(2) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_VCS2_PDP_LDW(2) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_VCS2_PDP_UDW(3) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_VCS2_PDP_LDW(3) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_BCS_PDP_UDW(0) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_BCS_PDP_LDW(0) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_BCS_PDP_UDW(1) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_BCS_PDP_LDW(1) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_BCS_PDP_UDW(2) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_BCS_PDP_LDW(2) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{_REG_BCS_PDP_UDW(3) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+{_REG_BCS_PDP_LDW(3) , 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+
+{0x2080, 4, F_RDR_ADRFIX, 0xFFFFF000, D_BDW_PLUS, NULL, NULL},
+{0x12080, 4, F_RDR_ADRFIX, 0xFFFFF000, D_BDW_PLUS, NULL, NULL},
+{0x1c080, 4, F_RDR_ADRFIX, 0xFFFFF000, D_BDW_PLUS, NULL, NULL},
+{0x1a080, 4, F_RDR_ADRFIX, 0xFFFFF000, D_BDW_PLUS, NULL, NULL},
+{0x22080, 4, F_RDR_ADRFIX, 0xFFFFF000, D_BDW_PLUS, NULL, NULL},
+
+{0xe100, 4, F_RDR, 0, D_IVB_PLUS, NULL, NULL},
+{0x7300, 4, F_RDR, 0, D_BDW_PLUS, NULL, NULL},
+
+{0x420b0, 4, F_DPY, 0, D_BDW, NULL, NULL},
+{0x420b4, 4, F_DPY, 0, D_BDW, NULL, NULL},
+{0x420b8, 4, F_DPY, 0, D_BDW, NULL, NULL},
+
+{0x45260, 4, F_DPY, 0, D_BDW, NULL, NULL},
+{0x6f800, 4, F_DPY, 0, D_BDW, NULL, NULL},
+
+{0x66c00, 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+{0x66c04, 4, F_VIRT, 0, D_BDW, NULL, NULL},
+
+{0x4024, 4, F_VIRT, 0, D_BDW, NULL, NULL},
+
+{0x9134, 4, F_VIRT, 0, D_BDW, NULL, NULL},
+{0x9138, 4, F_VIRT, 0, D_BDW, NULL, NULL},
+{0x913c, 4, F_VIRT, 0, D_BDW, NULL, NULL},
+
+/* WA */
+{0xfdc, 4, F_VIRT, 0, D_BDW, NULL, NULL},
+{0xe4f0, 4, F_RDR, 0, D_BDW, NULL, NULL},
+{0xe4f4, 4, F_RDR, 0, D_BDW, NULL, NULL},
+{0x9430, 4, F_RDR, 0, D_BDW, NULL, NULL},
+
+/* L3 */
+{0xb1f0, 4, F_RDR, 0, D_BDW, NULL, NULL},
+{0xb1c0, 4, F_RDR, 0, D_BDW, NULL, NULL},
+{0xb118, 4, F_RDR, 0, D_BDW, NULL, NULL},
+{0xb100, 4, F_RDR, 0, D_BDW, NULL, NULL},
+{0xb10c, 4, F_RDR, 0, D_BDW, NULL, NULL},
+{0xb110, 4, F_RDR, 0, D_BDW, NULL, NULL},
+
+/* NON-PRIV */
+{0x24d0, 4, F_RDR, 0, D_BDW, NULL, NULL},
+{0x24d4, 4, F_RDR, 0, D_BDW, NULL, NULL},
+{0x24d8, 4, F_RDR, 0, D_BDW, NULL, NULL},
+{0x24dc, 4, F_RDR, 0, D_BDW, NULL, NULL},
+
+{0x83a4, 4, F_RDR, 0, D_BDW, NULL, NULL},
+
+/* UCG */
+{0x8430, 4, F_PT, 0, D_BDW, NULL, NULL},
+
+{0x110000, 4, F_VIRT, 0, D_BDW_PLUS, NULL, NULL},
+};
+
+static void vgt_passthrough_execlist(struct pgt_device *pdev)
+{
+       int i;
+       reg_update_handlers(pdev, _REG_RCS_CTX_STATUS_BUF, 48, NULL, NULL);
+       reg_update_handlers(pdev, _REG_VCS_CTX_STATUS_BUF, 48, NULL, NULL);
+       reg_update_handlers(pdev, _REG_VECS_CTX_STATUS_BUF, 48, NULL, NULL);
+       reg_update_handlers(pdev, _REG_VCS2_CTX_STATUS_BUF, 48, NULL, NULL);
+       reg_update_handlers(pdev, _REG_BCS_CTX_STATUS_BUF, 48, NULL, NULL);
+
+       for (i = 0; i < 48; i += 4) {
+               reg_change_owner(pdev, _REG_RCS_CTX_STATUS_BUF + i, VGT_OT_RENDER);
+               reg_change_owner(pdev, _REG_VCS_CTX_STATUS_BUF + i, VGT_OT_RENDER);
+               reg_change_owner(pdev, _REG_VECS_CTX_STATUS_BUF + i, VGT_OT_RENDER);
+               reg_change_owner(pdev, _REG_VCS2_CTX_STATUS_BUF + i, VGT_OT_RENDER);
+               reg_change_owner(pdev, _REG_BCS_CTX_STATUS_BUF + i, VGT_OT_RENDER);
+       }
+
+       reg_update_handlers(pdev, _REG_RCS_CTX_STATUS_PTR, 4, NULL, NULL);
+       reg_update_handlers(pdev, _REG_VCS_CTX_STATUS_PTR, 4, NULL, NULL);
+       reg_update_handlers(pdev, _REG_VECS_CTX_STATUS_PTR, 4, NULL, NULL);
+       reg_update_handlers(pdev, _REG_VCS2_CTX_STATUS_PTR, 4, NULL, NULL);
+       reg_update_handlers(pdev, _REG_BCS_CTX_STATUS_PTR, 4, NULL, NULL);
+
+       reg_change_owner(pdev, _REG_RCS_CTX_STATUS_PTR, F_RDR);
+       reg_change_owner(pdev, _REG_VCS_CTX_STATUS_PTR, F_RDR);
+       reg_change_owner(pdev, _REG_VECS_CTX_STATUS_PTR, F_RDR);
+       reg_change_owner(pdev, _REG_VCS2_CTX_STATUS_PTR, F_RDR);
+       reg_change_owner(pdev, _REG_BCS_CTX_STATUS_PTR, F_RDR);
+}
+
+bool vgt_post_setup_mmio_hooks(struct pgt_device *pdev)
+{
+	printk("post mmio hooks initialized\n");
+
+	if (hvm_render_owner)
+		vgt_passthrough_execlist(pdev);
+
+	if (!pdev->enable_ppgtt)
+		return true;
+
+	vgt_dbg(VGT_DBG_MEM,"Hook up PPGTT register handlers\n");
+
+	if (IS_PREBDW(pdev)) {
+		/* trap PPGTT base register */
+		reg_update_handlers(pdev, _REG_RCS_PP_DIR_BASE_IVB, 4,
+				pp_dir_base_read, pp_dir_base_write);
+		reg_update_handlers(pdev, _REG_BCS_PP_DIR_BASE, 4,
+				pp_dir_base_read, pp_dir_base_write);
+		reg_update_handlers(pdev, _REG_VCS_PP_DIR_BASE, 4,
+				pp_dir_base_read, pp_dir_base_write);
+
+		reg_update_handlers(pdev, _REG_RCS_PP_DCLV, 4,
+				pp_dclv_read, pp_dclv_write);
+		reg_update_handlers(pdev, _REG_BCS_PP_DCLV, 4,
+				pp_dclv_read, pp_dclv_write);
+		reg_update_handlers(pdev, _REG_VCS_PP_DCLV, 4,
+				pp_dclv_read, pp_dclv_write);
+
+		if (IS_HSW(pdev)) {
+			reg_update_handlers(pdev, _REG_VECS_PP_DIR_BASE, 4,
+					pp_dir_base_read,
+					pp_dir_base_write);
+			reg_update_handlers(pdev, _REG_VECS_PP_DCLV, 4,
+					pp_dclv_read, pp_dclv_write);
+		}
+	}
+
+	/* XXX cache register? */
+	/* PPGTT enable register */
+	reg_update_handlers(pdev, _REG_RCS_GFX_MODE_IVB, 4,
+			ring_pp_mode_read, ring_pp_mode_write);
+	reg_update_handlers(pdev, _REG_BCS_BLT_MODE_IVB, 4,
+			ring_pp_mode_read, ring_pp_mode_write);
+	reg_update_handlers(pdev, _REG_VCS_MFX_MODE_IVB, 4,
+			ring_pp_mode_read, ring_pp_mode_write);
+
+	if (IS_HSW(pdev) || IS_BDWPLUS(pdev))
+		reg_update_handlers(pdev, _REG_VEBOX_MODE, 4,
+				ring_pp_mode_read,
+				ring_pp_mode_write);
+
+	if (IS_BDWGT3(pdev)) {
+		reg_update_handlers(pdev, _REG_VCS2_MFX_MODE_BDW, 4,
+				ring_pp_mode_read,
+				ring_pp_mode_write);
+
+		VGT_MMIO_WRITE(pdev, 0xfdc,
+				(1 << 28) | (1 << 24) | (1 << 25) | (1 << 26));
+	}
+
+	return true;
+}
+
+int vgt_get_base_reg_num()
+{
+	return ARRAY_NUM(vgt_base_reg_info);
+}
+
+/*
+ * This array lists registers which stick to original policy, as
+ * specified in vgt_base_reg_info, and not impacted by the super
+ * owner mode (which has most registers owned by HVM instead of
+ * dom0).
+ *
+ * Currently the registers in this list are those, which must be
+ * virtualized, with XenGT driver itself as the exclusive owner.
+ * Some features like monitor hotplug may be broken, due to the
+ * whole handling flow already fixed (first to dom0). But that
+ * should be fine, since super owner mode is used for analyze
+ * basic stability issues.
+ */
+reg_list_t vgt_gen7_sticky_regs[] = {
+	/* interrupt control registers */
+	{_REG_GTIMR, 4},
+	{_REG_GTIER, 4},
+	{_REG_GTIIR, 4},
+	{_REG_GTISR, 4},
+	{_REG_RCS_IMR, 4},
+	{_REG_BCS_IMR, 4},
+	{_REG_VCS_IMR, 4},
+	{_REG_VECS_IMR, 4},
+	{_REG_DEIMR, 4},
+	{_REG_DEIER, 4},
+	{_REG_DEIIR, 4},
+	{_REG_DEISR, 4},
+	{_REG_SDEIMR, 4},
+	{_REG_SDEIER, 4},
+	{_REG_SDEIIR, 4},
+	{_REG_SDEISR, 4},
+	{_REG_PMIMR, 4},
+	{_REG_PMIER, 4},
+	{_REG_PMIIR, 4},
+	{_REG_PMISR, 4},
+
+	/* PPGTT related registers */
+	{_REG_RCS_GFX_MODE_IVB, 4},
+	{_REG_VCS_MFX_MODE_IVB, 4},
+	{_REG_BCS_BLT_MODE_IVB, 4},
+	{_REG_VEBOX_MODE, 4},
+	{_REG_RCS_PP_DIR_BASE_IVB, 4},
+	{_REG_VCS_PP_DIR_BASE, 4},
+	{_REG_BCS_PP_DIR_BASE, 4},
+	{_REG_VECS_PP_DIR_BASE, 4},
+	{_REG_RCS_PP_DCLV, 4},
+	{_REG_VCS_PP_DCLV, 4},
+	{_REG_BCS_PP_DCLV, 4},
+	{_REG_VECS_PP_DCLV, 4},
+
+	/* forcewake */
+	{_REG_FORCEWAKE, 4},
+	{_REG_FORCEWAKE_ACK, 4},
+	{_REG_GT_CORE_STATUS, 4},
+	{_REG_GT_THREAD_STATUS, 4},
+	{_REG_GTFIFODBG, 4},
+	{_REG_GTFIFO_FREE_ENTRIES, 4},
+	{_REG_MUL_FORCEWAKE, 4},
+	{_REG_MUL_FORCEWAKE_ACK, 4},
+	{_REG_FORCEWAKE_ACK_HSW, 4},
+
+	/* misc */
+	{_REG_GEN6_GDRST, 4},
+	{_REG_FENCE_0_LOW, 0x80},
+	{VGT_PVINFO_PAGE, VGT_PVINFO_SIZE},
+	{_REG_CPU_VGACNTRL, 4},
+};
+
+reg_list_t vgt_gen8_sticky_regs[] = {
+	/* interrupt control registers */
+	{_REG_RCS_IMR, 4},
+	{_REG_BCS_IMR, 4},
+	{_REG_VCS_IMR, 4},
+	{_REG_VCS2_IMR, 4},
+	{_REG_VECS_IMR, 4},
+
+	{_REG_SDEIMR, 4},
+	{_REG_SDEIER, 4},
+	{_REG_SDEIIR, 4},
+	{_REG_SDEISR, 4},
+
+	/* Interrupt registers - BDW */
+	{_REG_GT_IMR(0), 4},
+	{_REG_GT_IER(0), 4},
+	{_REG_GT_IIR(0), 4},
+	{_REG_GT_ISR(0), 4},
+
+	{_REG_GT_IMR(1), 4},
+	{_REG_GT_IER(1), 4},
+	{_REG_GT_IIR(1), 4},
+	{_REG_GT_ISR(1), 4},
+
+	{_REG_GT_IMR(2), 4},
+	{_REG_GT_IER(2), 4},
+	{_REG_GT_IIR(2), 4},
+	{_REG_GT_ISR(2), 4},
+
+	{_REG_GT_IMR(3), 4},
+	{_REG_GT_IER(3), 4},
+	{_REG_GT_IIR(3), 4},
+	{_REG_GT_ISR(3), 4},
+
+	{_REG_DE_PIPE_IMR(PIPE_A), 4},
+	{_REG_DE_PIPE_IER(PIPE_A), 4},
+	{_REG_DE_PIPE_IIR(PIPE_A), 4},
+	{_REG_DE_PIPE_ISR(PIPE_A), 4},
+
+	{_REG_DE_PIPE_IMR(PIPE_B), 4},
+	{_REG_DE_PIPE_IER(PIPE_B), 4},
+	{_REG_DE_PIPE_IIR(PIPE_B), 4},
+	{_REG_DE_PIPE_ISR(PIPE_B), 4},
+
+	{_REG_DE_PIPE_IMR(PIPE_C), 4},
+	{_REG_DE_PIPE_IER(PIPE_C), 4},
+	{_REG_DE_PIPE_IIR(PIPE_C), 4},
+	{_REG_DE_PIPE_ISR(PIPE_C), 4},
+
+	{_REG_DE_PORT_IMR, 4},
+	{_REG_DE_PORT_IER, 4},
+	{_REG_DE_PORT_IIR, 4},
+	{_REG_DE_PORT_ISR, 4},
+
+	{_REG_DE_MISC_IMR, 4},
+	{_REG_DE_MISC_IER, 4},
+	{_REG_DE_MISC_IIR, 4},
+	{_REG_DE_MISC_ISR, 4},
+
+	{_REG_PCU_IMR, 4},
+	{_REG_PCU_IER, 4},
+	{_REG_PCU_IIR, 4},
+	{_REG_PCU_ISR, 4},
+
+	{_REG_MASTER_IRQ, 4},
+
+	/* PPGTT related registers */
+	{_REG_RCS_GFX_MODE_IVB, 4},
+	{_REG_VCS_MFX_MODE_IVB, 4},
+	{_REG_VCS2_MFX_MODE_BDW, 4},
+	{_REG_BCS_BLT_MODE_IVB, 4},
+	{_REG_VEBOX_MODE, 4},
+
+	/* forcewake */
+	{_REG_FORCEWAKE, 4},
+	{_REG_FORCEWAKE_ACK, 4},
+	{_REG_GT_CORE_STATUS, 4},
+	{_REG_GT_THREAD_STATUS, 4},
+	{_REG_GTFIFODBG, 4},
+	{_REG_GTFIFO_FREE_ENTRIES, 4},
+	{_REG_MUL_FORCEWAKE, 4},
+	{_REG_MUL_FORCEWAKE_ACK, 4},
+	{_REG_FORCEWAKE_ACK_HSW, 4},
+
+	/* misc */
+	{_REG_GEN6_GDRST, 4},
+	{_REG_FENCE_0_LOW, 0x80},
+	{VGT_PVINFO_PAGE, VGT_PVINFO_SIZE},
+	{_REG_CPU_VGACNTRL, 4},
+};
+
+reg_list_t *vgt_get_sticky_regs(struct pgt_device *pdev)
+{
+	if (IS_PREBDW(pdev))
+		return vgt_gen7_sticky_regs;
+	else
+		return vgt_gen8_sticky_regs;
+}
+
+int vgt_get_sticky_reg_num(struct pgt_device *pdev)
+{
+	if (IS_PREBDW(pdev))
+		return ARRAY_NUM(vgt_gen7_sticky_regs);
+	else
+		return ARRAY_NUM(vgt_gen8_sticky_regs);
+}
+
+reg_addr_sz_t vgt_reg_addr_sz[] = {
+	{_REG_RCS_HWS_PGA, 4096, D_ALL},
+	{_REG_VCS_HWS_PGA, 4096, D_ALL},
+	{_REG_BCS_HWS_PGA, 4096, D_SNB},
+	{_REG_BCS_HWS_PGA_GEN7, 4096, D_GEN7PLUS},
+	{_REG_VEBOX_HWS_PGA_GEN7, 4096, D_GEN7PLUS},
+	{_REG_VECS_HWS_PGA, 4096, D_HSW},
+	{_REG_CCID, HSW_CXT_TOTAL_SIZE, D_HSW},
+};
+
+int vgt_get_reg_addr_sz_num()
+{
+	return ARRAY_NUM(vgt_reg_addr_sz);
+}
diff --git a/drivers/gpu/drm/i915/vgt/host.c b/drivers/gpu/drm/i915/vgt/host.c
new file mode 100644
index 0000000..a049cb6
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/host.c
@@ -0,0 +1,183 @@
+#include <linux/io.h>
+#include <drm/drmP.h>
+
+#include "i915_drv.h"
+#include "host.h"
+
+static struct drm_i915_private *dev_priv = NULL;
+static unsigned long gtt_offset = 0;
+
+void i915_vgt_record_priv(struct drm_i915_private *priv)
+{
+	struct drm_device *dev = priv->dev;
+
+	BUG_ON(!dev);
+
+	dev_priv = priv;
+	gtt_offset = pci_resource_len(dev->pdev, 0) / 2;
+}
+
+bool vgt_native_mmio_read(u32 reg, void *val, int len, bool trace)
+{
+	BUG_ON(!dev_priv);
+
+	switch (len) {
+		case 1:
+			*(u8 *)val = dev_priv->uncore.funcs.mmio_readb(dev_priv, reg, trace);
+			break;
+		case 2:
+			*(u16 *)val = dev_priv->uncore.funcs.mmio_readw(dev_priv, reg, trace);
+			break;
+		case 4:
+			*(u32 *)val = dev_priv->uncore.funcs.mmio_readl(dev_priv, reg, trace);
+			break;
+		case 8:
+			*(u64 *)val = dev_priv->uncore.funcs.mmio_readq(dev_priv, reg, trace);
+			break;
+		default:
+			vgt_err("your len is wrong: %d\n", len);
+			return false;
+	}
+	return true;
+}
+
+bool vgt_native_mmio_write(u32 reg, void *val, int len, bool trace)
+{
+	BUG_ON(!dev_priv);
+
+	switch (len) {
+		case 1:
+			dev_priv->uncore.funcs.mmio_writeb(dev_priv, reg, *(u8 *)val, trace);
+			break;
+		case 2:
+			dev_priv->uncore.funcs.mmio_writew(dev_priv, reg, *(u16 *)val, trace);
+			break;
+		case 4:
+			dev_priv->uncore.funcs.mmio_writel(dev_priv, reg, *(u32 *)val, trace);
+			break;
+		case 8:
+			dev_priv->uncore.funcs.mmio_writeq(dev_priv, reg, *(u64 *)val, trace);
+			break;
+		default:
+			vgt_err("your len is wrong: %d\n", len);
+			return false;
+	}
+	return true;
+}
+
+bool vgt_native_gtt_read(u32 reg, void *val, int len)
+{
+	void *va = (void *)vgt_gttmmio_va(pdev_default, reg + gtt_offset);
+
+#if 0
+	if (dev_priv && vgt_ops && vgt_ops->initialized) {
+		switch (len) {
+		case 4:
+			*(u32 *)val = readl(reg + dev_priv->gtt.gsm);
+			break;
+		case 8:
+			*(u64 *)val = readq(reg + dev_priv->gtt.gsm);
+			break;
+		default:
+			vgt_err("your len is wrong: %d\n", len);
+			return false;
+		}
+		return true;
+	} else
+#endif
+	{
+		switch (len) {
+		case 4:
+			*(u32 *)val = readl(va);
+			break;
+		case 8:
+			*(u64 *)val = readq(va);
+			break;
+		default:
+			vgt_err("your len is wrong: %d\n", len);
+			return false;
+		}
+		return true;
+	}
+}
+
+bool vgt_native_gtt_write(u32 reg, void *val, int len)
+{
+	void *va = (void *)vgt_gttmmio_va(pdev_default, reg + gtt_offset);
+
+#if 0
+	if (dev_priv) {
+		switch (len) {
+		case 4:
+			writel(*(u32 *)val, reg + dev_priv->gtt.gsm);
+			break;
+		case 8:
+			writeq(*(u64 *)val, reg + dev_priv->gtt.gsm);
+			break;
+		default:
+			vgt_err("your len is wrong: %d\n", len);
+			return false;
+		}
+		return true;
+	} else
+#endif
+	{
+		switch (len) {
+		case 4:
+			writel(*(u32 *)val, va);
+			break;
+		case 8:
+			writeq(*(u64 *)val, va);
+			break;
+		default:
+			vgt_err("your len is wrong: %d\n", len);
+			return false;
+		}
+		return true;
+	}
+}
+
+bool vgt_host_read(u32 reg, void *val, int len, bool is_gtt, bool trace)
+{
+	uint64_t pa;
+
+	BUG_ON(!dev_priv);
+
+	pa = is_gtt ?
+		vgt_gttmmio_pa(pdev_default, reg + gtt_offset) :
+		vgt_gttmmio_pa(pdev_default, reg);
+	return vgt_ops->mem_read(vgt_dom0, pa, val, len);
+}
+
+bool vgt_host_write(u32 reg, void *val, int len, bool is_gtt, bool trace)
+{
+	uint64_t pa;
+
+	BUG_ON(!dev_priv);
+
+	pa = is_gtt ?
+		vgt_gttmmio_pa(pdev_default, reg + gtt_offset) :
+		vgt_gttmmio_pa(pdev_default, reg);
+	return vgt_ops->mem_write(vgt_dom0, pa, val, len);
+}
+
+void vgt_host_irq_sync(void)
+{
+	irq_work_sync(&dev_priv->irq_work);
+}
+
+void vgt_host_irq(int irq)
+{
+	struct drm_device *dev = pci_get_drvdata(pgt_to_pci(pdev_default));
+	vgt_schedule_host_isr(dev);
+}
+
+void vgt_force_wake_get(void)
+{
+	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
+}
+
+void vgt_force_wake_put(void)
+{
+	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
+}
diff --git a/drivers/gpu/drm/i915/vgt/host.h b/drivers/gpu/drm/i915/vgt/host.h
new file mode 100644
index 0000000..2b05c89
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/host.h
@@ -0,0 +1,41 @@
+#ifndef _VGT_HOST_MEDIATE_H_
+#define _VGT_HOST_MEDIATE_H_
+
+#define vgt_info(fmt, s...)	\
+	do { printk(KERN_INFO "vGT info:(%s:%d) " fmt, __FUNCTION__, __LINE__, ##s); } while (0)
+
+#define vgt_warn(fmt, s...)	\
+	do { printk(KERN_WARNING "vGT warning:(%s:%d) " fmt, __FUNCTION__, __LINE__, ##s); } while (0)
+
+#define vgt_err(fmt, s...)	\
+	do { printk(KERN_ERR "vGT error:(%s:%d) " fmt, __FUNCTION__, __LINE__, ##s); } while (0)
+
+struct pgt_device;
+struct vgt_device;
+struct vgt_ops;
+typedef struct {
+    bool (*mem_read)(struct vgt_device *vgt, uint64_t pa, void *p_data, int bytes);
+    bool (*mem_write)(struct vgt_device *vgt, uint64_t pa, void *p_data, int bytes);
+    bool (*cfg_read)(struct vgt_device *vgt, unsigned int off, void *p_data, int bytes);
+    bool (*cfg_write)(struct vgt_device *vgt, unsigned int off, void *p_data, int bytes);
+    bool initialized;	/* whether vgt_ops can be referenced */
+} vgt_ops_t;
+extern struct pgt_device *pdev_default;
+extern struct vgt_device *vgt_dom0;
+extern vgt_ops_t *vgt_ops;
+
+bool vgt_native_mmio_read(u32 reg, void *val, int len, bool trace);
+bool vgt_native_mmio_write(u32 reg, void *val, int len, bool trace);
+bool vgt_native_gtt_read(u32 reg, void *val, int len);
+bool vgt_native_gtt_write(u32 reg, void *val, int len);
+void vgt_host_irq(int);
+void vgt_host_irq_sync(void);
+
+void vgt_force_wake_get(void);
+void vgt_force_wake_put(void);
+
+uint64_t vgt_gttmmio_va(struct pgt_device *pdev, off_t reg);
+uint64_t vgt_gttmmio_pa(struct pgt_device *pdev, off_t reg);
+struct pci_dev *pgt_to_pci(struct pgt_device *pdev);
+
+#endif /* _VGT_HOST_MEDIATE_H_ */
diff --git a/drivers/gpu/drm/i915/vgt/hypercall.h b/drivers/gpu/drm/i915/vgt/hypercall.h
new file mode 100644
index 0000000..6f83bc4
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/hypercall.h
@@ -0,0 +1,45 @@
+/*
+ * Interface abstraction for hypervisor services
+ *
+ * Copyright(c) 2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of Version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#ifndef _VGT_HYPERCALL_H_
+#define _VGT_HYPERCALL_H_
+
+struct guest_page;
+struct vgt_device;
+struct kernel_dm {
+	unsigned long (*g2m_pfn)(int vm_id, unsigned long g_pfn);
+	int (*pause_domain)(int vm_id);
+	int (*shutdown_domain)(int vm_id);
+	int (*map_mfn_to_gpfn)(int vm_id, unsigned long gpfn,
+		unsigned long mfn, int nr, int map);
+	int (*set_trap_area)(struct vgt_device *vgt, uint64_t start, uint64_t end, bool map);
+	bool (*set_wp_pages)(struct vgt_device *vgt, struct guest_page *p);
+	bool (*unset_wp_pages)(struct vgt_device *vgt, struct guest_page *p);
+	int (*check_host)(void);
+	int (*from_virt_to_mfn)(void *addr);
+	void *(*from_mfn_to_virt)(int mfn);
+	int (*inject_msi)(int vm_id, u32 addr, u16 data);
+	int (*hvm_init)(struct vgt_device *vgt);
+	void (*hvm_exit)(struct vgt_device *vgt);
+	void *(*gpa_to_va)(struct vgt_device *vgt, unsigned long gap);
+	bool (*read_va)(struct vgt_device *vgt, void *va, void *val, int len, int atomic);
+	bool (*write_va)(struct vgt_device *vgt, void *va, void *val, int len, int atomic);
+};
+
+#endif /* _VGT_HYPERCALL_H_ */
diff --git a/drivers/gpu/drm/i915/vgt/instance.c b/drivers/gpu/drm/i915/vgt/instance.c
new file mode 100644
index 0000000..cb2a3e2
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/instance.c
@@ -0,0 +1,535 @@
+/*
+ * Instance life-cycle management
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "vgt.h"
+
+/*
+ * bitmap of allocated vgt_ids.
+ * bit = 0 means free ID, =1 means allocated ID.
+ */
+static unsigned long vgt_id_alloc_bitmap;
+
+struct vgt_device *vmid_2_vgt_device(int vmid)
+{
+	unsigned int bit;
+	struct vgt_device *vgt;
+
+	ASSERT(vgt_id_alloc_bitmap != ~0UL)
+	for_each_set_bit(bit, &vgt_id_alloc_bitmap, VGT_MAX_VMS) {
+		vgt = default_device.device[bit];
+		if (vgt && vgt->vm_id == vmid)
+			return vgt;
+	}
+	return NULL;
+}
+
+static int allocate_vgt_id(void)
+{
+	unsigned long bit_index;
+
+	ASSERT(vgt_id_alloc_bitmap != ~0UL)
+	do {
+		bit_index = ffz (vgt_id_alloc_bitmap);
+		if (bit_index >= VGT_MAX_VMS) {
+			vgt_err("vGT: allocate_vgt_id() failed\n");
+			return -ENOSPC;
+		}
+	} while (test_and_set_bit(bit_index, &vgt_id_alloc_bitmap) != 0);
+
+	return bit_index;
+}
+
+static void free_vgt_id(int vgt_id)
+{
+	ASSERT(vgt_id >= 0 && vgt_id < VGT_MAX_VMS);
+	ASSERT(vgt_id_alloc_bitmap & (1UL << vgt_id));
+	clear_bit(vgt_id, &vgt_id_alloc_bitmap);
+}
+
+/*
+ * Initialize the vgt state instance.
+ * Return:	0: failed
+ *		1: success
+ *
+ */
+static int create_state_instance(struct vgt_device *vgt)
+{
+	vgt_state_t	*state;
+	int i;
+
+	vgt_dbg(VGT_DBG_GENERIC, "create_state_instance\n");
+	state = &vgt->state;
+	state->vReg = vzalloc(vgt->pdev->mmio_size);
+	state->sReg = vzalloc(vgt->pdev->mmio_size);
+	if ( state->vReg == NULL || state->sReg == NULL )
+	{
+		printk("VGT: insufficient memory allocation at %s\n", __FUNCTION__);
+		if ( state->vReg )
+			vfree (state->vReg);
+		if ( state->sReg )
+			vfree (state->sReg);
+		state->sReg = state->vReg = NULL;
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < I915_MAX_PIPES; i++) {
+		vgt->pipe_mapping[i] = i;
+	}
+
+	for (i = 0; i < VGT_BAR_NUM; i++)
+		state->bar_mapped[i] = 0;
+	return 0;
+}
+
+/*
+ * priv: VCPU ?
+ */
+int create_vgt_instance(struct pgt_device *pdev, struct vgt_device **ptr_vgt, vgt_params_t vp)
+{
+	int cpu;
+	struct vgt_device *vgt;
+	char *cfg_space;
+	u16 *gmch_ctl;
+	int rc = -ENOMEM;
+	int i;
+
+	vgt_info("vm_id=%d, low_gm_sz=%dMB, high_gm_sz=%dMB, fence_sz=%d, vgt_primary=%d\n",
+		vp.vm_id, vp.aperture_sz, vp.gm_sz-vp.aperture_sz, vp.fence_sz, vp.vgt_primary);
+
+	vgt = vzalloc(sizeof(*vgt));
+	if (vgt == NULL) {
+		printk("Insufficient memory for vgt_device in %s\n", __FUNCTION__);
+		return rc;
+	}
+
+	atomic_set(&vgt->crashing, 0);
+
+	if ((rc = vgt->vgt_id = allocate_vgt_id()) < 0 )
+		goto err2;
+
+	vgt->vm_id = vp.vm_id;
+	vgt->pdev = pdev;
+
+	vgt->force_removal = 0;
+
+	INIT_LIST_HEAD(&vgt->list);
+
+	if ((rc = create_state_instance(vgt)) < 0)
+		goto err2;
+
+	for (i = 0; i < I915_MAX_PORTS; i++) {
+		vgt->ports[i].type = VGT_PORT_MAX;
+		vgt->ports[i].cache.type = VGT_PORT_MAX;
+		vgt->ports[i].port_override = i;
+		vgt->ports[i].cache.port_override = i;
+		vgt->ports[i].physcal_port = i;
+	}
+
+	/* Hard code ballooning now. We can support non-ballooning too in the future */
+	vgt->ballooning = 1;
+
+	/* present aperture to the guest at the same host address */
+	vgt->state.aperture_base = phys_aperture_base(pdev);
+
+	/* init aperture/gm ranges allocated to this vgt */
+	if ((rc = allocate_vm_aperture_gm_and_fence(vgt, vp)) < 0) {
+		printk("vGT: %s: no enough available aperture/gm/fence!\n", __func__);
+		goto err2;
+	}
+
+	vgt->aperture_offset = aperture_2_gm(pdev, vgt->aperture_base);
+	vgt->aperture_base_va = phys_aperture_vbase(pdev) +
+		vgt->aperture_offset;
+
+	alloc_vm_rsvd_aperture(vgt);
+
+	vgt->state.bar_size[0] = pdev->bar_size[0];	/* MMIOGTT */
+	vgt->state.bar_size[1] =			/* Aperture */
+		vgt->ballooning ? pdev->bar_size[1] : vgt_aperture_sz(vgt);
+	vgt->state.bar_size[2] = pdev->bar_size[2];	/* PIO */
+	vgt->state.bar_size[3] = pdev->bar_size[3];	/* ROM */
+
+	/* Set initial configuration space and MMIO space registers. */
+	cfg_space = &vgt->state.cfg_space[0];
+	memcpy (cfg_space, pdev->initial_cfg_space, VGT_CFG_SPACE_SZ);
+	cfg_space[VGT_REG_CFG_SPACE_MSAC] = vgt->state.bar_size[1];
+
+	/* Show guest that there isn't any stolen memory.*/
+	gmch_ctl = (u16 *)(cfg_space + _REG_GMCH_CONTRL);
+	if (IS_PREBDW(pdev))
+		*gmch_ctl &= ~(_REGBIT_SNB_GMCH_GMS_MASK << _REGBIT_SNB_GMCH_GMS_SHIFT);
+	else
+		*gmch_ctl &= ~(_REGBIT_BDW_GMCH_GMS_MASK << _REGBIT_BDW_GMCH_GMS_SHIFT);
+
+	vgt_pci_bar_write_32(vgt, VGT_REG_CFG_SPACE_BAR1, phys_aperture_base(pdev) );
+
+	/* mark HVM's GEN device's IO as Disabled. hvmloader will enable it */
+	if (vgt->vm_id != 0) {
+		cfg_space[VGT_REG_CFG_COMMAND] &= ~(_REGBIT_CFG_COMMAND_IO |
+						_REGBIT_CFG_COMMAND_MEMORY |
+						_REGBIT_CFG_COMMAND_MASTER);
+	}
+
+	vgt_info("aperture: [0x%llx, 0x%llx] guest [0x%llx, 0x%llx] "
+		"va(0x%llx)\n",
+		vgt_aperture_base(vgt),
+		vgt_aperture_end(vgt),
+		vgt_guest_aperture_base(vgt),
+		vgt_guest_aperture_end(vgt),
+		(uint64_t)vgt->aperture_base_va);
+
+	vgt_info("GM: [0x%llx, 0x%llx], [0x%llx, 0x%llx], "
+		"guest[0x%llx, 0x%llx], [0x%llx, 0x%llx]\n",
+		vgt_visible_gm_base(vgt),
+		vgt_visible_gm_end(vgt),
+		vgt_hidden_gm_base(vgt),
+		vgt_hidden_gm_end(vgt),
+		vgt_guest_visible_gm_base(vgt),
+		vgt_guest_visible_gm_end(vgt),
+		vgt_guest_hidden_gm_base(vgt),
+		vgt_guest_hidden_gm_end(vgt));
+
+	/* If the user explicitly specified a value, use it; or, use the
+	 * global vgt_primary.
+	 */
+	ASSERT(vgt->vm_id == 0 || (vp.vgt_primary >= -1 && vp.vgt_primary <= 1));
+	if (vgt->vm_id != 0 &&
+		(vp.vgt_primary == 0 || (vp.vgt_primary == -1 && !vgt_primary))) {
+		/* Mark vgt device as non primary VGA */
+		cfg_space[VGT_REG_CFG_CLASS_CODE] = VGT_PCI_CLASS_VGA;
+		cfg_space[VGT_REG_CFG_SUB_CLASS_CODE] = VGT_PCI_CLASS_VGA_OTHER;
+		cfg_space[VGT_REG_CFG_CLASS_PROG_IF] = VGT_PCI_CLASS_VGA_OTHER;
+	}
+
+	state_sreg_init (vgt);
+	state_vreg_init(vgt);
+
+	/* setup the ballooning information */
+	if (vgt->ballooning) {
+		__vreg64(vgt, vgt_info_off(magic)) = VGT_MAGIC;
+		__vreg(vgt, vgt_info_off(version_major)) = 1;
+		__vreg(vgt, vgt_info_off(version_minor)) = 0;
+		__vreg(vgt, vgt_info_off(display_ready)) = 0;
+		__vreg(vgt, vgt_info_off(vgt_id)) = vgt->vgt_id;
+		__vreg(vgt, vgt_info_off(avail_rs.low_gmadr.my_base)) = vgt_visible_gm_base(vgt);
+		__vreg(vgt, vgt_info_off(avail_rs.low_gmadr.my_size)) = vgt_aperture_sz(vgt);
+		__vreg(vgt, vgt_info_off(avail_rs.high_gmadr.my_base)) = vgt_hidden_gm_base(vgt);
+		__vreg(vgt, vgt_info_off(avail_rs.high_gmadr.my_size)) = vgt_hidden_gm_sz(vgt);
+
+		__vreg(vgt, vgt_info_off(avail_rs.fence_num)) = vgt->fence_sz;
+		vgt_info("filling VGT_PVINFO_PAGE for dom%d:\n"
+			"   visable_gm_base=0x%llx, size=0x%llx\n"
+			"   hidden_gm_base=0x%llx, size=0x%llx\n"
+			"   fence_base=%d, num=%d\n",
+			vgt->vm_id,
+			vgt_visible_gm_base(vgt), vgt_aperture_sz(vgt),
+			vgt_hidden_gm_base(vgt), vgt_hidden_gm_sz(vgt),
+			vgt->fence_base, vgt->fence_sz);
+
+		ASSERT(sizeof(struct vgt_if) == VGT_PVINFO_SIZE);
+	}
+
+	vgt->bypass_addr_check = bypass_dom0_addr_check && (vgt->vm_id == 0);
+
+	vgt_lock_dev(pdev, cpu);
+
+	pdev->device[vgt->vgt_id] = vgt;
+	list_add(&vgt->list, &pdev->rendering_idleq_head);
+
+	vgt_unlock_dev(pdev, cpu);
+
+	if (!vgt_init_vgtt(vgt)) {
+		vgt_err("fail to initialize vgt vgtt.\n");
+		goto err2;
+	}
+
+	if (vgt->vm_id != 0){
+		/* HVM specific init */
+		if ((rc = hypervisor_hvm_init(vgt)) < 0)
+			goto err;
+	}
+
+	if (vgt->vm_id) {
+		if (hvm_render_owner)
+			current_render_owner(pdev) = vgt;
+
+		if (hvm_display_owner)
+			current_display_owner(pdev) = vgt;
+
+		if (hvm_super_owner) {
+			ASSERT(hvm_render_owner);
+			ASSERT(hvm_display_owner);
+			ASSERT(hvm_boot_foreground);
+			current_config_owner(pdev) = vgt;
+			current_foreground_vm(pdev) = vgt;
+		}
+	}
+	bitmap_zero(vgt->enabled_rings, MAX_ENGINES);
+	bitmap_zero(vgt->started_rings, MAX_ENGINES);
+
+	for (i = 0; i < MAX_ENGINES; ++ i) {
+		vgt->rb[i].csb_write_ptr = DEFAULT_INV_SR_PTR;
+	}
+
+	/* create debugfs per vgt */
+	if ((rc = vgt_create_debugfs(vgt)) < 0) {
+		vgt_err("failed to create debugfs for vgt-%d\n",
+			vgt->vgt_id);
+		goto err;
+	}
+
+	if ((rc = vgt_create_mmio_dev(vgt)) < 0) {
+		vgt_err("failed to create mmio devnode for vgt-%d\n",
+				vgt->vgt_id);
+		goto err;
+	}
+
+	vgt_init_i2c_edid(vgt);
+
+	*ptr_vgt = vgt;
+
+	/* initialize context scheduler infor */
+	if (event_based_qos)
+		vgt_init_sched_info(vgt);
+
+	if (shadow_tail_based_qos)
+		vgt_init_rb_tailq(vgt);
+
+	vgt->warn_untrack = 1;
+	return 0;
+err:
+	vgt_clean_vgtt(vgt);
+err2:
+	hypervisor_hvm_exit(vgt);
+	if (vgt->aperture_base > 0)
+		free_vm_aperture_gm_and_fence(vgt);
+	vfree(vgt->state.vReg);
+	vfree(vgt->state.sReg);
+	if (vgt->vgt_id >= 0)
+		free_vgt_id(vgt->vgt_id);
+	vfree(vgt);
+	return rc;
+}
+
+void vgt_release_instance(struct vgt_device *vgt)
+{
+	int i;
+	struct pgt_device *pdev = vgt->pdev;
+	struct list_head *pos;
+	struct vgt_device *v = NULL;
+	int cpu;
+
+	printk("prepare to destroy vgt (%d)\n", vgt->vgt_id);
+
+	/* destroy vgt_mmio_device */
+	vgt_destroy_mmio_dev(vgt);
+
+	vgt_destroy_debugfs(vgt);
+
+	vgt_lock_dev(pdev, cpu);
+
+	printk("check render ownership...\n");
+	list_for_each (pos, &pdev->rendering_runq_head) {
+		v = list_entry (pos, struct vgt_device, list);
+		if (v == vgt)
+			break;
+	}
+
+	if (v != vgt)
+		printk("vgt instance has been removed from run queue\n");
+	else if (hvm_render_owner || current_render_owner(pdev) != vgt) {
+		printk("remove vgt(%d) from runqueue safely\n",
+			vgt->vgt_id);
+		vgt_disable_render(vgt);
+	} else {
+		printk("vgt(%d) is current owner, request reschedule\n",
+			vgt->vgt_id);
+		vgt_request_force_removal(vgt);
+	}
+
+	printk("check display ownership...\n");
+	if (!hvm_super_owner && (current_display_owner(pdev) == vgt)) {
+		vgt_dbg(VGT_DBG_DPY, "switch display ownership back to dom0\n");
+		current_display_owner(pdev) = vgt_dom0;
+	}
+
+	if (!hvm_super_owner && (current_foreground_vm(pdev) == vgt)) {
+		vgt_dbg(VGT_DBG_DPY, "switch foreground vm back to dom0\n");
+		pdev->next_foreground_vm = vgt_dom0;
+		do_vgt_fast_display_switch(pdev);
+	}
+
+	vgt_unlock_dev(pdev, cpu);
+	if (vgt->force_removal)
+		/* wait for removal completion */
+		wait_event(pdev->destroy_wq, !vgt->force_removal);
+
+	printk("release display/render ownership... done\n");
+
+	/* FIXME: any conflicts between destroy_wq ? */
+	if (shadow_tail_based_qos)
+		vgt_destroy_rb_tailq(vgt);
+
+	vgt_clean_vgtt(vgt);
+	hypervisor_hvm_exit(vgt);
+
+	if (vgt->state.opregion_va) {
+		vgt_hvm_opregion_map(vgt, 0);
+		free_pages((unsigned long)vgt->state.opregion_va,
+				VGT_OPREGION_PORDER);
+	}
+
+	vgt_lock_dev(pdev, cpu);
+
+	vgt->pdev->device[vgt->vgt_id] = NULL;
+	free_vgt_id(vgt->vgt_id);
+
+	/* already idle */
+	list_del(&vgt->list);
+
+	vgt_unlock_dev(pdev, cpu);
+
+	for (i = 0; i < I915_MAX_PORTS; i++) {
+		if (vgt->ports[i].edid) {
+			kfree(vgt->ports[i].edid);
+			vgt->ports[i].edid = NULL;
+		}
+
+		if (vgt->ports[i].dpcd) {
+			kfree(vgt->ports[i].dpcd);
+			vgt->ports[i].dpcd = NULL;
+		}
+
+		if (vgt->ports[i].cache.edid) {
+			kfree(vgt->ports[i].cache.edid);
+			vgt->ports[i].cache.edid = NULL;
+		}
+
+		if (vgt->ports[i].kobj.state_initialized) {
+			kobject_put(&vgt->ports[i].kobj);
+		}
+	}
+
+	/* clear the gtt entries for GM of this vgt device */
+	vgt_clear_gtt(vgt);
+
+	free_vm_aperture_gm_and_fence(vgt);
+	free_vm_rsvd_aperture(vgt);
+	vfree(vgt->state.vReg);
+	vfree(vgt->state.sReg);
+	vfree(vgt);
+	printk("vGT: vgt_release_instance done\n");
+}
+
+void vgt_reset_ppgtt(struct vgt_device *vgt, unsigned long ring_bitmap)
+{
+	struct vgt_mm *mm;
+	int bit;
+
+	if (!vgt->pdev->enable_ppgtt || !vgt->gtt.active_ppgtt_mm_bitmap)
+		return;
+
+	if (ring_bitmap == 0xff)
+		vgt_info("VM %d: Reset full virtual PPGTT state.\n", vgt->vm_id);
+
+	for_each_set_bit(bit, &ring_bitmap, sizeof(ring_bitmap)) {
+		if (bit >= vgt->pdev->max_engines)
+			break;
+
+		if (!test_bit(bit, &vgt->gtt.active_ppgtt_mm_bitmap))
+			continue;
+
+		mm = vgt->rb[bit].active_ppgtt_mm;
+
+		vgt_info("VM %d: Reset ring %d PPGTT state.\n", vgt->vm_id, bit);
+
+		vgt->rb[bit].has_ppgtt_mode_enabled = 0;
+		vgt->rb[bit].has_ppgtt_base_set = 0;
+		vgt->rb[bit].ppgtt_page_table_level = 0;
+		vgt->rb[bit].ppgtt_root_pointer_type = GTT_TYPE_INVALID;
+
+		vgt_destroy_mm(mm);
+
+		vgt->rb[bit].active_ppgtt_mm = NULL;
+		clear_bit(bit, &vgt->gtt.active_ppgtt_mm_bitmap);
+	}
+
+	return;
+}
+
+static void vgt_reset_ringbuffer(struct vgt_device *vgt, unsigned long ring_bitmap)
+{
+	vgt_state_ring_t *rb;
+	int bit;
+
+	for_each_set_bit(bit, &ring_bitmap, sizeof(ring_bitmap)) {
+		int i;
+		if (bit >= vgt->pdev->max_engines)
+			break;
+
+		rb = &vgt->rb[bit];
+
+		/* Drop all submitted commands. */
+		vgt_init_cmd_info(rb);
+
+		rb->uhptr = 0;
+		rb->request_id = rb->uhptr_id = 0;
+
+		rb->el_slots_head = rb->el_slots_tail = 0;
+		for (i = 0; i < EL_QUEUE_SLOT_NUM; ++ i)
+			memset(&rb->execlist_slots[i], 0,
+				sizeof(struct vgt_exec_list));
+
+		memset(&rb->vring, 0, sizeof(vgt_ringbuffer_t));
+		memset(&rb->sring, 0, sizeof(vgt_ringbuffer_t));
+		rb->csb_write_ptr = DEFAULT_INV_SR_PTR;
+
+		vgt_disable_ring(vgt, bit);
+
+		if (bit == RING_BUFFER_RCS) {
+			struct pgt_device *pdev = vgt->pdev;
+			struct vgt_rsvd_ring *ring = &pdev->ring_buffer[bit];
+
+			memcpy((char *)v_aperture(pdev, rb->context_save_area),
+					(char *)v_aperture(pdev, ring->null_context),
+					SZ_CONTEXT_AREA_PER_RING);
+
+			vgt->has_context = rb->active_vm_context = 0;
+		}
+	}
+
+	return;
+}
+
+void vgt_reset_virtual_states(struct vgt_device *vgt, unsigned long ring_bitmap)
+{
+	ASSERT(spin_is_locked(&vgt->pdev->lock));
+
+	vgt_reset_ringbuffer(vgt, ring_bitmap);
+
+	vgt_reset_ppgtt(vgt, ring_bitmap);
+
+	return;
+}
diff --git a/drivers/gpu/drm/i915/vgt/interrupt.c b/drivers/gpu/drm/i915/vgt/interrupt.c
new file mode 100644
index 0000000..ba1625c
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/interrupt.c
@@ -0,0 +1,2233 @@
+/*
+ * vGT interrupt handler
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/linkage.h>
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/bitops.h>
+#include <linux/slab.h>
+#include <linux/list.h>
+
+#include "vgt.h"
+
+/*
+ * TODO:
+ *   - IIR could store two pending interrupts. need emulate the behavior
+ *   - GT has 2nd level IMR registers (render/blitter/video)
+ *   - Handle more events (like hdmi/dp hotplug, pipe-c, watchdog, etc.)
+ */
+
+/*
+ * Below are necessary steps to add a new event handling:
+ *   a) (device specific) add bit<->event mapping information in
+ *      vgt_base_init_irq
+ *
+ *   b) (event specific) add event forwarding policy in vgt_init_events
+ *
+ *      Normally those are the only steps required, if the event is only
+ *      associated to the 1st leve interrupt control registers (iir/ier
+ *      imr/isr). The default handler will take care automatically
+ *
+ *      In the case where the event is associated with status/control
+ *      bits in other registers (e.g. monitor hotplug), you'll provide
+ *      specific handler for both physical event and virtual event
+ *
+ *   c) create a vgt_handle_XXX_phys handler, which deals with any required
+ *      housekeeping, and may optionally cache some state to be forwarded
+ *      to a VM
+ *
+ *   d) create a vgt_handle_XXX_virt handler, which emulates a virtual
+ *      event generation with any required state emulated accordingly, may
+ *      optionally use cached state from p_handler
+ *
+ *   e) setup virt/phys handler in vgt_init_events
+ */
+static void vgt_handle_events(struct vgt_irq_host_state *hstate, void *iir,
+		struct vgt_irq_info *info);
+
+static void update_upstream_irq(struct vgt_device *vgt,
+		struct vgt_irq_info *info);
+
+static int vgt_irq_warn_once[VGT_MAX_VMS+1][EVENT_MAX];
+
+char *vgt_irq_name[EVENT_MAX] = {
+	// GT
+	[RCS_MI_USER_INTERRUPT] = "Render Command Streamer MI USER INTERRUPT",
+	[RCS_DEBUG] = "Render EU debug from SVG",
+	[RCS_MMIO_SYNC_FLUSH] = "Render MMIO sync flush status",
+	[RCS_CMD_STREAMER_ERR] = "Render Command Streamer error interrupt",
+	[RCS_PIPE_CONTROL] = "Render PIPE CONTROL notify",
+	[RCS_WATCHDOG_EXCEEDED] = "Render Command Streamer Watchdog counter exceeded",
+	[RCS_PAGE_DIRECTORY_FAULT] = "Render page directory faults",
+	[RCS_AS_CONTEXT_SWITCH] = "Render AS Context Switch Interrupt",
+
+	[VCS_MI_USER_INTERRUPT] = "Video Command Streamer MI USER INTERRUPT",
+	[VCS_MMIO_SYNC_FLUSH] = "Video MMIO sync flush status",
+	[VCS_CMD_STREAMER_ERR] = "Video Command Streamer error interrupt",
+	[VCS_MI_FLUSH_DW] = "Video MI FLUSH DW notify",
+	[VCS_WATCHDOG_EXCEEDED] = "Video Command Streamer Watchdog counter exceeded",
+	[VCS_PAGE_DIRECTORY_FAULT] = "Video page directory faults",
+	[VCS_AS_CONTEXT_SWITCH] = "Video AS Context Switch Interrupt",
+	[VCS2_MI_USER_INTERRUPT] = "VCS2 Video Command Streamer MI USER INTERRUPT",
+	[VCS2_MI_FLUSH_DW] = "VCS2 Video MI FLUSH DW notify",
+	[VCS2_AS_CONTEXT_SWITCH] = "VCS2 Context Switch Interrupt",
+
+	[BCS_MI_USER_INTERRUPT] = "Blitter Command Streamer MI USER INTERRUPT",
+	[BCS_MMIO_SYNC_FLUSH] = "Billter MMIO sync flush status",
+	[BCS_CMD_STREAMER_ERR] = "Blitter Command Streamer error interrupt",
+	[BCS_MI_FLUSH_DW] = "Blitter MI FLUSH DW notify",
+	[BCS_PAGE_DIRECTORY_FAULT] = "Blitter page directory faults",
+	[BCS_AS_CONTEXT_SWITCH] = "Blitter AS Context Switch Interrupt",
+
+	[VECS_MI_FLUSH_DW] = "Video Enhanced Streamer MI FLUSH DW notify",
+	[VECS_AS_CONTEXT_SWITCH] = "VECS Context Switch Interrupt",
+
+	// DISPLAY
+	[PIPE_A_FIFO_UNDERRUN] = "Pipe A FIFO underrun",
+	[PIPE_A_CRC_ERR] = "Pipe A CRC error",
+	[PIPE_A_CRC_DONE] = "Pipe A CRC done",
+	[PIPE_A_VSYNC] = "Pipe A vsync",
+	[PIPE_A_LINE_COMPARE] = "Pipe A line compare",
+	[PIPE_A_ODD_FIELD] = "Pipe A odd field",
+	[PIPE_A_EVEN_FIELD] = "Pipe A even field",
+	[PIPE_A_VBLANK] = "Pipe A vblank",
+	[PIPE_B_FIFO_UNDERRUN] = "Pipe B FIFO underrun",
+	[PIPE_B_CRC_ERR] = "Pipe B CRC error",
+	[PIPE_B_CRC_DONE] = "Pipe B CRC done",
+	[PIPE_B_VSYNC] = "Pipe B vsync",
+	[PIPE_B_LINE_COMPARE] = "Pipe B line compare",
+	[PIPE_B_ODD_FIELD] = "Pipe B odd field",
+	[PIPE_B_EVEN_FIELD] = "Pipe B even field",
+	[PIPE_B_VBLANK] = "Pipe B vblank",
+	[PIPE_C_VBLANK] = "Pipe C vblank",
+	[DPST_PHASE_IN] = "DPST phase in event",
+	[DPST_HISTOGRAM] = "DPST histogram event",
+	[GSE] = "GSE",
+	[DP_A_HOTPLUG] = "DP A Hotplug",
+	[AUX_CHANNEL_A] = "AUX Channel A",
+	[PERF_COUNTER] = "Performance counter",
+	[POISON] = "Poison",
+	[GTT_FAULT] = "GTT fault",
+	[PRIMARY_A_FLIP_DONE] = "Primary Plane A flip done",
+	[PRIMARY_B_FLIP_DONE] = "Primary Plane B flip done",
+	[PRIMARY_C_FLIP_DONE] = "Primary Plane C flip done",
+	[SPRITE_A_FLIP_DONE] = "Sprite Plane A flip done",
+	[SPRITE_B_FLIP_DONE] = "Sprite Plane B flip done",
+	[SPRITE_C_FLIP_DONE] = "Sprite Plane C flip done",
+
+	// PM
+	[GV_DOWN_INTERVAL] = "Render geyserville Down evaluation interval interrupt",
+	[GV_UP_INTERVAL] = "Render geyserville UP evaluation interval interrupt",
+	[RP_DOWN_THRESHOLD] = "RP DOWN threshold interrupt",
+	[RP_UP_THRESHOLD] = "RP UP threshold interrupt",
+	[FREQ_DOWNWARD_TIMEOUT_RC6] = "Render Frequency Downward Timeout During RC6 interrupt",
+	[PCU_THERMAL] = "PCU Thermal Event",
+	[PCU_PCODE2DRIVER_MAILBOX] = "PCU pcode2driver mailbox event",
+
+	// PCH
+	[FDI_RX_INTERRUPTS_TRANSCODER_A] = "FDI RX Interrupts Combined A",
+	[AUDIO_CP_CHANGE_TRANSCODER_A] = "Audio CP Change Transcoder A",
+	[AUDIO_CP_REQUEST_TRANSCODER_A] = "Audio CP Request Transcoder A",
+	[FDI_RX_INTERRUPTS_TRANSCODER_B] = "FDI RX Interrupts Combined B",
+	[AUDIO_CP_CHANGE_TRANSCODER_B] = "Audio CP Change Transcoder B",
+	[AUDIO_CP_REQUEST_TRANSCODER_B] = "Audio CP Request Transcoder B",
+	[FDI_RX_INTERRUPTS_TRANSCODER_C] = "FDI RX Interrupts Combined C",
+	[AUDIO_CP_CHANGE_TRANSCODER_C] = "Audio CP Change Transcoder C",
+	[AUDIO_CP_REQUEST_TRANSCODER_C] = "Audio CP Request Transcoder C",
+	[ERR_AND_DBG] = "South Error and Debug Interupts Combined",
+	[GMBUS] = "Gmbus",
+	[SDVO_B_HOTPLUG] = "SDVO B hotplug",
+	[CRT_HOTPLUG] = "CRT Hotplug",
+	[DP_B_HOTPLUG] = "DisplayPort/HDMI/DVI B Hotplug",
+	[DP_C_HOTPLUG] = "DisplayPort/HDMI/DVI C Hotplug",
+	[DP_D_HOTPLUG] = "DisplayPort/HDMI/DVI D Hotplug",
+	[AUX_CHENNEL_B] = "AUX Channel B",
+	[AUX_CHENNEL_C] = "AUX Channel C",
+	[AUX_CHENNEL_D] = "AUX Channel D",
+	[AUDIO_POWER_STATE_CHANGE_B] = "Audio Power State change Port B",
+	[AUDIO_POWER_STATE_CHANGE_C] = "Audio Power State change Port C",
+	[AUDIO_POWER_STATE_CHANGE_D] = "Audio Power State change Port D",
+
+	[EVENT_RESERVED] = "RESERVED EVENTS!!!",
+};
+
+void reset_cached_interrupt_registers(struct pgt_device *pdev)
+{
+	struct vgt_irq_host_state *hstate = pdev->irq_hstate;
+	struct vgt_irq_info *info;
+	u32 reg_base, ier, imr;
+	int i;
+
+	for (i = 0; i < IRQ_INFO_MAX; i++) {
+		info = hstate->info[i];
+		if (!info)
+			continue;
+
+		reg_base = hstate->info[i]->reg_base;
+
+		imr = regbase_to_imr(reg_base);
+		ier = regbase_to_ier(reg_base);
+
+		__sreg(vgt_dom0, imr) = VGT_MMIO_READ(pdev, imr);
+		__sreg(vgt_dom0, ier) = VGT_MMIO_READ(pdev, ier);
+	}
+
+	for (i = 0; i < pdev->max_engines; i++) {
+		imr = pdev->ring_mmio_base[i] - 0x30 + 0xa8;
+		__sreg(vgt_dom0, imr) = VGT_MMIO_READ(pdev, imr);
+	}
+}
+
+static inline u32 vgt_read_cached_interrupt_register(struct pgt_device *pdev,
+		vgt_reg_t reg)
+{
+	return __sreg(vgt_dom0, reg);
+}
+
+static inline void vgt_write_cached_interrupt_register(struct pgt_device *pdev,
+		vgt_reg_t reg, u32 v)
+{
+	unsigned long flags;
+
+	if (__sreg(vgt_dom0, reg) == v)
+		return;
+
+	__sreg(vgt_dom0, reg) = v;
+
+	vgt_get_irq_lock(pdev, flags);
+
+	VGT_MMIO_WRITE(pdev, reg, v);
+	VGT_POST_READ(pdev, reg);
+
+	vgt_put_irq_lock(pdev, flags);
+}
+
+/* we need to translate interrupts that is pipe related.
+* for DE IMR or DE IER, bit 0~4 is interrupts for Pipe A, bit 5~9 is interrupts for Pipe B, bit 10~14 is interrupts
+* for pipe C. we can move the interrupts to the right bits when translating interrupts
+*/
+static u32 gen6_translate_pipe_interrupt(struct vgt_device *vgt, unsigned int reg)
+{
+	struct vgt_irq_host_state *irq_hstate = vgt->pdev->irq_hstate;
+	u32 interrupt = __vreg(vgt, reg);
+	int i = 0;
+	u32 mapped_interrupt = interrupt;
+	u32 temp;
+
+	if (_REG_DEIMR == reg) {
+		mapped_interrupt |= irq_hstate->pipe_mask;
+		mapped_interrupt |= (irq_hstate->pipe_mask << 5);
+		mapped_interrupt |= (irq_hstate->pipe_mask << 10);
+		// clear the initial mask bit in DEIMR for VBLANKS, so that when pipe mapping
+		// is not valid, physically there are still vblanks generated.
+		mapped_interrupt &= ~((1 << 0) | (1 << 5) | (1 << 10));
+		for (i = 0; i < I915_MAX_PIPES; i++) {
+			if (vgt->pipe_mapping[i] == I915_MAX_PIPES)
+				continue;
+
+			mapped_interrupt &= ~(irq_hstate->pipe_mask <<
+				(vgt->pipe_mapping[i] * 5));
+
+			temp = interrupt >> (i * 5);
+			temp &= irq_hstate->pipe_mask;
+			mapped_interrupt |= temp << (vgt->pipe_mapping[i] * 5);
+		}
+	} else if (_REG_DEIER == reg) {
+		mapped_interrupt &= ~irq_hstate->pipe_mask;
+		mapped_interrupt &= ~(irq_hstate->pipe_mask<<5);
+		mapped_interrupt &= ~(irq_hstate->pipe_mask<<10);
+		for (i = 0; i < I915_MAX_PIPES; i++) {
+			temp = interrupt >> (i * 5);
+			temp &= irq_hstate->pipe_mask;
+			if (vgt->pipe_mapping[i] != I915_MAX_PIPES) {
+				mapped_interrupt |= temp << (vgt->pipe_mapping[i] * 5);
+			}
+		}
+	}
+	return mapped_interrupt;
+}
+
+static u32 gen8_translate_pipe_interrupt(struct vgt_device *vgt, vgt_reg_t reg)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	u32 v, v_val, p_val, p_reg;
+	int v_pipe, p_pipe;
+	bool is_imr;
+
+	v_val = __vreg(vgt, reg);
+
+	if (reg < 0x44400 || reg > 0x4442c)
+		return v_val;
+
+	is_imr = ((reg & 0xf) == 0x4);
+	v_pipe = reg >> 4 & 0xf;
+	p_pipe = vgt->pipe_mapping[v_pipe];
+
+	if (p_pipe == I915_MAX_PIPES) {
+		// clear the initial mask bit VBLANKS, so that when pipe mapping
+		// is not valid, physically there are still vblanks generated.
+		return is_imr ? 0xfffffffe : 0;
+	}
+
+	if (v_pipe == p_pipe)
+		return v_val;
+
+	p_reg = (reg & ~(0xf << 4)) | p_pipe << 4;
+	p_val = __vreg(vgt, p_reg);
+
+	v = vgt_read_cached_interrupt_register(pdev, p_reg);
+	if (is_imr)
+		v &= v_val;
+	else
+		v |= v_val;
+	vgt_write_cached_interrupt_register(pdev, p_reg, v);
+
+	return p_val;
+}
+
+u32 pipe_mapping_interrupt_virt_to_phys(struct vgt_device *vgt, vgt_reg_t reg)
+{
+	if (IS_PREBDW(vgt->pdev))
+		return gen6_translate_pipe_interrupt(vgt, reg);
+	else
+		return gen8_translate_pipe_interrupt(vgt, reg);
+}
+
+
+/* =======================IRR/IMR/IER handlers===================== */
+
+/* Now we have physical mask bits generated by ANDing virtual
+ * mask bits from all VMs. That means, the event is physically unmasked
+ * as long as a VM wants it. This is safe because we still use a single
+ * big lock for all critical paths, but not efficient.
+ */
+u32 vgt_recalculate_mask_bits(struct pgt_device *pdev, unsigned int reg)
+{
+	int i;
+	u32 imr = 0xffffffff;
+	u32 mapped_interrupt;
+
+	ASSERT(spin_is_locked(&pdev->lock));
+	for (i = 0; i < VGT_MAX_VMS; i++) {
+		if (pdev->device[i]) {
+			mapped_interrupt = pipe_mapping_interrupt_virt_to_phys(
+					pdev->device[i], reg);
+			imr &= mapped_interrupt;
+		}
+	}
+
+	return imr;
+}
+
+/*
+ * Now we have physical enabling bits generated by ORing virtual
+ * enabling bits from all VMs. That means, the event is physically enabled
+ * as long as a VM wants it. This is safe because we still use a single
+ * big lock for all critical paths, but not efficient.
+ */
+u32 vgt_recalculate_ier(struct pgt_device *pdev, unsigned int reg)
+{
+	int i;
+	u32 ier = 0;
+	u32 mapped_interrupt;
+
+	ASSERT(spin_is_locked(&pdev->lock));
+	for (i = 0; i < VGT_MAX_VMS; i++) {
+		if (pdev->device[i]) {
+			mapped_interrupt = pipe_mapping_interrupt_virt_to_phys(
+					pdev->device[i], reg);
+			ier |= mapped_interrupt;
+		}
+	}
+
+	return ier;
+}
+
+static enum vgt_irq_type irq_reg_to_info(struct pgt_device *pdev, vgt_reg_t reg)
+{
+	enum vgt_irq_type irq_type;
+
+	switch (reg) {
+	case _REG_GTIMR:
+	case _REG_GTIIR:
+	case _REG_GTIER:
+	case _REG_GTISR:
+	case _REG_RCS_IMR:
+	case _REG_BCS_IMR:
+	case _REG_VCS_IMR:
+	case _REG_VECS_IMR:
+		irq_type = IRQ_INFO_GT;
+		break;
+	case _REG_DEIMR:
+	case _REG_DEIIR:
+	case _REG_DEIER:
+	case _REG_DEISR:
+		irq_type = IRQ_INFO_DPY;
+		break;
+	case _REG_SDEIMR:
+	case _REG_SDEIIR:
+	case _REG_SDEIER:
+	case _REG_SDEISR:
+		irq_type = IRQ_INFO_PCH;
+		break;
+	case _REG_PMIMR:
+	case _REG_PMIIR:
+	case _REG_PMIER:
+	case _REG_PMISR:
+		irq_type = IRQ_INFO_PM;
+		break;
+	default:
+		irq_type = IRQ_INFO_MAX;
+		break;
+	}
+	return irq_type;
+}
+
+void recalculate_and_update_imr(struct pgt_device *pdev, vgt_reg_t reg)
+{
+	struct vgt_irq_host_state *hstate = pdev->irq_hstate;
+	enum vgt_irq_type irq_type;
+	u32 new_imr;
+
+	new_imr = vgt_recalculate_mask_bits(pdev, reg);
+	/*
+	 * unmask the default bits and update imr
+	 */
+	if (irq_based_ctx_switch) {
+		irq_type = irq_reg_to_info(pdev, reg);
+		ASSERT(irq_type != IRQ_INFO_MAX);
+		new_imr &= ~(hstate->info[irq_type]->default_enabled_events);
+	}
+
+	vgt_write_cached_interrupt_register(pdev, reg, new_imr);
+}
+
+static inline struct vgt_irq_info *regbase_to_irq_info(struct pgt_device *pdev,
+		unsigned int reg)
+{
+	struct vgt_irq_host_state *hstate = pdev->irq_hstate;
+	int i;
+
+	for_each_set_bit(i, hstate->irq_info_bitmap, IRQ_INFO_MAX) {
+		if (hstate->info[i]->reg_base == reg)
+			return hstate->info[i];
+	}
+
+	return NULL;
+}
+
+/* general write handler for all imr registers */
+bool vgt_reg_imr_handler(struct vgt_device *vgt,
+	unsigned int reg, void *p_data, unsigned int bytes)
+{
+	uint32_t changed, masked, unmasked;
+	uint32_t imr = *(u32 *)p_data;
+	struct pgt_device *pdev = vgt->pdev;
+	struct vgt_irq_ops *ops = vgt_get_irq_ops(pdev);
+
+	vgt_dbg(VGT_DBG_IRQ, "IRQ: capture IMR write on reg (%x) with val (%x)\n",
+		reg, imr);
+
+	vgt_dbg(VGT_DBG_IRQ, "IRQ: old vIMR(%x), pIMR(%x)\n",
+		 __vreg(vgt, reg), VGT_MMIO_READ(pdev, reg));
+
+	/* figure out newly masked/unmasked bits */
+	changed = __vreg(vgt, reg) ^ imr;
+	if (reg == _REG_DEIMR)
+		changed &= ~_REGBIT_MASTER_INTERRUPT;
+	masked = (__vreg(vgt, reg) & changed) ^ changed;
+	unmasked = masked ^ changed;
+
+	vgt_dbg(VGT_DBG_IRQ, "IRQ: changed (%x), masked(%x), unmasked (%x)\n",
+		changed, masked, unmasked);
+
+	__vreg(vgt, reg) = imr;
+
+	if (changed || device_is_reseting(pdev))
+		recalculate_and_update_imr(pdev, reg);
+
+	ops->check_pending_irq(vgt);
+	vgt_dbg(VGT_DBG_IRQ, "IRQ: new vIMR(%x), pIMR(%x)\n",
+		 __vreg(vgt, reg), VGT_MMIO_READ(pdev, reg));
+	return true;
+}
+
+void recalculate_and_update_ier(struct pgt_device *pdev, vgt_reg_t reg)
+{
+	struct vgt_irq_host_state *hstate = pdev->irq_hstate;
+	enum vgt_irq_type irq_type;
+	u32 new_ier;
+
+	new_ier = vgt_recalculate_ier(pdev, reg);
+
+	/*
+	 * enable the default bits and update ier
+	 */
+	if (irq_based_ctx_switch) {
+		irq_type = irq_reg_to_info(pdev, reg);
+		ASSERT(irq_type != IRQ_INFO_MAX);
+		new_ier |= hstate->info[irq_type]->default_enabled_events;
+	}
+
+	if (device_is_reseting(pdev)) {
+		if (IS_BDWPLUS(pdev)) {
+			if (reg == _REG_MASTER_IRQ)
+				new_ier &= ~_REGBIT_MASTER_IRQ_CONTROL;
+		} else {
+			if (reg == _REG_DEIER)
+				new_ier &= ~_REGBIT_MASTER_INTERRUPT;
+		}
+	}
+
+	vgt_write_cached_interrupt_register(pdev, reg, new_ier);
+}
+
+bool vgt_reg_master_irq_handler(struct vgt_device *vgt,
+	unsigned int reg, void *p_data, unsigned int bytes)
+{
+	uint32_t changed, enabled, disabled;
+	uint32_t ier = *(u32 *)p_data;
+	uint32_t virtual_ier = __vreg(vgt, reg);
+	struct pgt_device *pdev = vgt->pdev;
+	struct vgt_irq_ops *ops = vgt_get_irq_ops(pdev);
+
+	vgt_dbg(VGT_DBG_IRQ, "IRQ: capture master irq write on reg (%x) with val (%x)\n",
+		reg, ier);
+
+	vgt_dbg(VGT_DBG_IRQ, "IRQ: old vreg(%x), preg(%x)\n",
+		 __vreg(vgt, reg), VGT_MMIO_READ(pdev, reg));
+
+	if (likely(vgt_track_nest) && !vgt->vgt_id &&
+		__get_cpu_var(in_vgt) != 1) {
+		vgt_err("i915 virq happens in nested vgt context(%d)!!!\n",
+			__get_cpu_var(in_vgt));
+		ASSERT(0);
+	}
+
+	/*
+	 * _REG_MASTER_IRQ is a special irq register,
+	 * only bit 31 is allowed to be modified
+	 * and treated as an IER bit.
+	 */
+	ier &= _REGBIT_MASTER_IRQ_CONTROL;
+	virtual_ier &= _REGBIT_MASTER_IRQ_CONTROL;
+	__vreg(vgt, reg) &= ~_REGBIT_MASTER_IRQ_CONTROL;
+	__vreg(vgt, reg) |= ier;
+
+	/* figure out newly enabled/disable bits */
+	changed = virtual_ier ^ ier;
+	enabled = (virtual_ier & changed) ^ changed;
+	disabled = enabled ^ changed;
+
+	vgt_dbg(VGT_DBG_IRQ, "vGT_IRQ: changed (%x), enabled(%x), disabled(%x)\n",
+			changed, enabled, disabled);
+
+	if (changed || device_is_reseting(pdev))
+		recalculate_and_update_ier(pdev, reg);
+
+	ops->check_pending_irq(vgt);
+	vgt_dbg(VGT_DBG_IRQ, "IRQ: new vreg(%x), preg(%x)\n",
+		 __vreg(vgt, reg), VGT_MMIO_READ(pdev, reg));
+	return true;
+}
+
+/* general write handler for all level-1 ier registers */
+bool vgt_reg_ier_handler(struct vgt_device *vgt,
+	unsigned int reg, void *p_data, unsigned int bytes)
+{
+	uint32_t changed, enabled, disabled;
+	uint32_t ier = *(u32 *)p_data;
+	struct pgt_device *pdev = vgt->pdev;
+	struct vgt_irq_ops *ops = vgt_get_irq_ops(pdev);
+	struct vgt_irq_info *info;
+
+	vgt_dbg(VGT_DBG_IRQ, "IRQ: capture IER write on reg (%x) with val (%x)\n",
+		reg, ier);
+
+	vgt_dbg(VGT_DBG_IRQ, "IRQ: old vIER(%x), pIER(%x)\n",
+		 __vreg(vgt, reg), VGT_MMIO_READ(pdev, reg));
+
+	if (likely(vgt_track_nest) && !vgt->vgt_id &&
+		__get_cpu_var(in_vgt) != 1) {
+		vgt_err("i915 virq happens in nested vgt context(%d)!!!\n",
+			__get_cpu_var(in_vgt));
+		ASSERT(0);
+	}
+
+	/* figure out newly enabled/disable bits */
+	changed = __vreg(vgt, reg) ^ ier;
+	enabled = (__vreg(vgt, reg) & changed) ^ changed;
+	disabled = enabled ^ changed;
+
+	vgt_dbg(VGT_DBG_IRQ, "vGT_IRQ: changed (%x), enabled(%x), disabled(%x)\n",
+			changed, enabled, disabled);
+	__vreg(vgt, reg) = ier;
+
+	info = regbase_to_irq_info(pdev, ier_to_regbase(reg));
+	if (!info)
+		return false;
+
+	if (info->has_upstream_irq)
+		update_upstream_irq(vgt, info);
+
+	if (changed || device_is_reseting(pdev))
+		recalculate_and_update_ier(pdev, reg);
+
+	ops->check_pending_irq(vgt);
+	vgt_dbg(VGT_DBG_IRQ, "IRQ: new vIER(%x), pIER(%x)\n",
+		 __vreg(vgt, reg), VGT_MMIO_READ(pdev, reg));
+	return true;
+}
+
+bool vgt_reg_iir_handler(struct vgt_device *vgt, unsigned int reg,
+	void *p_data, unsigned int bytes)
+{
+	struct vgt_irq_info *info = regbase_to_irq_info(vgt->pdev, iir_to_regbase(reg));
+	vgt_reg_t iir = *(vgt_reg_t *)p_data;
+
+	vgt_dbg(VGT_DBG_IRQ, "IRQ: capture IIR write on reg (%x) with val (%x)\n",
+		reg, iir);
+
+	if (!info)
+		return false;
+
+	/* TODO: need use an atomic operation. Now it's safe due to big lock */
+	__vreg(vgt, reg) &= ~iir;
+
+	if (info->has_upstream_irq)
+		update_upstream_irq(vgt, info);
+
+	return true;
+}
+
+bool vgt_reg_isr_read(struct vgt_device *vgt, unsigned int reg,
+	void *p_data, unsigned int bytes)
+{
+	vgt_reg_t isr_value;
+	if (is_current_display_owner(vgt) && reg == _REG_SDEISR) {
+		isr_value = VGT_MMIO_READ(vgt->pdev, _REG_SDEISR);
+		memcpy(p_data, (char *)&isr_value, bytes);
+		return true;
+	} else {
+		return default_mmio_read(vgt, reg, p_data, bytes);
+	}
+}
+
+bool vgt_reg_isr_write(struct vgt_device *vgt, unsigned int reg,
+	void *p_data, unsigned int bytes)
+{
+	vgt_dbg(VGT_DBG_IRQ, "IRQ: capture ISR write on reg (%x) with val (%x)." \
+		" Will be ignored!\n", reg, *(vgt_reg_t *)p_data);
+
+	return true;
+}
+
+#define IIR_WRITE_MAX	5
+
+static bool process_irq(struct vgt_irq_host_state *hstate,
+		struct vgt_irq_info *info)
+{
+	struct pgt_device *pdev = hstate->pdev;
+	u32 val;
+	u32 reg;
+	int count = 0;
+
+	if (info->group == IRQ_INFO_MASTER)
+		reg = info->reg_base;
+	else
+		reg = regbase_to_iir(info->reg_base);
+
+	val = VGT_MMIO_READ(pdev, reg);
+	if (!val)
+		return false;
+
+	vgt_handle_events(hstate, &val, info);
+
+	if (reg != _REG_SDEIIR) {
+		if (info->group != IRQ_INFO_MASTER)
+			VGT_MMIO_WRITE(pdev, reg, val);
+	} else {
+		while((count < IIR_WRITE_MAX) && (val != 0)) {
+			VGT_MMIO_WRITE(pdev, _REG_SDEIIR, val);
+			val = VGT_MMIO_READ(pdev, _REG_SDEIIR);
+			count ++;
+		}
+	}
+
+	return true;
+}
+
+struct vgt_irq_map snb_irq_map[] = {
+	{ IRQ_INFO_DPY, 21, IRQ_INFO_PCH, ~0 },
+	{ -1, -1, -1, ~0},
+};
+
+struct vgt_irq_map base_irq_map[] = {
+	{ IRQ_INFO_DPY, 28, IRQ_INFO_PCH, ~0 },
+	{ -1, -1, -1, ~0},
+};
+
+struct vgt_irq_map gen8_irq_map[] = {
+	{ IRQ_INFO_MASTER, 0, IRQ_INFO_GT0, 0xffff },
+	{ IRQ_INFO_MASTER, 1, IRQ_INFO_GT0, 0xffff0000 },
+	{ IRQ_INFO_MASTER, 2, IRQ_INFO_GT1, 0xffff },
+	{ IRQ_INFO_MASTER, 3, IRQ_INFO_GT1, 0xffff0000 },
+	{ IRQ_INFO_MASTER, 4, IRQ_INFO_GT2, 0xffff },
+	{ IRQ_INFO_MASTER, 6, IRQ_INFO_GT3, 0xffff },
+	{ IRQ_INFO_MASTER, 16, IRQ_INFO_DE_PIPE_A, ~0 },
+	{ IRQ_INFO_MASTER, 17, IRQ_INFO_DE_PIPE_B, ~0 },
+	{ IRQ_INFO_MASTER, 18, IRQ_INFO_DE_PIPE_C, ~0 },
+	{ IRQ_INFO_MASTER, 20, IRQ_INFO_DE_PORT, ~0 },
+	{ IRQ_INFO_MASTER, 22, IRQ_INFO_DE_MISC, ~0 },
+	{ IRQ_INFO_MASTER, 23, IRQ_INFO_PCH, ~0 },
+	{ IRQ_INFO_MASTER, 30, IRQ_INFO_PCU, ~0 },
+	{ -1, -1, ~0 },
+};
+
+static void process_downstream_irq(struct vgt_irq_host_state *hstate,
+		struct vgt_irq_info *info, int bit)
+{
+	struct vgt_irq_map *map;
+
+	for (map = hstate->irq_map; map->up_irq_bit != -1; map++) {
+		if (map->up_irq_group != info->group || map->up_irq_bit != bit)
+			continue;
+
+		process_irq(hstate, hstate->info[map->down_irq_group]);
+	}
+}
+
+static void update_upstream_irq(struct vgt_device *vgt,
+		struct vgt_irq_info *info)
+{
+	struct vgt_irq_host_state *hstate = vgt->pdev->irq_hstate;
+	struct vgt_irq_map *map = hstate->irq_map;
+	struct vgt_irq_info *up_irq_info = NULL;
+	u32 set_bits = 0;
+	u32 clear_bits = 0;
+	int bit;
+	u32 val = __vreg(vgt, regbase_to_iir(info->reg_base))
+			& __vreg(vgt, regbase_to_ier(info->reg_base));
+
+	if (!info->has_upstream_irq)
+		return;
+
+	for (map = hstate->irq_map; map->up_irq_bit != -1; map++) {
+		if (info->group != map->down_irq_group)
+			continue;
+
+		if (!up_irq_info)
+			up_irq_info = hstate->info[map->up_irq_group];
+		else
+			ASSERT(up_irq_info == hstate->info[map->up_irq_group]);
+
+		bit = map->up_irq_bit;
+
+		if (val & map->down_irq_bitmask)
+			set_bits |= (1 << bit);
+		else
+			clear_bits |= (1 << bit);
+	}
+
+	ASSERT(up_irq_info);
+
+	if (up_irq_info->group == IRQ_INFO_MASTER) {
+		u32 isr = up_irq_info->reg_base;
+		__vreg(vgt, isr) &= ~clear_bits;
+		__vreg(vgt, isr) |= set_bits;
+	} else {
+		u32 iir = regbase_to_iir(up_irq_info->reg_base);
+		u32 imr = regbase_to_imr(up_irq_info->reg_base);
+		__vreg(vgt, iir) |= (set_bits & ~__vreg(vgt, imr));
+	}
+
+	if (up_irq_info->has_upstream_irq)
+		update_upstream_irq(vgt, up_irq_info);
+}
+
+static void vgt_irq_map_init(struct vgt_irq_host_state *hstate)
+{
+	struct vgt_irq_map *map;
+	struct vgt_irq_info *up_info, *down_info;
+	int up_bit;
+
+	for (map = hstate->irq_map; map->up_irq_bit != -1; map++) {
+		up_info = hstate->info[map->up_irq_group];
+		up_bit = map->up_irq_bit;
+		down_info = hstate->info[map->down_irq_group];
+
+		set_bit(up_bit, up_info->downstream_irq_bitmap);
+		down_info->has_upstream_irq = true;
+
+		printk("vGT: irq map [upstream] group: %d, bit: %d -> [downstream] group: %d, bitmask: 0x%x\n",
+			up_info->group, up_bit, down_info->group, map->down_irq_bitmask);
+	}
+}
+
+/* =======================vEvent injection===================== */
+
+DEFINE_PER_CPU(unsigned long, delay_event_bitmap);
+static unsigned long next_avail_delay_event = 1;
+
+static void *delay_event_timers[BITS_PER_LONG];
+
+static bool vgt_check_delay_event(void *timer)
+{
+	int bit;
+
+	if (!vgt_delay_nest || !hypervisor_check_host()
+			|| !vgt_enabled || !__get_cpu_var(in_vgt))
+		return true;
+
+	if (timer == NULL) {
+		bit = 0;
+	} else {
+		for (bit = 1; bit < next_avail_delay_event; bit++)
+			if (delay_event_timers[bit] == timer)
+				break;
+
+		if (bit == next_avail_delay_event) {
+			vgt_warn("Unknown delay timer event: %p\n", timer);
+			return true;
+		}
+	}
+
+	set_bit(bit, &__get_cpu_var(delay_event_bitmap));
+	return false;
+}
+
+bool vgt_can_process_irq(void)
+{
+	return vgt_check_delay_event(NULL);
+}
+
+bool vgt_can_process_timer(void *timer)
+{
+	return vgt_check_delay_event(timer);
+}
+
+void vgt_new_delay_event_timer(void *timer)
+{
+	if (next_avail_delay_event == ARRAY_SIZE(delay_event_timers)) {
+		vgt_warn("cannot allocate new delay event timer.\n");
+		return;
+	}
+
+	delay_event_timers[next_avail_delay_event] = timer;
+	next_avail_delay_event++;
+
+	return;
+}
+
+static void vgt_flush_delay_events(void)
+{
+	int bit;
+
+	for_each_set_bit(bit, &__get_cpu_var(delay_event_bitmap), BITS_PER_LONG) {
+		if (bit == next_avail_delay_event)
+			break;
+
+		clear_bit(bit, &__get_cpu_var(delay_event_bitmap));
+
+		if (bit == 0) {
+			struct pgt_device *pdev = &default_device;
+			int i915_irq = pdev->irq_hstate->i915_irq;
+			vgt_host_irq(i915_irq);
+		} else {
+			struct timer_list *t = delay_event_timers[bit];
+			if (t)
+				mod_timer(t, jiffies);
+		}
+	}
+
+	return;
+}
+
+/*
+ * dom0 virtual interrupt can only be pended here. Immediate
+ * injection at this point may cause race condition on nested
+ * lock, regardless of whether the target vcpu is the current
+ * or not.
+ */
+static void pend_dom0_virtual_interrupt(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	int i915_irq = pdev->irq_hstate->i915_irq;
+
+	ASSERT(spin_is_locked(&pdev->lock));
+
+	/*
+	 * Some wired devices leave dirty IIR bits before system
+	 * booting. It will trigger unexpected interrupt injection
+	 * before VGT irq framework works.
+	 */
+	if (!pdev->irq_hstate->installed)
+		return;
+
+	if (unlikely(!vgt_track_nest)) {
+		vgt_host_irq(i915_irq);
+		return;
+	}
+
+	if (pdev->dom0_irq_pending)
+		return;
+
+	/*
+	 * set current cpu to do delayed check, wchih may
+	 * trigger ipi call function but at this piont irq
+	 * may be disabled already.
+	 */
+	pdev->dom0_irq_cpu = smp_processor_id();
+	wmb();
+	pdev->dom0_irq_pending = true;
+
+	/* TODO: may do a kick here */
+}
+
+/*
+ * actual virq injection happens here. called in vgt_exit()
+ * or IPI handler
+ */
+static void do_inject_dom0_virtual_interrupt(void *info, int ipi)
+{
+	unsigned long flags;
+	struct pgt_device *pdev = &default_device;
+	int i915_irq;
+	int this_cpu;
+
+	if (ipi)
+		clear_bit(0, &pdev->dom0_ipi_irq_injecting);
+
+	/* still in vgt. the injection will happen later */
+	if (__get_cpu_var(in_vgt))
+		return;
+
+	spin_lock_irqsave(&pdev->lock, flags);
+	if (!pdev->dom0_irq_pending) {
+		spin_unlock_irqrestore(&pdev->lock, flags);
+		return;
+	}
+
+	ASSERT(pdev->dom0_irq_cpu != -1);
+	this_cpu = smp_processor_id();
+	if (this_cpu != pdev->dom0_irq_cpu) {
+		spin_unlock_irqrestore(&pdev->lock, flags);
+		return;
+	}
+
+	i915_irq = pdev->irq_hstate->i915_irq;
+
+	//TODO: remove the logic used for injecting irq to dom0!
+	pdev->dom0_irq_pending = false;
+	wmb();
+	pdev->dom0_irq_cpu = -1;
+
+	spin_unlock_irqrestore(&pdev->lock, flags);
+	vgt_host_irq(i915_irq);
+}
+
+void inject_dom0_virtual_interrupt(void *info)
+{
+	if (vgt_delay_nest)
+		vgt_flush_delay_events();
+
+	do_inject_dom0_virtual_interrupt(info, 0);
+
+	return;
+}
+
+static int vgt_inject_virtual_interrupt(struct vgt_device *vgt)
+{
+	if (vgt->vm_id)
+		hypervisor_inject_msi(vgt);
+	else
+		pend_dom0_virtual_interrupt(vgt);
+
+	vgt->stat.irq_num++;
+	vgt->stat.last_injection = get_cycles();
+	return 0;
+}
+
+static void vgt_propagate_event(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event, struct vgt_device *vgt)
+{
+	int bit;
+	struct vgt_irq_info *info;
+	unsigned int reg_base;
+
+	info = vgt_get_irq_info(hstate, event);
+	if (!info) {
+		vgt_err("IRQ(%d): virt-inject: no irq reg info!!!\n",
+			vgt->vm_id);
+		return;
+	}
+
+	reg_base = info->reg_base;
+	bit = hstate->events[event].bit;
+
+	/*
+         * this function call is equivalent to a rising edge ISR
+         * TODO: need check 2nd level IMR for render events
+         */
+	if (!test_bit(bit, (void*)vgt_vreg(vgt, regbase_to_imr(reg_base)))) {
+		vgt_dbg(VGT_DBG_IRQ, "IRQ: set bit (%d) for (%s) for VM (%d)\n",
+			bit, vgt_irq_name[event], vgt->vm_id);
+		set_bit(bit, (void*)vgt_vreg(vgt, regbase_to_iir(reg_base)));
+	}
+}
+
+/* =======================vEvent Handlers===================== */
+
+static void vgt_handle_default_event_virt(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event, struct vgt_device *vgt)
+{
+	if (!vgt_irq_warn_once[vgt->vgt_id][event]) {
+		vgt_info("IRQ: VM(%d) receive event %d (%s)\n",
+			vgt->vm_id, event, vgt_irq_name[event]);
+		vgt_irq_warn_once[vgt->vgt_id][event] = 1;
+	}
+	vgt_propagate_event(hstate, event, vgt);
+	vgt->stat.events[event]++;
+}
+
+static void vgt_handle_ring_empty_notify_virt(struct vgt_irq_host_state *hstate,
+       enum vgt_event_type event, struct vgt_device *vgt)
+{
+	vgt_check_pending_context_switch(vgt);
+	vgt_handle_default_event_virt(hstate, event, vgt);
+}
+
+static void vgt_handle_phase_in_virt(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event, struct vgt_device *vgt)
+{
+	__vreg(vgt, _REG_BLC_PWM_CTL2) |= _REGBIT_PHASE_IN_IRQ_STATUS;
+	vgt_handle_default_event_virt(hstate, event, vgt);
+}
+
+static void vgt_handle_histogram_virt(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event, struct vgt_device *vgt)
+{
+	__vreg(vgt, _REG_HISTOGRAM_THRSH) |= _REGBIT_HISTOGRAM_IRQ_STATUS;
+	vgt_handle_default_event_virt(hstate, event, vgt);
+}
+
+static void vgt_handle_crt_hotplug_virt(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event, struct vgt_device *vgt)
+{
+	/* update channel status */
+	if (__vreg(vgt, _REG_PCH_ADPA) & _REGBIT_ADPA_CRT_HOTPLUG_ENABLE) {
+
+		if (!is_current_display_owner(vgt)) {
+			__vreg(vgt, _REG_PCH_ADPA) &=
+				~_REGBIT_ADPA_CRT_HOTPLUG_MONITOR_MASK;
+			if (dpy_has_monitor_on_port(vgt, PORT_E))
+				__vreg(vgt, _REG_PCH_ADPA) |=
+					_REGBIT_ADPA_CRT_HOTPLUG_MONITOR_MASK;
+		}
+
+		vgt_handle_default_event_virt(hstate, event, vgt);
+	}
+}
+
+static void vgt_handle_port_hotplug_virt(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event, struct vgt_device *vgt)
+{
+	vgt_reg_t enable_mask, status_mask;
+
+	if (event == DP_B_HOTPLUG) {
+		enable_mask = _REGBIT_DP_B_ENABLE;
+		status_mask = _REGBIT_DP_B_STATUS;
+	} else if (event == DP_C_HOTPLUG) {
+		enable_mask = _REGBIT_DP_C_ENABLE;
+		status_mask = _REGBIT_DP_C_STATUS;
+	} else {
+		ASSERT(event == DP_D_HOTPLUG);
+		enable_mask = _REGBIT_DP_D_ENABLE;
+		status_mask = _REGBIT_DP_D_STATUS;
+	}
+
+	if (__vreg(vgt, _REG_SHOTPLUG_CTL) & enable_mask) {
+
+		__vreg(vgt, _REG_SHOTPLUG_CTL) &= ~status_mask;
+		if (is_current_display_owner(vgt)) {
+			__vreg(vgt, _REG_SHOTPLUG_CTL) |=
+				vgt_get_event_val(hstate, event) & status_mask;
+		} else {
+			__vreg(vgt, _REG_SHOTPLUG_CTL) |= status_mask;
+		}
+
+		vgt_handle_default_event_virt(hstate, event, vgt);
+	}
+}
+
+static inline enum vgt_ring_id event_to_ring_id(enum vgt_event_type event)
+{
+	enum vgt_ring_id ring_id;
+
+	switch(event) {
+	case RCS_AS_CONTEXT_SWITCH:
+		ring_id = RING_BUFFER_RCS;
+		break;
+	case VCS_AS_CONTEXT_SWITCH:
+		ring_id = RING_BUFFER_VCS;
+		break;
+	case VCS2_AS_CONTEXT_SWITCH:
+		ring_id = RING_BUFFER_VCS2;
+		break;
+	case BCS_AS_CONTEXT_SWITCH:
+		ring_id = RING_BUFFER_BCS;
+		break;
+	case VECS_AS_CONTEXT_SWITCH:
+		ring_id = RING_BUFFER_VECS;
+		break;
+	default:
+		ring_id = MAX_ENGINES;
+		BUG();
+	}
+	return ring_id;
+}
+
+static void vgt_handle_ctx_switch_virt(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event, struct vgt_device *vgt)
+{
+	enum vgt_ring_id ring_id;
+	uint32_t ctx_ptr_reg;
+	struct ctx_st_ptr_format ctx_ptr_val;
+	int v_write_ptr;
+	int s_write_ptr;
+	bool csb_has_new_updates = false;
+
+	ring_id = event_to_ring_id(event);
+	ctx_ptr_reg = el_ring_mmio(ring_id, _EL_OFFSET_STATUS_PTR);
+	ctx_ptr_val.dw = __vreg(vgt, ctx_ptr_reg);
+	v_write_ptr = ctx_ptr_val.status_buf_write_ptr;
+	s_write_ptr = vgt->rb[ring_id].csb_write_ptr;
+
+	if (v_write_ptr != s_write_ptr)
+		csb_has_new_updates = true;
+
+	if (hvm_render_owner || csb_has_new_updates) {
+		ctx_ptr_val.status_buf_write_ptr = s_write_ptr;
+		__vreg(vgt, ctx_ptr_reg) = ctx_ptr_val.dw;
+		vgt_handle_default_event_virt(hstate, event, vgt);
+	}
+}
+
+static enum vgt_event_type translate_physical_event(struct vgt_device *vgt,
+	enum vgt_event_type event)
+{
+	enum vgt_pipe virtual_pipe = I915_MAX_PIPES;
+	enum vgt_pipe physical_pipe = I915_MAX_PIPES;
+	enum vgt_event_type virtual_event = event;
+	int i;
+
+	switch (event) {
+	case PIPE_A_VSYNC:
+	case PIPE_A_LINE_COMPARE:
+	case PIPE_A_VBLANK:
+	case PRIMARY_A_FLIP_DONE:
+	case SPRITE_A_FLIP_DONE:
+		physical_pipe = PIPE_A;
+		break;
+
+	case PIPE_B_VSYNC:
+	case PIPE_B_LINE_COMPARE:
+	case PIPE_B_VBLANK:
+	case PRIMARY_B_FLIP_DONE:
+	case SPRITE_B_FLIP_DONE:
+		physical_pipe = PIPE_B;
+		break;
+
+	case PIPE_C_VSYNC:
+	case PIPE_C_LINE_COMPARE:
+	case PIPE_C_VBLANK:
+	case PRIMARY_C_FLIP_DONE:
+	case SPRITE_C_FLIP_DONE:
+		physical_pipe = PIPE_C;
+		break;
+	default:
+		physical_pipe = I915_MAX_PIPES;
+	}
+
+	for (i = 0; i < I915_MAX_PIPES; i++) {
+		if (vgt->pipe_mapping[i] == physical_pipe) {
+			virtual_pipe = i;
+			break;
+		}
+	}
+
+	if (virtual_pipe != I915_MAX_PIPES && physical_pipe  != I915_MAX_PIPES) {
+		virtual_event = event + ((int)virtual_pipe - (int)physical_pipe);
+	}
+
+	return virtual_event;
+}
+
+
+/* =======================pEvent Handlers===================== */
+
+static void vgt_handle_default_event_phys(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event)
+{
+	if (!vgt_irq_warn_once[VGT_MAX_VMS][event]) {
+		vgt_info("IRQ: receive event (%s)\n",
+				vgt_irq_name[event]);
+		vgt_irq_warn_once[VGT_MAX_VMS][event] = 1;
+	}
+}
+
+static void vgt_handle_phase_in_phys(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event)
+{
+	uint32_t val;
+	struct pgt_device *pdev = hstate->pdev;
+
+	val = VGT_MMIO_READ(pdev, _REG_BLC_PWM_CTL2);
+	val &= ~_REGBIT_PHASE_IN_IRQ_STATUS;
+	VGT_MMIO_WRITE(pdev, _REG_BLC_PWM_CTL2, val);
+
+	vgt_handle_default_event_phys(hstate, event);
+}
+
+static void vgt_handle_histogram_phys(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event)
+{
+	uint32_t val;
+	struct pgt_device *pdev = hstate->pdev;
+
+	val = VGT_MMIO_READ(pdev, _REG_HISTOGRAM_THRSH);
+	val &= ~_REGBIT_HISTOGRAM_IRQ_STATUS;
+	VGT_MMIO_WRITE(pdev, _REG_HISTOGRAM_THRSH, val);
+
+	vgt_handle_default_event_phys(hstate, event);
+}
+
+/*
+ * It's said that CRT hotplug detection through below method does not
+ * always work. For example in Linux i915 not hotplug handler is installed
+ * for CRT (likely through some other polling method). But let's use this
+ * as the example for how hotplug event is generally handled here.
+ */
+static void vgt_handle_crt_hotplug_phys(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event)
+{
+	vgt_reg_t adpa_ctrl;
+	struct pgt_device *pdev = hstate->pdev;
+
+	adpa_ctrl = VGT_MMIO_READ(pdev, _REG_PCH_ADPA);
+	if (!(adpa_ctrl & _REGBIT_ADPA_DAC_ENABLE)) {
+		vgt_warn("IRQ: captured CRT hotplug event when CRT is disabled\n");
+	}
+
+	/* check blue/green channel status for attachment status */
+	if (adpa_ctrl & _REGBIT_ADPA_CRT_HOTPLUG_MONITOR_MASK) {
+		vgt_info("IRQ: detect crt insert event!\n");
+		vgt_set_uevent(vgt_dom0, CRT_HOTPLUG_IN);
+	} else {
+		vgt_info("IRQ: detect crt removal event!\n");
+		vgt_set_uevent(vgt_dom0, CRT_HOTPLUG_OUT);
+	}
+
+	if (propagate_monitor_to_guest)
+		vgt_set_uevent(vgt_dom0, VGT_DETECT_PORT_E);
+
+	/* send out udev events when handling physical interruts */
+	vgt_raise_request(pdev, VGT_REQUEST_UEVENT);
+
+	vgt_handle_default_event_phys(hstate, event);
+}
+
+static void vgt_handle_port_hotplug_phys(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event)
+{
+	vgt_reg_t hotplug_ctrl;
+	vgt_reg_t enable_mask, status_mask, tmp;
+	enum vgt_uevent_type hotplug_event;
+	enum vgt_uevent_type detect_event;
+	struct pgt_device *pdev = hstate->pdev;
+
+	if (event == DP_B_HOTPLUG) {
+		enable_mask = _REGBIT_DP_B_ENABLE;
+		status_mask = _REGBIT_DP_B_STATUS;
+		hotplug_event = PORT_B_HOTPLUG_IN;
+		detect_event = VGT_DETECT_PORT_B;
+	} else if (event == DP_C_HOTPLUG) {
+		enable_mask = _REGBIT_DP_C_ENABLE;
+		status_mask = _REGBIT_DP_C_STATUS;
+		hotplug_event = PORT_C_HOTPLUG_IN;
+		detect_event = VGT_DETECT_PORT_C;
+	} else {
+		ASSERT(event == DP_D_HOTPLUG);
+		enable_mask = _REGBIT_DP_D_ENABLE;
+		status_mask = _REGBIT_DP_D_STATUS;
+		hotplug_event = PORT_D_HOTPLUG_IN;
+		detect_event = VGT_DETECT_PORT_D;
+	}
+
+	hotplug_ctrl = VGT_MMIO_READ(pdev, _REG_SHOTPLUG_CTL);
+
+	if (!(hotplug_ctrl & enable_mask)) {
+		vgt_warn("IRQ: captured port hotplug event when HPD is disabled\n");
+	}
+
+	tmp = hotplug_ctrl & ~(_REGBIT_DP_B_STATUS |
+				_REGBIT_DP_C_STATUS |
+				_REGBIT_DP_D_STATUS);
+	tmp |= hotplug_ctrl & status_mask;
+	/* write back value to clear specific port status */
+	VGT_MMIO_WRITE(pdev, _REG_SHOTPLUG_CTL, tmp);
+
+	if (hotplug_ctrl & status_mask) {
+		vgt_info("IRQ: detect monitor insert event on port!\n");
+		vgt_set_uevent(vgt_dom0, hotplug_event);
+	} else {
+		vgt_info("IRQ: detect monitor removal eventon port!\n");
+		vgt_set_uevent(vgt_dom0, hotplug_event + 1);
+	}
+
+	if (propagate_monitor_to_guest)
+		vgt_set_uevent(vgt_dom0, detect_event);
+
+	vgt_set_event_val(hstate, event, hotplug_ctrl);
+	/* send out udev events when handling physical interruts */
+	vgt_raise_request(pdev, VGT_REQUEST_UEVENT);
+
+	vgt_handle_default_event_phys(hstate, event);
+}
+
+static void vgt_handle_ctx_switch_phys(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event)
+{
+	uint32_t ctx_ptr_reg;
+	struct ctx_st_ptr_format ctx_st_ptr;
+	struct pgt_device *pdev = hstate->pdev;
+	enum vgt_ring_id ring_id = event_to_ring_id(event);
+
+	ctx_ptr_reg = el_ring_mmio(ring_id, _EL_OFFSET_STATUS_PTR);
+	ctx_st_ptr.dw = VGT_MMIO_READ(pdev, ctx_ptr_reg);
+	el_write_ptr(pdev, ring_id) = ctx_st_ptr.status_buf_write_ptr;
+
+	vgt_raise_request(pdev, VGT_REQUEST_CTX_EMULATION_RCS + ring_id);
+
+	vgt_handle_default_event_phys(hstate, event);
+}
+
+/* =====================GEN specific logic======================= */
+
+/*
+ * Here we only check IIR/IER. IMR/ISR is not checked
+ * because only rising-edge of ISR is captured as an event,
+ * so that current value of vISR doesn't matter.
+ */
+static void vgt_base_check_pending_irq(struct vgt_device *vgt)
+{
+	struct vgt_irq_host_state *hstate = vgt->pdev->irq_hstate;
+	struct vgt_irq_info *info = hstate->info[IRQ_INFO_PCH];
+
+	if (!(__vreg(vgt, _REG_DEIER) & _REGBIT_MASTER_INTERRUPT))
+		return;
+
+	if ((__vreg(vgt, regbase_to_iir(info->reg_base))
+				& __vreg(vgt, regbase_to_ier(info->reg_base))))
+		update_upstream_irq(vgt, info);
+
+	/* then check 1st level pending events */
+	if ((__vreg(vgt, _REG_DEIIR) & __vreg(vgt, _REG_DEIER)) ||
+	    (__vreg(vgt, _REG_GTIIR) & __vreg(vgt, _REG_GTIER)) ||
+	    (__vreg(vgt, _REG_PMIIR) & __vreg(vgt, _REG_PMIER))) {
+		vgt_inject_virtual_interrupt(vgt);
+	}
+}
+
+/* base interrupt handler, for snb/ivb/hsw */
+static irqreturn_t vgt_base_irq_handler(struct vgt_irq_host_state *hstate)
+{
+	struct pgt_device *pdev = hstate->pdev;
+	bool rc = false;
+
+	vgt_dbg(VGT_DBG_IRQ, "IRQ: receive interrupt (de-%x, gt-%x, pch-%x, pm-%x)\n",
+			VGT_MMIO_READ(pdev, _REG_DEIIR),
+			VGT_MMIO_READ(pdev, _REG_GTIIR),
+			VGT_MMIO_READ(pdev, _REG_SDEIIR),
+			VGT_MMIO_READ(pdev, _REG_PMIIR));
+
+	rc |= process_irq(hstate, hstate->info[IRQ_INFO_GT]);
+	rc |= process_irq(hstate, hstate->info[IRQ_INFO_DPY]);
+	rc |= process_irq(hstate, hstate->info[IRQ_INFO_PM]);
+
+	return rc ? IRQ_HANDLED : IRQ_NONE;
+}
+
+/* SNB/IVB/HSW share the similar interrupt register scheme */
+static struct vgt_irq_info vgt_base_gt_info = {
+	.name = "GT-IRQ",
+	.reg_base = _REG_GTISR,
+	.bit_to_event = {[0 ... VGT_IRQ_BITWIDTH-1] = EVENT_RESERVED},
+};
+
+static struct vgt_irq_info vgt_base_dpy_info = {
+	.name = "DPY-IRQ",
+	.reg_base = _REG_DEISR,
+	.bit_to_event = {[0 ... VGT_IRQ_BITWIDTH-1] = EVENT_RESERVED},
+};
+
+static struct vgt_irq_info vgt_base_pch_info = {
+	.name = "PCH-IRQ",
+	.reg_base = _REG_SDEISR,
+	.bit_to_event = {[0 ... VGT_IRQ_BITWIDTH-1] = EVENT_RESERVED},
+};
+
+static struct vgt_irq_info vgt_base_pm_info = {
+	.name = "PM-IRQ",
+	.reg_base = _REG_PMISR,
+	.bit_to_event = {[0 ... VGT_IRQ_BITWIDTH-1] = EVENT_RESERVED},
+};
+
+/* associate gen specific register bits to general events */
+/* TODO: add all hardware bit definitions */
+static void vgt_base_init_irq(
+	struct vgt_irq_host_state *hstate)
+{
+	struct pgt_device *pdev = hstate->pdev;
+
+#define SET_BIT_INFO(s, b, e, i)		\
+	do {					\
+		s->events[e].bit = b;		\
+		s->events[e].info = s->info[i];	\
+		s->info[i]->bit_to_event[b] = e;\
+	} while (0);
+
+#define SET_DEFAULT_ENABLED_EVENTS(s, e, i)			      \
+	set_bit(s->events[e].bit, &(s->info[i]->default_enabled_events));\
+
+#define SET_IRQ_GROUP(s, g, i) \
+	do { \
+		s->info[g] = i; \
+		(i)->group = g; \
+		set_bit(g, s->irq_info_bitmap); \
+	} while (0);
+
+	hstate->pipe_mask = REGBIT_INTERRUPT_PIPE_MASK;
+
+	SET_IRQ_GROUP(hstate, IRQ_INFO_GT, &vgt_base_gt_info);
+	SET_IRQ_GROUP(hstate, IRQ_INFO_DPY, &vgt_base_dpy_info);
+	SET_IRQ_GROUP(hstate, IRQ_INFO_PM, &vgt_base_pm_info);
+	SET_IRQ_GROUP(hstate, IRQ_INFO_PCH, &vgt_base_pch_info);
+
+	/* Render events */
+	SET_BIT_INFO(hstate, 0, RCS_MI_USER_INTERRUPT, IRQ_INFO_GT);
+	SET_BIT_INFO(hstate, 4, RCS_PIPE_CONTROL, IRQ_INFO_GT);
+	SET_BIT_INFO(hstate, 12, VCS_MI_USER_INTERRUPT, IRQ_INFO_GT);
+	SET_BIT_INFO(hstate, 16, VCS_MI_FLUSH_DW, IRQ_INFO_GT);
+	SET_BIT_INFO(hstate, 22, BCS_MI_USER_INTERRUPT, IRQ_INFO_GT);
+	SET_BIT_INFO(hstate, 26, BCS_MI_FLUSH_DW, IRQ_INFO_GT);
+	/* No space in GT, so put it in PM */
+	SET_BIT_INFO(hstate, 13, VECS_MI_FLUSH_DW, IRQ_INFO_PM);
+
+	/* Display events */
+	if (IS_IVB(pdev) || IS_HSW(pdev)) {
+		SET_BIT_INFO(hstate, 0, PIPE_A_VBLANK, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 3, PRIMARY_A_FLIP_DONE, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 4, SPRITE_A_FLIP_DONE, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 5, PIPE_B_VBLANK, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 8, PRIMARY_B_FLIP_DONE, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 9, SPRITE_B_FLIP_DONE, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 10, PIPE_C_VBLANK, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 13, PRIMARY_C_FLIP_DONE, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 14, SPRITE_C_FLIP_DONE, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 24, DPST_PHASE_IN, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 25, DPST_HISTOGRAM, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 26, AUX_CHANNEL_A, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 27, DP_A_HOTPLUG, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 29, GSE, IRQ_INFO_DPY);
+	} else if (IS_SNB(pdev)) {
+		SET_BIT_INFO(hstate, 7, PIPE_A_VBLANK, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 15, PIPE_B_VBLANK, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 16, DPST_PHASE_IN, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 17, DPST_HISTOGRAM, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 18, GSE, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 19, DP_A_HOTPLUG, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 20, AUX_CHANNEL_A, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 26, PRIMARY_A_FLIP_DONE, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 27, PRIMARY_B_FLIP_DONE, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 28, SPRITE_A_FLIP_DONE, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 29, SPRITE_B_FLIP_DONE, IRQ_INFO_DPY);
+	}
+
+	/* PM events */
+	SET_BIT_INFO(hstate, 1, GV_DOWN_INTERVAL, IRQ_INFO_PM);
+	SET_BIT_INFO(hstate, 2, GV_UP_INTERVAL, IRQ_INFO_PM);
+	SET_BIT_INFO(hstate, 4, RP_DOWN_THRESHOLD, IRQ_INFO_PM);
+	SET_BIT_INFO(hstate, 5, RP_UP_THRESHOLD, IRQ_INFO_PM);
+	SET_BIT_INFO(hstate, 6, FREQ_DOWNWARD_TIMEOUT_RC6, IRQ_INFO_PM);
+	SET_BIT_INFO(hstate, 24, PCU_THERMAL, IRQ_INFO_PM);
+	SET_BIT_INFO(hstate, 25, PCU_PCODE2DRIVER_MAILBOX, IRQ_INFO_PM);
+
+	/* PCH events */
+	SET_BIT_INFO(hstate, 17, GMBUS, IRQ_INFO_PCH);
+	SET_BIT_INFO(hstate, 19, CRT_HOTPLUG, IRQ_INFO_PCH);
+	SET_BIT_INFO(hstate, 21, DP_B_HOTPLUG, IRQ_INFO_PCH);
+	SET_BIT_INFO(hstate, 22, DP_C_HOTPLUG, IRQ_INFO_PCH);
+	SET_BIT_INFO(hstate, 23, DP_D_HOTPLUG, IRQ_INFO_PCH);
+	SET_BIT_INFO(hstate, 25, AUX_CHENNEL_B, IRQ_INFO_PCH);
+	SET_BIT_INFO(hstate, 26, AUX_CHENNEL_C, IRQ_INFO_PCH);
+	SET_BIT_INFO(hstate, 27, AUX_CHENNEL_D, IRQ_INFO_PCH);
+
+	SET_DEFAULT_ENABLED_EVENTS(hstate, RCS_MI_USER_INTERRUPT, IRQ_INFO_GT);
+	SET_DEFAULT_ENABLED_EVENTS(hstate, RCS_PIPE_CONTROL, IRQ_INFO_GT);
+	SET_DEFAULT_ENABLED_EVENTS(hstate, VCS_MI_USER_INTERRUPT, IRQ_INFO_GT);
+	SET_DEFAULT_ENABLED_EVENTS(hstate, VCS_MI_FLUSH_DW, IRQ_INFO_GT);
+	SET_DEFAULT_ENABLED_EVENTS(hstate, BCS_MI_USER_INTERRUPT, IRQ_INFO_GT);
+
+	if (IS_HSW(pdev))
+		SET_DEFAULT_ENABLED_EVENTS(hstate, VECS_MI_FLUSH_DW, IRQ_INFO_PM);
+
+}
+
+static void vgt_base_disable_irq(struct vgt_irq_host_state *hstate)
+{
+	struct pgt_device *pdev = hstate->pdev;
+
+	ASSERT(spin_is_locked(&pdev->irq_lock));
+
+	VGT_MMIO_WRITE(pdev, _REG_DEIER,
+			VGT_MMIO_READ(pdev, _REG_DEIER) & ~_REGBIT_MASTER_INTERRUPT);
+}
+
+static void vgt_base_enable_irq(struct vgt_irq_host_state *hstate)
+{
+	struct pgt_device *pdev = hstate->pdev;
+
+	ASSERT(spin_is_locked(&pdev->irq_lock));
+
+	VGT_MMIO_WRITE(pdev, _REG_DEIER,
+			VGT_MMIO_READ(pdev, _REG_DEIER) | _REGBIT_MASTER_INTERRUPT);
+}
+
+struct vgt_irq_ops vgt_base_irq_ops = {
+	.irq_handler = vgt_base_irq_handler,
+	.init_irq = vgt_base_init_irq,
+	.check_pending_irq = vgt_base_check_pending_irq,
+	.disable_irq = vgt_base_disable_irq,
+	.enable_irq = vgt_base_enable_irq,
+};
+
+/* GEN8 interrupt routines. */
+
+#define DEFINE_VGT_GEN8_IRQ_INFO(regname, regbase) \
+       static struct vgt_irq_info vgt_gen8_##regname##_info = { \
+               .name = #regname"-IRQ", \
+               .reg_base = regbase, \
+               .bit_to_event = {[0 ... VGT_IRQ_BITWIDTH-1] = EVENT_RESERVED}, \
+       };
+
+DEFINE_VGT_GEN8_IRQ_INFO(gt0, _REG_GT_ISR(0));
+DEFINE_VGT_GEN8_IRQ_INFO(gt1, _REG_GT_ISR(1));
+DEFINE_VGT_GEN8_IRQ_INFO(gt2, _REG_GT_ISR(2));
+DEFINE_VGT_GEN8_IRQ_INFO(gt3, _REG_GT_ISR(3));
+DEFINE_VGT_GEN8_IRQ_INFO(de_pipe_a, _REG_DE_PIPE_ISR(PIPE_A));
+DEFINE_VGT_GEN8_IRQ_INFO(de_pipe_b, _REG_DE_PIPE_ISR(PIPE_B));
+DEFINE_VGT_GEN8_IRQ_INFO(de_pipe_c, _REG_DE_PIPE_ISR(PIPE_C));
+DEFINE_VGT_GEN8_IRQ_INFO(de_port, _REG_DE_PORT_ISR);
+DEFINE_VGT_GEN8_IRQ_INFO(de_misc, _REG_DE_MISC_ISR);
+DEFINE_VGT_GEN8_IRQ_INFO(pcu, _REG_PCU_ISR);
+DEFINE_VGT_GEN8_IRQ_INFO(master, _REG_MASTER_IRQ);
+
+static void vgt_gen8_check_pending_irq(struct vgt_device *vgt)
+{
+	struct vgt_irq_host_state *hstate = vgt->pdev->irq_hstate;
+	int i;
+
+	if (!(__vreg(vgt, _REG_MASTER_IRQ) &
+				_REGBIT_MASTER_IRQ_CONTROL))
+		return;
+
+	for_each_set_bit(i, hstate->irq_info_bitmap, IRQ_INFO_MAX) {
+		struct vgt_irq_info *info = hstate->info[i];
+
+		if (!info->has_upstream_irq)
+			continue;
+
+		if ((__vreg(vgt, regbase_to_iir(info->reg_base))
+					& __vreg(vgt, regbase_to_ier(info->reg_base))))
+			update_upstream_irq(vgt, info);
+	}
+
+	if (__vreg(vgt, _REG_MASTER_IRQ) & ~_REGBIT_MASTER_IRQ_CONTROL)
+		vgt_inject_virtual_interrupt(vgt);
+}
+
+/* GEN8 interrupt handler */
+static irqreturn_t vgt_gen8_irq_handler(struct vgt_irq_host_state *hstate)
+{
+	struct pgt_device *pdev = hstate->pdev;
+	u32 master_ctl;
+	bool rc;
+
+	master_ctl = VGT_MMIO_READ(pdev, _REG_MASTER_IRQ);
+	master_ctl &= ~_REGBIT_MASTER_IRQ_CONTROL;
+
+	if (!master_ctl)
+		return IRQ_NONE;
+
+	vgt_dbg(VGT_DBG_IRQ, "IRQ: receive interrupt master_ctl %x\n", master_ctl);
+
+	rc = process_irq(hstate, hstate->info[IRQ_INFO_MASTER]);
+
+	return rc ? IRQ_HANDLED : IRQ_NONE;
+}
+
+static void vgt_gen8_init_irq(
+		struct vgt_irq_host_state *hstate)
+{
+	struct pgt_device *pdev = hstate->pdev;
+
+	hstate->pipe_mask = REGBIT_INTERRUPT_PIPE_MASK;
+
+	hstate->info[IRQ_INFO_MASTER] = &vgt_gen8_master_info;
+	hstate->info[IRQ_INFO_MASTER]->group = IRQ_INFO_MASTER;
+
+	SET_IRQ_GROUP(hstate, IRQ_INFO_GT0, &vgt_gen8_gt0_info);
+	SET_IRQ_GROUP(hstate, IRQ_INFO_GT1, &vgt_gen8_gt1_info);
+	SET_IRQ_GROUP(hstate, IRQ_INFO_GT2, &vgt_gen8_gt2_info);
+	SET_IRQ_GROUP(hstate, IRQ_INFO_GT3, &vgt_gen8_gt3_info);
+	SET_IRQ_GROUP(hstate, IRQ_INFO_DE_PIPE_A, &vgt_gen8_de_pipe_a_info);
+	SET_IRQ_GROUP(hstate, IRQ_INFO_DE_PIPE_B, &vgt_gen8_de_pipe_b_info);
+	SET_IRQ_GROUP(hstate, IRQ_INFO_DE_PIPE_C, &vgt_gen8_de_pipe_c_info);
+	SET_IRQ_GROUP(hstate, IRQ_INFO_DE_PORT, &vgt_gen8_de_port_info);
+	SET_IRQ_GROUP(hstate, IRQ_INFO_DE_MISC, &vgt_gen8_de_misc_info);
+	SET_IRQ_GROUP(hstate, IRQ_INFO_PCU, &vgt_gen8_pcu_info);
+	SET_IRQ_GROUP(hstate, IRQ_INFO_PCH, &vgt_base_pch_info);
+
+	/* GEN8 level 2 interrupts. */
+
+	/* GEN8 interrupt GT0 events */
+	SET_BIT_INFO(hstate, 0, RCS_MI_USER_INTERRUPT, IRQ_INFO_GT0);
+	SET_BIT_INFO(hstate, 4, RCS_PIPE_CONTROL, IRQ_INFO_GT0);
+	SET_BIT_INFO(hstate, 8, RCS_AS_CONTEXT_SWITCH, IRQ_INFO_GT0);
+
+	SET_BIT_INFO(hstate, 16, BCS_MI_USER_INTERRUPT, IRQ_INFO_GT0);
+	SET_BIT_INFO(hstate, 20, BCS_MI_FLUSH_DW, IRQ_INFO_GT0);
+	SET_BIT_INFO(hstate, 24, BCS_AS_CONTEXT_SWITCH, IRQ_INFO_GT0);
+
+	/* GEN8 interrupt GT1 events */
+	SET_BIT_INFO(hstate, 0, VCS_MI_USER_INTERRUPT, IRQ_INFO_GT1);
+	SET_BIT_INFO(hstate, 4, VCS_MI_FLUSH_DW, IRQ_INFO_GT1);
+	SET_BIT_INFO(hstate, 8, VCS_AS_CONTEXT_SWITCH, IRQ_INFO_GT1);
+
+	if (IS_BDWGT3(pdev)) {
+		SET_BIT_INFO(hstate, 16, VCS2_MI_USER_INTERRUPT, IRQ_INFO_GT1);
+		SET_BIT_INFO(hstate, 20, VCS2_MI_FLUSH_DW, IRQ_INFO_GT1);
+		SET_BIT_INFO(hstate, 24, VCS2_AS_CONTEXT_SWITCH, IRQ_INFO_GT1);
+	}
+
+	/* GEN8 interrupt GT2 events */
+	SET_BIT_INFO(hstate, 1, GV_DOWN_INTERVAL, IRQ_INFO_GT2);
+	SET_BIT_INFO(hstate, 2, GV_UP_INTERVAL, IRQ_INFO_GT2);
+	SET_BIT_INFO(hstate, 4, RP_DOWN_THRESHOLD, IRQ_INFO_GT2);
+	SET_BIT_INFO(hstate, 5, RP_UP_THRESHOLD, IRQ_INFO_GT2);
+	SET_BIT_INFO(hstate, 6, FREQ_DOWNWARD_TIMEOUT_RC6, IRQ_INFO_GT2);
+
+	/* GEN8 interrupt GT3 events */
+	SET_BIT_INFO(hstate, 0, VECS_MI_USER_INTERRUPT, IRQ_INFO_GT3);
+	SET_BIT_INFO(hstate, 4, VECS_MI_FLUSH_DW, IRQ_INFO_GT3);
+	SET_BIT_INFO(hstate, 8, VECS_AS_CONTEXT_SWITCH, IRQ_INFO_GT3);
+
+	SET_BIT_INFO(hstate, 0, PIPE_A_VBLANK, IRQ_INFO_DE_PIPE_A);
+	SET_BIT_INFO(hstate, 4, PRIMARY_A_FLIP_DONE, IRQ_INFO_DE_PIPE_A);
+	SET_BIT_INFO(hstate, 5, SPRITE_A_FLIP_DONE, IRQ_INFO_DE_PIPE_A);
+
+	SET_BIT_INFO(hstate, 0, PIPE_B_VBLANK, IRQ_INFO_DE_PIPE_B);
+	SET_BIT_INFO(hstate, 4, PRIMARY_B_FLIP_DONE, IRQ_INFO_DE_PIPE_B);
+	SET_BIT_INFO(hstate, 5, SPRITE_B_FLIP_DONE, IRQ_INFO_DE_PIPE_B);
+
+	SET_BIT_INFO(hstate, 0, PIPE_C_VBLANK, IRQ_INFO_DE_PIPE_C);
+	SET_BIT_INFO(hstate, 4, PRIMARY_C_FLIP_DONE, IRQ_INFO_DE_PIPE_C);
+	SET_BIT_INFO(hstate, 5, SPRITE_C_FLIP_DONE, IRQ_INFO_DE_PIPE_C);
+
+	/* GEN8 interrupt DE PORT events */
+	SET_BIT_INFO(hstate, 0, AUX_CHANNEL_A, IRQ_INFO_DE_PORT);
+	SET_BIT_INFO(hstate, 3, DP_A_HOTPLUG, IRQ_INFO_DE_PORT);
+
+	/* GEN8 interrupt DE MISC events */
+	SET_BIT_INFO(hstate, 0, GSE, IRQ_INFO_DE_MISC);
+
+	/* PCH events */
+	SET_BIT_INFO(hstate, 17, GMBUS, IRQ_INFO_PCH);
+	SET_BIT_INFO(hstate, 19, CRT_HOTPLUG, IRQ_INFO_PCH);
+	SET_BIT_INFO(hstate, 21, DP_B_HOTPLUG, IRQ_INFO_PCH);
+	SET_BIT_INFO(hstate, 22, DP_C_HOTPLUG, IRQ_INFO_PCH);
+	SET_BIT_INFO(hstate, 23, DP_D_HOTPLUG, IRQ_INFO_PCH);
+	SET_BIT_INFO(hstate, 25, AUX_CHENNEL_B, IRQ_INFO_PCH);
+	SET_BIT_INFO(hstate, 26, AUX_CHENNEL_C, IRQ_INFO_PCH);
+	SET_BIT_INFO(hstate, 27, AUX_CHENNEL_D, IRQ_INFO_PCH);
+
+	/* GEN8 interrupt PCU events */
+	SET_BIT_INFO(hstate, 24, PCU_THERMAL, IRQ_INFO_PCU);
+	SET_BIT_INFO(hstate, 25, PCU_PCODE2DRIVER_MAILBOX, IRQ_INFO_PCU);
+
+	irq_based_ctx_switch = false;
+}
+
+static void vgt_gen8_disable_irq(struct vgt_irq_host_state *hstate)
+{
+	struct pgt_device *pdev = hstate->pdev;
+
+	ASSERT(spin_is_locked(&pdev->irq_lock));
+
+	VGT_MMIO_WRITE(pdev, _REG_MASTER_IRQ,
+			(VGT_MMIO_READ(pdev, _REG_MASTER_IRQ)
+			 & ~_REGBIT_MASTER_IRQ_CONTROL));
+	VGT_POST_READ(pdev, _REG_MASTER_IRQ);
+}
+
+static void vgt_gen8_enable_irq(struct vgt_irq_host_state *hstate)
+{
+	struct pgt_device *pdev = hstate->pdev;
+
+	ASSERT(spin_is_locked(&pdev->irq_lock));
+
+	VGT_MMIO_WRITE(pdev, _REG_MASTER_IRQ,
+			(VGT_MMIO_READ(pdev, _REG_MASTER_IRQ)
+			 | _REGBIT_MASTER_IRQ_CONTROL));
+	VGT_POST_READ(pdev, _REG_MASTER_IRQ);
+}
+
+struct vgt_irq_ops vgt_gen8_irq_ops = {
+	.irq_handler = vgt_gen8_irq_handler,
+	.init_irq = vgt_gen8_init_irq,
+	.check_pending_irq = vgt_gen8_check_pending_irq,
+	.disable_irq = vgt_gen8_disable_irq,
+	.enable_irq = vgt_gen8_enable_irq,
+};
+
+/* ======================common event logic====================== */
+
+/*
+ * Trigger a virtual event which comes from other requests like hotplug agent
+ * instead of from pirq.
+ */
+void vgt_trigger_virtual_event(struct vgt_device *vgt,
+	enum vgt_event_type event)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	struct vgt_irq_host_state *hstate = pdev->irq_hstate;
+	vgt_event_virt_handler_t handler;
+	struct vgt_irq_ops *ops = vgt_get_irq_ops(pdev);
+
+	ASSERT(spin_is_locked(&pdev->lock));
+
+	handler = vgt_get_event_virt_handler(hstate, event);
+	ASSERT(handler);
+
+	handler(hstate, event, vgt);
+
+	ops->check_pending_irq(vgt);
+}
+
+/*
+ * Forward cached physical events to VMs, invoked from kernel thread
+ */
+void vgt_forward_events(struct pgt_device *pdev)
+{
+	int i, event;
+	cycles_t delay;
+	struct vgt_irq_host_state *hstate = pdev->irq_hstate;
+	vgt_event_virt_handler_t handler;
+	struct vgt_irq_ops *ops = vgt_get_irq_ops(pdev);
+	enum vgt_event_type virtual_event;
+
+	/* WARING: this should be under lock protection */
+	//raise_ctx_sched(vgt_dom0);
+
+	pdev->stat.last_virq = get_cycles();
+	delay = pdev->stat.last_virq - pdev->stat.last_pirq;
+
+	/*
+	 * it's possible a new pirq coming before last request is handled.
+	 * or the irq may come before kthread is ready. So skip the 1st 5.
+	 */
+	if (delay > 0 && pdev->stat.irq_num > 5)
+		pdev->stat.irq_delay_cycles += delay;
+
+	ASSERT(spin_is_locked(&pdev->lock));
+	for_each_set_bit(event, hstate->pending_events, EVENT_MAX) {
+		clear_bit(event, hstate->pending_events);
+
+		handler = vgt_get_event_virt_handler(hstate, event);
+		ASSERT(handler);
+
+		switch (vgt_get_event_policy(hstate, event)) {
+		case EVENT_FW_ALL:
+			for (i = 0; i < VGT_MAX_VMS; i++) {
+				if (pdev->device[i]) {
+					virtual_event = translate_physical_event(pdev->device[i], event);
+					handler(hstate, virtual_event, pdev->device[i]);
+				}
+			}
+			break;
+		case EVENT_FW_DOM0:
+			virtual_event = translate_physical_event(vgt_dom0, event);
+			handler(hstate, virtual_event, vgt_dom0);
+			break;
+		case EVENT_FW_NONE:
+		default:
+			break;
+		}
+	}
+
+	for (i = 0; i < VGT_MAX_VMS; i++) {
+		if (pdev->device[i])
+			ops->check_pending_irq(pdev->device[i]);
+	}
+
+	pdev->stat.virq_cycles += get_cycles() - pdev->stat.last_virq;
+}
+
+inline bool vgt_need_emulated_irq(struct vgt_device *vgt, enum vgt_pipe pipe)
+{
+	bool rc = false;
+	if (vgt_has_pipe_enabled(vgt, pipe)) {
+		enum vgt_pipe phys_pipe = vgt->pipe_mapping[pipe];
+		if ((phys_pipe == I915_MAX_PIPES) ||
+			!pdev_has_pipe_enabled(vgt->pdev, phys_pipe))
+			rc = true;
+	}
+	return rc;
+}
+
+static inline void vgt_emulate_vblank(struct vgt_device *vgt,
+			enum vgt_pipe pipe)
+{
+	enum vgt_event_type vblank;
+	switch (pipe) {
+	case PIPE_A:
+		vblank = PIPE_A_VBLANK; break;
+	case PIPE_B:
+		vblank = PIPE_B_VBLANK; break;
+	case PIPE_C:
+		vblank = PIPE_C_VBLANK; break;
+	default:
+		ASSERT(0);
+	}
+
+	if (vgt_has_pipe_enabled(vgt, pipe)) {
+		enum vgt_pipe phys_pipe = vgt->pipe_mapping[pipe];
+		if ((phys_pipe == I915_MAX_PIPES) ||
+			!pdev_has_pipe_enabled(vgt->pdev, phys_pipe)) {
+			uint32_t delta = vgt->frmcount_delta[pipe];
+			vgt->frmcount_delta[pipe] = ((delta == 0xffffffff) ?
+						0 : ++ delta);
+			vgt_trigger_virtual_event(vgt, vblank);
+		}
+	}
+}
+
+/*TODO
+ * In vgt_emulate_dpy_events(), so far only one virtual virtual
+ * event is injected into VM. If more than one events are injected, we
+ * should use a new function other than vgt_trigger_virtual_event(),
+ * that new one can combine multiple virtual events into a single
+ * virtual interrupt.
+ */
+void vgt_emulate_dpy_events(struct pgt_device *pdev)
+{
+	int i;
+
+	ASSERT(spin_is_locked(&pdev->lock));
+	for (i = 0; i < VGT_MAX_VMS; i ++) {
+		struct vgt_device *vgt = pdev->device[i];
+
+		if (!vgt || is_current_display_owner(vgt))
+			continue;
+
+		vgt_emulate_vblank(vgt, PIPE_A);
+		vgt_emulate_vblank(vgt, PIPE_B);
+		vgt_emulate_vblank(vgt, PIPE_C);
+	}
+}
+
+/*
+ * Scan all pending events in the specified category, and then invoke
+ * registered handler accordingly
+ */
+static void vgt_handle_events(struct vgt_irq_host_state *hstate, void *iir,
+		struct vgt_irq_info *info)
+{
+	int bit;
+	enum vgt_event_type event;
+	vgt_event_phys_handler_t handler;
+	struct pgt_device *pdev = hstate->pdev;
+
+	ASSERT(spin_is_locked(&pdev->irq_lock));
+
+	for_each_set_bit(bit, iir, VGT_IRQ_BITWIDTH) {
+		if (test_bit(bit, info->downstream_irq_bitmap)) {
+			process_downstream_irq(hstate, info, bit);
+			continue;
+		}
+
+		event = info->bit_to_event[bit];
+		pdev->stat.events[event]++;
+
+		if (unlikely(event == EVENT_RESERVED)) {
+			if (!test_and_set_bit(bit, &info->warned))
+				vgt_err("IRQ: abandon non-registered [%s, bit-%d] event (%s)\n",
+						info->name, bit, vgt_irq_name[event]);
+			continue;
+		}
+
+		handler = vgt_get_event_phys_handler(hstate, event);
+		ASSERT(handler);
+
+		handler(hstate, event);
+		set_bit(event, hstate->pending_events);
+	}
+}
+
+/*
+ * Physical interrupt handler for Intel HD serious graphics
+ *   - handle various interrupt reasons
+ *   - may trigger virtual interrupt instances to dom0 or other VMs
+ */
+irqreturn_t vgt_interrupt(int irq, void *data)
+{
+	struct pgt_device *pdev = i915_drm_to_pgt(data);
+	struct vgt_irq_host_state *hstate = pdev->irq_hstate;
+	irqreturn_t ret;
+	int cpu;
+
+	cpu = vgt_enter();
+
+	pdev->stat.irq_num++;
+	pdev->stat.last_pirq = get_cycles();
+
+	spin_lock(&pdev->irq_lock);
+
+	/* avoid nested handling by disabling master interrupt */
+	hstate->ops->disable_irq(hstate);
+
+	ret = hstate->ops->irq_handler(hstate);
+	if (ret == IRQ_NONE) {
+		vgt_dbg(VGT_DBG_IRQ, "Spurious interrupt received (or shared vector)\n");
+		goto out;
+	}
+
+	vgt_raise_request(pdev, VGT_REQUEST_IRQ);
+
+out:
+	/* re-enable master interrupt */
+	hstate->ops->enable_irq(hstate);
+
+	spin_unlock(&pdev->irq_lock);
+
+	pdev->stat.pirq_cycles += get_cycles() - pdev->stat.last_pirq;
+
+	vgt_exit(cpu);
+	return IRQ_HANDLED;
+}
+
+/* default handler will be invoked, if not explicitly specified here */
+static void vgt_init_events(
+	struct vgt_irq_host_state *hstate)
+{
+	int i;
+
+#define SET_POLICY_ALL(h, e)	\
+	((h)->events[e].policy = EVENT_FW_ALL)
+#define SET_POLICY_DOM0(h, e)	\
+	((h)->events[e].policy = EVENT_FW_DOM0)
+#define SET_POLICY_NONE(h, e)	\
+	((h)->events[e].policy = EVENT_FW_NONE)
+#define SET_P_HANDLER(s, e, h)	\
+	((s)->events[e].p_handler = h)
+#define SET_V_HANDLER(s, e, h)	\
+	((s)->events[e].v_handler = h)
+
+	for (i = 0; i < EVENT_MAX; i++) {
+		hstate->events[i].info = NULL;
+		/* Default forwarding to all VMs (render and most display events) */
+		SET_POLICY_ALL(hstate, i);
+		hstate->events[i].p_handler = vgt_handle_default_event_phys;
+		hstate->events[i].v_handler = vgt_handle_default_event_virt;;
+	}
+
+	SET_P_HANDLER(hstate, DPST_PHASE_IN, vgt_handle_phase_in_phys);
+	SET_P_HANDLER(hstate, DPST_HISTOGRAM, vgt_handle_histogram_phys);
+	SET_P_HANDLER(hstate, CRT_HOTPLUG, vgt_handle_crt_hotplug_phys);
+	SET_P_HANDLER(hstate, DP_B_HOTPLUG, vgt_handle_port_hotplug_phys);
+	SET_P_HANDLER(hstate, DP_C_HOTPLUG, vgt_handle_port_hotplug_phys);
+	SET_P_HANDLER(hstate, DP_D_HOTPLUG, vgt_handle_port_hotplug_phys);
+
+	SET_P_HANDLER(hstate, RCS_AS_CONTEXT_SWITCH, vgt_handle_ctx_switch_phys);
+	SET_P_HANDLER(hstate, VCS_AS_CONTEXT_SWITCH, vgt_handle_ctx_switch_phys);
+	SET_P_HANDLER(hstate, VCS2_AS_CONTEXT_SWITCH, vgt_handle_ctx_switch_phys);
+	SET_P_HANDLER(hstate, BCS_AS_CONTEXT_SWITCH, vgt_handle_ctx_switch_phys);
+	SET_P_HANDLER(hstate, VECS_AS_CONTEXT_SWITCH, vgt_handle_ctx_switch_phys);
+
+	SET_V_HANDLER(hstate, DPST_PHASE_IN, vgt_handle_phase_in_virt);
+	SET_V_HANDLER(hstate, DPST_HISTOGRAM, vgt_handle_histogram_virt);
+	SET_V_HANDLER(hstate, CRT_HOTPLUG, vgt_handle_crt_hotplug_virt);
+	SET_V_HANDLER(hstate, DP_B_HOTPLUG, vgt_handle_port_hotplug_virt);
+	SET_V_HANDLER(hstate, DP_C_HOTPLUG, vgt_handle_port_hotplug_virt);
+	SET_V_HANDLER(hstate, DP_D_HOTPLUG, vgt_handle_port_hotplug_virt);
+
+	SET_V_HANDLER(hstate, RCS_AS_CONTEXT_SWITCH, vgt_handle_ctx_switch_virt);
+	SET_V_HANDLER(hstate, VCS_AS_CONTEXT_SWITCH, vgt_handle_ctx_switch_virt);
+	SET_V_HANDLER(hstate, VCS2_AS_CONTEXT_SWITCH, vgt_handle_ctx_switch_virt);
+	SET_V_HANDLER(hstate, BCS_AS_CONTEXT_SWITCH, vgt_handle_ctx_switch_virt);
+	SET_V_HANDLER(hstate, VECS_AS_CONTEXT_SWITCH, vgt_handle_ctx_switch_virt);
+
+	SET_V_HANDLER(hstate, RCS_MI_USER_INTERRUPT, vgt_handle_ring_empty_notify_virt);
+	SET_V_HANDLER(hstate, VCS_MI_USER_INTERRUPT, vgt_handle_ring_empty_notify_virt);
+	SET_V_HANDLER(hstate, BCS_MI_USER_INTERRUPT, vgt_handle_ring_empty_notify_virt);
+	SET_V_HANDLER(hstate, RCS_PIPE_CONTROL, vgt_handle_ring_empty_notify_virt);
+	SET_V_HANDLER(hstate, VCS_MI_FLUSH_DW, vgt_handle_ring_empty_notify_virt);
+	SET_V_HANDLER(hstate, VECS_MI_FLUSH_DW, vgt_handle_ring_empty_notify_virt);
+
+	/* for engine specific reset */
+	SET_POLICY_DOM0(hstate, RCS_WATCHDOG_EXCEEDED);
+	SET_POLICY_DOM0(hstate, VCS_WATCHDOG_EXCEEDED);
+
+	/* ACPI OpRegion belongs to dom0 */
+	SET_POLICY_DOM0(hstate, GSE);
+
+	/* render-p/c fully owned by Dom0 */
+	SET_POLICY_DOM0(hstate, GV_DOWN_INTERVAL);
+	SET_POLICY_DOM0(hstate, GV_UP_INTERVAL);
+	SET_POLICY_DOM0(hstate, RP_DOWN_THRESHOLD);
+	SET_POLICY_DOM0(hstate, RP_UP_THRESHOLD);
+	SET_POLICY_DOM0(hstate, FREQ_DOWNWARD_TIMEOUT_RC6);
+	SET_POLICY_DOM0(hstate, PCU_THERMAL);
+	SET_POLICY_DOM0(hstate, PCU_PCODE2DRIVER_MAILBOX);
+
+	/* Audio owned by Dom0 */
+	SET_POLICY_DOM0(hstate, AUDIO_CP_CHANGE_TRANSCODER_A);
+	SET_POLICY_DOM0(hstate, AUDIO_CP_REQUEST_TRANSCODER_A);
+	SET_POLICY_DOM0(hstate, AUDIO_CP_CHANGE_TRANSCODER_B);
+	SET_POLICY_DOM0(hstate, AUDIO_CP_REQUEST_TRANSCODER_B);
+	SET_POLICY_DOM0(hstate, AUDIO_CP_CHANGE_TRANSCODER_C);
+	SET_POLICY_DOM0(hstate, AUDIO_CP_REQUEST_TRANSCODER_C);
+
+	/* Aux Channel owned by Dom0 */
+	SET_POLICY_DOM0(hstate, AUX_CHANNEL_A);
+	SET_POLICY_DOM0(hstate, AUX_CHENNEL_B);
+	SET_POLICY_DOM0(hstate, AUX_CHENNEL_C);
+	SET_POLICY_DOM0(hstate, AUX_CHENNEL_D);
+
+	/* Monitor interfaces are controlled by XenGT driver */
+	SET_POLICY_DOM0(hstate, DP_A_HOTPLUG);
+	SET_POLICY_DOM0(hstate, DP_B_HOTPLUG);
+	SET_POLICY_DOM0(hstate, DP_C_HOTPLUG);
+	SET_POLICY_DOM0(hstate, DP_D_HOTPLUG);
+	SET_POLICY_DOM0(hstate, SDVO_B_HOTPLUG);
+	SET_POLICY_DOM0(hstate, CRT_HOTPLUG);
+
+	SET_POLICY_DOM0(hstate, GMBUS);
+}
+
+static enum hrtimer_restart vgt_dpy_timer_fn(struct hrtimer *data)
+{
+	struct vgt_emul_timer *dpy_timer;
+	struct vgt_irq_host_state *hstate;
+	struct pgt_device *pdev;
+
+	dpy_timer = container_of(data, struct vgt_emul_timer, timer);
+	hstate = container_of(dpy_timer, struct vgt_irq_host_state, dpy_timer);
+	pdev = hstate->pdev;
+
+	vgt_raise_request(pdev, VGT_REQUEST_EMUL_DPY_EVENTS);
+
+	hrtimer_add_expires_ns(&dpy_timer->timer, dpy_timer->period);
+	return HRTIMER_RESTART;
+}
+
+/*
+ * Do interrupt initialization for vGT driver
+ */
+int vgt_irq_init(struct pgt_device *pdev)
+{
+	struct vgt_irq_host_state *hstate;
+	struct vgt_emul_timer *dpy_timer;
+
+	hstate = kzalloc(sizeof(struct vgt_irq_host_state), GFP_KERNEL);
+	if (hstate == NULL)
+		return -ENOMEM;
+
+	if (IS_SNB(pdev)) {
+		hstate->ops = &vgt_base_irq_ops;
+		hstate->irq_map = snb_irq_map;
+	} else if (IS_IVB(pdev) || IS_HSW(pdev)) {
+		hstate->ops = &vgt_base_irq_ops;
+		hstate->irq_map = base_irq_map;
+	} else if (IS_BDW(pdev)) {
+		hstate->ops = &vgt_gen8_irq_ops;
+		hstate->irq_map = gen8_irq_map;
+	} else {
+		vgt_err("Unsupported device\n");
+		kfree(hstate);
+		return -EINVAL;
+	}
+
+	spin_lock_init(&pdev->irq_lock);
+
+	hstate->pdev = pdev;
+	hstate->i915_irq = -1;
+	//hstate.pirq = IRQ_INVALID;
+
+	/* common event initialization */
+	vgt_init_events(hstate);
+
+	/* gen specific initialization */
+	hstate->ops->init_irq(hstate);
+
+	vgt_irq_map_init(hstate);
+
+	pdev->irq_hstate = hstate;
+	pdev->dom0_irq_cpu = -1;
+	pdev->dom0_irq_pending = false;
+	pdev->dom0_ipi_irq_injecting = 0;
+
+	dpy_timer = &hstate->dpy_timer;
+	hrtimer_init(&dpy_timer->timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
+	dpy_timer->timer.function = vgt_dpy_timer_fn;
+	dpy_timer->period = VGT_DPY_EMUL_PERIOD;
+
+	return 0;
+}
+
+void vgt_irq_exit(struct pgt_device *pdev)
+{
+	free_irq(pdev->irq_hstate->pirq, pdev);
+	hrtimer_cancel(&pdev->irq_hstate->dpy_timer.timer);
+
+	/* TODO: recover i915 handler? */
+	//unbind_from_irq(vgt_i915_irq(pdev));
+
+	kfree(pdev->irq_hstate);
+}
+
+void *vgt_init_irq(struct pci_dev *pdev, struct drm_device *dev)
+{
+	struct pgt_device *node, *pgt = NULL;
+	int irq;
+	struct vgt_irq_host_state *hstate;
+
+	if (!hypervisor_check_host() || !vgt_enabled)
+		return NULL;
+
+	if (list_empty(&pgt_devices)) {
+		printk("vGT: no valid pgt_device registered when installing irq\n");
+		return NULL;
+	}
+
+	list_for_each_entry(node, &pgt_devices, list) {
+		if (node->pdev == pdev) {
+			pgt = node;
+			break;
+		}
+	}
+
+	if (!pgt) {
+		printk("vGT: no matching pgt_device when registering irq\n");
+		return NULL;
+	}
+
+	printk("vGT: found matching pgt_device when registering irq for dev (0x%x)\n", pdev->devfn);
+
+	hstate = pgt->irq_hstate;
+	if (hstate->installed) {
+		printk("vGT: IRQ has been installed already.\n");
+		return NULL;
+	}
+	irq = -1;
+	vgt_dbg(VGT_DBG_IRQ, "not requesting irq here!\n");
+	hstate->pirq = pdev->irq;
+	hstate->i915_irq = irq;
+
+	hstate->installed = true;
+
+	printk("vGT: allocate virq (%d) for i915, while keep original irq (%d) for vgt\n",
+		hstate->i915_irq, hstate->pirq);
+	printk("vGT: track_nest: %s\n", vgt_track_nest ? "enabled" : "disabled");
+
+	return pgt;
+}
+
+void vgt_fini_irq(struct pci_dev *pdev)
+{
+	struct pgt_device *node, *pgt = NULL;
+	struct vgt_irq_host_state *hstate;
+
+	if (!hypervisor_check_host() || !vgt_enabled)
+		return;
+
+	if (list_empty(&pgt_devices)) {
+		printk("vGT: no valid pgt_device registered when installing irq\n");
+		return;
+	}
+
+	list_for_each_entry(node, &pgt_devices, list) {
+		if (node->pdev == pdev) {
+			pgt = node;
+			break;
+		}
+	}
+
+	if (!pgt) {
+		printk("vGT: no matching pgt_device when registering irq\n");
+		return;
+	}
+
+	hstate = pgt->irq_hstate;
+	if (!hstate->installed) {
+		printk("vGT: IRQ hasn't been installed yet.\n");
+		return;
+	}
+
+	/* Mask all GEN interrupts */
+	VGT_MMIO_WRITE(pgt, _REG_DEIER,
+		VGT_MMIO_READ(pgt, _REG_DEIER) & ~_REGBIT_MASTER_INTERRUPT);
+
+	hstate->installed = false;
+}
+
+void vgt_inject_flip_done(struct vgt_device *vgt, enum vgt_pipe pipe)
+{
+	enum vgt_event_type event = EVENT_MAX;
+	if (current_foreground_vm(vgt->pdev) != vgt) {
+		if (pipe == PIPE_A) {
+			event = PRIMARY_A_FLIP_DONE;
+		} else if (pipe == PIPE_B) {
+			event = PRIMARY_B_FLIP_DONE;
+		} else if (pipe == PIPE_C) {
+			event = PRIMARY_C_FLIP_DONE;
+		}
+
+		if (event != EVENT_MAX) {
+			vgt_trigger_virtual_event(vgt, event);
+		}
+	}
+}
diff --git a/drivers/gpu/drm/i915/vgt/klog.c b/drivers/gpu/drm/i915/vgt/klog.c
new file mode 100644
index 0000000..9b94f57
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/klog.c
@@ -0,0 +1,716 @@
+/*
+ *  klog - facility to transfer buck kernel log between kernel and userspace
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of Version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Copyright (C) IBM Corporation, 2005
+ *
+ * 2005		Tom Zanussi <zanussi@us.ibm.com>
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/relay.h>
+#include <linux/debugfs.h>
+#include <linux/slab.h>
+
+/* maximum size of klog formatting buffer beyond which truncation will occur */
+#define KLOG_TMPBUF_SIZE (1024)
+/* per-cpu klog formatting temporary buffer */
+static char klog_buf[NR_CPUS][KLOG_TMPBUF_SIZE];
+
+/* This app's channel/control files will appear in /debug/klog */
+#define APP_DIR		"klog"
+
+static struct rchan *	chan;
+
+/* app data */
+static struct dentry *	dir;
+static int		logging;
+static int		mappings;
+static int		suspended;
+static size_t		dropped;
+static size_t		subbuf_size = 262144;
+static size_t		n_subbufs = 4;
+
+/* channel-management control files */
+static struct dentry	*enabled_control;
+static struct dentry	*create_control;
+static struct dentry	*subbuf_size_control;
+static struct dentry	*n_subbufs_control;
+static struct dentry	*dropped_control;
+
+/* produced/consumed control files */
+static struct dentry	*produced_control[NR_CPUS];
+static struct dentry	*consumed_control[NR_CPUS];
+
+/* control file fileop declarations */
+struct file_operations	enabled_fops;
+struct file_operations	create_fops;
+struct file_operations	subbuf_size_fops;
+struct file_operations	n_subbufs_fops;
+struct file_operations	dropped_fops;
+struct file_operations	produced_fops;
+struct file_operations	consumed_fops;
+
+/* forward declarations */
+static int create_controls(void);
+static void destroy_channel(void);
+static void remove_controls(void);
+
+/**
+ *	module init - creates channel management control files
+ *
+ *	Returns 0 on success, negative otherwise.
+ */
+int vgt_klog_init(void)
+{
+	dir = debugfs_create_dir(APP_DIR, NULL);
+	if (!dir) {
+		printk("Couldn't create relay app directory.\n");
+		return -ENOMEM;
+	}
+
+	if (create_controls()) {
+		debugfs_remove(dir);
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+void vgt_klog_cleanup(void)
+{
+	destroy_channel();
+	remove_controls();
+	if (dir)
+		debugfs_remove(dir);
+}
+
+/* Boilerplate code below here */
+
+/**
+ *	remove_channel_controls - removes produced/consumed control files
+ */
+static void remove_channel_controls(void)
+{
+	int i;
+
+	for (i = 0; i < NR_CPUS; i++) {
+		if (produced_control[i]) {
+			debugfs_remove(produced_control[i]);
+			produced_control[i] = NULL;
+			continue;
+		}
+		break;
+	}
+
+	for (i = 0; i < NR_CPUS; i++) {
+		if (consumed_control[i]) {
+			debugfs_remove(consumed_control[i]);
+			consumed_control[i] = NULL;
+			continue;
+		}
+		break;
+	}
+}
+
+/**
+ *	create_channel_controls - creates produced/consumed control files
+ *
+ *	Returns channel on success, negative otherwise.
+ */
+static int create_channel_controls(struct dentry *parent,
+				const char *base_filename,
+				struct rchan *chan)
+{
+	unsigned int i;
+	char *tmpname = kmalloc(NAME_MAX + 1, GFP_KERNEL);
+	if (!tmpname)
+		return -ENOMEM;
+
+	for_each_online_cpu(i) {
+		sprintf(tmpname, "%s%d.produced", base_filename, i);
+		produced_control[i] = debugfs_create_file(tmpname, 0, parent, chan->buf[i], &produced_fops);
+		if (!produced_control[i]) {
+			printk("Couldn't create relay control file %s.\n",
+				tmpname);
+			goto cleanup_control_files;
+		}
+
+		sprintf(tmpname, "%s%d.consumed", base_filename, i);
+		consumed_control[i] = debugfs_create_file(tmpname, 0, parent, chan->buf[i], &consumed_fops);
+		if (!consumed_control[i]) {
+			printk("Couldn't create relay control file %s.\n",
+				tmpname);
+			goto cleanup_control_files;
+		}
+	}
+	kfree(tmpname);
+	return 0;
+
+cleanup_control_files:
+	remove_channel_controls();
+	kfree(tmpname);
+	return -ENOMEM;
+}
+
+/*
+ * subbuf_start() relay callback.
+ *
+ * Defined so that we can 1) reserve padding counts in the sub-buffers, and
+ * 2) keep a count of events dropped due to the buffer-full condition.
+ */
+static int subbuf_start_handler(struct rchan_buf *buf,
+				void *subbuf,
+				void *prev_subbuf,
+				size_t prev_padding)
+{
+	if (prev_subbuf)
+		*((size_t *)prev_subbuf) = prev_padding;
+
+	if (relay_buf_full(buf)) {
+		if (!suspended) {
+			suspended = 1;
+			printk("cpu %d buffer full!!!\n", smp_processor_id());
+		}
+		dropped++;
+		return 0;
+	} else if (suspended) {
+		suspended = 0;
+		printk("cpu %d buffer no longer full.\n", smp_processor_id());
+	}
+
+	subbuf_start_reserve(buf, sizeof(size_t));
+
+	return 1;
+}
+
+/*
+ * file_create() callback.  Creates relay file in debugfs.
+ */
+static struct dentry *create_buf_file_handler(const char *filename,
+						struct dentry *parent,
+						umode_t mode,
+						struct rchan_buf *buf,
+						int *is_global)
+{
+	struct dentry *buf_file;
+
+	buf_file = debugfs_create_file(filename, mode, parent, buf,
+					&relay_file_operations);
+
+	return buf_file;
+}
+
+/*
+ * file_remove() default callback.  Removes relay file in debugfs.
+ */
+static int remove_buf_file_handler(struct dentry *dentry)
+{
+	debugfs_remove(dentry);
+
+	return 0;
+}
+
+/*
+ * relay callbacks
+ */
+static struct rchan_callbacks relay_callbacks =
+{
+	.subbuf_start = subbuf_start_handler,
+	.create_buf_file = create_buf_file_handler,
+	.remove_buf_file = remove_buf_file_handler,
+};
+
+/**
+ *	create_channel - creates channel /debug/klog/cpuXXX
+ *
+ *	Creates channel along with associated produced/consumed control files
+ *
+ *	Returns channel on success, NULL otherwise
+ */
+static struct rchan *create_channel(unsigned subbuf_size,
+					unsigned n_subbufs)
+{
+	struct rchan *chan;
+
+	printk("create_channel: subbuf_size %u, n_subbufs %u, dir %p\n", subbuf_size, n_subbufs, dir);
+
+	chan = relay_open("cpu", dir, subbuf_size,
+			n_subbufs, &relay_callbacks, NULL);
+
+	if (!chan) {
+		printk("relay app channel creation failed\n");
+		return NULL;
+	}
+
+	if (create_channel_controls(dir, "cpu", chan)) {
+		relay_close(chan);
+		return NULL;
+	}
+
+	logging = 0;
+	mappings = 0;
+	suspended = 0;
+	dropped = 0;
+
+	return chan;
+}
+
+/**
+ *	destroy_channel - destroys channel /debug/APP_DIR/cpuXXX
+ *
+ *	Destroys channel along with associated produced/consumed control files
+ */
+static void destroy_channel(void)
+{
+	if (chan) {
+		relay_close(chan);
+		chan = NULL;
+	}
+	remove_channel_controls();
+}
+
+/**
+ *	remove_controls - removes channel management control files
+ */
+static void remove_controls(void)
+{
+	if (enabled_control)
+		debugfs_remove(enabled_control);
+
+	if (subbuf_size_control)
+		debugfs_remove(subbuf_size_control);
+
+	if (n_subbufs_control)
+		debugfs_remove(n_subbufs_control);
+
+	if (create_control)
+		debugfs_remove(create_control);
+
+	if (dropped_control)
+		debugfs_remove(dropped_control);
+}
+
+/**
+ *	create_controls - creates channel management control files
+ *
+ *	Returns 0 on success, negative otherwise.
+ */
+static int create_controls(void)
+{
+	enabled_control = debugfs_create_file("enabled", 0, dir,
+						NULL, &enabled_fops);
+	if (!enabled_control) {
+		printk("Couldn't create relay control file 'enabled'.\n");
+		goto fail;
+	}
+
+	subbuf_size_control = debugfs_create_file("subbuf_size", 0, dir,
+						NULL, &subbuf_size_fops);
+	if (!subbuf_size_control) {
+		printk("Couldn't create relay control file 'subbuf_size'.\n");
+		goto fail;
+	}
+
+	n_subbufs_control = debugfs_create_file("n_subbufs", 0, dir,
+						NULL, &n_subbufs_fops);
+	if (!n_subbufs_control) {
+		printk("Couldn't create relay control file 'n_subbufs'.\n");
+		goto fail;
+	}
+
+	create_control = debugfs_create_file("create", 0, dir,
+						NULL, &create_fops);
+	if (!create_control) {
+		printk("Couldn't create relay control file 'create'.\n");
+		goto fail;
+	}
+
+	dropped_control = debugfs_create_file("dropped", 0, dir,
+						NULL, &dropped_fops);
+	if (!dropped_control) {
+		printk("Couldn't create relay control file 'dropped'.\n");
+		goto fail;
+	}
+
+	return 0;
+fail:
+	remove_controls();
+	return -1;
+}
+
+/*
+ * control file fileop definitions
+ */
+
+/*
+ * control files for relay channel management
+ */
+
+static ssize_t enabled_read(struct file *filp, char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	char buf[16];
+
+	snprintf(buf, sizeof(buf), "%d\n", logging);
+	return simple_read_from_buffer(buffer, count, ppos,
+					buf, strlen(buf));
+}
+
+static ssize_t enabled_write(struct file *filp, const char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	char buf[16];
+	char *tmp;
+	int enabled;
+
+	if (count > sizeof(buf))
+		return -EINVAL;
+
+	memset(buf, 0, sizeof(buf));
+
+	if (copy_from_user(buf, buffer, count))
+		return -EFAULT;
+
+	enabled = simple_strtol(buf, &tmp, 10);
+	if (tmp == buf)
+		return -EINVAL;
+
+	if (enabled && chan)
+		logging = 1;
+	else if (!enabled) {
+		logging = 0;
+		if (chan)
+			relay_flush(chan);
+	}
+
+	return count;
+}
+
+/*
+ * 'enabled' file operations - boolean r/w
+ *
+ *  toggles logging to the relay channel
+ */
+struct file_operations enabled_fops = {
+	.owner	=	THIS_MODULE,
+	.read	=	enabled_read,
+	.write	=	enabled_write,
+};
+
+static ssize_t create_read(struct file *filp, char __user *buffer,
+			size_t count, loff_t *ppos)
+{
+	char buf[16];
+
+	snprintf(buf, sizeof(buf), "%d\n", !!chan);
+
+	return simple_read_from_buffer(buffer, count, ppos,
+					buf, strlen(buf));
+}
+
+static ssize_t create_write(struct file *filp, const char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	char buf[16];
+	char *tmp;
+	int create;
+
+	if (count > sizeof(buf))
+		return -EINVAL;
+
+	memset(buf, 0, sizeof(buf));
+
+	if (copy_from_user(buf, buffer, count))
+		return -EFAULT;
+
+	create = simple_strtol(buf, &tmp, 10);
+	if (tmp == buf)
+		return -EINVAL;
+
+	if (create) {
+		destroy_channel();
+		chan = create_channel(subbuf_size, n_subbufs);
+		if(!chan)
+			return count;
+	} else
+		destroy_channel();
+
+	return count;
+}
+
+/*
+ * 'create' file operations - boolean r/w
+ *
+ *  creates/destroys the relay channel
+ */
+struct file_operations create_fops = {
+	.owner	=	THIS_MODULE,
+	.read	=	create_read,
+	.write	=	create_write,
+};
+
+static ssize_t subbuf_size_read(struct file *filp, char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	char buf[16];
+
+	snprintf(buf, sizeof(buf), "%zu\n", subbuf_size);
+
+	return simple_read_from_buffer(buffer, count, ppos,
+					buf, strlen(buf));
+}
+
+static ssize_t subbuf_size_write(struct file *filp, const char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	char buf[16];
+	char *tmp;
+	size_t size;
+
+	if (count > sizeof(buf))
+		return -EINVAL;
+
+	memset(buf, 0, sizeof(buf));
+
+	if (copy_from_user(buf, buffer, count))
+		return -EFAULT;
+
+	size = simple_strtol(buf, &tmp, 10);
+	if (tmp == buf)
+		return -EINVAL;
+
+	subbuf_size = size;
+
+	return count;
+}
+
+/*
+ * 'subbuf_size' file operations - r/w
+ *
+ *  gets/sets the subbuffer size to use in channel creation
+ */
+struct file_operations subbuf_size_fops = {
+	.owner =	THIS_MODULE,
+	.read =		subbuf_size_read,
+	.write =	subbuf_size_write,
+};
+
+static ssize_t n_subbufs_read(struct file *filp, char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	char buf[16];
+
+	snprintf(buf, sizeof(buf), "%zu\n", n_subbufs);
+
+	return simple_read_from_buffer(buffer, count, ppos,
+					buf, strlen(buf));
+}
+
+static ssize_t n_subbufs_write(struct file *filp, const char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	char buf[16];
+	char *tmp;
+	size_t n;
+
+	if (count > sizeof(buf))
+		return -EINVAL;
+
+	memset(buf, 0, sizeof(buf));
+
+	if (copy_from_user(buf, buffer, count))
+		return -EFAULT;
+
+	n = simple_strtol(buf, &tmp, 10);
+	if (tmp == buf)
+		return -EINVAL;
+
+	n_subbufs = n;
+
+	return count;
+}
+
+/*
+ * 'n_subbufs' file operations - r/w
+ *
+ *  gets/sets the number of subbuffers to use in channel creation
+ */
+struct file_operations n_subbufs_fops = {
+	.owner =	THIS_MODULE,
+	.read =		n_subbufs_read,
+	.write =	n_subbufs_write,
+};
+
+static ssize_t dropped_read(struct file *filp, char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	char buf[16];
+
+	snprintf(buf, sizeof(buf), "%zu\n", dropped);
+
+	return simple_read_from_buffer(buffer, count, ppos,
+					buf, strlen(buf));
+}
+
+/*
+ * 'dropped' file operations - r
+ *
+ *  gets the number of dropped events seen
+ */
+struct file_operations dropped_fops = {
+	.owner =	THIS_MODULE,
+	.read =		dropped_read,
+};
+
+
+/*
+ * control files for relay produced/consumed sub-buffer counts
+ */
+
+static int produced_open(struct inode *inode, struct file *filp)
+{
+	filp->private_data = inode->i_private;
+
+	return 0;
+}
+
+static ssize_t produced_read(struct file *filp, char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	struct rchan_buf *buf = filp->private_data;
+
+	return simple_read_from_buffer(buffer, count, ppos,
+					&buf->subbufs_produced,
+					sizeof(buf->subbufs_produced));
+}
+
+/*
+ * 'produced' file operations - r, binary
+ *
+ *  There is a .produced file associated with each per-cpu relay file.
+ *  Reading a .produced file returns the number of sub-buffers so far
+ *  produced for the associated relay buffer.
+ */
+struct file_operations produced_fops = {
+	.owner =	THIS_MODULE,
+	.open =		produced_open,
+	.read =		produced_read,
+	.llseek = default_llseek,
+};
+
+static int consumed_open(struct inode *inode, struct file *filp)
+{
+	filp->private_data = inode->i_private;
+
+	return 0;
+}
+
+static ssize_t consumed_read(struct file *filp, char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	struct rchan_buf *buf = filp->private_data;
+
+	return simple_read_from_buffer(buffer, count, ppos,
+					&buf->subbufs_consumed,
+					sizeof(buf->subbufs_consumed));
+}
+
+static ssize_t consumed_write(struct file *filp, const char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	struct rchan_buf *buf = filp->private_data;
+	size_t consumed;
+
+	if (copy_from_user(&consumed, buffer, sizeof(consumed)))
+		return -EFAULT;
+
+	relay_subbufs_consumed(buf->chan, buf->cpu, consumed);
+
+	return count;
+}
+
+/**
+ *	klog_printk - send a formatted string to the klog handler
+ *	@fmt: format string, same as printk
+ */
+
+static int new_text_line[NR_CPUS] = {1};
+void klog_printk(const char *fmt, ...)
+{
+	va_list args;
+	int tlen, len,cpu,i;
+	char *cbuf;
+	char tbuf[KLOG_TMPBUF_SIZE];
+	unsigned long flags;
+
+	unsigned long long t;
+	unsigned long nanosec_rem;
+
+	if (!logging)
+		return;
+
+	local_irq_save(flags);
+
+	cpu = smp_processor_id();
+	cbuf = klog_buf[cpu];
+
+	va_start(args, fmt);
+	len = vsnprintf(tbuf, KLOG_TMPBUF_SIZE , fmt, args);
+	va_end(args);
+
+	for (i=0; i<len; i++){
+		if (new_text_line[cpu]){
+			/* Add the current time stamp */
+			t = cpu_clock(cpu);
+			nanosec_rem = do_div(t, 1000000000);
+			tlen = sprintf(cbuf, "[%5lu.%06lu] ",
+					(unsigned long) t,
+					nanosec_rem / 1000);
+			cbuf += tlen;
+			new_text_line[cpu] = 0;
+		}
+		*cbuf++ = tbuf[i];
+		if (tbuf[i] == '\n')
+			new_text_line[cpu] = 1;
+	}
+
+	relay_write(chan, klog_buf[cpu], cbuf - klog_buf[cpu]);
+
+	local_irq_restore(flags);
+}
+
+EXPORT_SYMBOL_GPL(klog_printk);
+
+/*
+ * 'consumed' file operations - r/w, binary
+ *
+ *  There is a .consumed file associated with each per-cpu relay file.
+ *  Writing to a .consumed file adds the value written to the
+ *  subbuffers-consumed count of the associated relay buffer.
+ *  Reading a .consumed file returns the number of sub-buffers so far
+ *  consumed for the associated relay buffer.
+ */
+struct file_operations consumed_fops = {
+	.owner	=	THIS_MODULE,
+	.open	=	consumed_open,
+	.read	=	consumed_read,
+	.write	=	consumed_write,
+	.llseek	=	default_llseek,
+};
+
diff --git a/drivers/gpu/drm/i915/vgt/mmio.c b/drivers/gpu/drm/i915/vgt/mmio.c
new file mode 100644
index 0000000..61676ef
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/mmio.c
@@ -0,0 +1,798 @@
+/*
+ * MMIO virtualization framework
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/acpi.h>
+#include <linux/kthread.h>
+#include <linux/freezer.h>
+
+#include "vgt.h"
+
+#define CREATE_TRACE_POINTS
+#include "trace.h"
+
+DEFINE_HASHTABLE(vgt_mmio_table, VGT_HASH_BITS);
+
+void vgt_add_mmio_entry(struct vgt_mmio_entry *e)
+{
+	hash_add(vgt_mmio_table, &e->hlist, e->base);
+}
+
+struct vgt_mmio_entry * vgt_find_mmio_entry(unsigned int base)
+{
+	struct vgt_mmio_entry *e;
+
+	hash_for_each_possible(vgt_mmio_table, e, hlist, base) {
+		if (base == e->base)
+			return e;
+	}
+	return NULL;
+}
+
+void vgt_del_mmio_entry(unsigned int base)
+{
+	struct vgt_mmio_entry *e;
+
+	if ((e = vgt_find_mmio_entry(base))) {
+		hash_del(&e->hlist);
+		kfree(e);
+	}
+}
+
+void vgt_clear_mmio_table(void)
+{
+	int i;
+	struct hlist_node *tmp;
+	struct vgt_mmio_entry *e;
+
+	hash_for_each_safe(vgt_mmio_table, i, tmp, e, hlist)
+		kfree(e);
+
+	hash_init(vgt_mmio_table);
+}
+
+/* Default MMIO handler registration
+ * These MMIO are registered as at least 4-byte aligned
+ */
+bool vgt_register_mmio_handler(unsigned int start, int bytes,
+	vgt_mmio_read read, vgt_mmio_write write)
+{
+	int i, j, end;
+	struct vgt_mmio_entry *mht;
+
+	end = start + bytes -1;
+
+	vgt_dbg(VGT_DBG_GENERIC, "start=0x%x end=0x%x\n", start, end);
+
+	ASSERT((start & 3) == 0);
+	ASSERT(((end+1) & 3) == 0);
+
+	for ( i = start; i < end; i += 4 ) {
+		mht = vgt_find_mmio_entry(i);
+		if (mht) {
+			mht->read = read;
+			mht->write = write;
+			continue;
+		}
+
+		mht = kmalloc(sizeof(*mht), GFP_KERNEL);
+		if (mht == NULL) {
+			printk("Insufficient memory in %s\n", __FUNCTION__);
+			for (j = start; j < i; j += 4) {
+				vgt_del_mmio_entry (j);
+			}
+			BUG();
+		}
+		mht->base = i;
+
+		/*
+		 * Win7 GFX driver uses memcpy to access the vgt PVINFO regs,
+		 * hence align_bytes can be 1.
+		 */
+		if (start >= VGT_PVINFO_PAGE &&
+			start < VGT_PVINFO_PAGE + VGT_PVINFO_SIZE)
+			mht->align_bytes = 1;
+		else
+			mht->align_bytes = 4;
+
+		mht->read = read;
+		mht->write = write;
+		INIT_HLIST_NODE(&mht->hlist);
+		vgt_add_mmio_entry(mht);
+	}
+	return true;
+}
+
+static inline unsigned long vgt_get_passthrough_reg(struct vgt_device *vgt,
+		unsigned int reg)
+{
+	__sreg(vgt, reg) = VGT_MMIO_READ(vgt->pdev, reg);
+	__vreg(vgt, reg) = mmio_h2g_gmadr(vgt, reg, __sreg(vgt, reg));
+	return __vreg(vgt, reg);
+}
+
+static unsigned long vgt_get_reg(struct vgt_device *vgt, unsigned int reg)
+{
+	/* check whether to update vreg from HW */
+//	if (reg_hw_status(pdev, reg) &&
+	if (reg_hw_access(vgt, reg))
+		return vgt_get_passthrough_reg(vgt, reg);
+	else
+		return __vreg(vgt, reg);
+}
+
+static inline unsigned long vgt_get_passthrough_reg_64(struct vgt_device *vgt, unsigned int reg)
+{
+	__sreg64(vgt, reg) = VGT_MMIO_READ_BYTES(vgt->pdev, reg, 8);
+	__vreg(vgt, reg) = mmio_h2g_gmadr(vgt, reg, __sreg(vgt, reg));
+	__vreg(vgt, reg + 4) = mmio_h2g_gmadr(vgt, reg + 4, __sreg(vgt, reg + 4));
+	return __vreg64(vgt, reg);
+}
+/*
+ * for 64bit reg access, we split into two 32bit accesses since each part may
+ * require address fix
+ *
+ * TODO: any side effect with the split? or instead install specific handler
+ * for 64bit regs like fence?
+ */
+static unsigned long vgt_get_reg_64(struct vgt_device *vgt, unsigned int reg)
+{
+	/* check whether to update vreg from HW */
+//	if (reg_hw_status(pdev, reg) &&
+	if (reg_hw_access(vgt, reg))
+		return vgt_get_passthrough_reg_64(vgt, reg);
+	else
+		return __vreg64(vgt, reg);
+}
+
+static void vgt_update_reg(struct vgt_device *vgt, unsigned int reg)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	/*
+	 * update sreg if pass through;
+	 * update preg if boot_time or vgt is reg's cur owner
+	 */
+	__sreg(vgt, reg) = mmio_g2h_gmadr(vgt, reg, __vreg(vgt, reg));
+	if (reg_hw_access(vgt, reg))
+		VGT_MMIO_WRITE(pdev, reg, __sreg(vgt, reg));
+}
+
+static void vgt_update_reg_64(struct vgt_device *vgt, unsigned int reg)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	/*
+	 * update sreg if pass through;
+	 * update preg if boot_time or vgt is reg's cur owner
+	 */
+	__sreg(vgt, reg) = mmio_g2h_gmadr(vgt, reg, __vreg(vgt, reg));
+	__sreg(vgt, reg + 4) = mmio_g2h_gmadr(vgt, reg + 4, __vreg(vgt, reg + 4));
+	if (reg_hw_access(vgt, reg))
+			VGT_MMIO_WRITE_BYTES(pdev, reg, __sreg64(vgt, reg), 8);
+}
+
+bool default_mmio_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	unsigned int reg;
+	unsigned long wvalue;
+	reg = offset & ~(bytes - 1);
+
+	if (bytes <= 4) {
+		wvalue = vgt_get_reg(vgt, reg);
+	} else {
+		wvalue = vgt_get_reg_64(vgt, reg);
+	}
+
+	memcpy(p_data, &wvalue + (offset & (bytes - 1)), bytes);
+
+	return true;
+}
+
+bool default_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	memcpy((char *)vgt->state.vReg + offset,
+			p_data, bytes);
+
+	offset &= ~(bytes - 1);
+	if (bytes <= 4)
+		vgt_update_reg(vgt, offset);
+	else
+		vgt_update_reg_64(vgt, offset);
+
+	return true;
+}
+
+bool default_passthrough_mmio_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	unsigned int reg;
+	unsigned long wvalue;
+	reg = offset & ~(bytes - 1);
+
+	if (bytes <= 4) {
+		wvalue = vgt_get_passthrough_reg(vgt, reg);
+	} else {
+		wvalue = vgt_get_passthrough_reg_64(vgt, reg);
+	}
+
+	memcpy(p_data, &wvalue + (offset & (bytes - 1)), bytes);
+
+	return true;
+}
+
+unsigned int vgt_pa_to_mmio_offset(struct vgt_device *vgt,
+	uint64_t pa)
+{
+#define PCI_BAR_ADDR_MASK (~0xFUL)  /* 4 LSB bits are not address */
+	return (vgt->vm_id == 0)?
+		pa - vgt->pdev->gttmmio_base :
+		pa - ( (*(uint64_t*)(vgt->state.cfg_space + VGT_REG_CFG_SPACE_BAR0))
+				& PCI_BAR_ADDR_MASK );
+}
+
+static inline bool valid_mmio_alignment(struct vgt_mmio_entry *mht,
+		unsigned int offset, int bytes)
+{
+	if ((bytes >= mht->align_bytes) && !(offset & (bytes - 1)))
+		return true;
+	vgt_err("Invalid MMIO offset(%08x), bytes(%d)\n",offset, bytes);
+	return false;
+}
+
+/*
+ * Emulate the VGT MMIO register read ops.
+ * Return : true/false
+ * */
+bool vgt_emulate_read(struct vgt_device *vgt, uint64_t pa, void *p_data,int bytes)
+{
+	struct vgt_mmio_entry *mht;
+	struct pgt_device *pdev = vgt->pdev;
+	unsigned int offset;
+	unsigned long flags;
+	bool rc;
+	cycles_t t0, t1;
+	struct vgt_statistics *stat = &vgt->stat;
+	int cpu;
+
+	t0 = get_cycles();
+
+	vgt_lock_dev_flags(pdev, cpu, flags);
+
+	if (atomic_read(&vgt->gtt.n_write_protected_guest_page)) {
+		guest_page_t *gp;
+		gp = vgt_find_guest_page(vgt, pa >> PAGE_SHIFT);
+		if (gp) {
+			memcpy(p_data, gp->vaddr + (pa & ~PAGE_MASK), bytes);
+			vgt_unlock_dev_flags(pdev, cpu, flags);
+			return true;
+		}
+	}
+
+	offset = vgt_pa_to_mmio_offset(vgt, pa);
+
+	/* FENCE registers / GTT entries(sometimes) are accessed in 8 bytes. */
+	if (bytes > 8 || (offset & (bytes - 1)))
+		goto err_common_chk;
+
+	if (bytes > 4)
+		vgt_dbg(VGT_DBG_GENERIC,"vGT: capture >4 bytes read to %x\n", offset);
+
+	raise_ctx_sched(vgt);
+
+	if (reg_is_gtt(pdev, offset)) {
+		rc = gtt_emulate_read(vgt, offset, p_data, bytes);
+		vgt_unlock_dev_flags(pdev, cpu, flags);
+		return rc;
+	}
+
+	if (!reg_is_mmio(pdev, offset + bytes))
+		goto err_mmio;
+
+	mht = vgt_find_mmio_entry(offset);
+	if ( mht && mht->read ) {
+		if (!valid_mmio_alignment(mht, offset, bytes))
+			goto err_mmio;
+		if (!mht->read(vgt, offset, p_data, bytes))
+			goto err_mmio;
+	} else
+		if (!default_mmio_read(vgt, offset, p_data, bytes))
+			goto err_mmio;
+
+	if (!reg_is_tracked(pdev, offset) && vgt->warn_untrack) {
+		vgt_warn("vGT: untracked MMIO read: vm_id(%d), offset=0x%x,"
+			"len=%d, val=0x%x!!!\n",
+			vgt->vm_id, offset, bytes, *(u32 *)p_data);
+
+		if (offset == 0x206c) {
+			printk("------------------------------------------\n");
+			printk("VM(%d) likely triggers a gfx reset\n", vgt->vm_id);
+			printk("Disable untracked MMIO warning for VM(%d)\n", vgt->vm_id);
+			printk("------------------------------------------\n");
+			vgt->warn_untrack = 0;
+			show_debug(pdev);
+		}
+
+		//WARN_ON(vgt->vm_id == 0); /* The call stack is meaningless for HVM */
+	}
+
+	reg_set_accessed(pdev, offset);
+
+	vgt_unlock_dev_flags(pdev, cpu, flags);
+	trace_vgt_mmio_rw(VGT_TRACE_READ, vgt->vm_id, offset, p_data, bytes);
+
+	t1 = get_cycles();
+	stat->mmio_rcnt++;
+	stat->mmio_rcycles += t1 - t0;
+	return true;
+err_mmio:
+	vgt_unlock_dev_flags(pdev, cpu, flags);
+err_common_chk:
+	vgt_err("VM(%d): invalid MMIO offset(%08x), bytes(%d)!\n",
+		vgt->vm_id, offset, bytes);
+	show_debug(pdev);
+	return false;
+}
+
+/*
+ * Emulate the VGT MMIO register write ops.
+ * Return : true/false
+ * */
+bool vgt_emulate_write(struct vgt_device *vgt, uint64_t pa,
+	void *p_data, int bytes)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	struct vgt_mmio_entry *mht;
+	unsigned int offset;
+	unsigned long flags;
+	int cpu;
+	vgt_reg_t old_vreg=0, old_sreg=0;
+	bool rc;
+	cycles_t t0, t1;
+	struct vgt_statistics *stat = &vgt->stat;
+
+	vgt_lock_dev_flags(pdev, cpu, flags);
+
+	t0 = get_cycles();
+
+	if (atomic_read(&vgt->gtt.n_write_protected_guest_page)) {
+		guest_page_t *guest_page;
+		guest_page = vgt_find_guest_page(vgt, pa >> PAGE_SHIFT);
+		if (guest_page) {
+			rc = guest_page->handler(guest_page, pa, p_data, bytes);
+			vgt_unlock_dev_flags(pdev, cpu, flags);
+			return rc;
+		}
+	}
+
+	offset = vgt_pa_to_mmio_offset(vgt, pa);
+
+	/* FENCE registers / GTT entries(sometimes) are accessed in 8 bytes. */
+	if (bytes > 8 || (offset & (bytes - 1)))
+		goto err_mmio;
+
+	if (bytes > 4)
+		vgt_dbg(VGT_DBG_GENERIC,"vGT: capture >4 bytes write to %x with val (%lx)\n", offset, *(unsigned long*)p_data);
+/*
+	if (reg_rdonly(pdev, offset & (~(bytes - 1)))) {
+		printk("vGT: captured write to read-only reg (%x)\n", offset);
+		return true;
+	}
+*/
+
+	raise_ctx_sched(vgt);
+
+	if (reg_is_gtt(pdev, offset)) {
+		rc = gtt_emulate_write(vgt, offset, p_data, bytes);
+		vgt_unlock_dev_flags(pdev, cpu, flags);
+		return rc;
+	}
+
+	if (!reg_is_mmio(pdev, offset + bytes))
+		goto err_mmio;
+
+	if (reg_mode_ctl(pdev, offset)) {
+		old_vreg = __vreg(vgt, offset);
+		old_sreg = __sreg(vgt, offset);
+	}
+
+	if (!reg_is_tracked(pdev, offset) && vgt->warn_untrack) {
+		vgt_warn("vGT: untracked MMIO write: vm_id(%d), offset=0x%x,"
+			"len=%d, val=0x%x!!!\n",
+			vgt->vm_id, offset, bytes, *(u32 *)p_data);
+
+		//WARN_ON(vgt->vm_id == 0); /* The call stack is meaningless for HVM */
+	}
+
+	mht = vgt_find_mmio_entry(offset);
+	if ( mht && mht->write ) {
+		if (!valid_mmio_alignment(mht, offset, bytes))
+			goto err_mmio;
+		if (!mht->write(vgt, offset, p_data, bytes))
+			goto err_mmio;
+	} else
+		if (!default_mmio_write(vgt, offset, p_data, bytes))
+			goto err_mmio;
+
+	/* higher 16bits of mode ctl regs are mask bits for change */
+	if (reg_mode_ctl(pdev, offset)) {
+		u32 mask = __vreg(vgt, offset) >> 16;
+
+		vgt_dbg(VGT_DBG_GENERIC,"old mode (%x): %x/%x, mask(%x)\n", offset,
+			__vreg(vgt, offset), __sreg(vgt, offset),
+			reg_aux_mode_mask(pdev, offset));
+		/*
+		 * share the global mask among VMs, since having one VM touch a bit
+		 * not changed by another VM should be still saved/restored later
+		 */
+		reg_aux_mode_mask(pdev, offset) |= mask << 16;
+		__vreg(vgt, offset) = (old_vreg & ~mask) | (__vreg(vgt, offset) & mask);
+		__sreg(vgt, offset) = (old_sreg & ~mask) | (__sreg(vgt, offset) & mask);
+		vgt_dbg(VGT_DBG_GENERIC,"new mode (%x): %x/%x, mask(%x)\n", offset,
+			__vreg(vgt, offset), __sreg(vgt, offset),
+			reg_aux_mode_mask(pdev, offset));
+		//show_mode_settings(vgt->pdev);
+	}
+
+	if (offset == _REG_RCS_UHPTR)
+		vgt_dbg(VGT_DBG_GENERIC,"vGT: write to UHPTR (%x,%x)\n", __vreg(vgt, offset), __sreg(vgt, offset));
+
+	reg_set_accessed(pdev, offset);
+	vgt_unlock_dev_flags(pdev, cpu, flags);
+	trace_vgt_mmio_rw(VGT_TRACE_WRITE, vgt->vm_id, offset, p_data, bytes);
+
+	t1 = get_cycles();
+	stat->mmio_wcycles += t1 - t0;
+	stat->mmio_wcnt++;
+	return true;
+err_mmio:
+	vgt_unlock_dev_flags(pdev, cpu, flags);
+	vgt_err("VM(%d): invalid MMIO offset(0x%08x, pa:0x%016llx),"
+		"bytes(%d)!\n", vgt->vm_id, offset, pa, bytes);
+	show_debug(pdev);
+	return false;
+}
+
+static bool vgt_hvm_opregion_resinit(struct vgt_device *vgt, uint32_t gpa)
+{
+	void *orig_va = vgt->pdev->opregion_va;
+	uint8_t	*buf;
+	int i;
+
+	if (vgt->state.opregion_va) {
+		vgt_err("VM%d tried to init opregion multiple times!\n",
+				vgt->vm_id);
+		return false;
+	}
+	if (orig_va == NULL) {
+		vgt_err("VM%d: No mapped OpRegion available\n", vgt->vm_id);
+		return false;
+	}
+
+	vgt->state.opregion_va = (void *)__get_free_pages(GFP_ATOMIC |
+			GFP_DMA32 | __GFP_ZERO,
+			VGT_OPREGION_PORDER);
+	if (vgt->state.opregion_va == NULL) {
+		vgt_err("VM%d: failed to allocate memory for opregion\n",
+				vgt->vm_id);
+		return false;
+	}
+
+	memcpy_fromio(vgt->state.opregion_va, orig_va, VGT_OPREGION_SIZE);
+
+	for (i = 0; i < VGT_OPREGION_PAGES; i++)
+		vgt->state.opregion_gfn[i] = (gpa >> PAGE_SHIFT) + i;
+
+	/* for unknown reason, the value in LID field is incorrect
+	 * which block the windows guest, so workaround it by force
+	 * setting it to "OPEN"
+	 */
+	buf = (uint8_t *)vgt->state.opregion_va;
+	buf[VGT_OPREGION_REG_CLID] = 0x3;
+
+	return true;
+}
+
+int vgt_hvm_opregion_map(struct vgt_device *vgt, int map)
+{
+	void *opregion;
+	int rc;
+	int i;
+
+	opregion = vgt->state.opregion_va;
+
+	for (i = 0; i < VGT_OPREGION_PAGES; i++) {
+		rc = hypervisor_map_mfn_to_gpfn(vgt,
+			vgt->state.opregion_gfn[i],
+			hypervisor_virt_to_mfn(opregion + i*PAGE_SIZE),
+			1,
+			map);
+		if (rc != 0)
+			vgt_err("hypervisor_map_mfn_to_gpfn fail with %d!\n", rc);
+	}
+
+	return rc;
+}
+
+int vgt_hvm_opregion_init(struct vgt_device *vgt, uint32_t gpa)
+{
+	if (vgt_hvm_opregion_resinit(vgt, gpa)) {
+
+		/* modify the vbios parameters for PORTs,
+		 * Let guest see full port capability.
+		 */
+		if (!propagate_monitor_to_guest && !is_current_display_owner(vgt)) {
+			vgt_prepare_vbios_general_definition(vgt);
+		}
+
+		return vgt_hvm_opregion_map(vgt, 1);
+	}
+
+	return false;
+}
+
+void vgt_initial_opregion_setup(struct pgt_device *pdev)
+{
+	pci_read_config_dword(pdev->pdev, VGT_REG_CFG_OPREGION,
+			&pdev->opregion_pa);
+	pdev->opregion_va = acpi_os_ioremap(pdev->opregion_pa,
+			VGT_OPREGION_SIZE);
+	if (pdev->opregion_va == NULL)
+		vgt_err("Directly map OpRegion failed\n");
+}
+
+static void vgt_set_reg_attr(struct pgt_device *pdev,
+	u32 reg, reg_attr_t *attr, bool track)
+{
+	/* ensure one entry per reg */
+	ASSERT_NUM(!reg_is_tracked(pdev, reg) || !track, reg);
+
+	if (reg_is_tracked(pdev, reg)) {
+		if (track)
+			printk("vGT: init a tracked reg (%x)!!!\n", reg);
+
+		return;
+	}
+
+	reg_set_owner(pdev, reg, attr->flags & VGT_REG_OWNER);
+	if (attr->flags & VGT_REG_PASSTHROUGH)
+		reg_set_passthrough(pdev, reg);
+	if (attr->flags & VGT_REG_ADDR_FIX ) {
+		if (!attr->addr_mask)
+			printk("vGT: ZERO addr fix mask for %x\n", reg);
+		reg_set_addr_fix(pdev, reg, attr->addr_mask);
+
+		/* set the default range size to 4, might be updated later */
+		reg_aux_addr_size(pdev, reg) = 4;
+	}
+	if (attr->flags & VGT_REG_MODE_CTL)
+		reg_set_mode_ctl(pdev, reg);
+	if (attr->flags & VGT_REG_VIRT)
+		reg_set_virt(pdev, reg);
+	if (attr->flags & VGT_REG_HW_STATUS)
+		reg_set_hw_status(pdev, reg);
+
+	/* last mark the reg as tracked */
+	if (track)
+		reg_set_tracked(pdev, reg);
+}
+
+static void vgt_initialize_reg_attr(struct pgt_device *pdev,
+	reg_attr_t *info, int num, bool track)
+{
+	int i, cnt = 0, tot = 0;
+	u32 reg;
+	reg_attr_t *attr;
+
+	attr = info;
+	for (i = 0; i < num; i++, attr++) {
+		if (!vgt_match_device_attr(pdev, attr))
+			continue;
+
+		cnt++;
+		if (track)
+			vgt_dbg(VGT_DBG_GENERIC,"reg(%x): size(%x), device(%d), flags(%x), mask(%x), read(%llx), write(%llx)\n",
+				attr->reg, attr->size, attr->device,
+				attr->flags,
+				attr->addr_mask,
+				(u64)attr->read, (u64)attr->write);
+		for (reg = attr->reg;
+			reg < attr->reg + attr->size;
+			reg += REG_SIZE) {
+			vgt_set_reg_attr(pdev, reg, attr, track);
+			tot++;
+		}
+
+		if (attr->read || attr->write)
+			vgt_register_mmio_handler(attr->reg, attr->size,
+				attr->read, attr->write);
+	}
+	printk("%d listed, %d used\n", num, cnt);
+	printk("total %d registers tracked\n", tot);
+}
+
+void vgt_setup_reg_info(struct pgt_device *pdev)
+{
+	int i, reg;
+	struct vgt_mmio_entry *mht;
+	reg_addr_sz_t *reg_addr_sz;
+	reg_list_t *reg_list = vgt_get_sticky_regs(pdev);
+
+	printk("vGT: setup tracked reg info\n");
+	vgt_initialize_reg_attr(pdev, vgt_base_reg_info,
+		vgt_get_base_reg_num(), true);
+
+	/* GDRST can be accessed by byte */
+	mht = vgt_find_mmio_entry(_REG_GEN6_GDRST);
+	if (mht)
+		mht->align_bytes = 1;
+
+	for (i = 0; i < vgt_get_sticky_reg_num(pdev); i++) {
+		for (reg = reg_list[i].reg;
+		     reg < reg_list[i].reg + reg_list[i].size;
+		     reg += REG_SIZE)
+			reg_set_sticky(pdev, reg);
+	}
+
+	/* update the address range size in aux table */
+	for (i =0; i < vgt_get_reg_addr_sz_num(); i++) {
+		reg_addr_sz = &vgt_reg_addr_sz[i];
+		if (reg_addr_sz->device & vgt_gen_dev_type(pdev))
+			reg_aux_addr_size(pdev, reg_addr_sz->reg) = reg_addr_sz->size;
+	}
+}
+
+static void __vgt_initial_mmio_space (struct pgt_device *pdev,
+					reg_attr_t *info, int num)
+{
+	int i, j;
+	reg_attr_t *attr;
+
+	attr = info;
+
+	for (i = 0; i < num; i++, attr++) {
+		if (!vgt_match_device_attr(pdev, attr))
+			continue;
+
+		for (j = 0; j < attr->size; j += 4) {
+			pdev->initial_mmio_state[REG_INDEX(attr->reg + j)] =
+				VGT_MMIO_READ(pdev, attr->reg + j);
+		}
+	}
+
+}
+
+bool vgt_initial_mmio_setup (struct pgt_device *pdev)
+{
+	vgt_reg_t val;
+
+	if (!pdev->initial_mmio_state) {
+		pdev->initial_mmio_state = vzalloc(pdev->mmio_size);
+		if (!pdev->initial_mmio_state) {
+			printk("vGT: failed to allocate initial_mmio_state\n");
+			return false;
+		}
+	}
+
+	__vgt_initial_mmio_space(pdev, vgt_base_reg_info, vgt_get_base_reg_num());
+
+	/* customize the initial MMIO
+	 * 1, GMBUS status
+	 * 2, Initial port status. 
+	 */
+
+	/* GMBUS2 has an in-use bit as the hw semaphore, and we should recover
+	 * it after the snapshot.
+	 */
+	pdev->initial_mmio_state[REG_INDEX(_REG_PCH_GMBUS2)] &= ~0x8000;
+
+	val = (DEFAULT_INV_SR_PTR << _CTXBUF_READ_PTR_SHIFT) | DEFAULT_INV_SR_PTR;
+
+	pdev->initial_mmio_state[REG_INDEX(_REG_RCS_CTX_STATUS_PTR)] = val;
+	pdev->initial_mmio_state[REG_INDEX(_REG_VCS_CTX_STATUS_PTR)] = val;
+	pdev->initial_mmio_state[REG_INDEX(_REG_VECS_CTX_STATUS_PTR)] = val;
+	pdev->initial_mmio_state[REG_INDEX(_REG_VCS2_CTX_STATUS_PTR)] = val;
+	pdev->initial_mmio_state[REG_INDEX(_REG_BCS_CTX_STATUS_PTR)] = val;
+
+	val = ((_CTXBUF_READ_PTR_MASK << 16) |
+			(DEFAULT_INV_SR_PTR << _CTXBUF_READ_PTR_SHIFT));
+
+	VGT_MMIO_WRITE(pdev, _REG_RCS_CTX_STATUS_PTR, val);
+	VGT_MMIO_WRITE(pdev, _REG_VCS_CTX_STATUS_PTR, val);
+	VGT_MMIO_WRITE(pdev, _REG_VECS_CTX_STATUS_PTR, val);
+	VGT_MMIO_WRITE(pdev, _REG_VCS2_CTX_STATUS_PTR, val);
+	VGT_MMIO_WRITE(pdev, _REG_BCS_CTX_STATUS_PTR, val);
+
+	VGT_MMIO_WRITE(pdev, _REG_PCH_GMBUS2,
+			VGT_MMIO_READ(pdev, _REG_PCH_GMBUS2) | 0x8000);
+
+	vgt_dpy_init_modes(pdev->initial_mmio_state);
+
+	pdev->initial_mmio_state[REG_INDEX(_REG_WRPLL_CTL1)] &= ~(1 << 31);
+	pdev->initial_mmio_state[REG_INDEX(_REG_WRPLL_CTL2)] &= ~(1 << 31);
+
+	return true;
+}
+
+void state_vreg_init(struct vgt_device *vgt)
+{
+	int i;
+	struct pgt_device *pdev = vgt->pdev;
+
+	for (i = 0; i < pdev->mmio_size; i += sizeof(vgt_reg_t)) {
+		/*
+		 * skip the area of VGT PV INFO PAGE because we need keep
+		 * its content across Dom0 S3.
+		*/
+		if (i >= VGT_PVINFO_PAGE &&
+			i < VGT_PVINFO_PAGE + VGT_PVINFO_SIZE)
+			continue;
+
+		__vreg(vgt, i) = pdev->initial_mmio_state[i/sizeof(vgt_reg_t)];
+	}
+
+	/* set the bit 0:2 (Thread C-State) to C0
+	 * TODO: consider other bit 3:31
+	 */
+	__vreg(vgt, _REG_GT_THREAD_STATUS) = 0;
+
+	/* set the bit 0:2(Core C-State ) to C0 */
+	__vreg(vgt, _REG_GT_CORE_STATUS) = 0;
+
+	/*TODO: init other regs that need different value from pdev */
+
+	if (IS_HSW(vgt->pdev)) {
+		/*
+		 * Clear _REGBIT_FPGA_DBG_RM_NOCLAIM for not causing DOM0
+		 * or Ubuntu HVM complains about unclaimed MMIO registers.
+		 */
+		__vreg(vgt, _REG_FPGA_DBG) &= ~_REGBIT_FPGA_DBG_RM_NOCLAIM;
+	}
+}
+
+/* TODO: figure out any security holes by giving the whole initial state */
+void state_sreg_init(struct vgt_device *vgt)
+{
+	vgt_reg_t *sreg;
+
+	sreg = vgt->state.sReg;
+	memcpy (sreg, vgt->pdev->initial_mmio_state, vgt->pdev->mmio_size);
+
+	/*
+	 * Do we really need address fix for initial state? Any address information
+	 * there is meaningless to a VM, unless that address is related to allocated
+	 * GM space to the VM. Translate a host address '0' to a guest GM address
+	 * is just a joke.
+	 */
+#if 0
+	/* FIXME: add off in addr table to avoid checking all regs */
+	for (i = 0; i < vgt->pdev->reg_num; i++) {
+		if (reg_addr_fix(vgt->pdev, i * REG_SIZE)) {
+			__sreg(vgt, i) = mmio_g2h_gmadr(vgt, i, __vreg(vgt, i));
+			vgt_dbg(VGT_DBG_GENERIC,"vGT: address fix for reg (%x): (%x->%x)\n",
+				i, __vreg(vgt, i), __sreg(vgt, i));
+		}
+	}
+#endif
+}
diff --git a/drivers/gpu/drm/i915/vgt/reg.h b/drivers/gpu/drm/i915/vgt/reg.h
new file mode 100644
index 0000000..50dad26
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/reg.h
@@ -0,0 +1,1975 @@
+/*
+ * vGT core headers
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef _VGT_REG_H_
+#define _VGT_REG_H_
+
+/*
+ * Definition of MMIO registers.
+ */
+#define _VGT_MMIO_THROUGH_OFFSET(index, a, b)	((a) + (index)*((b)-(a)))
+#define _VGT_MMIO_GET_INDEX(reg, a, b)		(((reg)-(a))/((b)-(a)))
+
+#define _VGT_PIPE(pipe, a, b)		_VGT_MMIO_THROUGH_OFFSET(pipe, a, b)
+#define _VGT_PORT(port, a, b)		_VGT_MMIO_THROUGH_OFFSET(port, a, b)
+#define _VGT_TRANSCODER(tran, a, b)   _VGT_MMIO_THROUGH_OFFSET(tran, a, b)
+
+#define _VGT_GET_PIPE(reg, a, b)	_VGT_MMIO_GET_INDEX(reg, a, b)
+#define _VGT_GET_PORT(reg, a, b)	_VGT_MMIO_GET_INDEX(reg, a, b)
+
+static inline uint32_t __RING_REG(int32_t ring_id, uint32_t rcs_reg)
+{
+	if (ring_id == 0 /* RING_BUFFER_RCS */) return rcs_reg;
+	if (ring_id == 1 /* RING_BUFFER_VCS */) return rcs_reg+0x10000;
+	if (ring_id == 2 /* RING_BUFFER_BCS */) return rcs_reg+0x20000;
+	if (ring_id == 3 /* RING_BUFFER_VECS*/) return rcs_reg+0x18000;
+	if (ring_id == 4 /* RING_BUFFER_VCS2*/) return rcs_reg+0x1a000;
+	return 0; /* invalid ring_id, trigger crash */
+}
+
+#define _REG_INVALID	0xFFFFFFFF
+
+#define _MASKED_BIT_ENABLE(a) (((a) << 16) | (a))
+#define _MASKED_BIT_DISABLE(a) ((a) << 16)
+
+/*
+ * Registers used only by the command parser
+ */
+#define _REG_BCS_SWCTRL 0x22200
+
+#define _REG_HS_INVOCATION_COUNT 0x2300
+#define _REG_DS_INVOCATION_COUNT 0x2308
+#define _REG_IA_VERTICES_COUNT   0x2310
+#define _REG_IA_PRIMITIVES_COUNT 0x2318
+#define _REG_VS_INVOCATION_COUNT 0x2320
+#define _REG_GS_INVOCATION_COUNT 0x2328
+#define _REG_GS_PRIMITIVES_COUNT 0x2330
+#define _REG_CL_INVOCATION_COUNT 0x2338
+#define _REG_CL_PRIMITIVES_COUNT 0x2340
+#define _REG_PS_INVOCATION_COUNT 0x2348
+#define _REG_PS_DEPTH_COUNT      0x2350
+
+
+/* PRB0, RCS */
+#define _REG_RCS_TAIL		0x02030
+#define _REG_RCS_HEAD		0x02034
+#define _REG_RCS_START		0x02038
+#define _REG_RCS_CTL		0x0203c
+
+/* VECS: HSW+ */
+#define _REG_VECS_TAIL		0x1A030
+#define _REG_VECS_HEAD		0x1A034
+#define _REG_VECS_START		0x1A038
+#define _REG_VECS_CTL		0x1A03c
+
+/* VCS */
+#define _REG_VCS_TAIL		0x12030
+#define _REG_VCS_HEAD		0x12034
+#define _REG_VCS_START		0x12038
+#define _REG_VCS_CTL		0x1203c
+
+/* VCS2 for BDW GT3 */
+#define _REG_VCS2_TAIL		0x1C030
+#define _REG_VCS2_HEAD		0x1C034
+#define _REG_VCS2_START		0x1C038
+#define _REG_VCS2_CTL		0x1C03C
+
+/* BCS */
+#define _REG_BCS_TAIL		0x22030
+#define _REG_BCS_HEAD		0x22034
+#define _REG_BCS_START		0x22038
+#define _REG_BCS_CTL		0x2203c
+
+#define RB_OFFSET_TAIL		0
+#define RB_OFFSET_HEAD		4
+#define RB_OFFSET_START		8
+#define RB_OFFSET_CTL		0xC
+#define RB_REGS_SIZE		0x10
+
+#define RB_TAIL(pdev, id)	(pdev->ring_mmio_base[id] + RB_OFFSET_TAIL)
+#define RB_HEAD(pdev, id)	(pdev->ring_mmio_base[id] + RB_OFFSET_HEAD)
+#define RB_START(pdev, id)	(pdev->ring_mmio_base[id] + RB_OFFSET_START)
+#define RB_CTL(pdev, id)	(pdev->ring_mmio_base[id] + RB_OFFSET_CTL)
+
+#define RB_HEAD_OFF_MASK	((1U << 21) - (1U << 2))	/* bit 2 to 20 */
+#define RB_HEAD_OFF_SHIFT	2
+#define RB_TAIL_OFF_MASK	((1U << 21) - (1U << 3))	/* bit 3 to 20 */
+#define RB_TAIL_OFF_SHIFT	3
+
+#define RB_TAIL_SIZE_MASK	((1U << 21) - (1U << 12))	/* bit 12 to 20 */
+#define _RING_CTL_BUF_SIZE(ctl)	(((ctl) & RB_TAIL_SIZE_MASK) + GTT_PAGE_SIZE)
+#define _RING_CTL_ENABLE	0x1	/* bit 0 */
+#define _RING_CTL_RB_WAIT	(1 << 11)
+
+#define _REG_CCID		0x02180
+#define CCID_MBO_BITS		(1 << 8)	/* bit 8 must be one */
+#define CCID_EXTENDED_STATE_SAVE_ENABLE		(1 << 3)
+#define CCID_EXTENDED_STATE_RESTORE_ENABLE	(1 << 2)
+#define CCID_VALID		(1 << 0)
+#define _REG_CXT_SIZE		0x021a0
+#define _REG_GEN7_CXT_SIZE	0x021a8
+#define _REG_VECS_CXT_SIZE	0x1A1A8
+
+#define _REG_RCS_MI_MODE	0x209C
+#define        _REGBIT_MI_ASYNC_FLIP_PERFORMANCE_MODE	(1 << 14)
+#define        _REGBIT_MI_FLUSH_PERFORMANCE_MODE	(1 << 13)
+//#define        _REGBIT_MI_FLUSH			(3 << 11)
+#define        _REGBIT_MI_FLUSH				(1 << 12)
+#define        _REGBIT_MI_INVALIDATE_UHPTR		(1 << 11)
+#define        _REGBIT_MI_RINGS_IDLE			(1 << 9)
+#define        _REGBIT_MI_STOP_RINGS			(1 << 8)
+#define _REG_VCS_MI_MODE	0x1209C
+#define _REG_VCS2_MI_MODE	0x1c09C
+#define _REG_BCS_MI_MODE	0x2209C
+#define _REG_VECS_MI_MODE	0x1A09c
+#define _REG_GFX_MODE	0x2520
+#define        _REGBIT_FLUSH_TLB_INVALIDATION_MODE	(1 << 13)
+#define        _REGBIT_REPLAY_MODE			(1 << 11)
+#define        _REGBIT_PPGTT_ENABLE			(1 << 9)
+#define        _REGBIT_PPGTT64_ENABLE                   (1 << 7)
+#define _REG_ARB_MODE	0x4030
+#define        _REGBIT_ADDRESS_SWIZZLING		(3 << 4)
+#define _REG_GT_MODE	0x20D0
+
+#define _REG_GAC_MODE		0x120A0
+#define _REG_GAB_MODE		0x220A0
+
+#define _REG_RCS_INSTPM		0x20C0
+#define _REG_VCS_INSTPM		0x120C0
+#define _REG_VCS2_INSTPM	0x1c0C0
+#define _REG_BCS_INSTPM		0x220C0
+#define _REG_VECS_INSTPM	0x1A0C0
+#define     _REGBIT_INSTPM_SYNC_FLUSH		(1<<5)
+#define     _REGBIT_INSTPM_FORCE_ORDERING	(1<<7) /* GEN6+ */
+#define     _REGBIT_INSTPM_TLB_INVALIDATE	(1<<9)
+
+#define INSTPM_CONS_BUF_ADDR_OFFSET_DIS (1<<6)
+
+/* IVB+ */
+#define _REG_BCS_BLT_MODE_IVB	0x2229C
+#define _REG_RCS_GFX_MODE_IVB	0x0229C
+#define _REG_VCS_MFX_MODE_IVB	0x1229C
+#define _REG_VCS2_MFX_MODE_BDW	0x1c29C
+#define  _REGBIT_EXECLIST_ENABLE       (1 << 15)
+#define _REG_CACHE_MODE_0_IVB	0x7000
+#define _REG_CACHE_MODE_1_IVB	0x7004
+#define _REG_GT_MODE_IVB	0x7008
+#define _REG_VEBOX_MODE		0x1A29C
+
+#define GFX_MODE_BIT_SET_IN_MASK(val, bit) \
+		((((bit) & 0xffff0000) == 0) && !!((val) & (((bit) << 16))))
+
+/* PPGTT entry */
+#define _REGBIT_PDE_VALID	(1<<0)
+#define _REGBIT_PDE_PAGE_32K	(1<<1)
+#define _REGBIT_PTE_VALID	(1<<0)
+/* control bits except address and valid bit */
+#define _REGBIT_PTE_CTL_MASK_GEN7	0xe	/* SNB/IVB */
+#define _REGBIT_PTE_CTL_MASK_GEN7_5	0x80e	/* HSW */
+
+#define _REG_RCS_IMR		0x20A8
+#define _REG_VCS_IMR		0x120A8
+#define _REG_BCS_IMR		0x220A8
+#define _REG_VECS_IMR		0x1A0A8
+#define _REG_VCS2_IMR		0x1c0A8
+
+#define _REG_RCS_BB_ADDR	0x2140
+#define _REG_VCS_BB_ADDR	0x12140
+#define _REG_BCS_BB_ADDR	0x22140
+#define _REG_VECS_BB_ADDR	0x1A140
+#define _REG_VCS2_BB_ADDR	0x1c140
+
+#define _REG_VECS_CTX_WA_BB_ADDR 0x1A144
+
+#define _REG_RCS_HWS_PGA	0x4080
+#define _REG_VCS_HWS_PGA	0x4180
+#define _REG_BCS_HWS_PGA	0x24080
+#define _REG_BCS_HWS_PGA_GEN7	0x4280
+#define _REG_VECS_HWS_PGA	0x1A080
+#define _REG_VEBOX_HWS_PGA_GEN7	0x4380
+
+#define _REG_RCS_EXCC		0x2028
+#define _REG_VCS_EXCC		0x12028
+#define _REG_BCS_EXCC		0x22028
+#define _REG_VECS_EXCC		0x1A028
+#define _REG_VCS2_EXCC		0x1c028
+
+#define _REG_RCS_UHPTR		0x2134
+#define _REG_VCS_UHPTR		0x12134
+#define _REG_BCS_UHPTR		0x22134
+#define _REG_VECS_UHPTR		0x1A134
+#define _REG_VCS2_UHPTR		0x1c134
+#define 	_REGBIT_UHPTR_VALID	(1 << 0)
+#define VGT_UHPTR(ring_id) __RING_REG(ring_id, _REG_RCS_UHPTR)
+
+#define _REG_RCS_ACTHD		0x2074
+#define _REG_VCS_ACTHD		0x12074
+#define _REG_BCS_ACTHD		0x22074
+#define _REG_VECS_ACTHD		0x1A074
+#define _REG_VCS2_ACTHD		0x1c074
+
+#define _REG_RCS_ACTHD_UDW	0x205c
+#define _REG_VCS_ACTHD_UDW	0x1205c
+#define _REG_BCS_ACTHD_UDW	0x2205c
+#define _REG_VECS_ACTHD_UDW	0x1A05c
+#define _REG_VCS2_ACTHD_UDW	0x1c05c
+
+#define VGT_ACTHD(ring_id) __RING_REG(ring_id, _REG_RCS_ACTHD)
+
+#define _REG_RCS_HWSTAM		0x2098
+#define _REG_VCS_HWSTAM		0x12098
+#define _REG_BCS_HWSTAM		0x22098
+#define _REG_VECS_HWSTAM	0x1A098
+#define _REG_VCS2_HWSTAM	0x1c098
+
+#define _REG_RCS_BB_PREEMPT_ADDR	0x2148
+
+#define _REG_RCS_BB_ADDR_DIFF		0x2154
+#define _REG_RCS_FBC_RT_BASE_ADDR	0x2128
+#define _REG_IVB_RCS_FBC_RT_BASE_ADDR	0X7020
+
+#define _REG_RCS_PP_DIR_BASE_READ	0x2518
+#define _REG_RCS_PP_DIR_BASE_IVB	0x2228
+#define _REG_RCS_PP_DCLV		0x2220
+#define _REG_BCS_PP_DIR_BASE		0x22228
+#define _REG_BCS_PP_DCLV		0x22220
+#define _REG_VCS_PP_DIR_BASE		0x12228
+#define _REG_VCS_PP_DCLV		0x12220
+#define _REG_VECS_PP_DIR_BASE		0x1A228
+#define _REG_VECS_PP_DCLV		0x1A220
+
+#define _REG_RVSYNC		0x2040
+#define _REG_RBSYNC		0x2044
+#define _REG_RVESYNC		0x2048
+
+#define _REG_BRSYNC		0x22040
+#define _REG_BVSYNC		0x22044
+#define _REG_BVESYNC		0x22048
+
+#define _REG_VBSYNC		0x12040
+#define _REG_VRSYNC		0x12044
+#define _REG_VVESYNC		0x12048
+
+#define _REG_VEBSYNC		0x1A040
+#define _REG_VERSYNC		0x1A044
+#define _REG_VEVSYNC		0x1A048
+
+#define _REG_RCS_TIMESTAMP	0x2358
+#define _REG_VCS_TIMESTAMP	0x12358
+#define _REG_VCS2_TIMESTAMP	0x1c358
+#define _REG_BCS_TIMESTAMP	0x22358
+
+#define _EL_BASE_RCS		0x02000
+#define _EL_BASE_VCS		0x12000
+#define _EL_BASE_VECS		0x1A000
+#define _EL_BASE_VCS2		0x1C000
+#define _EL_BASE_BCS		0x22000
+
+#define _EL_OFFSET_SUBMITPORT	0x230
+#define _EL_OFFSET_STATUS	0x234
+#define _EL_OFFSET_SR_CTL	0x244
+#define _EL_OFFSET_STATUS_BUF	0x370
+#define _EL_OFFSET_STATUS_PTR	0x3A0
+
+#define _REG_RCS_EXECLIST_SUBMITPORT	0x02230
+#define _REG_VCS_EXECLIST_SUBMITPORT	0x12230
+#define _REG_VECS_EXECLIST_SUBMITPORT	0x1A230
+#define _REG_VCS2_EXECLIST_SUBMITPORT	0x1C230
+#define _REG_BCS_EXECLIST_SUBMITPORT	0x22230
+
+#define _EXECLIST_LRCA_MASK		0xfffff000
+
+#define _REG_RCS_EXECLIST_STATUS	0x02234
+#define _REG_VCS_EXECLIST_STATUS	0x12234
+#define _REG_VECS_EXECLIST_STATUS	0x1A234
+#define _REG_VCS2_EXECLIST_STATUS	0x1C234
+#define _REG_BCS_EXECLIST_STATUS	0x22234
+
+#define _REG_RCS_CTX_SR_CTL	0x02244
+#define _REG_VCS_CTX_SR_CTL	0x12244
+#define _REG_VECS_CTX_SR_CTL	0x1A244
+#define _REG_VCS2_CTX_SR_CTL	0x1C244
+#define _REG_BCS_CTX_SR_CTL	0x22244
+
+#define _REG_RCS_CTX_STATUS_BUF		0x02370
+#define _REG_VCS_CTX_STATUS_BUF		0x12370
+#define _REG_VECS_CTX_STATUS_BUF	0x1A370
+#define _REG_VCS2_CTX_STATUS_BUF	0x1C370
+#define _REG_BCS_CTX_STATUS_BUF		0x22370
+
+#define _REG_RCS_CTX_STATUS_PTR		0x023A0
+#define _REG_VCS_CTX_STATUS_PTR		0x123A0
+#define _REG_VECS_CTX_STATUS_PTR	0x1A3A0
+#define _REG_VCS2_CTX_STATUS_PTR	0x1C3A0
+#define _REG_BCS_CTX_STATUS_PTR		0x223A0
+
+#define  _CTXBUF_READ_PTR_SHIFT		8
+#define  _CTXBUF_READ_PTR_MASK		(0x7 << _CTXBUF_READ_PTR_SHIFT)
+#define  _CTXBUF_WRITE_PTR_MASK		(0x7)
+
+#define _REG_FENCE_0_LOW	0x100000
+#define 	_REGBIT_FENCE_VALID	(1 << 0)
+
+#define _REG_CURACNTR		0x70080
+#define    _CURSOR_MODE			0x3f
+#define    _CURSOR_MODE_DISABLE		0x00
+#define    _CURSOR_ALPHA_FORCE_SHIFT	8
+#define    _CURSOR_ALPHA_FORCE_MASK	(0x3 << _CURSOR_ALPHA_FORCE_SHIFT)
+#define    _CURSOR_ALPHA_PLANE_SHIFT	10
+#define    _CURSOR_ALPHA_PLANE_MASK	(0x3 << _CURSOR_ALPHA_PLANE_SHIFT)
+#define _REG_CURABASE		0x70084
+#define _REG_CURAPOS		0x70088
+#define    _CURSOR_POS_X_SHIFT		0
+#define    _CURSOR_POS_X_MASK		(0x1fff << _CURSOR_POS_X_SHIFT)
+#define    _CURSOR_SIGN_X_SHIFT	15
+#define    _CURSOR_SIGN_X_MASK		(1 << _CURSOR_SIGN_X_SHIFT)
+#define    _CURSOR_POS_Y_SHIFT		16
+#define    _CURSOR_POS_Y_MASK		(0xfff << _CURSOR_POS_Y_SHIFT)
+#define    _CURSOR_SIGN_Y_SHIFT	31
+#define    _CURSOR_SIGN_Y_MASK		(1 << _CURSOR_SIGN_Y_SHIFT)
+#define _REG_CURASURFLIVE	0x700AC
+
+#define _REG_CURAPALET_0	0x70090
+#define _REG_CURAPALET_1	0x70094
+#define _REG_CURAPALET_2	0x70098
+#define _REG_CURAPALET_3	0x7009C
+
+#define _REG_CURBCNTR_SNB	0x700C0
+#define _REG_CURBBASE_SNB	0x700C4
+#define _REG_CURBPOS_SNB	0x700C8
+#define _REG_CURBSURFLIVE_SNB	0x700EC
+
+#define _REG_CURBCNTR		0x71080
+#define _REG_CURBBASE		0x71084
+#define _REG_CURBPOS		0x71088
+#define _REG_CURBSURFLIVE	0x710AC
+
+#define _REG_CURCCNTR		0x72080
+#define _REG_CURCBASE		0x72084
+#define _REG_CURCPOS		0x72088
+#define _REG_CURCSURFLIVE	0x720AC
+
+#define VGT_CURCNTR_SNB(pipe)	_VGT_PIPE(pipe, _REG_CURACNTR, _REG_CURBCNTR_SNB)
+#define VGT_CURBASE_SNB(pipe)	_VGT_PIPE(pipe, _REG_CURABASE, _REG_CURBBASE_SNB)
+#define VGT_CURPOS_SNB(pipe)	_VGT_PIPE(pipe, _REG_CURAPOS, _REG_CURBPOS_SNB)
+
+#define VGT_CURCNTR(pipe)	_VGT_PIPE(pipe, _REG_CURACNTR, _REG_CURBCNTR)
+#define VGT_CURBASE(pipe)	_VGT_PIPE(pipe, _REG_CURABASE, _REG_CURBBASE)
+#define VGT_CURPOS(pipe)	_VGT_PIPE(pipe, _REG_CURAPOS, _REG_CURBPOS)
+
+#define _REG_DSPACNTR		0x70180
+#define    _PRI_PLANE_ENABLE		(1 << 31)
+#define    _PRI_PLANE_GAMMA_ENABLE	(1 << 30)
+#define    _PRI_PLANE_FMT_SHIFT		26
+#define    _PRI_PLANE_FMT_MASK		(0xf << _PRI_PLANE_FMT_SHIFT)
+#define    _PRI_PLANE_TRICKLE_FEED_DISABLE	(1 << 14)
+#define    _PRI_PLANE_TILE_SHIFT		10
+#define    _PRI_PLANE_TILE_MASK		(1 << _PRI_PLANE_TILE_SHIFT)
+
+#define _REG_DSPALINOFF		0x70184
+#define _REG_DSPASTRIDE		0x70188
+#define    _PRI_PLANE_STRIDE_SHIFT	6
+#define    _PRI_PLANE_STRIDE_MASK	(0x3ff << _PRI_PLANE_STRIDE_SHIFT)
+#define _REG_DSPAPOS		0x7018C /* reserved */
+#define _REG_DSPASIZE		0x70190
+#define _REG_DSPASURF		0x7019C
+#define _REG_DSPATILEOFF	0x701A4
+#define     _PRI_PLANE_X_OFF_SHIFT	0
+#define     _PRI_PLANE_X_OFF_MASK	(0x1fff << _PRI_PLANE_X_OFF_SHIFT)
+#define     _PRI_PLANE_Y_OFF_SHIFT	16
+#define     _PRI_PLANE_Y_OFF_MASK	(0xfff << _PRI_PLANE_Y_OFF_SHIFT)
+#define _REG_DSPASURFLIVE	0x701AC
+
+#define _REG_DSPBCNTR		0x71180
+#define _REG_DSPBLINOFF		0x71184
+#define _REG_DSPBSTRIDE		0x71188
+#define _REG_DSPBPOS		0x7118C
+#define _REG_DSPBSIZE		0x71190
+#define _REG_DSPBSURF		0x7119C
+#define _REG_DSPBTILEOFF	0x711A4
+#define _REG_DSPBSURFLIVE	0x711AC
+
+#define _REG_DSPCCNTR		0x72180
+#define _REG_DSPCLINOFF		0x72184
+#define _REG_DSPCSTRIDE		0x72188
+#define _REG_DSPCPOS		0x7218C
+#define _REG_DSPCSIZE		0x72190
+#define _REG_DSPCSURF		0x7219C
+#define _REG_DSPCTILEOFF	0x721A4
+#define _REG_DSPCSURFLIVE	0x721AC
+
+#define VGT_DSPSURF(pipe)	_VGT_PIPE(pipe, _REG_DSPASURF, _REG_DSPBSURF)
+#define VGT_DSPCNTR(pipe)	_VGT_PIPE(pipe, _REG_DSPACNTR, _REG_DSPBCNTR)
+#define VGT_DSPCNTRPIPE(dspcntr) _VGT_GET_PIPE(dspcntr, _REG_DSPACNTR,_REG_DSPBCNTR)
+
+#define VGT_DSPLINOFF(plane) _VGT_PIPE(plane, _REG_DSPALINOFF, _REG_DSPBLINOFF)
+#define VGT_DSPSTRIDE(plane) _VGT_PIPE(plane, _REG_DSPASTRIDE, _REG_DSPBSTRIDE)
+#define VGT_DSPTILEOFF(plane) _VGT_PIPE(plane, _REG_DSPATILEOFF, _REG_DSPBTILEOFF)
+
+#define VGT_DSPSURFPIPE(dspsurf) _VGT_GET_PIPE(dspsurf, _REG_DSPASURF,_REG_DSPBSURF)
+#define VGT_DSPSURFLIVEPIPE(dspsurf) _VGT_GET_PIPE(dspsurf, _REG_DSPASURFLIVE, \
+							_REG_DSPBSURFLIVE)
+#define VGT_DSPSURFLIVE(pipe)	_VGT_PIPE(pipe, _REG_DSPASURFLIVE, _REG_DSPBSURFLIVE)
+
+#define VGT_CURSURFPIPE(cursurf) _VGT_GET_PIPE(cursurf, _REG_CURABASE, _REG_CURBBASE)
+#define VGT_CURSURF(pipe)	_VGT_PIPE(pipe, _REG_CURABASE, _REG_CURBBASE)
+
+/* sprite */
+
+#define _REG_SPRA_CTL				0x70280
+#define    _SPRITE_ENABLE		(1 << 31)
+#define    _SPRITE_FMT_SHIFT		25
+#define    _SPRITE_FMT_MASK		(0x7 << _SPRITE_FMT_SHIFT)
+#define    _SPRITE_COLOR_ORDER_SHIFT	20
+#define    _SPRITE_COLOR_ORDER_MASK	(0x1 << _SPRITE_COLOR_ORDER_SHIFT)
+#define    _SPRITE_YUV_ORDER_SHIFT	16
+#define    _SPRITE_YUV_ORDER_MASK	(0x3 << _SPRITE_YUV_ORDER_SHIFT)
+#define    _SPRITE_TILED		(1 << 10)
+
+#define _REG_SPRA_STRIDE			0x70288
+#define    _SPRITE_STRIDE_SHIFT		6
+#define    _SPRITE_STRIDE_MASK		(0x1ff << _SPRITE_STRIDE_SHIFT)
+
+#define _REG_SPRASURF				0x7029C
+
+#define _REG_SPRASURFLIVE			0x702AC
+
+#define _REG_SPRA_SCALE				0x70304
+
+#define _REG_SPRA_POS				0x7028c
+#define    _SPRITE_POS_X_SHIFT		0
+#define    _SPRITE_POS_Y_SHIFT		16
+#define    _SPRITE_POS_X_MASK		(0x1fff << _SPRITE_POS_X_SHIFT)
+#define    _SPRITE_POS_Y_MASK		(0xfff << _SPRITE_POS_Y_SHIFT)
+
+#define _REG_SPRA_SIZE				0x70290
+#define    _SPRITE_SIZE_WIDTH_SHIFT		0
+#define    _SPRITE_SIZE_HEIGHT_SHIFT		16
+#define    _SPRITE_SIZE_WIDTH_MASK		(0x1fff << _SPRITE_SIZE_WIDTH_SHIFT)
+#define    _SPRITE_SIZE_HEIGHT_MASK		(0xfff << _SPRITE_SIZE_HEIGHT_SHIFT)
+
+#define _REG_SPRA_OFFSET			0x702a4
+#define    _SPRITE_OFFSET_START_X_SHIFT	0
+#define    _SPRITE_OFFSET_START_Y_SHIFT	16
+#define    _SPRITE_OFFSET_START_X_MASK	(0x1fff << _SPRITE_OFFSET_START_X_SHIFT)
+#define    _SPRITE_OFFSET_START_Y_MASK	(0xfff << _SPRITE_OFFSET_START_Y_SHIFT)
+
+#define _REG_SPRB_CTL				0x71280
+#define _REG_SPRB_STRIDE			0x71288
+#define _REG_SPRB_POS				0x7128c
+#define _REG_SPRB_SIZE				0x71290
+#define _REG_SPRB_OFFSET			0x712a4
+#define _REG_SPRB_SCALE				0x71304
+
+#define _REG_SPRB_CTL				0x71280
+#define _REG_SPRB_STRIDE			0x71288
+#define _REG_SPRBSURF				0x7129C
+#define _REG_SPRBSURFLIVE			0x712AC
+#define _REG_SPRB_SCALE				0x71304
+
+#define _REG_SPRC_CTL				0x72280
+#define _REG_SPRC_STRIDE			0x72288
+#define _REG_SPRCSURF				0x7229C
+#define _REG_SPRCSURFLIVE			0x722AC
+#define _REG_SPRC_SCALE				0x72304
+
+#define VGT_SPRCTL(pipe)	_VGT_PIPE(pipe, _REG_SPRA_CTL, _REG_SPRB_CTL)
+#define VGT_SPRSTRIDE(pipe)	_VGT_PIPE(pipe, _REG_SPRA_STRIDE, _REG_SPRB_STRIDE)
+#define VGT_SPRPOS(pipe)	_VGT_PIPE(pipe, _REG_SPRA_POS, _REG_SPRB_POS)
+#define VGT_SPRSIZE(pipe)	_VGT_PIPE(pipe, _REG_SPRA_SIZE, _REG_SPRB_SIZE)
+#define VGT_SPRSURF(pipe)	_VGT_PIPE(pipe, _REG_SPRASURF, _REG_SPRBSURF)
+#define VGT_SPRSURFPIPE(sprsurf) _VGT_GET_PIPE(sprsurf, _REG_SPRASURF, _REG_SPRBSURF)
+#define VGT_SPRSURFLIVE(pipe)	_VGT_PIPE(pipe, _REG_SPRASURFLIVE, _REG_SPRBSURFLIVE)
+#define VGT_SPROFFSET(pipe)	_VGT_PIPE(pipe, _REG_SPRA_OFFSET, _REG_SPRB_OFFSET)
+
+#define VGT_SPRCNTRPIPE(sprcntr) _VGT_GET_PIPE(sprcntr, _REG_SPRA_CTL,_REG_SPRB_CTL)
+#define VGT_CURCNTRPIPE(curcntr) _VGT_GET_PIPE(curcntr, _REG_CURACNTR,_REG_CURBCNTR)
+
+#define _REG_DVSACNTR		0x72180
+#define _REG_DVSALINOFF		0x72184
+#define _REG_DVSASTRIDE		0x72188
+#define _REG_DVSAPOS		0x7218C
+#define _REG_DVSASIZE		0x72190
+#define _REG_DVSAKEYVAL		0x72194
+#define _REG_DVSAKEYMSK		0x72198
+#define _REG_DVSASURF		0x7219C
+#define _REG_DVSAKEYMAXVAL	0x721A0
+#define _REG_DVSATILEOFF	0x721A4
+#define _REG_DVSASURFLIVE	0x721AC
+#define _REG_DVSASCALE		0x72204
+/* DVSAGAMC: 0x72300 - 0x7234B */
+
+#define _REG_DVSBCNTR		0x73180
+#define _REG_DVSBLINOFF		0x73184
+#define _REG_DVSBSTRIDE		0x73188
+#define _REG_DVSBPOS		0x7318C
+#define _REG_DVSBSIZE		0x73190
+#define _REG_DVSBKEYVAL		0x73194
+#define _REG_DVSBKEYMSK		0x73198
+#define _REG_DVSBSURF		0x7319C
+#define _REG_DVSBKEYMAXVAL	0x731A0
+#define _REG_DVSBTILEOFF	0x731A4
+#define _REG_DVSBSURFLIVE	0x731AC
+#define _REG_DVSBSCALE		0x73204
+/* DVSBGAMC: 0x73300 - 0x7334B */
+
+#define _REG_PCH_DPB_AUX_CH_CTL		0xe4110
+#define _REG_PCH_DPB_AUX_CH_DATA1	0xe4114
+#define _REG_PCH_DPB_AUX_CH_DATA2	0xe4118
+#define _REG_PCH_DPB_AUX_CH_DATA3	0xe411c
+#define _REG_PCH_DPB_AUX_CH_DATA4	0xe4120
+#define _REG_PCH_DPB_AUX_CH_DATA5	0xe4124
+
+#define _REG_PCH_DPC_AUX_CH_CTL		0xe4210
+#define _REG_PCH_DPC_AUX_CH_DATA1	0xe4214
+#define _REG_PCH_DPC_AUX_CH_DATA2	0xe4218
+#define _REG_PCH_DPC_AUX_CH_DATA3	0xe421c
+#define _REG_PCH_DPC_AUX_CH_DATA4	0xe4220
+#define _REG_PCH_DPC_AUX_CH_DATA5	0xe4224
+
+#define _REG_PCH_DPD_AUX_CH_CTL		0xe4310
+#define _REG_PCH_DPD_AUX_CH_DATA1	0xe4314
+#define _REG_PCH_DPD_AUX_CH_DATA2	0xe4318
+#define _REG_PCH_DPD_AUX_CH_DATA3	0xe431c
+#define _REG_PCH_DPD_AUX_CH_DATA4	0xe4320
+#define _REG_PCH_DPD_AUX_CH_DATA5	0xe4324
+
+#define _REGBIT_DP_AUX_CH_CTL_SEND_BUSY		(1 << 31)
+#define _REGBIT_DP_AUX_CH_CTL_DONE		(1 << 30)
+#define _REGBIT_DP_AUX_CH_CTL_INTERRUPT		(1 << 29)
+#define _REGBIT_DP_AUX_CH_CTL_TIME_OUT_ERR	(1 << 28)
+#define _REGBIT_DP_AUX_CH_CTL_RECV_ERR		(1 << 25)
+#define _REGBIT_DP_AUX_CH_CTL_MESSAGE_SIZE_MASK	(0x1f << 20)
+#define _REGBIT_DP_AUX_CH_CTL_MESSAGE_SIZE_SHIFT	20
+#define _REGBIT_DP_AUX_CH_CTL_TIME_OUT_400us	(0 << 26)
+#define _DP_DETECTED				(1 << 2)
+#define _DP_AUX_CH_CTL_BIT_CLOCK_2X_SHIFT	0
+#define _DP_AUX_CH_CTL_PRECHARGE_2US_SHIFT	16
+#define _REG_FORCEWAKE		0xA18C
+#define _REG_FORCEWAKE_ACK	0x130090
+#define _REG_MUL_FORCEWAKE	0xA188
+#define _REG_MUL_FORCEWAKE_ACK	 0x130040
+#define _REG_FORCEWAKE_ACK_HSW	0x130044
+#define _REG_ECOBUS		0xA180
+#define        ECOBUS_FORCEWAKE_MT_ENABLE	(1<<5)
+#define _REGBIT_MUL_FORCEWAKE_ENABLE		(1<<5)
+
+#define _REG_GEN6_GDRST	0x941c
+#define    _REGBIT_GEN6_GRDOM_FULL		(1 << 0)
+#define    _REGBIT_GEN6_GRDOM_RENDER		(1 << 1)
+#define    _REGBIT_GEN6_GRDOM_MEDIA		(1 << 2)
+#define    _REGBIT_GEN6_GRDOM_BLT		(1 << 3)
+
+#define _REG_GT_THREAD_STATUS	0x13805C
+#define _REG_GT_CORE_STATUS	0x138060
+
+#define _REG_RC_CONTROL				0xA090
+#define _REGBIT_RC_HW_CTRL_ENABLE	(1<<31)
+#define _REGBIT_RC_RC1_ENABLE		(1<<20)
+#define _REGBIT_RC_RC6_ENABLE		(1<<18)
+#define _REGBIT_RC_DEEP_RC6_ENABLE	(1<<17)
+#define _REGBIT_RC_DEEPEST_RC6_ENABLE	(1<<16)
+
+#define _REG_RPNSWREQ				0xA008
+#define _REG_RC_VIDEO_FREQ			0xA00C
+#define _REG_RP_DOWN_TIMEOUT			0xA010
+#define _REG_RP_INTERRUPT_LIMITS		0xA014
+#define _REG_RPSTAT1				0xA01C
+#define _REG_RP_CONTROL				0xA024
+#define _REG_RP_UP_THRESHOLD			0xA02C
+#define _REG_RP_DOWN_THRESHOLD			0xA030
+#define _REG_RP_CUR_UP_EI			0xA050
+#define _REG_RP_CUR_UP				0xA054
+#define _REG_RP_PREV_UP				0xA058
+#define _REG_RP_CUR_DOWN_EI			0xA05C
+#define _REG_RP_CUR_DOWN			0xA060
+#define _REG_RP_PREV_DOWN			0xA064
+#define _REG_RP_UP_EI				0xA068
+#define _REG_RP_DOWN_EI				0xA06C
+#define _REG_RP_IDLE_HYSTERSIS			0xA070
+#define _REG_RC_STATE				0xA094
+#define _REG_RC1_WAKE_RATE_LIMIT		0xA098
+#define _REG_RC6_WAKE_RATE_LIMIT		0xA09C
+#define _REG_RC6pp_WAKE_RATE_LIMIT		0xA0A0
+#define _REG_RC_EVALUATION_INTERVAL		0xA0A8
+#define _REG_RC_IDLE_HYSTERSIS			0xA0AC
+#define _REG_RC_SLEEP				0xA0B0
+#define _REG_RC1e_THRESHOLD			0xA0B4
+#define _REG_RC6_THRESHOLD			0xA0B8
+#define _REG_RC6p_THRESHOLD			0xA0BC
+#define _REG_RC6pp_THRESHOLD			0xA0C0
+#define _REG_PMINTRMSK				0xA168
+
+#define MI_NOOP				0
+#define MI_FLUSH			(0x4 << 23)
+#define MI_SUSPEND_FLUSH		(0xb << 23)
+#define    MI_SUSPEND_FLUSH_EN		(1<<0)
+#define MI_SET_CONTEXT			(0x18 << 23)
+#define    MI_MM_SPACE_GTT		(1<<8)
+#define    MI_MM_SPACE_PHYSICAL		(0<<8)	/* deprecated */
+#define    MI_SAVE_EXT_STATE_EN		(1<<3)
+#define    MI_RESTORE_EXT_STATE_EN	(1<<2)
+#define    MI_FORCE_RESTORE		(1<<1)
+#define    MI_RESTORE_INHIBIT		(1<<0)
+#define MI_ARB_ON_OFF			(0x08 << 23)
+#define    MI_ARB_ENABLE		(1<<0)
+#define	   MI_ARB_DISABLE		(0<<0)
+/*
+ * We use _IMM instead of _INDEX, to avoid switching hardware
+ * status page
+ */
+#define MI_STORE_DATA_IMM		((0x20<<23) | 2)
+#define MI_STORE_DATA_IMM_QWORD		((0x20<<23) | 3)
+#define   MI_SDI_USE_GTT		(1<<22)
+#define MI_LOAD_REGISTER_IMM		(0x22<<23 | 1)
+#define   MI_LRI_BYTE0_DISABLE		(1<<8)
+#define   MI_LRI_BYTE1_DISABLE		(1<<9)
+#define   MI_LRI_BYTE2_DISABLE		(1<<10)
+#define   MI_LRI_BYTE3_DISABLE		(1<<11)
+
+#define   MI_WAIT_FOR_PLANE_C_FLIP_PENDING      (1<<15)
+#define   MI_WAIT_FOR_PLANE_B_FLIP_PENDING      (1<<9)
+#define   MI_WAIT_FOR_PLANE_A_FLIP_PENDING      (1<<1)
+
+#define   MI_WAIT_FOR_SPRITE_C_FLIP_PENDING      (1<<20)
+#define   MI_WAIT_FOR_SPRITE_B_FLIP_PENDING      (1<<10)
+#define   MI_WAIT_FOR_SPRITE_A_FLIP_PENDING      (1<<2)
+
+#define PIPE_CONTROL(len)		((0x3<<29)|(0x3<<27)|(0x2<<24)|(len-2))
+#define   PIPE_CONTROL_POST_SYNC_GLOBAL_GTT		(1<<24)
+#define   PIPE_CONTROL_POST_SYNC			(1<<23)
+#define   PIPE_CONTROL_CS_STALL				(1<<20)
+#define   PIPE_CONTROL_TLB_INVALIDATE			(1<<18)
+#define   PIPE_CONTROL_MEDIA_STATE_CLEAR		(1<<16)
+#define   PIPE_CONTROL_POST_SYNC_IMM			(1<<14)
+#define   PIPE_CONTROL_DEPTH_STALL			(1<<13)
+#define   PIPE_CONTROL_RENDER_TARGET_CACHE_FLUSH	(1<<12)
+#define   PIPE_CONTROL_INSTRUCTION_CACHE_INVALIDATE	(1<<11) /* MBZ on Ironlake */
+#define   PIPE_CONTROL_TEXTURE_CACHE_INVALIDATE		(1<<10) /* GM45+ only */
+#define   PIPE_CONTROL_INDIRECT_STATE_DISABLE		(1<<9)
+#define   PIPE_CONTROL_NOTIFY				(1<<8)
+#define   PIPE_CONTROL_FLUSH_ENABLE			(1<<7)
+#define   PIPE_CONTROL_DC_FLUSH_ENABLE			(1<<5)
+#define   PIPE_CONTROL_VF_CACHE_INVALIDATE		(1<<4)
+#define   PIPE_CONTROL_CONST_CACHE_INVALIDATE		(1<<3)
+#define   PIPE_CONTROL_STATE_CACHE_INVALIDATE		(1<<2)
+#define   PIPE_CONTROL_STALL_AT_SCOREBOARD		(1<<1)
+#define   PIPE_CONTROL_DEPTH_CACHE_FLUSH		(1<<0)
+
+#define DUMMY_3D		(0x6d800005)
+#define PRIM_TRILIST		(0x4)
+/* PCI config space */
+#define _REG_LBB	0xf4
+
+/* VGA stuff */
+#define _REG_VGA_MSR_WRITE	0x3c2
+#define _REG_VGA_MSR_READ	0x3cc
+#define    VGA_MSR_CGA_MODE	(1<<0)
+
+#define _REG_VGA_CR_INDEX_MDA	0x3b4
+#define _REG_VGA_CR_DATA_MDA	0x3b5
+#define _REG_VGA_ST01_MDA	0x3ba
+
+#define _REG_VGA_CR_INDEX_CGA	0x3d4
+#define _REG_VGA_CR_DATA_CGA	0x3d5
+#define _REG_VGA_ST01_CGA	0x3da
+
+#define _REG_VGA_SR_INDEX	0x3c4
+#define _REG_VGA_SR_DATA	0x3c5
+
+#define _REG_VGA_GR_INDEX	0x3ce
+#define _REG_VGA_GR_DATA	0x3cf
+
+#define _REG_VGA_AR_INDEX	0x3c0
+#define _REG_VGA_AR_DATA_WRITE	0x3c0
+#define _REG_VGA_AR_DATA_READ	0x3c1
+
+#define _REG_VGA_DACMASK	0x3c6
+/*
+ * Display engine regs
+ */
+
+#define _ACTIVE_WIDTH_MASK (0xFFF)
+
+/* Pipe A timing regs */
+#define _REG_HTOTAL_A		0x60000
+#define _REG_HBLANK_A		0x60004
+#define _REG_HSYNC_A		0x60008
+#define _REG_VTOTAL_A		0x6000c
+#define _REG_VBLANK_A		0x60010
+#define _REG_VSYNC_A		0x60014
+#define _REG_PIPEASRC		0x6001c
+#define     _PIPE_V_SRCSZ_SHIFT	0
+#define     _PIPE_V_SRCSZ_MASK	(0xfff << _PIPE_V_SRCSZ_SHIFT)
+#define     _PIPE_H_SRCSZ_SHIFT	16
+#define     _PIPE_H_SRCSZ_MASK	(0x1fff << _PIPE_H_SRCSZ_SHIFT)
+#define _REG_BCLRPAT_A		0x60020
+#define _REG_VSYNCSHIFT_A	0x60028
+
+/* Pipe B timing regs */
+#define _REG_HTOTAL_B		0x61000
+#define _REG_HBLANK_B		0x61004
+#define _REG_HSYNC_B		0x61008
+#define _REG_VTOTAL_B		0x6100c
+#define _REG_VBLANK_B		0x61010
+#define _REG_VSYNC_B		0x61014
+#define _REG_PIPEBSRC		0x6101c
+#define _REG_BCLRPAT_B		0x61020
+#define _REG_VSYNCSHIFT_B	0x61028
+
+/* Pipe C timing regs */
+#define _REG_HTOTAL_C		0x62000
+#define _REG_HBLANK_C		0x62004
+#define _REG_HSYNC_C		0x62008
+#define _REG_VTOTAL_C		0x6200c
+#define _REG_VBLANK_C		0x62010
+#define _REG_VSYNC_C		0x62014
+#define _REG_PIPECSRC		0x6201c
+#define _REG_BCLRPAT_C		0x62020	/*not needed*/
+#define _REG_VSYNCSHIFT_C	0x62028
+
+
+/* Pipe EDP timing regs */
+#define _REG_HTOTAL_EDP		0x6F000
+#define _REG_HBLANK_EDP		0x6F004
+#define _REG_HSYNC_EDP			0x6F008
+#define _REG_VTOTAL_EDP		0x6F00c
+#define _REG_VBLANK_EDP		0x6F010
+#define _REG_VSYNC_EDP			0x6F014
+#define _REG_VSYNCSHIFT_EDP	0x6F028
+
+
+#define VGT_HTOTAL(pipe)	_VGT_PIPE(pipe, _REG_HTOTAL_A, _REG_HTOTAL_B)
+#define VGT_HBLANK(pipe)	_VGT_PIPE(pipe, _REG_HBLANK_A, _REG_HBLANK_B)
+#define VGT_HSYNC(pipe)		_VGT_PIPE(pipe, _REG_HSYNC_A, _REG_HSYNC_B)
+#define VGT_VTOTAL(pipe)	_VGT_PIPE(pipe, _REG_VTOTAL_A, _REG_VTOTAL_B)
+#define VGT_VBLANK(pipe)	_VGT_PIPE(pipe, _REG_VBLANK_A, _REG_VBLANK_B)
+#define VGT_VSYNC(pipe)		_VGT_PIPE(pipe, _REG_VSYNC_A, _REG_VSYNC_B)
+
+#define VGT_BCLRPAT(pipe)	_VGT_PIPE(pipe, _REG_BCLRPAT_A, _REG_BCLRPAT_B)
+#define VGT_VSYNCSHIFT(pipe)	_VGT_PIPE(pipe, _REG_VSYNCSHIFT_A, _REG_VSYNCSHIFT_B)
+#define VGT_PIPESRC(pipe)	_VGT_PIPE(pipe, _REG_PIPEASRC, _REG_PIPEBSRC)
+
+#define _REG_DISP_ARB_CTL	0x45000
+#define _REG_DISP_ARB_CTL2	0x45004
+#define _REG_TILECTL		0x101000
+
+/* PCH */
+#define _REG_PCH_DREF_CONTROL			0xc6200
+#define    _REGBIT_DREF_CONTROL_MASK			0x7fc3
+#define    _REGBIT_DREF_CPU_SOURCE_OUTPUT_DISABLE	(0<<13)
+#define    _REGBIT_DREF_CPU_SOURCE_OUTPUT_DOWNSPREAD	(2<<13)
+#define    _REGBIT_DREF_CPU_SOURCE_OUTPUT_NONSPREAD	(3<<13)
+#define    _REGBIT_DREF_CPU_SOURCE_OUTPUT_MASK		(3<<13)
+#define    _REGBIT_DREF_SSC_SOURCE_DISABLE		(0<<11)
+#define    _REGBIT_DREF_SSC_SOURCE_ENABLE		(2<<11)
+#define    _REGBIT_DREF_SSC_SOURCE_MASK			(3<<11)
+#define    _REGBIT_DREF_NONSPREAD_SOURCE_DISABLE	(0<<9)
+#define    _REGBIT_DREF_NONSPREAD_CK505_ENABLE		(1<<9)
+#define    _REGBIT_DREF_NONSPREAD_SOURCE_ENABLE		(2<<9)
+#define    _REGBIT_DREF_NONSPREAD_SOURCE_MASK		(3<<9)
+#define    _REGBIT_DREF_SUPERSPREAD_SOURCE_DISABLE	(0<<7)
+#define    _REGBIT_DREF_SUPERSPREAD_SOURCE_ENABLE	(2<<7)
+#define    _REGBIT_DREF_SUPERSPREAD_SOURCE_MASK		(3<<7)
+#define    _REGBIT_DREF_SSC4_DOWNSPREAD			(0<<6)
+#define    _REGBIT_DREF_SSC4_CENTERSPREAD		(1<<6)
+#define    _REGBIT_DREF_SSC1_DISABLE			(0<<1)
+#define    _REGBIT_DREF_SSC1_ENABLE			(1<<1)
+#define    _REGBIT_DREF_SSC4_DISABLE			(0)
+#define    _REGBIT_DREF_SSC4_ENABLE			(1)
+
+#define _REG_PCH_RAWCLK_FREQ		0xc6204
+#define  _REGBIT_RAWCLK_FREQ_MASK       0x3ff
+
+/*
+ * digital port hotplug
+ */
+#define _REG_PCH_DPLL_A			0xc6014
+#define _REG_PCH_DPLL_B			0xc6018
+
+#define _REGBIT_DPLL_VCO_ENABLE		(1 << 31)
+#define VGT_PCH_DPLL(pipe)	_VGT_PIPE(pipe, _REG_PCH_DPLL_A, _REG_PCH_DPLL_B)
+
+#define _REG_PCH_FPA0				0xc6040
+#define    FP_CB_TUNE				(0x3<<22)
+#define _REG_PCH_FPA1				0xc6044
+#define _REG_PCH_FPB0				0xc6048
+#define _REG_PCH_FPB1				0xc604c
+#define VGT_PCH_FP0(pipe)	_VGT_PIPE(pipe, _REG_PCH_FPA0, _REG_PCH_FPB0)
+#define VGT_PCH_FP1(pipe)	_VGT_PIPE(pipe, _REG_PCH_FPA1, _REG_PCH_FPB1)
+
+#define _REG_PCH_DPLL_SEL			0xc7000
+#define _REGBIT_TRANSA_DPLL_ENABLE		(1 << 3)
+#define    _REGBIT_TRANSA_DPLLB_SEL		(1 << 0)
+#define    _REGBIT_TRANSA_DPLLA_SEL		0
+#define _REGBIT_TRANSB_DPLL_ENABLE		(1 << 7)
+#define    _REGBIT_TRANSB_DPLLB_SEL		(1 << 4)
+#define    _REGBIT_TRANSB_DPLLA_SEL		0
+#define _REGBIT_TRANSC_DPLL_ENABLE		(1 << 11)
+#define    _REGBIT_TRANSC_DPLLB_SEL		(1 << 8)
+#define    _REGBIT_TRANSC_DPLLA_SEL		0
+
+/*
+ * Clock control & power management
+ */
+#define _REG_VGA0	0x6000
+#define _REG_VGA1	0x6004
+#define _REG_VGA_PD	0x6010
+
+/* refresh rate hardware control */
+#define _REG_PIPEA_DATA_M1		0x60030
+#define _REG_PIPEA_DATA_N1		0x60034
+#define _REG_PIPEA_LINK_M1		0x60040
+#define _REG_PIPEA_LINK_N1		0x60044
+
+#define _REG_PIPEA_DATA_M2		0x60038
+#define _REG_PIPEA_DATA_N2		0x6003c
+#define _REG_PIPEA_LINK_M2		0x60048
+#define _REG_PIPEA_LINK_N2		0x6004c
+
+/* PIPE B timing regs are same start from 0x61000 */
+#define _REG_PIPEB_DATA_M1		0x61030
+#define _REG_PIPEB_DATA_N1		0x61034
+#define _REG_PIPEB_LINK_M1		0x61040
+#define _REG_PIPEB_LINK_N1		0x61044
+
+#define _REG_PIPEB_DATA_M2		0x61038
+#define _REG_PIPEB_DATA_N2		0x6103c
+#define _REG_PIPEB_LINK_M2		0x61048
+#define _REG_PIPEB_LINK_N2		0x6104c
+
+/* PIPE C timing regs are same start from 0x61000 */
+#define _REG_PIPEC_DATA_M1		0x62030
+#define _REG_PIPEC_DATA_N1		0x62034
+#define _REG_PIPEC_LINK_M1		0x62040
+#define _REG_PIPEC_LINK_N1		0x62044
+
+#define _REG_PIPEC_DATA_M2		0x62038
+#define _REG_PIPEC_DATA_N2		0x6203c
+#define _REG_PIPEC_LINK_M2		0x62048
+#define _REG_PIPEC_LINK_N2		0x6204c
+
+#define VGT_PIPE_DATA_M1(pipe) _VGT_PIPE(pipe, _REG_PIPEA_DATA_M1, _REG_PIPEB_DATA_M1)
+#define VGT_PIPE_DATA_N1(pipe) _VGT_PIPE(pipe, _REG_PIPEA_DATA_N1, _REG_PIPEB_DATA_N1)
+#define VGT_PIPE_DATA_M2(pipe) _VGT_PIPE(pipe, _REG_PIPEA_DATA_M2, _REG_PIPEB_DATA_M2)
+#define VGT_PIPE_DATA_N2(pipe) _VGT_PIPE(pipe, _REG_PIPEA_DATA_N2, _REG_PIPEB_DATA_N2)
+#define VGT_PIPE_LINK_M1(pipe) _VGT_PIPE(pipe, _REG_PIPEA_LINK_M1, _REG_PIPEB_LINK_M1)
+#define VGT_PIPE_LINK_N1(pipe) _VGT_PIPE(pipe, _REG_PIPEA_LINK_N1, _REG_PIPEB_LINK_N1)
+#define VGT_PIPE_LINK_M2(pipe) _VGT_PIPE(pipe, _REG_PIPEA_LINK_M2, _REG_PIPEB_LINK_M2)
+#define VGT_PIPE_LINK_N2(pipe) _VGT_PIPE(pipe, _REG_PIPEA_LINK_N2, _REG_PIPEB_LINK_N2)
+
+/* VGA port control */
+#define _REG_ADPA			0x61100
+
+/* FDI_RX, FDI_X is hard-wired to Transcoder_X */
+#define _REG_FDI_RXA_CTL			0xf000c
+#define _REG_FDI_RXB_CTL			0xf100c
+#define _REG_FDI_RXC_CTL			0xf200c
+
+#define _REGBIT_FDI_RX_ENABLE			(1 << 31)
+#define _REGBIT_FDI_RX_PLL_ENABLE		(1 << 13)
+#define _REGBIT_FDI_RX_PORT_WIDTH_MASK		(0x7 << 19)
+#define _REGBIT_FDI_RX_FDI_AUTO_TRAIN_ENABLE	(0x1 << 10)
+#define _REGBIT_FDI_LINK_TRAIN_PATTERN_1_CPT	(0 << 8)
+#define _REGBIT_FDI_LINK_TRAIN_PATTERN_2_CPT	(1 << 8)
+#define _REGBIT_FDI_LINK_TRAIN_NORMAL_CPT	(3 << 8)
+#define _REGBIT_FDI_LINK_TRAIN_PATTERN_MASK_CPT	(3 << 8)
+#define _REGBIT_FDI_RX_ENHANCE_FRAME_ENABLE	(1 << 6)
+#define _REGBIT_FDI_PCDCLK			(1 << 4)
+
+#define _REG_FDI_RXA_IIR			0xf0014
+#define _REG_FDI_RXB_IIR			0xf1014
+#define _REG_FDI_RXC_IIR			0xf2014
+#define _REG_FDI_RXA_IMR			0xf0018
+#define _REG_FDI_RXB_IMR			0xf1018
+#define _REG_FDI_RXC_IMR			0xf2018
+#define VGT_FDI_RX_IIR(pipe) _VGT_PIPE(pipe, _REG_FDI_RXA_IIR, _REG_FDI_RXB_IIR)
+#define VGT_FDI_RX_IMR(pipe) _VGT_PIPE(pipe, _REG_FDI_RXA_IMR, _REG_FDI_RXB_IMR)
+
+#define _REGBIT_FDI_RX_INTER_LANE_ALIGN		(1<<10)
+#define _REGBIT_FDI_RX_SYMBOL_LOCK		(1 << 9) /* train 2*/
+#define _REGBIT_FDI_RX_BIT_LOCK			(1 << 8) /* train 1*/
+#define _REGBIT_FDI_RX_TRAIN_PATTERN_2_FAIL	(1<<7)
+#define _REGBIT_FDI_RX_FS_CODE_ERR		(1<<6)
+#define _REGBIT_FDI_RX_FE_CODE_ERR		(1<<5)
+#define _REGBIT_FDI_RX_SYMBOL_ERR_RATE_ABOVE	(1<<4)
+#define _REGBIT_FDI_RX_HDCP_LINK_FAIL		(1<<3)
+#define _REGBIT_FDI_RX_PIXEL_FIFO_OVERFLOW	(1<<2)
+#define _REGBIT_FDI_RX_CROSS_CLOCK_OVERFLOW	(1<<1)
+#define _REGBIT_FDI_RX_SYMBOL_QUEUE_OVERFLOW	(1<<0)
+
+
+#define VGT_FDI_RX_CTL_BPC_MASK		(0x7 << 16)
+#define VGT_FDI_RX_CTL(pipe) _VGT_PIPE(pipe, _REG_FDI_RXA_CTL, _REG_FDI_RXB_CTL)
+
+#define _REG_FDI_RXA_MISC			0xf0010
+#define _REG_FDI_RXB_MISC			0xf1010
+#define _REG_FDI_RXA_TUSIZE1		0xf0030
+#define _REG_FDI_RXA_TUSIZE2		0xf0038
+#define _REG_FDI_RXB_TUSIZE1		0xf1030
+#define _REG_FDI_RXB_TUSIZE2		0xf1038
+
+#define VGT_FDI_RX_TUSIZE1(pipe) _VGT_PIPE(pipe, _REG_FDI_RXA_TUSIZE1,_REG_FDI_RXB_TUSIZE1)
+
+/* CPU: FDI_TX */
+#define _REG_FDI_TXA_CTL		0x60100
+#define _REG_FDI_TXB_CTL		0x61100
+#define _REG_FDI_TXC_CTL		0x62100
+
+#define _REGBIT_FDI_TX_ENABLE				(1 << 31)
+#define _REGBIT_FDI_LINK_TRAIN_PATTERN_1		(0 << 28)
+#define _REGBIT_FDI_LINK_TRAIN_PATTERN_2		(1 << 28)
+#define _REGBIT_FDI_LINK_TRAIN_NONE			(3 << 28)
+#define _REGBIT_FDI_TX_PLL_ENABLE			(1 << 14)
+#define _REGBIT_FDI_LINK_TRAIN_400MV_0DB_SNB_B		(0x0<<22)
+#define _REGBIT_FDI_LINK_TRAIN_400MV_6DB_SNB_B		(0x3a<<22)
+#define _REGBIT_FDI_LINK_TRAIN_600MV_3_5DB_SNB_B	(0x39<<22)
+#define _REGBIT_FDI_LINK_TRAIN_800MV_0DB_SNB_B		(0x38<<22)
+#define _REGBIT_FDI_LINK_TRAIN_VOL_EMP_MASK		(0x3f<<22)
+#define _REGBIT_FDI_TX_ENHANCE_FRAME_ENABLE		(1<<18)
+
+#define VGT_FDI_TX_CTL(pipe) _VGT_PIPE(pipe, _REG_FDI_TXA_CTL, _REG_FDI_TXB_CTL)
+
+/* CRT */
+#define _REG_PCH_ADPA				0xe1100
+#define _REGBIT_ADPA_DAC_ENABLE			(1 << 31)
+#define PORT_TRANS_SEL_SHIFT			29
+#define PORT_TRANS_SEL_MASK			(3 << PORT_TRANS_SEL_SHIFT)
+#define VGT_PORT_TRANS_SEL_CPT(pipe)		((pipe) << PORT_TRANS_SEL_SHIFT)
+#define _REGBIT_ADPA_CRT_HOTPLUG_MONITOR_MASK	(3 << 24)
+#define _REGBIT_ADPA_CRT_HOTPLUG_ENABLE		(1 << 23)
+#define _REGBIT_ADPA_CRT_HOTPLUG_PERIOD_128	(1 << 22)
+#define _REGBIT_ADPA_CRT_HOTPLUG_WARMUP_10MS	(1 << 21)
+#define _REGBIT_ADPA_CRT_HOTPLUG_SAMPLE_4S	(1 << 20)
+#define _REGBIT_ADPA_CRT_HOTPLUG_VOLTAGE_50	(1 << 18)
+#define _REGBIT_ADPA_CRT_HOTPLUG_VOLREF_325MV	(0 << 17)
+#define _REGBIT_ADPA_CRT_HOTPLUG_FORCE_TRIGGER	(1 << 16)
+#define _REGBIT_ADPA_VSYNC_ACTIVE_HIGH		(1 << 4)
+#define _REGBIT_ADPA_HSYNC_ACTIVE_HIGH		(1 << 3)
+
+/* Display port */
+#define _REG_DP_B_CTL	0xe4100
+#define _REG_DP_C_CTL	0xe4200
+#define _REG_DP_D_CTL	0xe4300
+#define _REGBIT_DP_PORT_ENABLE		(1 << 31)
+
+#define  _REGBIT_DP_VOLTAGE_0_4		(0 << 25)
+#define  _REGBIT_DP_VOLTAGE_0_6		(1 << 25)
+#define  _REGBIT_DP_VOLTAGE_0_8		(2 << 25)
+#define  _REGBIT_DP_VOLTAGE_1_2		(3 << 25)
+#define  _REGBIT_DP_VOLTAGE_MASK	(7 << 25)
+#define  DP_VOLTAGE_SHIFT		25
+
+#define _REGBIT_DP_PRE_EMPHASIS_0		(0 << 22)
+#define _REGBIT_DP_PRE_EMPHASIS_3_5		(1 << 22)
+#define _REGBIT_DP_PRE_EMPHASIS_6		(2 << 22)
+#define _REGBIT_DP_PRE_EMPHASIS_9_5		(3 << 22)
+#define _REGBIT_DP_PRE_EMPHASIS_MASK		(7 << 22)
+
+#define _REGBIT_DP_LINK_TRAIN_PAT_1_CPT		(0 << 8)
+#define _REGBIT_DP_LINK_TRAIN_PAT_2_CPT		(1 << 8)
+#define _REGBIT_DP_LINK_TRAIN_PAT_IDLE_CPT	(2 << 8)
+#define _REGBIT_DP_LINK_TRAIN_OFF_CPT		(3 << 8)
+#define _REGBIT_DP_LINK_TRAIN_MASK_CPT		(7 << 8)
+#define _REGBIT_DP_AUDIO_OUTPUT_ENABLE		(1 << 6)
+#define _REGBIT_DP_PORT_DETECTED		(1 << 2)
+
+/* legacy or PCH_IBX ? */
+#define _REGBIT_DP_LINK_TRAIN_MASK		(3 << 28)
+
+
+#define _REG_TRANS_DP_A_CTL	0xe0300
+#define _REG_TRANS_DP_B_CTL 0xe1300
+#define _REG_TRANS_DP_C_CTL 0xe2300
+#define _REGBIT_TRANS_DP_PORT_SEL_MASK	(3 << 29)
+#define _REGBIT_TRANS_DP_PORT_SEL_NONE	(3 << 29)
+#define _REGBIT_TRANS_DP_OUTPUT_ENABLE	(1 << 31)
+#define VGT_TRANS_DP_CTL(pipe)	(_REG_TRANS_DP_A_CTL + (pipe) * 0x01000)
+#define _REGBIT_TRANS_DP_PORT_SEL_B	(0 << 29)
+#define _REGBIT_TRANS_DP_PORT_SEL_C	(1 << 29)
+#define _REGBIT_TRANS_DP_PORT_SEL_D	(2 << 29)
+
+
+/* Digital display A (DP_A, embedded) */
+#define _REGBIT_DP_PORT_A_DETECTED	(1 << 2)
+
+/* HDMI/DVI/SDVO port */
+#define _REG_HDMI_B_CTL	0xe1140
+#define _REG_HDMI_C_CTL	0xe1150
+#define _REG_HDMI_D_CTL	0xe1160
+#define HDMI_TRANS_SEL_MASK		(3 << 29)
+#define _REGBIT_HDMI_PORT_ENABLE	(1 << 31)
+#define _REGBIT_HDMI_PORT_DETECTED	(1 << 2)
+
+/* PCH SDVOB multiplex with HDMIB */
+#define _REG_PCH_LVDS	0xe1180
+
+#define _REG_BLC_PWM_CPU_CTL2	0x48250
+
+#define _REG_BLC_PWM_CPU_CTL	0x48254
+#define VGT_BACKLIGHT_DUTY_CYCLE_MASK		(0xffff)
+
+#define _REG_BLC_PWM_PCH_CTL1	0xc8250
+#define _REG_BLC_PWM_PCH_CTL2	0xc8254
+#define _REG_PCH_PP_ON_DELAYS	0xc7208
+#define _REG_PCH_PP_OFF_DELAYS	0xc720c
+#define _REGBIT_PANEL_POWER_DOWN_DELAY_MASK	(0x1fff0000)
+#define _REGBIT_PANEL_POWER_DOWN_DELAY_SHIFT	16
+#define _REGBIT_PANEL_LIGHT_OFF_DELAY_MASK	(0x1fff)
+#define _REGBIT_PANEL_LIGHT_OFF_DELAY_SHIFT	0
+
+#define _REG_PCH_PP_DIVISOR		0xc7210
+
+#define _REG_PCH_PP_STATUS		0xc7200
+#define _REGBIT_PANEL_POWER_ON		(1 << 31)
+#define _REG_PCH_PP_CONTROL		0xc7204
+#define _REGBIT_POWER_TARGET_ON		(1 << 0)
+#define _REGBIT_PANEL_UNLOCK_REGS	(0xabcd << 16) /* Write Protect Key is 0xABCD */
+
+
+/* Watermark register (Ironlake) */
+#define _REG_WM0_PIPEA_ILK	0x45100
+#define _REG_WM0_PIPEB_ILK	0x45104
+#define _REG_WM0_PIPEC_IVB	0x45200
+#define _REG_WM1_LP_ILK		0x45108
+#define _REG_WM2_LP_ILK		0x4510c
+#define _REG_WM3_LP_ILK		0x45110
+#define _REG_WM1S_LP_ILK	0x45120
+#define _REG_WM2S_LP_IVB	0x45124
+#define _REG_WM3S_LP_IVB	0x45128
+
+#define  _REGBIT_WM0_PIPE_PLANE_MASK	(0x7f<<16)
+#define  _REGBIT_WM0_PIPE_PLANE_SHIFT	16
+#define  _REGBIT_WM0_PIPE_SPRITE_MASK	(0x3f<<8)
+#define  _REGBIT_WM0_PIPE_SPRITE_SHIFT	8
+#define  _REGBIT_WM0_PIPE_CURSOR_MASK	(0x1f)
+
+#define DISPLAY_MAXWM	0x7f	/* bit 16:22 */
+#define CURSOR_MAXWM	0x1f	/* bit 4:0 */
+
+union PCH_PP_CONTROL
+{
+	uint32_t data;
+	struct
+	{
+		uint32_t power_state_target	: 1; // bit 0
+		uint32_t power_down_on_reset	: 1; // bit 1
+		uint32_t backlight_enable	: 1; // bit 2
+		uint32_t edp_vdd_override_for_aux : 1; // bit 3
+		uint32_t reserve : 12;			// bits 15:4
+		uint32_t write_protect_key :16; // bits 31:16 0xABCD to disable protected)
+	};
+};
+
+union PCH_PP_STAUTS
+{
+	uint32_t data;
+	struct
+	{
+		uint32_t reserv1	: 4;	// bit 3:0
+		uint32_t reserv2	: 23;	// bit 26:4
+		uint32_t power_cycle_delay_active	:1;	// bit 27
+		uint32_t power_sequence_progress	:2;	// bits 29:28
+		uint32_t require_asset_status		:1;	// bit 30
+		uint32_t panel_powere_on_statue		:1;	// bit 31 (0 - Disable, 1 - Enable)
+	};
+};
+
+/* Clocking configuration register */
+#define _REG_RSTDBYCTL		0x111b8
+
+/* CPU panel fitter */
+#define _REG_PF_CTL_0			0x68080
+#define _REG_PF_CTL_1			0x68880
+#define _REG_PF_CTL_2			0x69080
+#define _REGBIT_PF_ENABLE		(1 << 31)
+#define  _REGBIT_PF_PIPE_SEL_MASK	(3<<29)
+#define  _REGBIT_PF_PIPE_SEL(pipe)	((pipe)<<29)
+#define _REGBIT_PF_FILTER_MASK		(3 << 23)
+#define _REGBIT_PF_FILTER_PROGRAMMED	(0 << 23)
+#define _REGBIT_PF_FILTER_MED_3x3	(1 << 23)
+#define _REGBIT_PF_FILTER_EDGE_ENHANCE	(2 << 23)
+#define _REGBIT_PF_FILTER_EDGE_SOFTEN	(3 << 23)
+
+#define _REG_PF_WIN_SZ_0		0x68074
+#define _REG_PF_WIN_SZ_1		0x68874
+#define _REG_PF_WIN_SZ_2		0x69074
+
+#define _REG_PF_WIN_POS_0		0x68070
+#define _REG_PF_WIN_POS_1		0x68870
+#define _REG_PF_WIN_POS_2		0x69070
+
+#define VGT_PF_CTL(pipe)	_VGT_PIPE(pipe, _REG_PF_CTL_0, _REG_PF_CTL_1)
+#define VGT_PF_WIN_SZ(pipe)	_VGT_PIPE(pipe, _REG_PF_WIN_SZ_0, _REG_PF_WIN_SZ_1)
+#define    VGT_PF_WIN_POS(pipe) _VGT_PIPE(pipe, _REG_PF_WIN_POS_0, _REG_PF_WIN_POS_1)
+
+/* Per-transcoder DIP controls */
+#define _REG_TRANSACONF			0xf0008
+#define _REG_TRANSBCONF			0xf1008
+#define _REGBIT_TRANS_ENABLE		(1 << 31)
+#define _REGBIT_TRANS_STATE_ENABLED	(1 << 30)
+#define _REGBIT_TRANS_INTERLACE_MASK	(7 << 21)
+#define VGT_TRANSCONF(plane)	_VGT_PIPE(plane, _REG_TRANSACONF, _REG_TRANSBCONF)
+
+union _TRANS_CONFIG
+{
+	uint32_t data;
+	struct
+	{
+		uint32_t reserve1 : 10;			// bit 9:0
+		uint32_t xvycc_color_range_limit : 1;	// bit 10
+		uint32_t reserve2 : 10;			// bit 20:11
+		uint32_t interlaced_mode: 3;		// bit 23:21
+		uint32_t reserve3 : 6;			// bit 29:24
+		uint32_t transcoder_state : 1;		// bit 30
+		uint32_t transcoder_enable : 1;		// bit 31
+	};
+};
+
+#define _REG_TRANSA_CHICKEN1	0xf0060
+#define _REG_TRANSA_CHICKEN2	0xf0064
+#define _REG_TRANSB_CHICKEN1	0xf1060
+#define _REG_TRANSB_CHICKEN2	0xf1064
+#define VGT_TRANS_CHICKEN2(pipe) _VGT_PIPE(pipe, _REG_TRANSA_CHICKEN2, _REG_TRANSB_CHICKEN2)
+#define _REGBIT_TRANS_AUTOTRAIN_GEN_STALL_DISABLE	(1<<31)
+
+/* transcoder */
+#define _REG_TRANS_HTOTAL_A		0xe0000
+#define _REG_TRANS_HBLANK_A		0xe0004
+#define _REG_TRANS_HSYNC_A		0xe0008
+#define _REG_TRANS_VTOTAL_A		0xe000c
+#define _REG_TRANS_VBLANK_A		0xe0010
+#define _REG_TRANS_VSYNC_A		0xe0014
+#define _REG_TRANS_VSYNCSHIFT_A		0xe0028
+#define _REG_TRANS_HTOTAL_B		0xe1000
+#define _REG_TRANS_HBLANK_B		0xe1004
+#define _REG_TRANS_HSYNC_B		0xe1008
+#define _REG_TRANS_VTOTAL_B		0xe100c
+#define _REG_TRANS_VBLANK_B		0xe1010
+#define _REG_TRANS_VSYNC_B		0xe1014
+#define _REG_TRANS_VSYNCSHIFT_B		0xe1028
+
+#define VGT_TRANS_HTOTAL(pipe)	_VGT_PIPE(pipe, _REG_TRANS_HTOTAL_A, _REG_TRANS_HTOTAL_B)
+#define VGT_TRANS_HBLANK(pipe)	_VGT_PIPE(pipe, _REG_TRANS_HBLANK_A, _REG_TRANS_HBLANK_B)
+#define VGT_TRANS_HSYNC(pipe)	 _VGT_PIPE(pipe, _REG_TRANS_HSYNC_A, _REG_TRANS_HSYNC_B)
+#define VGT_TRANS_VTOTAL(pipe)	_VGT_PIPE(pipe, _REG_TRANS_VTOTAL_A, _REG_TRANS_VTOTAL_B)
+#define VGT_TRANS_VBLANK(pipe)	_VGT_PIPE(pipe, _REG_TRANS_VBLANK_A, _REG_TRANS_VBLANK_B)
+#define VGT_TRANS_VSYNC(pipe)	 _VGT_PIPE(pipe, _REG_TRANS_VSYNC_A, _REG_TRANS_VSYNC_B)
+#define VGT_TRANS_VSYNCSHIFT(pipe)	_VGT_PIPE(pipe, _REG_TRANS_VSYNCSHIFT_A, \
+					_REG_TRANS_VSYNCSHIFT_B)
+
+
+
+#define _REG_SOUTH_CHICKEN1			0xc2000
+#define    VGT_FDIA_PHASE_SYNC_SHIFT_EN	18
+#define VGT_FDIA_PHASE_SYNC_SHIFT_OVR	19
+#define    VGT_FDI_PHASE_SYNC_EN(pipe)	(1 << (VGT_FDIA_PHASE_SYNC_SHIFT_EN - ((pipe) * 2)))
+#define VGT_FDI_PHASE_SYNC_OVR(pipe)(1 << (VGT_FDIA_PHASE_SYNC_SHIFT_OVR - ((pipe) *2)))
+#define _REG_SOUTH_CHICKEN2			0xc2004
+#define    _REGBIT_FDI_MPHY_IOSFSB_RESET_STATUS	(1<<13)
+#define    _REGBIT_MPHY_IOSFSB_RESET_CTL	(1<<12)
+#define _REG_SOUTH_DSPCLK_GATE_D		0xc2020
+
+#define _REG_TRANSA_DATA_M1		0xe0030
+#define _REG_TRANSA_DATA_N1		0xe0034
+#define _REG_TRANSA_DATA_M2		0xe0038
+#define _REG_TRANSA_DATA_N2		0xe003c
+#define _REG_TRANSA_DP_LINK_M1		0xe0040
+#define _REG_TRANSA_DP_LINK_N1		0xe0044
+#define _REG_TRANSA_DP_LINK_M2		0xe0048
+#define _REG_TRANSA_DP_LINK_N2		0xe004c
+
+#define _REG_TRANSB_DATA_M1		0xe1030
+#define _REG_TRANSB_DATA_N1		0xe1034
+#define _REG_TRANSB_DATA_M2		0xe1038
+#define _REG_TRANSB_DATA_N2		0xe103c
+#define _REG_TRANSB_DP_LINK_M1		0xe1040
+#define _REG_TRANSB_DP_LINK_N1		0xe1044
+#define _REG_TRANSB_DP_LINK_M2		0xe1048
+#define _REG_TRANSB_DP_LINK_N2		0xe104c
+
+#define VGT_TRANSDATA_M1(pipe)	_VGT_PIPE(pipe, _REG_TRANSA_DATA_M1, _REG_TRANSB_DATA_M1)
+#define VGT_TRANSDATA_N1(pipe)	_VGT_PIPE(pipe, _REG_TRANSA_DATA_N1, _REG_TRANSB_DATA_N1)
+#define VGT_TRANSDATA_M2(pipe)	_VGT_PIPE(pipe, _REG_TRANSA_DATA_M2, _REG_TRANSB_DATA_M2)
+#define VGT_TRANSDATA_N2(pipe)	_VGT_PIPE(pipe, _REG_TRANSA_DATA_N2, _REG_TRANSB_DATA_N2)
+
+#define _REG_TRANSA_VIDEO_DIP_CTL	0xE0200
+#define _REG_TRANSA_VIDEO_DIP_DATA	0xE0208
+#define _REG_TRANSA_VIDEO_DIP_GCP	0xE0210
+#define _REG_TRANSA_DP_CTL		0xE0300
+#define _REG_TRANSB_VIDEO_DIP_CTL	0xE1200
+#define _REG_TRANSB_VIDEO_DIP_DATA	0xE1208
+#define _REG_TRANSB_VIDEO_DIP_GCP	0xE1210
+#define _REG_TRANSB_DP_CTL		0xE1300
+#define _REG_TRANSC_VIDEO_DIP_CTL	0xE2200
+#define _REG_TRANSC_VIDEO_DIP_DATA	0xE2208
+#define _REG_TRANSC_VIDEO_DIP_GCP	0xE2210
+#define _REG_TRANSC_DP_CTL		0xE2300
+
+/* Display & cursor control */
+
+/* Pipe A */
+#define _REG_PIPEADSL		0x70000
+#define _REG_PIPEACONF		0x70008
+#define _REG_PIPEASTAT		0x70024
+#define _REG_PIPEA_FRMCOUNT 	0x70040
+#define _REG_PIPEA_FLIPCOUNT	0x70044
+#define _REG_PIPE_MISC_A	0x70030
+
+/* Pipe B */
+#define _REG_PIPEBDSL		0x71000
+#define _REG_PIPEBCONF		0x71008
+#define _REG_PIPEBSTAT		0x71024
+#define _REG_PIPEB_FRMCOUNT 	0x71040
+#define _REG_PIPEB_FLIPCOUNT	0x71044
+#define _REG_PIPE_MISC_B	0x71030
+
+/* Pipe C */
+#define _REG_PIPECDSL		0x72000
+#define _REG_PIPECCONF		0x72008
+#define _REG_PIPECSTAT		0x72024
+#define _REG_PIPEC_FRMCOUNT 	0x72040
+#define _REG_PIPEC_FLIPCOUNT	0x72044
+#define _REG_PIPE_MISC_C	0x72030
+
+/* eDP */
+#define _REG_PIPE_EDP_CONF	0x7f008
+
+/* bit fields of pipeconf */
+#define _REGBIT_PIPE_ENABLE		(1 << 31)
+#define _REGBIT_PIPE_STAT_ENABLED	(1 << 30)
+#define _REGBIT_PIPE_BPC_MASK		(7 << 5) /* ironlake */
+#define _REGBIT_PIPE_8BPC		(0 << 5)
+
+/* bit fields of pipestat */
+#define _REGBIT_PIPE_VBLANK_INTERRUPT_STATUS	(1 << 1)
+
+#define VGT_PIPEDSL(pipe)	_VGT_PIPE(pipe, _REG_PIPEADSL, _REG_PIPEBDSL)
+#define VGT_PIPECONF(pipe)	_VGT_PIPE(pipe, _REG_PIPEACONF, _REG_PIPEBCONF)
+#define VGT_PIPESTAT(pipe)	_VGT_PIPE(pipe, _REG_PIPEASTAT, _REG_PIPEBSTAT)
+#define VGT_PIPE_FRMCOUNT(pipe)	_VGT_PIPE(pipe, _REG_PIPEA_FRMCOUNT, _REG_PIPEB_FRMCOUNT)
+#define VGT_PIPE_FLIPCOUNT(pipe) _VGT_PIPE(pipe, _REG_PIPEA_FLIPCOUNT, _REG_PIPEB_FLIPCOUNT)
+
+#define VGT_PIPECONFPIPE(pipeconf) _VGT_GET_PIPE(pipeconf, _REG_PIPEACONF, _REG_PIPEBCONF)
+#define VGT_FRMCOUNTPIPE(frmcount) _VGT_GET_PIPE(frmcount, _REG_PIPEA_FRMCOUNT, _REG_PIPEB_FRMCOUNT)
+
+/* For Gen 2 */
+#define _REG_CURSIZE		0x700a0
+/*
+ * Palette regs
+ */
+#define _REG_PALETTE_A		0x0a000
+#define _REG_PALETTE_B		0x0a800
+#define VGT_PALETTE(pipe) _VGT_PIPE(pipe, _REG_PALETTE_A, _REG_PALETTE_B)
+
+/* legacy palette */
+#define _REG_LGC_PALETTE_A		0x4a000
+#define _REG_LGC_PALETTE_B		0x4a800
+#define _REG_LGC_PALETTE_C		0x4b000
+#define VGT_LGC_PALETTE(pipe) _VGT_PIPE(pipe, _REG_LGC_PALETTE_A, _REG_LGC_PALETTE_B)
+
+/* Display Port */
+#define _REG_DP_TP_CTL_A		0x64040
+#define _REG_DP_TP_CTL_B		0x64140
+#define _REG_DP_TP_CTL_C		0x64240
+#define _REG_DP_TP_CTL_D		0x64340
+#define _REG_DP_TP_CTL_E		0x64440
+#define  _REGBIT_DP_TP_ENABLE		(1 << 31)
+#define  _REGBIT_DP_TP_FDI_AUTO_TRAIN_ENABLE	(1 << 15)
+#define _REG_DDI_BUF_CTL_A		0x64000
+#define  _DDI_BUFCTL_DETECT_MASK	0x1
+#define  _REGBIT_DDI_BUF_ENABLE		(1 << 31)
+#define  _REGBIT_DDI_BUF_IS_IDLE	(1<<7)
+#define _REG_DDI_BUF_CTL_B		0x64100
+#define _REG_DDI_BUF_CTL_C		0x64200
+#define _REG_DDI_BUF_CTL_D		0x64300
+#define _REG_DDI_BUF_CTL_E		0x64400
+
+#define _REG_DP_TP_STATUS_A			0x64044
+#define _REG_DP_TP_STATUS_B			0x64144
+#define _REG_DP_TP_STATUS_C			0x64244
+#define _REG_DP_TP_STATUS_D			0x64344
+#define _REG_DP_TP_STATUS_E			0x64444
+#define  _REGBIT_DP_TP_STATUS_AUTOTRAIN_DONE	(1 << 12)
+
+#define VGT_DP_TP_CTL(port)		_VGT_PORT(port, _REG_DP_TP_CTL_A, \
+						_REG_DP_TP_CTL_B)
+#define VGT_DP_TP_CTL_PORT(reg)		_VGT_GET_PORT(reg, _REG_DP_TP_CTL_A, \
+						_REG_DP_TP_CTL_B)
+#define VGT_DP_TP_STATUS(port)		_VGT_PORT(port, _REG_DP_TP_STATUS_A, \
+						_REG_DP_TP_STATUS_B)
+#define VGT_DP_TP_STATUS_PORT(reg)	_VGT_GET_PORT(reg, _REG_DP_TP_STATUS_A, \
+						_REG_DP_TP_STATUS_B)
+#define VGT_DDI_BUF_CTL(port)		_VGT_PORT(port, _REG_DDI_BUF_CTL_A, \
+						_REG_DDI_BUF_CTL_B)
+
+#define DRM_MODE_DPMS_ON		0
+
+/* DPCD */
+#define DP_SET_POWER		0x600
+#define DP_SET_POWER_D0		0x1
+#define AUX_NATIVE_WRITE	0x8
+#define AUX_NATIVE_READ		0x9
+
+#define AUX_NATIVE_REPLY_MASK	(0x3 << 4)
+#define AUX_NATIVE_REPLY_ACK	(0x0 << 4)
+#define AUX_NATIVE_REPLY_NAK	(0x1 << 4)
+#define AUX_NATIVE_REPLY_DEFER	(0x2 << 4)
+
+#define AUX_BURST_SIZE		16
+
+/* DPCD 0x106 */
+
+#define DP_TRAINING_PATTERN_SET			0x102
+#define DP_TRAINING_PATTERN_DISABLE		0
+#define DP_TRAINING_PATTERN_1			1
+#define DP_TRAINING_PATTERN_2			2
+#define DP_LINK_SCRAMBLING_DISABLE		(1 << 5)
+
+#define DP_LINK_CONFIGURATION_SIZE		9
+#define    DP_LINK_BW_SET			0x100
+# define DP_SET_ANSI_8B10B			(1 << 0)
+
+#define DP_LINK_STATUS_SIZE			6
+#define DP_TRAIN_MAX_SWING_REACHED		(1 << 2)
+
+#define DP_TRAINING_LANE0_SET			0x103
+
+#define DP_TRAIN_VOLTAGE_SWING_MASK		0x3
+#define DP_TRAIN_VOLTAGE_SWING_SHIFT		0
+#define DP_TRAIN_VOLTAGE_SWING_400		(0 << 0)
+#define DP_TRAIN_VOLTAGE_SWING_600		(1 << 0)
+#define DP_TRAIN_VOLTAGE_SWING_800		(2 << 0)
+#define DP_TRAIN_VOLTAGE_SWING_1200		(3 << 0)
+
+#define DP_TRAIN_PRE_EMPHASIS_MASK		(3 << 3)
+#define DP_TRAIN_PRE_EMPHASIS_0			(0 << 3)
+#define DP_TRAIN_PRE_EMPHASIS_3_5		(1 << 3)
+#define DP_TRAIN_PRE_EMPHASIS_6			(2 << 3)
+#define DP_TRAIN_PRE_EMPHASIS_9_5		(3 << 3)
+
+#define DP_TRAIN_PRE_EMPHASIS_SHIFT		3
+#define DP_TRAIN_MAX_PRE_EMPHASIS_REACHED	(1 << 5)
+
+#define DP_LANE0_1_STATUS			0x202
+#define DP_LANE_CR_DONE				(1 << 0)
+
+#define DP_LANE_ALIGN_STATUS_UPDATED		0x204
+#define DP_INTERLANE_ALIGN_DONE			(1 << 0)
+#define DP_LANE_CHANNEL_EQ_DONE			(1 << 1)
+#define DP_LANE_SYMBOL_LOCKED			(1 << 2)
+
+#define DP_ADJUST_REQUEST_LANE0_1		0x206
+
+#define DP_ADJUST_VOLTAGE_SWING_LANE0_SHIFT 0
+#define DP_ADJUST_VOLTAGE_SWING_LANE1_SHIFT 4
+#define DP_ADJUST_PRE_EMPHASIS_LANE0_SHIFT  2
+#define DP_ADJUST_PRE_EMPHASIS_LANE1_SHIFT  6
+/* Ironlake */
+#define _REG_CPU_VGACNTRL	0x41000
+#define _REGBIT_VGA_DISPLAY_DISABLE	(1UL << 31)
+
+#define _REG_DISPLAY_CHICKEN_BITS_1	0x42000
+#define _REG_DISPLAY_CHICKEN_BITS_2	0x42004
+#define _REG_DSPCLK_GATE_D		0x42020
+
+#define _REG_DPFC_CB_BASE		0x43200
+#define _REG_DPFC_CONTROL		0x43208
+#define _REG_DPFC_RECOMP_CTL		0x4320c
+#define _REG_DPFC_CPU_FENCE_OFFSET	0x43218
+#define _REG_DPFC_CONTROL_SA		0x100100
+#define _REG_DPFC_CPU_FENCE_OFFSET_SA	0x100104
+
+#define _REG_CSC_A_COEFFICIENTS		0x49010
+#define _REG_CSC_A_MODE			0x49028
+#define _REG_PRECSC_A_HIGH_COLOR_CHANNEL_OFFSET		0x49030
+#define _REG_PRECSC_A_MEDIUM_COLOR_CHANNEL_OFFSET	0x49034
+#define _REG_PRECSC_A_LOW_COLOR_CHANNEL_OFFSET		0x49038
+
+#define _REG_CSC_B_COEFFICIENTS		0x49110
+#define _REG_CSC_B_MODE			0x49128
+#define _REG_PRECSC_B_HIGH_COLOR_CHANNEL_OFFSET		0x49130
+#define _REG_PRECSC_B_MEDIUM_COLOR_CHANNEL_OFFSET	0x49134
+#define _REG_PRECSC_B_LOW_COLOR_CHANNEL_OFFSET		0x49138
+
+#define _REG_CSC_C_COEFFICIENTS		0x49210
+#define _REG_CSC_C_MODE			0x49228
+#define _REG_PRECSC_C_HIGH_COLOR_CHANNEL_OFFSET		0x49230
+#define _REG_PRECSC_C_MEDIUM_COLOR_CHANNEL_OFFSET	0x49234
+#define _REG_PRECSC_C_LOW_COLOR_CHANNEL_OFFSET		0x49238
+
+/*
+ * Instruction and interrupt control regs
+ */
+#define _REG_HWS_PGA		0x02080
+#define _REG_IER		0x020a0
+#define _REG_IMR		0x020a8
+#define _REG_DE_RRMR		0x44050
+
+#define _REG_CACHE_MODE_0	0x02120 /* 915+ only */
+#define _REG_CACHE_MODE_1	0x02124
+#define _REG_GEN3_MI_ARB_STATE	0x020e4 /* 915+ only */
+
+#define _REG_SWF		0x4f000
+
+#define _REG_DP_BUFTRANS	0xe4f00
+
+/* digital port hotplug */
+
+#define _REG_PCH_GPIOA		0xc5010
+#define _REG_PCH_GPIOB		0xc5014
+#define _REG_PCH_GPIOC		0xc5018
+#define _REG_PCH_GPIOD		0xc501c
+#define _REG_PCH_GPIOE		0xc5020
+#define _REG_PCH_GPIOF		0xc5024
+
+#define _REG_PCH_GMBUS0		0xc5100
+#define _REG_PCH_GMBUS1		0xc5104
+#define _REG_PCH_GMBUS2		0xc5108
+#define _REG_PCH_GMBUS3		0xc510c
+#define _REG_PCH_GMBUS4		0xc5110
+#define _REG_PCH_GMBUS5		0xc5120
+
+/* GMBUS1 bits definitions */
+#define _GMBUS_SW_CLR_INT	(1 << 31)
+#define _GMBUS_SW_RDY		(1 << 30)
+#define _GMBUS_CYCLE_WAIT	(1 << 25)
+#define _GMBUS_CYCLE_INDEX	(1 << 26)
+#define _GMBUS_CYCLE_STOP	(1 << 27)
+#define _GMBUS_SLAVE_READ	(1 << 0)
+#define GMBUS1_TOTAL_BYTES_SHIFT 16
+#define GMBUS1_TOTAL_BYTES_MASK 0x1ff
+#define gmbus1_total_byte_count(v) (((v) >> GMBUS1_TOTAL_BYTES_SHIFT) & GMBUS1_TOTAL_BYTES_MASK)
+#define gmbus1_slave_addr(v) (((v) & 0xff) >> 1)
+#define gmbus1_slave_index(v) (((v) >> 8) & 0xff)
+#define gmbus1_bus_cycle(v) (((v) >> 25) & 0x7)
+
+/* GMBUS0 bits definitions */
+#define _GMBUS_PIN_SEL_MASK	(0x7)
+
+/* GMBUS2 bits definitions */
+#define _GMBUS_IN_USE		(1 << 15)
+#define _GMBUS_HW_WAIT		(1 << 14)
+#define _GMBUS_HW_RDY		(1 << 11)
+#define _GMBUS_INT_STAT		(1 << 12)
+#define _GMBUS_NAK		(1 << 10)
+#define _GMBUS_ACTIVE		(1 << 9)
+
+#define _GMBUS_SLAVE_READ	(1 << 0)
+#define _GMBUS_SLAVE_WRITE	(0 << 0)
+#define _GMBUS_BYTE_COUNT_SHIFT	16
+#define _GMBUS_SLAVE_ADDR_SHIFT	1
+#define _GMBUS_TRANS_MAX_BYTES	((1 << 9) - 1)
+
+#define _REG_GTFIFODBG			0x120000
+#define _REG_GTFIFO_FREE_ENTRIES	0x120008
+#define _REG_MCHBAR_MIRROR		0x140000
+#define _REG_UCG_CTL1			0x9400
+#define _REG_UCG_CTL2			0x9404
+#define _REG_RC_PWRCTX_MAXCNT		0x2054
+#define _REG_3D_CHICKEN1		0x2084
+#define _REG_3D_CHICKEN2		0x208C
+#define _REG_3D_CHICKEN3		0x2090
+#define _REG_RCS_ECOSKPD		0x21d0
+#define _REG_BCS_ECOSKPD		0x221d0
+#define _REG_VFSKPD			0x2470
+#define _REG_ECOCHK			0x4090
+#define _REG_GAC_ECOCHK			0x14090
+#define _REG_2D_CG_DIS			0x6200
+#define _REG_3D_CG_DIS			0x6204
+#define _REG_3D_CG_DIS2			0x6208
+#define _REG_SNPCR			0x900c
+#define _REG_MBCTL			0x907c
+#define _REG_GAB_CTL			0x24000
+#define _REG_SUPER_QUEUE_CONFIG		0x902c
+#define _REG_MISC_CLOCK_GATING		0x9424
+#define _REG_GTDRIVER_MAILBOX_INTERFACE	0x138124
+#define _REG_GTDRIVER_MAILBOX_DATA0	0x138128
+
+/*
+ * GPIO regs
+ */
+#define _REG_GMBUS0			0x5100 /* clock/port select */
+
+enum vgt_pipe {
+	PIPE_A = 0,
+	PIPE_B,
+	PIPE_C,
+	I915_MAX_PIPES
+};
+
+enum vgt_port {
+	PORT_A = 0,
+	PORT_B,
+	PORT_C,
+	PORT_D,
+	PORT_E,
+	I915_MAX_PORTS
+};
+
+#define VGT_PORT_NAME(p)	\
+	((p) == PORT_A ? "PORT_A" : \
+	((p) == PORT_B ? "PORT_B" : \
+	((p) == PORT_C ? "PORT_C" : \
+	((p) == PORT_D ? "PORT_D" : \
+	((p) == PORT_E ? "PORT_E" : "PORT_X")))))
+
+#define VGT_PIPE_NAME(p)	\
+	((p) == PIPE_A ? "Pipe A" : \
+		((p) == PIPE_B ? "Pipe B" : \
+			((p) == PIPE_C ? "Pipe C" : "PIPE X")))
+#define VGT_PIPE_CHAR(p)	\
+	((p) == PIPE_A ? 'A' : \
+		((p) == PIPE_B ? 'B' : \
+			((p) == PIPE_C ? 'C' : 'X')))
+
+enum vgt_plane_type {
+	PRIMARY_PLANE = 0,
+	CURSOR_PLANE,
+	SPRITE_PLANE,
+	MAX_PLANE
+};
+
+enum vgt_port_type {
+	VGT_CRT = 0,
+	VGT_DP_A,
+	VGT_DP_B,
+	VGT_DP_C,
+	VGT_DP_D,
+	VGT_HDMI_B,
+	VGT_HDMI_C,
+	VGT_HDMI_D,
+	VGT_PORT_MAX
+};
+
+#define VGT_PORT_TYPE_NAME(p)	\
+        ((p) == VGT_CRT ? "VGT_CRT" : \
+        ((p) == VGT_DP_A ? "VGT_DP_A" : \
+        ((p) == VGT_DP_B ? "VGT_DP_B" : \
+        ((p) == VGT_DP_C ? "VGT_DP_C" : \
+	((p) == VGT_DP_D ? "VGT_DP_D" : \
+	((p) == VGT_HDMI_B ? "VGT_HDMI_B" : \
+	((p) == VGT_HDMI_C ? "VGT_HDMI_C" : \
+	((p) == VGT_HDMI_D ? "VGT_HDMI_D" : "UNKNOWN"))))))))
+
+static inline int port_to_port_type(int port_sel)
+{
+        switch(port_sel) {
+        case PORT_A:
+                return VGT_DP_A;
+        case PORT_B:
+                return VGT_DP_B;
+        case PORT_C:
+                return VGT_DP_C;
+        case PORT_D:
+                return VGT_DP_D;
+        case PORT_E:
+                return VGT_CRT;
+	}
+        return VGT_PORT_MAX;
+}
+
+static inline int port_type_to_port(int port_sel)
+{
+	switch(port_sel) {
+	case VGT_DP_A:
+		return PORT_A;
+	case VGT_DP_B:
+	case VGT_HDMI_B:
+		return PORT_B;
+	case VGT_DP_C:
+	case VGT_HDMI_C:
+		return PORT_C;
+	case VGT_DP_D:
+	case VGT_HDMI_D:
+		return PORT_D;
+	case VGT_CRT:
+		return PORT_E;
+	}
+
+	return I915_MAX_PORTS;
+}
+
+/* interrupt related definitions */
+#define _REG_DEISR	0x44000
+#define _REG_DEIMR	0x44004
+#define _REG_DEIIR	0x44008
+#define _REG_DEIER	0x4400C
+#define        _REGSHIFT_MASTER_INTERRUPT	31
+#define        _REGBIT_MASTER_INTERRUPT	(1 << 31)
+#define        _REGBIT_DP_A_HOTPLUG		(1 << 19)
+#define        _REGBIT_PIPE_A_VBLANK		(1 << 7)
+#define        _REGSHIFT_PCH			21
+#define        _REGBIT_PCH			(1 << 21)
+/* GEN7 */
+#define        _REGSHIFT_PCH_GEN7		28
+#define        _REGBIT_PCH_GEN7			(1 << 28)
+#define _REG_GTISR	0x44010
+#define _REG_GTIMR	0x44014
+#define _REG_GTIIR	0x44018
+#define _REG_GTIER	0x4401C
+#define _REG_PMISR	0x44020
+#define _REG_PMIMR	0x44024
+#define _REG_PMIIR	0x44028
+#define _REG_PMIER	0x4402C
+#define _REG_DP_A_HOTPLUG_CNTL	0x44030
+#define        _REGBIT_DP_A_HOTPLUG_STATUS		(3 << 0)
+#define        _REGBIT_DP_A_PULSE_DURATION		(3 << 2)
+#define        _REGBIT_DP_A_HOTPLUG_ENABLE		(1 << 4)
+#define _REG_GTT_FAULT_STATUS	0x44040
+
+#define    _REG_SDEISR	0xC4000
+#define    _REG_SDEIMR	0xC4004
+#define    _REG_SDEIIR	0xC4008
+#define        _REGBIT_CRT_HOTPLUG		(1 << 19)
+#define        _REGBIT_DP_B_HOTPLUG		(1 << 21)
+#define        _REGBIT_DP_C_HOTPLUG		(1 << 22)
+#define        _REGBIT_DP_D_HOTPLUG		(1 << 23)
+#define    _REG_SDEIER	0xC400C
+#define _REG_SHOTPLUG_CTL	0xC4030
+#define        _REGBIT_DP_B_STATUS			(3 << 0)
+#define        _REGBIT_DP_B_PULSE_DURATION		(3 << 2)
+#define        _REGBIT_DP_B_ENABLE			(1 << 4)
+#define        _REGBIT_DP_C_STATUS			(3 << 8)
+#define        _REGBIT_DP_C_PULSE_DURATION		(3 << 10)
+#define        _REGBIT_DP_C_ENABLE			(1 << 12)
+#define        _REGBIT_DP_D_STATUS			(3 << 16)
+#define        _REGBIT_DP_D_PULSE_DURATION		(3 << 18)
+#define        _REGBIT_DP_D_ENABLE			(1 << 20)
+
+#define RING_IMR(ring) \
+	__RING_REG((ring), _REG_RCS_IMR)
+
+#define _REG_RCS_WATCHDOG_CTL	0x2178
+#define _REG_RCS_WATCHDOG_THRSH	0x217C
+#define _REG_RCS_WATCHDOG_CTR	0x2190
+#define _REG_VCS_WATCHDOG_CTR	0x12178
+#define _REG_VCS_WATCHDOG_THRSH	0x1217C
+
+#define _REG_RCS_EIR	0x20B0
+#define _REG_RCS_EMR	0x20B4
+#define _REG_RCS_ESR	0x20B8
+#define _REG_BCS_EIR	0x220B0
+#define _REG_BCS_EMR	0x220B4
+#define _REG_BCS_ESR	0x220B8
+#define _REG_VCS_EIR	0x120B0
+#define _REG_VCS_EMR	0x120B4
+#define _REG_VCS_ESR	0x120B8
+#define _REG_VECS_EIR	0x1A0B0
+#define _REG_VECS_EMR	0x1A0B4
+#define _REG_VECS_ESR	0x1A0B8
+
+#define RING_EIR(ring) \
+	__RING_REG((ring), _REG_RCS_EIR)
+#define RING_EMR(ring) \
+	__RING_REG((ring), _REG_RCS_EMR)
+#define RING_ESR(ring) \
+	__RING_REG((ring), _REG_RCS_ESR)
+
+#define RING_REG_2064(ring) \
+	({ASSERT((ring) > 0); \
+	 __RING_REG((ring), 0x2064);})
+
+#define RING_REG_2068(ring) \
+	__RING_REG((ring), 0x2068)
+
+#define RING_REG_2078(ring) \
+	__RING_REG((ring), 0x2078)
+
+#define RING_REG_206C(ring) \
+	__RING_REG((ring), 0x206C)
+
+/* blacklight PWM control */
+#define _REG_BLC_PWM_CTL2	0x48250
+#define        _REGBIT_PHASE_IN_IRQ_ENABLE	(1 << 24)
+#define        _REGBIT_PHASE_IN_IRQ_STATUS	(1 << 26)
+#define _REG_HISTOGRAM_THRSH	0x48268
+#define        _REGBIT_HISTOGRAM_IRQ_ENABLE	(1 << 31)
+#define        _REGBIT_HISTOGRAM_IRQ_STATUS	(1 << 30)
+
+/*
+ * Next MACROs for GT configuration space.
+ */
+#define VGT_PCI_CLASS_VGA			0x03
+#define VGT_PCI_CLASS_VGA_OTHER			0x80
+
+#define VGT_REG_CFG_VENDOR_ID			0x00
+#define VGT_REG_CFG_COMMAND			0x04
+#define _REGBIT_CFG_COMMAND_IO			(1 << 0)
+#define _REGBIT_CFG_COMMAND_MEMORY		(1 << 1)
+#define _REGBIT_CFG_COMMAND_MASTER		(1 << 2)
+#define VGT_REG_CFG_CLASS_PROG_IF		0x09
+#define VGT_REG_CFG_SUB_CLASS_CODE		0x0A
+#define VGT_REG_CFG_CLASS_CODE			0x0B
+#define VGT_REG_CFG_SPACE_BAR0			0x10
+#define VGT_REG_CFG_SPACE_BAR1			0x18
+#define VGT_REG_CFG_SPACE_BAR2			0x20
+#define VGT_REG_CFG_SPACE_BAR_ROM		0x30
+#define VGT_REG_CFG_SPACE_MSAC			0x62
+#define VGT_REG_CFG_SWSCI_TRIGGER		0xE8
+#define	_REGBIT_CFG_SWSCI_SCI_SELECT		(1 << 15)
+#define	_REGBIT_CFG_SWSCI_SCI_TRIGGER		1
+#define VGT_REG_CFG_OPREGION			0xFC
+
+#define VGT_OPREGION_PAGES			2
+#define VGT_OPREGION_PORDER			1
+#define VGT_OPREGION_SIZE			(8 * 1024)
+#define VGT_OPREGION_REG_CLID			0x1AC
+#define VGT_OPREGION_REG_SCIC			0x200
+#define _REGBIT_OPREGION_SCIC_FUNC_MASK		0x1E
+#define _REGBIT_OPREGION_SCIC_FUNC_SHIFT	1
+#define _REGBIT_OPREGION_SCIC_SUBFUNC_MASK	0xFF00
+#define _REGBIT_OPREGION_SCIC_SUBFUNC_SHIFT	8
+#define _REGBIT_OPREGION_SCIC_EXIT_MASK		0xE0
+#define VGT_OPREGION_SCIC_F_GETBIOSDATA		4
+#define VGT_OPREGION_SCIC_F_GETBIOSCALLBACKS	6
+#define VGT_OPREGION_SCIC_SF_SUPPRTEDCALLS	0
+#define VGT_OPREGION_SCIC_SF_REQEUSTEDCALLBACKS	1
+#define VGT_OPREGION_REG_PARM			0x204
+
+//#define MSAC_APERTURE_SIZE_MASK		0x3
+#define MSAC_APERTURE_SIZE_128M			(0 << 1)
+#define MSAC_APERTURE_SIZE_256M			(1 << 1)
+#define MSAC_APERTURE_SIZE_512M			(3 << 1)
+
+
+/*
+ * Configuration register definition for BDF: 0:0:0.
+ */
+#define _REG_GMCH_CONTRL		0x50
+#define    _REGBIT_SNB_GMCH_GMS_SHIFT   3 /* Graphics Mode Select */
+#define    _REGBIT_SNB_GMCH_GMS_MASK    0x1f
+#define    _REGBIT_BDW_GMCH_GMS_SHIFT   8
+#define    _REGBIT_BDW_GMCH_GMS_MASK    0xff
+
+/* HSW */
+#define _REG_LCPLL_CTL		0x130040
+#define  _REGBIT_LCPLL_PLL_DISABLE		(1<<31)
+#define  _REGBIT_LCPLL_PLL_LOCK			(1<<30)
+#define  _REGBIT_LCPLL_CLK_FREQ_MASK		(3<<26)
+#define  _REGBIT_LCPLL_CD_SOURCE_FCLK		(1<<21)
+#define  _REGBIT_LCPLL_CD_SOURCE_FCLK_DONE	(1<<19)
+#define  _LCPLL_CLK_FREQ_450		(0<<26)
+#define _REG_HSW_FUSE_STRAP	0x42014
+#define  _REGBIT_HSW_CDCLK_LIMIT	(1 << 24)
+#define _REG_GFX_FLSH_CNT	0x101008
+
+#define _REG_HSW_PWR_WELL_CTL1	0x45400
+#define _REG_HSW_PWR_WELL_CTL2	0x45404
+#define _REG_HSW_PWR_WELL_CTL3	0x45408
+#define _REG_HSW_PWR_WELL_CTL4	0x4540C
+#define _REG_HSW_PWR_WELL_CTL5	0x45410
+#define _REG_HSW_PWR_WELL_CTL6	0x45414
+
+#define   _REGBIT_HSW_PWR_WELL_ENABLE			(1<<31)
+#define   _REGBIT_HSW_PWR_WELL_STATE				(1<<30)
+#define   _REGBIT_HSW_PWR_WELL_ENABLE_SINGLE_STEP	(1<<31)
+#define   _REGBIT_HSW_PWR_WELL_PWR_GATE_OVERRIDE	(1<<20)
+#define   _REGBIT_HSW_PWR_WELL_FORCE_ON			(1<<19)
+
+#define _REG_SPLL_CTL		0x46020
+#define  _REGBIT_SPLL_CTL_ENABLE	(1 << 31)
+
+#define _REG_WRPLL_CTL1		0x46040
+#define _REG_WRPLL_CTL2		0x46060
+#define  _REGBIT_WRPLL_ENABLE	(1 << 31)
+
+#define _REG_PORT_CLK_SEL_DDIA	0x46100
+#define _REG_PORT_CLK_SEL_DDIB	0x46104
+#define _REG_PORT_CLK_SEL_DDIC	0x46108
+#define _REG_PORT_CLK_SEL_DDID	0x4610C
+#define _REG_PORT_CLK_SEL_DDIE	0x46110
+
+#define _REG_TRANS_CLK_SEL_A	0x46140
+#define _REG_TRANS_CLK_SEL_B	0x46144
+#define _REG_TRANS_CLK_SEL_C	0x46148
+
+#define _REG_SBI_ADDR			0xc6000
+#define _REG_SBI_DATA			0xc6004
+#define _REG_SBI_CTL_STAT		0xc6008
+#define _SBI_RESPONSE_MASK		0x3
+#define _SBI_RESPONSE_SHIFT		0x1
+#define _SBI_STAT_MASK			0x1
+#define _SBI_STAT_SHIFT			0x0
+#define _SBI_RESPONSE_FAIL		(0x1<<_SBI_RESPONSE_SHIFT)
+#define _SBI_RESPONSE_SUCCESS		(0x0<<_SBI_RESPONSE_SHIFT)
+#define _SBI_BUSY			(0x1<<_SBI_STAT_SHIFT)
+#define _SBI_READY			(0x0<<_SBI_STAT_SHIFT)
+#define _SBI_OPCODE_SHIFT		8
+#define _SBI_OPCODE_MASK		(0xff << _SBI_OPCODE_SHIFT)
+#define _SBI_CMD_IORD			2
+#define _SBI_CMD_IOWR			3
+#define _SBI_CMD_CRRD			6
+#define _SBI_CMD_CRWR			7
+#define _SBI_ADDR_OFFSET_SHIFT		16
+#define _SBI_ADDR_OFFSET_MASK		(0xffff << _SBI_ADDR_OFFSET_SHIFT)
+
+#define _REG_TRANS_DDI_FUNC_CTL_A	0x60400
+#define _REG_TRANS_DDI_FUNC_CTL_B	0x61400
+#define _REG_TRANS_DDI_FUNC_CTL_C	0x62400
+#define _REG_TRANS_DDI_FUNC_CTL_EDP	0x6F400
+
+#define _VGT_TRANS_DDI_FUNC_CTL(tran)   _VGT_TRANSCODER(tran, _REG_TRANS_DDI_FUNC_CTL_A, \
+						   _REG_TRANS_DDI_FUNC_CTL_B)
+
+
+#define  _REGBIT_TRANS_DDI_FUNC_ENABLE		(1<<31)
+/* Those bits are ignored by pipe EDP since it can only connect to DDI A */
+#define  _TRANS_DDI_PORT_SHIFT			28
+#define  _REGBIT_TRANS_DDI_PORT_MASK		(7<<_TRANS_DDI_PORT_SHIFT)
+#define  _REGBIT_TRANS_DDI_SELECT_PORT(x)	((x)<<_TRANS_DDI_PORT_SHIFT)
+#define  _REGBIT_TRANS_DDI_PORT_NONE		(0<<_TRANS_DDI_PORT_SHIFT)
+#define  _TRANS_DDI_MODE_SELECT_HIFT		24
+#define  _REGBIT_TRANS_DDI_MODE_SELECT_MASK	(7<<_TRANS_DDI_MODE_SELECT_HIFT)
+#define  _REGBIT_TRANS_DDI_MODE_SELECT_HDMI	(0<<_TRANS_DDI_MODE_SELECT_HIFT)
+#define  _REGBIT_TRANS_DDI_MODE_SELECT_DVI	(1<<_TRANS_DDI_MODE_SELECT_HIFT)
+#define  _REGBIT_TRANS_DDI_MODE_SELECT_DP_SST	(2<<_TRANS_DDI_MODE_SELECT_HIFT)
+#define  _REGBIT_TRANS_DDI_MODE_SELECT_DP_MST	(3<<_TRANS_DDI_MODE_SELECT_HIFT)
+#define  _REGBIT_TRANS_DDI_MODE_SELECT_FDI	(4<<_TRANS_DDI_MODE_SELECT_HIFT)
+#define  _REGBIT_TRANS_DDI_BPC_MASK		(7<<20)
+#define  _REGBIT_TRANS_DDI_BPC_8		(0<<20)
+#define  _REGBIT_TRANS_DDI_BPC_10		(1<<20)
+#define  _REGBIT_TRANS_DDI_BPC_6		(2<<20)
+#define  _REGBIT_TRANS_DDI_BPC_12		(3<<20)
+#define  _REGBIT_TRANS_DDI_PVSYNC		(1<<17)
+#define  _REGBIT_TRANS_DDI_PHSYNC		(1<<16)
+#define  _TRANS_DDI_EDP_INPUT_SHIFT		12
+#define  _REGBIT_TRANS_DDI_EDP_INPUT_MASK	(7<<_TRANS_DDI_EDP_INPUT_SHIFT)
+#define  _REGBIT_TRANS_DDI_EDP_INPUT_A_ON	(0<<_TRANS_DDI_EDP_INPUT_SHIFT)
+#define  _REGBIT_TRANS_DDI_EDP_INPUT_A_ONOFF	(4<<_TRANS_DDI_EDP_INPUT_SHIFT)
+#define  _REGBIT_TRANS_DDI_EDP_INPUT_B_ONOFF	(5<<_TRANS_DDI_EDP_INPUT_SHIFT)
+#define  _REGBIT_TRANS_DDI_EDP_INPUT_C_ONOFF	(6<<_TRANS_DDI_EDP_INPUT_SHIFT)
+#define  _REGBIT_TRANS_DDI_BFI_ENABLE		(1<<4)
+#define  _REGBIT_TRANS_DDI_PORT_WIDTH_X1	(0<<1)
+#define  _REGBIT_TRANS_DDI_PORT_WIDTH_X2	(1<<1)
+#define  _REGBIT_TRANS_DDI_PORT_WIDTH_X4	(3<<1)
+
+
+#define _REG_TRANS_MSA_MISC_A	0x60410
+#define _REG_TRANS_MSA_MISC_B	0x61410
+#define _REG_TRANS_MSA_MISC_C	0x62410
+
+#define _REG_GEN7_COMMON_SLICE_CHICKEN1		0x7010
+#define _REG_GEN7_COMMON_SLICE_CHICKEN2		0x7014
+#define _REG_GEN7_L3CNTLREG1			0xB01C
+#define _REG_GEN7_L3_CHICKEN_MODE_REGISTER	0xB030
+#define _REG_GEN7_SQ_CHICKEN_MBCUNIT_CONFIG	0x9030
+#define _REG_WM_DBG				0x45280
+
+#define _REG_PIPE_WM_LINETIME_A			0x45270
+#define _REG_PIPE_WM_LINETIME_B			0x45274
+#define _REG_PIPE_WM_LINETIME_C			0x45278
+
+#define _REG_HSW_VIDEO_DIP_CTL_A		0x60200
+#define _REG_HSW_VIDEO_DIP_CTL_B		0x61200
+#define _REG_HSW_VIDEO_DIP_CTL_C		0x62200
+#define _REG_HSW_VIDEO_DIP_CTL_EDP		0x6F200
+
+#define _REG_DPA_AUX_CH_CTL			0x64010
+#define _REG_DPA_AUX_CH_DATA1			0x64014
+
+#define _REG_DDI_BUF_TRANS_A			0x64E00
+#define _REG_HSW_AUD_CONFIG_A			0x65000
+
+#define _REG_SFUSE_STRAP			0xC2014
+#define  _REGBIT_SFUSE_STRAP_B_PRESENTED	(1 << 2)
+#define  _REGBIT_SFUSE_STRAP_C_PRESENTED	(1 << 1)
+#define  _REGBIT_SFUSE_STRAP_D_PRESENTED	(1 << 0)
+
+#define _REG_PIXCLK_GATE			0xC6020
+
+#define _REG_SCRATCH1				0xB038
+#define _REG_GEN7_L3SQCREG4			0xB034
+#define _REG_ROW_CHICKEN3			0xE49C
+
+#define _REG_FPGA_DBG				0x42300
+#define _REGBIT_FPGA_DBG_RM_NOCLAIM		(1 << 31)
+
+/* GEN8 interrupt registers definations */
+#define _REG_MASTER_IRQ			0x44200
+#define  _REGBIT_MASTER_IRQ_CONTROL	(1<<31)
+#define  _REGBIT_PCU_IRQ			(1<<30)
+#define  _REGBIT_DE_PCH_IRQ		(1<<23)
+#define  _REGBIT_DE_MISC_IRQ		(1<<22)
+#define  _REGBIT_DE_PORT_IRQ		(1<<20)
+#define  _REGBIT_DE_PIPE_C_IRQ		(1<<18)
+#define  _REGBIT_DE_PIPE_B_IRQ		(1<<17)
+#define  _REGBIT_DE_PIPE_A_IRQ		(1<<16)
+#define  _REGBIT_GT_VECS_IRQ		(1<<6)
+#define  _REGBIT_GT_VCS2_IRQ		(1<<3)
+#define  _REGBIT_GT_VCS1_IRQ		(1<<2)
+#define  _REGBIT_GT_BCS_IRQ		(1<<1)
+#define  _REGBIT_GT_RCS_IRQ		(1<<0)
+
+#define _REG_GT_ISR(which) (0x44300 + (0x10 * (which)))
+#define _REG_GT_IMR(which) (0x44304 + (0x10 * (which)))
+#define _REG_GT_IIR(which) (0x44308 + (0x10 * (which)))
+#define _REG_GT_IER(which) (0x4430c + (0x10 * (which)))
+
+#define _REG_DE_PIPE_ISR(pipe) (0x44400 + (0x10 * (pipe)))
+#define _REG_DE_PIPE_IMR(pipe) (0x44404 + (0x10 * (pipe)))
+#define _REG_DE_PIPE_IIR(pipe) (0x44408 + (0x10 * (pipe)))
+#define _REG_DE_PIPE_IER(pipe) (0x4440c + (0x10 * (pipe)))
+
+#define _REG_DE_PORT_ISR 0x44440
+#define _REG_DE_PORT_IMR 0x44444
+#define _REG_DE_PORT_IIR 0x44448
+#define _REG_DE_PORT_IER 0x4444c
+#define  _REGBIT_PORT_DP_A_HOTPLUG	(1 << 3)
+#define  _REGBIT_AUX_CHANNEL_A	(1 << 0)
+
+#define _REG_DE_MISC_ISR 0x44460
+#define _REG_DE_MISC_IMR 0x44464
+#define _REG_DE_MISC_IIR 0x44468
+#define _REG_DE_MISC_IER 0x4446c
+
+#define _REG_PCU_ISR 0x444e0
+#define _REG_PCU_IMR 0x444e4
+#define _REG_PCU_IIR 0x444e8
+#define _REG_PCU_IER 0x444ec
+
+#define _REG_GEN8_PRIVATE_PAT  0x40e0
+
+#define _REG_GAMTARBMODE		0x04a08
+
+#define _REG_RING_PDP_UDW(base, n)      (base + 0x270 + ((n) * 8 + 4))
+#define _REG_RING_PDP_LDW(base, n)      (base + 0x270 + (n) * 8)
+
+#define _REG_RCS_PDP_UDW(n)	_REG_RING_PDP_UDW(0x2000, n)
+#define _REG_RCS_PDP_LDW(n)	_REG_RING_PDP_LDW(0x2000, n)
+
+#define _REG_VCS_PDP_UDW(n)	_REG_RING_PDP_UDW(0x12000, n)
+#define _REG_VCS_PDP_LDW(n)	_REG_RING_PDP_LDW(0x12000, n)
+
+#define _REG_VCS2_PDP_UDW(n)	_REG_RING_PDP_UDW(0x1c000, n)
+#define _REG_VCS2_PDP_LDW(n)	_REG_RING_PDP_LDW(0x1c000, n)
+
+#define _REG_VECS_PDP_UDW(n)	_REG_RING_PDP_UDW(0x1a000, n)
+#define _REG_VECS_PDP_LDW(n)	_REG_RING_PDP_LDW(0x1a000, n)
+
+#define _REG_BCS_PDP_UDW(n)	_REG_RING_PDP_UDW(0x22000, n)
+#define _REG_BCS_PDP_LDW(n)	_REG_RING_PDP_LDW(0x22000, n)
+
+#endif	/* _VGT_REG_H_ */
diff --git a/drivers/gpu/drm/i915/vgt/render.c b/drivers/gpu/drm/i915/vgt/render.c
new file mode 100644
index 0000000..5668e2a
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/render.c
@@ -0,0 +1,2218 @@
+/*
+ * Render context management
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/module.h>
+#include <linux/kthread.h>
+#include <linux/delay.h>
+#include "vgt.h"
+
+/*
+ * NOTE list:
+ *	- hook with i915 driver (now invoke vgt_initalize from i915_init directly)
+ *	 also the hooks in AGP driver
+ *	- need a check on "unsigned long" vs. "u64" usage
+ *	- need consider cache related issues, e.g. Linux/Windows may have different
+ *	 TLB invalidation mode setting, which may impact vGT's context switch logic
+ */
+u64	context_switch_cost = 0;
+u64	context_switch_num = 0;
+u64	ring_idle_wait = 0;
+
+int vgt_ctx_switch = 1;
+bool vgt_validate_ctx_switch = false;
+
+static struct vgt_render_context_ops *context_ops;
+
+void vgt_toggle_ctx_switch(bool enable)
+{
+	/*
+	 * No need to hold lock as this will be observed
+	 * in the next check in kthread.
+	 */
+	if (enable)
+		vgt_ctx_switch = 1;
+	else
+		vgt_ctx_switch = 0;
+}
+
+/*
+ * TODO: the context layout could be different on generations.
+ * e.g. ring head/tail, ccid, etc. when PPGTT is enabled
+ */
+#define OFF_CACHE_MODE_0	0x4A
+#define OFF_CACHE_MODE_1	0x4B
+#define OFF_INSTPM		0x4D
+#define OFF_EXCC		0x4E
+#define OFF_MI_MODE		0x4F
+void update_context(struct vgt_device *vgt, uint64_t context)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	uint64_t ptr;
+	u32 *vptr;
+
+	ptr = (uint64_t)phys_aperture_vbase(pdev) + context;
+	vptr = (u32 *)ptr;
+#define UPDATE_FIELD(off, reg) \
+	*(vptr + off) = 0xFFFF0000 | (__sreg(vgt, reg) & 0xFFFF);
+
+	UPDATE_FIELD(OFF_CACHE_MODE_0, _REG_CACHE_MODE_0);
+	UPDATE_FIELD(OFF_CACHE_MODE_1, _REG_CACHE_MODE_1);
+	UPDATE_FIELD(OFF_INSTPM, _REG_RCS_INSTPM);
+	UPDATE_FIELD(OFF_EXCC, _REG_RCS_EXCC);
+	UPDATE_FIELD(OFF_MI_MODE, _REG_RCS_MI_MODE);
+}
+
+static bool ring_is_empty(struct pgt_device *pdev,
+	int id)
+{
+	if ( is_ring_enabled(pdev, id) && !is_ring_empty(pdev, id) )
+		return false;
+
+	return true;
+}
+
+static bool ring_is_xxx(struct pgt_device *pdev,
+	int id)
+{
+	if (pdev->ring_xxx_valid &&
+	    !(VGT_MMIO_READ(pdev, pdev->ring_xxx[id]) &
+		      (1 << pdev->ring_xxx_bit[id])))
+		return false;
+
+	return true;
+}
+
+static bool ring_is_stopped(struct pgt_device *pdev, int id)
+{
+	vgt_reg_t val;
+
+	val = VGT_MMIO_READ(pdev, pdev->ring_mi_mode[id]);
+	if ((val & (_REGBIT_MI_STOP_RINGS | _REGBIT_MI_RINGS_IDLE)) ==
+	    (_REGBIT_MI_STOP_RINGS | _REGBIT_MI_RINGS_IDLE))
+		return true;
+
+	return false;
+}
+
+static bool ring_wait_for_completion(struct pgt_device *pdev, int id)
+{
+	u32 *ptr;
+
+	/* now a single magic number, because only RCS supports hw switch */
+	if (id != RING_BUFFER_RCS)
+		return true;
+
+	ptr = (u32 *)(phys_aperture_vbase(pdev) + vgt_data_ctx_magic(pdev));
+	if (wait_for_atomic((*ptr == pdev->magic), VGT_RING_TIMEOUT) != 0) {
+		vgt_err("Timeout %d ms for CMD comletion on ring %d\n",
+			VGT_RING_TIMEOUT, id);
+		vgt_err("expected(%d), actual(%d)\n", pdev->magic, *ptr);
+		return false;
+	}
+
+	return true;
+}
+
+/* make a render engine idle */
+bool idle_render_engine(struct pgt_device *pdev, int id)
+{
+	if (wait_for_atomic(ring_is_empty(pdev, id), VGT_RING_TIMEOUT) != 0) {
+		int i, busy = 1;
+		vgt_reg_t acthd1, acthd2;
+		vgt_warn("Timeout wait %d ms for ring(%d) empty\n",
+			VGT_RING_TIMEOUT, id);
+
+		/*
+		 * TODO:
+		 * The timeout value may be not big enough, for some specific
+		 * workloads in the VM, if they submit a big trunk of commands
+		 * in a batch. The problem in current implementation is, ctx
+		 * switch request is made asynchronous to the cmd submission.
+		 * it's possible to have a request handled right after the
+		 * current owner submits a big trunk of commands, and thus
+		 * need to wait for a long time for completion.
+		 *
+		 * a better way is to detect ring idle in a delayed fashion,
+		 * e.g. in interrupt handler. That should remove this tricky
+		 * multi-iteration logic simpler
+		 */
+		acthd1 = VGT_MMIO_READ(pdev, VGT_ACTHD(id));
+		busy = wait_for_atomic(ring_is_empty(pdev, id), 50);
+		for (i = 0; i < 3; i++) {
+			if (!busy)
+				break;
+
+			vgt_info("(%d) check whether ring actually stops\n", i);
+			acthd2 = VGT_MMIO_READ(pdev, VGT_ACTHD(id));
+			if (acthd1 != acthd2) {
+				vgt_info("ring still moves (%x->%x)\n",
+					acthd1, acthd2);
+				acthd1 = acthd2;
+			}
+
+			vgt_info("trigger another wait...\n");
+			busy = wait_for_atomic(ring_is_empty(pdev, id),
+				VGT_RING_TIMEOUT);
+		}
+
+		if (busy) {
+			vgt_err("Ugh...it's a real hang!!!\n");
+			return false;
+		} else {
+			vgt_warn("ring idle now... after extra wait\n");
+		}
+	}
+
+	/* may do some jobs here to make sure ring idle */
+	if (wait_for_atomic(ring_is_xxx(pdev, id), VGT_RING_TIMEOUT) != 0) {
+		vgt_err("Timeout wait %d ms for ring(%d) xxx\n",
+			VGT_RING_TIMEOUT, id);
+		return false;
+	}
+
+	return true;
+}
+
+static bool vgt_ring_check_offset_passed(struct pgt_device *pdev, int ring_id,
+	u32 head, u32 tail, u32 offset)
+{
+	bool rc = true;
+	/*
+	 * Check if ring buffer is wrapped, otherwise there
+	 * are remaining instruction in ringbuffer, but no
+	 * interrupt anymore, so switch to polling mode.
+	 */
+	rc = (head > tail) ? false : true;
+
+	if (head > offset)
+		rc = (offset > tail) ? true : rc;
+	else
+		rc = (offset > tail) ? rc : false;
+
+	return rc;
+}
+
+static bool vgt_rings_need_idle_notification(struct pgt_device *pdev)
+{
+	int i;
+	u32 head, tail, offset;
+
+	for (i=0; i < pdev->max_engines; i++) {
+		if (pdev->ring_buffer[i].need_irq) {
+			head = VGT_MMIO_READ(pdev, RB_HEAD(pdev, i)) & RB_HEAD_OFF_MASK;
+			tail = VGT_MMIO_READ(pdev, RB_TAIL(pdev, i)) & RB_TAIL_OFF_MASK;
+			if (head != tail) {
+				offset = pdev->ring_buffer[i].ip_offset;
+				if (!vgt_ring_check_offset_passed(pdev, i, head, tail, offset))
+					break;
+			}
+		}
+	}
+	/*
+	 * If all the rings has been checked, mean there are no more user
+	 * interrupt instructions remain in the ring buffer, so switch to
+	 * polling mode, otherwise return false and wait for next interrupt.
+	 */
+	return (i == pdev->max_engines) ? true : false;
+}
+
+void vgt_check_pending_context_switch(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+
+	if (pdev->ctx_switch_pending) {
+		if (vgt_rings_need_idle_notification(pdev)) {
+			pdev->ctx_switch_pending = false;
+			vgt_raise_request(pdev, VGT_REQUEST_CTX_SWITCH);
+		}
+	}
+}
+
+bool idle_rendering_engines(struct pgt_device *pdev, int *id)
+{
+	int i;
+
+	/*
+	 * Though engines are checked in order, no further CMDs
+	 * are allowed from VM due to holding the big lock.
+	 */
+	for (i=0; i < pdev->max_engines; i++) {
+		struct vgt_rsvd_ring *ring = &pdev->ring_buffer[i];
+
+		if (!ring->need_switch)
+			continue;
+
+		if ( !idle_render_engine(pdev, i) ) {
+			*id = i;
+			return false;
+		}
+	}
+	return true;
+}
+
+static inline bool stop_ring(struct pgt_device *pdev, int id)
+{
+	VGT_MMIO_WRITE(pdev, pdev->ring_mi_mode[id],
+			_REGBIT_MI_STOP_RINGS | (_REGBIT_MI_STOP_RINGS << 16));
+
+	if (wait_for_atomic(ring_is_stopped(pdev, id), VGT_RING_TIMEOUT)) {
+		vgt_err("Timeout stop ring (%d) for %d ms\n",
+			id, VGT_RING_TIMEOUT);
+		return false;
+	}
+
+	return true;
+}
+
+static inline void start_ring(struct pgt_device *pdev, int id)
+{
+	VGT_MMIO_WRITE(pdev, pdev->ring_mi_mode[id],
+			_REGBIT_MI_STOP_RINGS << 16);
+	VGT_POST_READ(pdev, pdev->ring_mi_mode[id]);
+}
+
+/*
+ * write to head is undefined when ring is enabled.
+ *
+ * so always invoke this disable action when recovering a new ring setting
+ */
+static inline void disable_ring(struct pgt_device *pdev, int id)
+{
+	/* disable the ring */
+	VGT_WRITE_CTL(pdev, id, 0);
+	/* by ktian1. no source for this trick */
+	VGT_POST_READ_CTL(pdev, id);
+}
+
+static inline void restore_ring_ctl(struct pgt_device *pdev, int id,
+		vgt_reg_t val)
+{
+	VGT_WRITE_CTL(pdev, id, val);
+	VGT_POST_READ_CTL(pdev, id);
+}
+
+static inline void enable_ring(struct pgt_device *pdev, int id, vgt_reg_t val)
+{
+	ASSERT(val & _RING_CTL_ENABLE);
+	restore_ring_ctl(pdev, id, val);
+}
+
+bool vgt_vrings_empty(struct vgt_device *vgt)
+{
+	int id;
+	vgt_ringbuffer_t *vring;
+	for (id = 0; id < vgt->pdev->max_engines; id++)
+		if (test_bit(id, vgt->enabled_rings)) {
+			vring = &vgt->rb[id].vring;
+			if (vgt->pdev->enable_execlist) {
+				int i;
+				struct vgt_exec_list *el_slots;
+				el_slots = vgt->rb[id].execlist_slots;
+				for (i = 0; i < EL_QUEUE_SLOT_NUM; ++ i) {
+					if (el_slots[i].status != EL_EMPTY)
+						return false;
+				}
+			} else {
+				if (!RB_HEAD_TAIL_EQUAL(vring->head, vring->tail))
+					return false;
+			}
+		}
+
+	return true;
+}
+
+static void vgt_save_ringbuffer(struct vgt_device *vgt, int id)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	vgt_state_ring_t *rb = &vgt->rb[id];
+
+	/* only head is HW updated */
+	rb->sring.head = VGT_READ_HEAD(pdev, id);
+	rb->vring.head = rb->sring.head;
+}
+
+/* restore ring buffer structures to a empty state (head==tail) */
+static void vgt_restore_ringbuffer(struct vgt_device *vgt, int id)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	vgt_ringbuffer_t *srb = &vgt->rb[id].sring;
+	struct vgt_rsvd_ring *ring = &pdev->ring_buffer[id];
+
+	if (pdev->enable_execlist)
+		return;
+
+	if (!ring->need_switch)
+		return;
+
+	vgt_dbg(VGT_DBG_RENDER, "restore ring: [%x, %x, %x, %x] \n", srb->head, srb->tail,
+		VGT_READ_HEAD(pdev, id),
+		VGT_READ_TAIL(pdev, id));
+
+	disable_ring(pdev, id);
+
+	VGT_WRITE_START(pdev, id, srb->start);
+
+	/* make head==tail when enabling the ring buffer */
+	VGT_WRITE_HEAD(pdev, id, srb->head);
+	VGT_WRITE_TAIL(pdev, id, srb->head);
+
+	restore_ring_ctl(pdev, id, srb->ctl);
+	/*
+	 * FIXME: One weird issue observed when switching between dom0
+	 * and win8 VM. The video ring #1 is not used by both dom0 and
+	 * win8 (head=tail=0), however sometimes after switching back
+	 * to win8 the video ring may enter a weird state that VCS cmd
+	 * parser continues to parse the whole ring (fulled with ZERO).
+	 * Sometimes it ends for one whole loop when head reaches back
+	 * to 0. Sometimes it may parse indefinitely so that there's
+	 * no way to wait for the ring empty.
+	 *
+	 * Add a posted read works around the issue. In the future we
+	 * can further optimize by not switching unused ring.
+	 */
+	VGT_POST_READ_HEAD(pdev, id);
+	vgt_dbg(VGT_DBG_RENDER, "restore ring: [%x, %x]\n",
+		VGT_READ_HEAD(pdev, id),
+		VGT_READ_TAIL(pdev, id));
+}
+
+void vgt_kick_off_ringbuffers(struct vgt_device *vgt)
+{
+	int i;
+	struct pgt_device *pdev = vgt->pdev;
+
+	for (i = 0; i < pdev->max_engines; i++) {
+		struct vgt_rsvd_ring *ring = &pdev->ring_buffer[i];
+
+		if (!ring->need_switch)
+			continue;
+
+		start_ring(pdev, i);
+		vgt_submit_commands(vgt, i);
+	}
+}
+
+void vgt_kick_off_execution(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	if (pdev->enable_execlist)
+		vgt_kick_off_execlists(vgt);
+	else
+		vgt_kick_off_ringbuffers(vgt);
+}
+
+/* FIXME: need audit all render resources carefully */
+vgt_reg_t vgt_render_regs[] = {
+	/* mode ctl regs. sync with vgt_mode_ctl_regs */
+	_REG_ARB_MODE,
+
+	_REG_CACHE_MODE_0,
+	_REG_RCS_MI_MODE,
+	_REG_GFX_MODE,
+
+	_REG_VCS_MI_MODE,
+	_REG_BCS_MI_MODE,
+
+	_REG_RCS_INSTPM,
+	_REG_VCS_INSTPM,
+	_REG_BCS_INSTPM,
+
+	_REG_GT_MODE,
+	_REG_CACHE_MODE_1,
+
+	/* other regs */
+
+	_REG_RCS_HWSTAM,
+	_REG_BCS_HWSTAM,
+	_REG_VCS_HWSTAM,
+
+	_REG_RCS_HWS_PGA,
+	_REG_BCS_HWS_PGA,
+	_REG_VCS_HWS_PGA,
+
+	_REG_RCS_EXCC,
+	_REG_BCS_EXCC,
+	_REG_VCS_EXCC,
+
+	_REG_RCS_UHPTR,
+	_REG_BCS_UHPTR,
+	_REG_VCS_UHPTR,
+
+	_REG_TILECTL,
+
+	_REG_BRSYNC,
+	_REG_BVSYNC,
+	_REG_RBSYNC,
+	_REG_RVSYNC,
+	_REG_VBSYNC,
+	_REG_VRSYNC,
+};
+
+vgt_reg_t vgt_gen7_render_regs[] = {
+	/* Add IVB register, so they all got pass-through */
+
+	_REG_ARB_MODE,
+
+	_REG_BCS_HWS_PGA_GEN7,
+	_REG_RCS_HWS_PGA,
+	_REG_VCS_HWS_PGA,
+	_REG_VECS_HWS_PGA,
+
+	_REG_BCS_MI_MODE,
+	_REG_BCS_BLT_MODE_IVB,
+	_REG_BCS_INSTPM,
+	_REG_BCS_HWSTAM,
+	_REG_BCS_EXCC,
+	_REG_BCS_UHPTR,
+	_REG_BRSYNC,
+	_REG_BVSYNC,
+	_REG_BVESYNC,
+
+	_REG_RCS_GFX_MODE_IVB,
+	_REG_RCS_HWSTAM,
+	_REG_RCS_UHPTR,
+	_REG_RBSYNC,
+	_REG_RVSYNC,
+	_REG_RVESYNC,
+
+	_REG_VCS_MI_MODE,
+	_REG_VCS_MFX_MODE_IVB,
+	_REG_VCS_INSTPM,
+	_REG_VCS_HWSTAM,
+	_REG_VCS_EXCC,
+	_REG_VCS_UHPTR,
+	_REG_VBSYNC,
+	_REG_VRSYNC,
+	_REG_VVESYNC,
+
+	_REG_VECS_MI_MODE,
+	_REG_VEBOX_MODE,
+	_REG_VECS_INSTPM,
+	_REG_VECS_HWSTAM,
+	_REG_VECS_EXCC,
+	_REG_VERSYNC,
+	_REG_VEBSYNC,
+	_REG_VEVSYNC,
+
+	_REG_RCS_BB_PREEMPT_ADDR,
+
+	/* more check for this group later */
+	0x23bc,
+	0x2448,
+	0x244c,
+	0x2450,
+	0x2454,
+	0x7034,
+	0x2b00,
+	0x91b8,
+	0x91bc,
+	0x91c0,
+	0x91c4,
+	0x9150,
+	0x9154,
+	0x9160,
+	0x9164,
+
+	0x4040,
+	0xb010,
+	0xb020,
+	0xb024,
+
+	//_REG_UCG_CTL1,
+	//_REG_UCG_CTL2,
+	//_REG_DISPLAY_CHICKEN_BITS_1,
+	//_REG_DSPCLK_GATE_D,
+	//_REG_SUPER_QUEUE_CONFIG,
+	_REG_ECOCHK,
+	//_REG_MISC_CLOCK_GATING,
+
+	0x2450,
+	0x20dc,
+	_REG_3D_CHICKEN3,
+	0x2088,
+	0x20e4,
+	_REG_GEN7_COMMON_SLICE_CHICKEN1,
+	_REG_GEN7_L3CNTLREG1,
+	_REG_GEN7_L3_CHICKEN_MODE_REGISTER,
+	_REG_GEN7_SQ_CHICKEN_MBCUNIT_CONFIG,
+	0x20a0,
+	0x20e8,
+	0xb038,
+};
+
+vgt_reg_t vgt_gen8_render_regs[] = {
+	/* Hardware Status Page Address Registers. */
+	_REG_HWS_PGA,
+	0x12080,
+	0x1a080,
+	0x1c080,
+	0x22080,
+
+	_REG_GEN8_PRIVATE_PAT,
+	_REG_GEN8_PRIVATE_PAT + 4,
+
+	_REG_BCS_MI_MODE,
+	_REG_BCS_BLT_MODE_IVB,
+	_REG_BCS_INSTPM,
+	_REG_BCS_HWSTAM,
+	_REG_BCS_EXCC,
+
+        /* Execlist Status Registers */
+        _REG_RCS_EXECLIST_STATUS,
+        _REG_VCS_EXECLIST_STATUS,
+        _REG_VECS_EXECLIST_STATUS,
+        _REG_VCS2_EXECLIST_STATUS,
+        _REG_BCS_EXECLIST_STATUS,
+};
+
+static void __vgt_rendering_save(struct vgt_device *vgt, int num, vgt_reg_t *regs)
+{
+	vgt_reg_t	*sreg, *vreg;	/* shadow regs */
+	int i;
+
+	sreg = vgt->state.sReg;
+	vreg = vgt->state.vReg;
+
+	for (i=0; i<num; i++) {
+		int reg = regs[i];
+		//if (reg_hw_status(vgt->pdev, reg)) {
+		/* FIXME: only hw update reg needs save */
+		if (!reg_mode_ctl(vgt->pdev, reg))
+		{
+			__sreg(vgt, reg) = VGT_MMIO_READ(vgt->pdev, reg);
+			__vreg(vgt, reg) = mmio_h2g_gmadr(vgt, reg, __sreg(vgt, reg));
+			vgt_dbg(VGT_DBG_RENDER, "....save mmio (%x) with (%x)\n", reg, __sreg(vgt, reg));
+		}
+	}
+}
+
+/* For save/restore global states difference between VMs.
+ * Other context states should be covered by normal context switch later. */
+static void vgt_rendering_save_mmio(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+
+	/*
+	 * both save/restore refer to the same array, so it's
+	 * enough to track only save part
+	 */
+	pdev->in_ctx_switch = 1;
+	if (IS_SNB(pdev))
+		__vgt_rendering_save(vgt,
+				ARRAY_NUM(vgt_render_regs),
+				&vgt_render_regs[0]);
+	else if (IS_IVB(pdev) || IS_HSW(pdev))
+		__vgt_rendering_save(vgt,
+				ARRAY_NUM(vgt_gen7_render_regs),
+				&vgt_gen7_render_regs[0]);
+	else if (IS_BDW(pdev))
+		__vgt_rendering_save(vgt,
+				ARRAY_NUM(vgt_gen8_render_regs),
+				&vgt_gen8_render_regs[0]);
+
+	pdev->in_ctx_switch = 0;
+}
+
+static void __vgt_rendering_restore (struct vgt_device *vgt, int num_render_regs, vgt_reg_t *render_regs)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	vgt_reg_t	*sreg, *vreg;	/* shadow regs */
+	vgt_reg_t  res_val; /*restored value of mmio register*/
+	int i;
+
+	sreg = vgt->state.sReg;
+	vreg = vgt->state.vReg;
+
+	for (i = 0; i < num_render_regs; i++) {
+		int reg = render_regs[i];
+		vgt_reg_t val = __sreg(vgt, reg);
+
+		if (reg_mode_ctl(pdev, reg) && reg_aux_mode_mask(pdev, reg))
+			val |= reg_aux_mode_mask(pdev, reg);
+
+		/*
+		 * FIXME: there's regs only with some bits updated by HW. Need
+		 * OR vm's update with hw's bits?
+		 */
+		//if (!reg_hw_status(vgt->pdev, reg))
+		VGT_MMIO_WRITE(vgt->pdev, reg, val);
+		vgt_dbg(VGT_DBG_RENDER, "....restore mmio (%x) with (%x)\n", reg, val);
+
+		/* Use this post-read as a workaround for a gpu hang issue */
+		res_val = VGT_MMIO_READ(vgt->pdev, reg);
+
+		if(!vgt_validate_ctx_switch)
+			continue;
+		if(res_val == val)
+			continue;
+		if (!reg_mode_ctl(pdev, reg) ||
+			 ((res_val ^ val) & (reg_aux_mode_mask(pdev, reg) >> 16)))
+			vgt_warn("restore %x: failed:  val=%x, val_read_back=%x\n",
+				reg, val, res_val);
+	}
+}
+
+/*
+ * Restore MMIO registers per rendering context.
+ * (Not include ring buffer registers).
+ */
+static void vgt_rendering_restore_mmio(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+
+	if (IS_SNB(pdev))
+		__vgt_rendering_restore(vgt,
+				ARRAY_NUM(vgt_render_regs),
+				&vgt_render_regs[0]);
+	else if (IS_IVB(pdev) || IS_HSW(pdev))
+		__vgt_rendering_restore(vgt,
+				ARRAY_NUM(vgt_gen7_render_regs),
+				&vgt_gen7_render_regs[0]);
+	else if (IS_BDW(pdev))
+		__vgt_rendering_restore(vgt,
+				ARRAY_NUM(vgt_gen8_render_regs),
+				&vgt_gen8_render_regs[0]);
+}
+
+/*
+ * ring buffer usage in vGT driver is simple. We allocate a ring buffer
+ * big enough to contain all emitted CMDs in a render context switch,
+ * and thus emit function is implemented simply by sequentially advancing
+ * tail point, w/o the wrap handling requirement.
+ */
+static inline void vgt_ring_emit(struct vgt_rsvd_ring *ring,
+				u32 data)
+{
+	ASSERT(ring->tail + 4 < ring->size);
+	writel(data, ring->virtual_start + ring->tail);
+	ring->tail += 4;
+}
+
+static void vgt_ring_emit_cmds(struct vgt_rsvd_ring *ring, char *buf, int size)
+{
+	ASSERT(ring->tail + size < ring->size);
+	memcpy(ring->virtual_start + ring->tail, buf, size);
+	ring->tail += size;
+}
+
+void vgt_ring_init(struct pgt_device *pdev, int id)
+{
+	struct vgt_rsvd_ring *ring = &pdev->ring_buffer[id];
+
+	ring->pdev = pdev;
+	ring->id = id;
+	ring->size = VGT_RSVD_RING_SIZE;
+	ring->start = aperture_2_gm(pdev,
+			rsvd_aperture_alloc(pdev, ring->size));
+	ring->virtual_start = v_aperture(pdev, ring->start);
+	ring->head = 0;
+	ring->tail = 0;
+
+	switch (id) {
+	case RING_BUFFER_RCS:
+		ring->stateless = 0;
+		ring->need_switch = 1;
+		break;
+	case RING_BUFFER_VCS:
+	case RING_BUFFER_VCS2:
+	case RING_BUFFER_VECS:
+		ring->stateless = 1;
+		if (enable_video_switch)
+			ring->need_switch = 1;
+		else
+			ring->need_switch = 0;
+		break;
+	case RING_BUFFER_BCS:
+		ring->stateless = 1;
+		ring->need_switch = 1;
+		break;
+	default:
+		vgt_err("Unknown ring ID (%d)\n", id);
+		ASSERT(0);
+		break;
+	}
+}
+
+static bool vgt_setup_rsvd_ring(struct vgt_rsvd_ring *ring)
+{
+	struct pgt_device *pdev = ring->pdev;
+	u32 head;
+	int id = ring->id;
+
+	ring->head = 0;
+	ring->tail = 0;
+
+	disable_ring(pdev, id);
+
+	/* execute our ring */
+	VGT_WRITE_HEAD(pdev, id, 0);
+	VGT_WRITE_TAIL(pdev, id, 0);
+
+	head = VGT_READ_HEAD(pdev, id);
+	if (head != 0) {
+		VGT_WRITE_HEAD(pdev, id, 0);
+	}
+
+	VGT_WRITE_START(pdev, id, ring->start);
+
+	enable_ring(pdev, id, ((ring->size - PAGE_SIZE) & 0x1FF000) | 1);
+
+	if (wait_for_atomic(((VGT_READ_CTL(pdev, id) & 1) != 0 &&
+			VGT_READ_START(pdev, id) == ring->start &&
+			(VGT_READ_HEAD(pdev, id) & RB_HEAD_OFF_MASK) == 0),
+			VGT_RING_TIMEOUT)) {
+		vgt_err("Timeout setup rsvd ring-%d for %dms\n",
+			VGT_RING_TIMEOUT, id);
+		return false;
+	}
+
+	vgt_dbg(VGT_DBG_RENDER, "start vgt ring at 0x%x\n", ring->start);
+	return true;
+}
+
+static void vgt_ring_advance(struct vgt_rsvd_ring *ring)
+{
+	ring->tail &= ring->size - 1;
+	VGT_WRITE_TAIL(ring->pdev, ring->id, ring->tail);
+}
+
+static u32 hsw_null_context_cmds[] = {
+	0x7a000003, 0x01000000, 0x00000000, 0x00000000, 0x00000000, //0
+	0x69040000, //0x14
+	0x0, 0x0, 0x0, //0x18  //{0x12400001, 0x00138064, <addr>+0x124};
+	0x11000001, 0x0000b020, 0x00800040, //0x24
+	0x11000001, 0x0000b024, 0x00080410, //0x30
+	0x11000001, 0x0000b010, 0x00610000, //0x3c
+	0x78140001, 0x24000000, 0x80000000, //0x48
+	0x78200006, 0x00000000, 0x80000000, 0x00000000,
+	0x55800400, 0x00000000, 0x00000000, 0x00000000, //0x54
+	0x78130005, 0x00000000, 0x20000000, 0x02001808,
+	0x00000000, 0x00000000, 0x00000000, //0x74
+	0x781f000c, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, //
+	0x78100004, 0x00000000, 0x80010000, 0x0, 0x0, 0x0, //0x0c8
+	0x781b0005, 0x00010023, 0x0, 0x0, 0x0, 0x00000800, 0x0, //0x0e0
+	0x78110005, 0x0, 0x0, 0x0, 0x0, 0x01000000, 0x0, //0x0fc
+	0x781e0001, 0x0, 0x0, //0x118
+	0x781d0004, 0x0, 0x00010000, 0x0, 0x0, 0x0, //0x124
+	0x78120002, 0x0, 0x20000001, 0x0, //0x13c
+	0x781c0002, 0x0, 0x427c0000, 0x42800000, //0x14c
+	0x780c0000, 0x0, //0x15c
+	0x78300000, 0x00000018, //0x164
+	0x78310000, 0x0, //0x16c
+	0x78320000, 0x0, //0x174
+	0x78330000, 0x0, //0x17c
+	0x79120000, 0x0, //0x184
+	0x79130000, 0x0, //0x18c
+	0x79140000, 0x0, //0x194
+	0x79150000, 0x0, //0x19c
+	0x79160000, 0x0, //0x1a4
+	0x78150005, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, //0x1ac
+	0x78190005, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, //0x1c8
+	0x781a0005, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, //0x1e4
+	0x78160005, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, //0x200
+	0x78170005, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, //0x21c
+	0x79170101, 0x00000000, 0x80808080, 0x00000000,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, //0x238
+	0x79180002, 0x00000000, 0x00000000, 0x00000000,
+	0x79180002, 0x20000000, 0x00000000, 0x00000000,
+	0x79180002, 0x40000000, 0x00000000, 0x00000000,
+	0x79180002, 0x60000000, 0x00000000, 0x00000000, //0x644
+	0x6101000a, 0x00000001, 0x00000001, 0x00000001, //{0x6101000a, <addr> | 0x1, <addr> | 0x1, <addr> | 0x1,
+	0x00000001, 0x00000001, 0x00000001, 0x00000001, //0x1, <addr>|0x1, 0x1, 0x1,
+	0x00000001, 0x00000001, 0x00000001, 0x00000001,  //0x1, 0x1, 0x0, 0x0}//0x684
+	0x61020000, 0x00000000, //0x684
+	0x79000002, 0x00000000, 0x1fff1fff, 0x00000000, //0x6bc
+	0x78050005, 0xe0000000, 0x00000000, 0x00000000,
+	0x00000000, 0x00000000, 0x00000000, //0x6cc
+	0x79040002, 0x00000000, 0x00000000, 0x00000000,
+	0x79040002, 0x40000000, 0x00000000, 0x00000000,
+	0x79040002, 0x80000000, 0x00000000, 0x00000000,
+	0x79040002, 0xc0000000, 0x00000000, 0x00000000, //0x6e8
+	0x79080001, 0x00000000, 0x00000000, //0x728
+	0x790a0001, 0x00000000, 0x00000000, //0x734
+	0x8060001, 0x00000000, 0x00000000, //0x740
+	0x78070001, 0x00000000, 0x00000000, //0x74c
+	0x78040001, 0x0, 0x1, //0x758
+	0x79110000, 0x0, //0x764
+	0x79060000, 0x0, //0x76c
+	0x7907001f, //0x774
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, //0x778
+	0x790200ff,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, //0x7f8
+	0x790c00ff,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, //0xbfc
+	0x780a0001, 0x00000000, 0x00000000, //0x1000
+	0x78080083, //0x100c
+	0x00005000, 0x00000000, 0x00000000, 0x00000000,
+	0x04005000, 0x00000000, 0x00000000, 0x00000000,
+	0x08005000, 0x00000000, 0x00000000, 0x00000000,
+	0x0c005000, 0x00000000, 0x00000000, 0x00000000,
+	0x10005000, 0x00000000, 0x00000000, 0x00000000,
+	0x14005000, 0x00000000, 0x00000000, 0x00000000,
+	0x18005000, 0x00000000, 0x00000000, 0x00000000,
+	0x1c005000, 0x00000000, 0x00000000, 0x00000000,
+	0x20005000, 0x00000000, 0x00000000, 0x00000000,
+	0x24005000, 0x00000000, 0x00000000, 0x00000000,
+	0x28005000, 0x00000000, 0x00000000, 0x00000000,
+	0x2c005000, 0x00000000, 0x00000000, 0x00000000,
+	0x30005000, 0x00000000, 0x00000000, 0x00000000,
+	0x34005000, 0x00000000, 0x00000000, 0x00000000,
+	0x38005000, 0x00000000, 0x00000000, 0x00000000,
+	0x3c005000, 0x00000000, 0x00000000, 0x00000000,
+	0x40005000, 0x00000000, 0x00000000, 0x00000000,
+	0x44005000, 0x00000000, 0x00000000, 0x00000000,
+	0x48005000, 0x00000000, 0x00000000, 0x00000000,
+	0x4c005000, 0x00000000, 0x00000000, 0x00000000,
+	0x50005000, 0x00000000, 0x00000000, 0x00000000,
+	0x54005000, 0x00000000, 0x00000000, 0x00000000,
+	0x58005000, 0x00000000, 0x00000000, 0x00000000,
+	0x5c005000, 0x00000000, 0x00000000, 0x00000000,
+	0x60005000, 0x00000000, 0x00000000, 0x00000000,
+	0x64005000, 0x00000000, 0x00000000, 0x00000000,
+	0x68005000, 0x00000000, 0x00000000, 0x00000000,
+	0x6c005000, 0x00000000, 0x00000000, 0x00000000,
+	0x70005000, 0x00000000, 0x00000000, 0x00000000,
+	0x74005000, 0x00000000, 0x00000000, 0x00000000,
+	0x78005000, 0x00000000, 0x00000000, 0x00000000,
+	0x7c005000, 0x00000000, 0x00000000, 0x00000000,
+	0x80005000, 0x00000000, 0x00000000, 0x00000000, //0x1010
+	0x78090043, //0x1220
+	0x02000000, 0x22220000, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, //0x1224
+	0x680b0001, //0x1334
+	0x78260000, 0x0, //0x1338
+	0x78270000, 0x0, //0x1340
+	0x78280000, 0x0, //0x1348
+	0x78290000, 0x0, //0x1350
+	0x782a0000, 0x0, //0x1358
+	0x79190001, 0x00000060, 0x00000000, //0x1360
+	0x791a0001, 0x00000030, 0x00000000, //0x136c
+	0x791b0001, 0x00000030, 0x00000000, //0x1378
+	0x780e0000, 0x00000041, //0x1384
+	0x78240000, 0x000000c1, //0x138c
+	0x78250000, 0x00000081, //0x1394
+	0x782b0000, 0x00000000, //0x139c
+	0x782c0000, 0x00000000, //0x13a4
+	0x782d0000, 0x00000000, //0x13ac
+	0x782e0000, 0x00000000, //0x13b4
+	0x782f0000, 0x00000000, //0x13bc
+	0x790e0004, 0x0, 0x0, 0x0, 0x0, 0x0, //0x13c4
+	0x780f0000, 0x0, //0x13dc
+	0x78230000, 0x0, //0x13e4
+	0x78210000, 0xe0, //0x13ec
+	0x7b000005, 0x00000004, 0x00000001, 0x0, 0x1, 0x0, 0x0, //0x13f4
+};
+
+static u32 hsw_null_indirect_state[] = {
+	0x0, 0x0, //0x0
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, //0x08
+	0x0, 0x0, 0x3f800000, 0x3f800000, 0x3f800000, 0x3f800000, //0x040
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, //0x058
+	0x0, 0x0, 0x08000000, //0x080
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, //0x08c
+	0x00188221, 0x0, //0x0c0
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, //0x0c8
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, //0x0e0
+	0x0, //0x120
+	0x0, //0x124
+};
+
+static bool gen7_init_null_context(struct pgt_device *pdev, int id)
+{
+	struct vgt_rsvd_ring *ring = &pdev->ring_buffer[id];
+	int i;
+	vgt_reg_t	ccid;
+
+	if (!IS_HSW(pdev))
+		return false;
+
+	/* only RCS support HW context on HSW */
+	if ((id != RING_BUFFER_RCS) || ring->null_context)
+		return true;
+
+	/* assume no active usage so far */
+	ccid = VGT_MMIO_READ (pdev, _REG_CCID);
+	ASSERT(ccid == 0);
+	ASSERT(!VGT_READ_TAIL(pdev, id));
+
+	/* current ring buffer is dom0's. so switch first */
+	if (!vgt_setup_rsvd_ring(ring))
+		goto err;
+
+	ring->null_context = aperture_2_gm(pdev,
+			rsvd_aperture_alloc(pdev, SZ_CONTEXT_AREA_PER_RING));
+	ring->indirect_state = aperture_2_gm(pdev,
+			rsvd_aperture_alloc(pdev, SZ_INDIRECT_STATE));
+	memcpy((char *)v_aperture(pdev, ring->indirect_state),
+	       (char *)hsw_null_indirect_state,
+	       sizeof(hsw_null_indirect_state));
+
+	/* Make NULL context active */
+	vgt_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_DISABLE);
+	vgt_ring_emit(ring, MI_SET_CONTEXT);
+	vgt_ring_emit(ring, ring->null_context |
+			    MI_MM_SPACE_GTT |
+			    MI_SAVE_EXT_STATE_EN |
+			    MI_RESTORE_INHIBIT);
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_ENABLE);
+	/* 64 Byte alignment */
+	for (i = 5; i < 16; i++)
+		vgt_ring_emit(ring, MI_NOOP);
+
+	hsw_null_context_cmds[0x18/4] = 0x12400001;
+	hsw_null_context_cmds[(0x18 + 0x4)/4] = 0x138064;
+	hsw_null_context_cmds[(0x18 + 0x8)/4] = ring->indirect_state + 0x124;
+	hsw_null_context_cmds[(0x684 + 0x4)/4] = ring->indirect_state | 0x1;
+	hsw_null_context_cmds[(0x684 + 0x8)/4] = ring->indirect_state | 0x1;
+	hsw_null_context_cmds[(0x684 + 0xc)/4] = ring->indirect_state | 0x1;
+	hsw_null_context_cmds[(0x684 + 0x14)/4] = ring->indirect_state | 0x1;
+	vgt_ring_emit_cmds(ring, (char *)hsw_null_context_cmds,
+		sizeof(hsw_null_context_cmds));
+
+#if 0
+	{
+		char *p_contents = ring->virtual_start;
+		int i;
+		for (i = 0; i < ring->tail/4; i++) {
+			if (!(i % 8))
+				printk("\n[%08x]:", i * 4);
+			printk(" %8x", *((u32*)p_contents + i));
+		}
+		printk("\n");
+	}
+#endif
+
+	vgt_ring_advance(ring);
+	if (!idle_render_engine(pdev, id)) {
+		vgt_err("Ring-%d hang in null context init\n", id);
+		goto err;
+	}
+
+	vgt_ring_emit(ring, PIPE_CONTROL(5));
+	vgt_ring_emit(ring, PIPE_CONTROL_CS_STALL |
+			    PIPE_CONTROL_MEDIA_STATE_CLEAR |
+			    PIPE_CONTROL_RENDER_TARGET_CACHE_FLUSH |
+			    PIPE_CONTROL_STATE_CACHE_INVALIDATE);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+	/* save internal state to NULL context */
+	vgt_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_DISABLE);
+	vgt_ring_emit(ring, MI_SET_CONTEXT);
+	vgt_ring_emit(ring, 0 |
+			    MI_MM_SPACE_GTT |
+			    MI_RESTORE_INHIBIT);
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_ENABLE);
+	/* make sure no active context after this point */
+	vgt_ring_emit(ring, MI_LOAD_REGISTER_IMM |
+			    MI_LRI_BYTE1_DISABLE |
+			    MI_LRI_BYTE2_DISABLE |
+			    MI_LRI_BYTE3_DISABLE);
+	vgt_ring_emit(ring, _REG_CCID);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, PIPE_CONTROL(5));
+	vgt_ring_emit(ring, PIPE_CONTROL_CS_STALL |
+			    PIPE_CONTROL_RENDER_TARGET_CACHE_FLUSH |
+			    PIPE_CONTROL_INDIRECT_STATE_DISABLE);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+
+	vgt_ring_advance(ring);
+	if (!idle_render_engine(pdev, id)) {
+		vgt_err("Ring-%d hang in saving null context\n", id);
+		goto err;
+	}
+
+	ccid = VGT_MMIO_READ (pdev, _REG_CCID);
+	if (ccid != 0) {
+		vgt_err("Fail to invalidate CCID after null context init\n");
+		goto err;
+	}
+
+	/* Then recover dom0's ring structure */
+	if (!stop_ring(pdev, id))
+		goto err;
+	vgt_restore_ringbuffer(vgt_dom0, id);
+	start_ring(pdev, id);
+
+	/* Update dom0's initial context area */
+	memcpy((char *)v_aperture(pdev, vgt_dom0->rb[id].context_save_area),
+	       (char *)v_aperture(pdev, ring->null_context),
+	       SZ_CONTEXT_AREA_PER_RING);
+	return true;
+
+err:
+	ring->null_context = 0;
+	ring->indirect_state = 0;
+	vgt_err("NULL context initialization fails!\n");
+	return false;
+}
+
+static bool gen7_save_hw_context(int id, struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	vgt_state_ring_t *rb = &vgt->rb[id];
+	struct vgt_rsvd_ring *ring = &pdev->ring_buffer[id];
+	vgt_reg_t	ccid, new_ccid;
+
+	if (test_bit(RESET_INPROGRESS, &vgt->reset_flags))
+		return true;
+
+	/* pipeline flush */
+	vgt_ring_emit(ring, PIPE_CONTROL(5));
+	vgt_ring_emit(ring, PIPE_CONTROL_CS_STALL |
+			    PIPE_CONTROL_TLB_INVALIDATE |
+			    PIPE_CONTROL_FLUSH_ENABLE);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+
+	vgt_ring_emit(ring, PIPE_CONTROL(5));
+	vgt_ring_emit(ring, PIPE_CONTROL_RENDER_TARGET_CACHE_FLUSH |
+			    PIPE_CONTROL_FLUSH_ENABLE |
+			    PIPE_CONTROL_VF_CACHE_INVALIDATE |
+			    PIPE_CONTROL_CONST_CACHE_INVALIDATE |
+			    PIPE_CONTROL_STATE_CACHE_INVALIDATE);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+
+#if 0
+	/*
+	 * Activate XenGT context for prev
+	 * Guest may have an active context already. Better to not clobber
+	 * that area, and instead have full control on the context save
+	 * area directly in XenGT driver.
+	 */
+	ccid = rb->context_save_area |
+	       CCID_EXTENDED_STATE_SAVE_ENABLE |
+	       CCID_EXTENDED_STATE_RESTORE_ENABLE |
+	       CCID_VALID;
+	vgt_ring_emit(ring, MI_LOAD_REGISTER_IMM);
+	vgt_ring_emit(ring, _REG_CCID);
+	vgt_ring_emit(ring, ccid);
+
+	/* pipeline flush */
+	vgt_ring_emit(ring, PIPE_CONTROL(5));
+	vgt_ring_emit(ring, PIPE_CONTROL_CS_STALL |
+			    PIPE_CONTROL_TLB_INVALIDATE |
+			    PIPE_CONTROL_FLUSH_ENABLE |
+			    PIPE_CONTROL_POST_SYNC_IMM |
+			    PIPE_CONTROL_POST_SYNC_GLOBAL_GTT);
+	vgt_ring_emit(ring, vgt_data_ctx_magic(pdev));
+	vgt_ring_emit(ring, ++pdev->magic);
+	vgt_ring_emit(ring, 0);
+
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_NOOP);
+
+	/* submit cmds */
+	vgt_ring_advance(ring);
+
+	if (!ring_wait_for_completion(pdev, id)) {
+		vgt_err("change CCID to XenGT save context: commands unfinished\n");
+		return false;
+	}
+
+	if (VGT_MMIO_READ(pdev, _REG_CCID) != ccid) {
+		vgt_err("change CCID to XenGT save context: fail [%x, %x]\n",
+			VGT_MMIO_READ(pdev, _REG_CCID), ccid);
+		return false;
+	}
+
+	/* Save context and switch to NULL context */
+	vgt_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_DISABLE);
+	vgt_ring_emit(ring, MI_SET_CONTEXT);
+	vgt_ring_emit(ring, ring->null_context |
+			    MI_MM_SPACE_GTT |
+			    MI_SAVE_EXT_STATE_EN |
+			    MI_RESTORE_EXT_STATE_EN |
+			    MI_FORCE_RESTORE);
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_ENABLE);
+#else
+	/* FIXME: too many CCID changes looks not working. So
+	 * fall back to original style by using guest context directly
+	 */
+	if (vgt->has_context) {
+		rb->active_vm_context = VGT_MMIO_READ(pdev, _REG_CCID);
+		rb->active_vm_context &= 0xfffff000;
+	}
+
+	vgt_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_DISABLE);
+	vgt_ring_emit(ring, MI_SET_CONTEXT);
+	vgt_ring_emit(ring, rb->context_save_area |
+			    MI_RESTORE_INHIBIT |
+			    MI_SAVE_EXT_STATE_EN |
+			    MI_RESTORE_EXT_STATE_EN);
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_ENABLE);
+#endif
+
+	/* pipeline flush */
+	vgt_ring_emit(ring, PIPE_CONTROL(5));
+	vgt_ring_emit(ring, PIPE_CONTROL_CS_STALL |
+			    PIPE_CONTROL_TLB_INVALIDATE |
+			    PIPE_CONTROL_FLUSH_ENABLE |
+			    PIPE_CONTROL_MEDIA_STATE_CLEAR |
+			    PIPE_CONTROL_DC_FLUSH_ENABLE |
+			    PIPE_CONTROL_RENDER_TARGET_CACHE_FLUSH |
+			    PIPE_CONTROL_POST_SYNC_IMM |
+			    PIPE_CONTROL_POST_SYNC_GLOBAL_GTT);
+	vgt_ring_emit(ring, vgt_data_ctx_magic(pdev));
+	vgt_ring_emit(ring, ++pdev->magic);
+	vgt_ring_emit(ring, 0);
+
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_NOOP);
+
+	/* submit cmds */
+	vgt_ring_advance(ring);
+
+	if (!ring_wait_for_completion(pdev, id)) {
+		vgt_err("save context commands unfinished\n");
+		return false;
+	}
+
+	ccid = VGT_MMIO_READ (pdev, _REG_CCID);
+#if 0
+	new_ccid = ring->null_context;
+#else
+	new_ccid = rb->context_save_area;
+#endif
+	if ((ccid & GTT_PAGE_MASK) != (new_ccid & GTT_PAGE_MASK)) {
+		vgt_err("vGT: CCID isn't changed [%x, %lx]\n", ccid, (unsigned long)new_ccid);
+		return false;
+	}
+
+	return true;
+}
+
+struct reg_mask_t {
+	u32		reg;
+	u8		mask;
+	vgt_reg_t	val;
+};
+
+static struct reg_mask_t rcs_reset_mmio[] = {
+	{0x4080, 0},
+	{0x2134, 0},
+	{0x20c0, 1},
+	{0x20a8, 0},
+
+	{0x7000, 1},
+	{0x209c, 1},
+	{0x2090, 1},
+	{0x9400, 0},
+
+	{0x9404, 0},
+	{0x42000, 0},
+	{0x42020, 0},
+	{0x902c, 0},
+
+	{0x4090, 0},
+	{0x9424, 0},
+	{0x229c, 1},
+	{0x2044, 0},
+
+	{0x20a0, 0},
+	{0x7004, 1},
+	{0x20dc, 1},
+
+	{0x2220, 0},
+	{0x2228, 0},
+	{0x2180, 0},
+
+	{0x2054, 0},
+};
+
+static bool vgt_reset_engine(struct pgt_device *pdev, int id)
+{
+	int i;
+	vgt_reg_t head, tail, start, ctl;
+	vgt_reg_t val, val1;
+
+	if (id != RING_BUFFER_RCS) {
+		vgt_err("ring-%d reset unsupported\n", id);
+		return false;
+	}
+
+	/* save reset context */
+	for (i = 0; i < ARRAY_NUM(rcs_reset_mmio); i++) {
+		struct reg_mask_t *r = &rcs_reset_mmio[i];
+
+		if (r->reg == 0x2220 || r->reg == 0x2228)
+			r->val = VGT_MMIO_READ(pdev, r->reg + 0x10000);
+		else
+			r->val = VGT_MMIO_READ(pdev, r->reg);
+
+		if (r->mask)
+			r->val |= 0xFFFF0000;
+	}
+
+	head = VGT_READ_HEAD(pdev, id);
+	tail = VGT_READ_TAIL(pdev, id);
+	start = VGT_READ_START(pdev, id);
+	ctl = VGT_READ_CTL(pdev, id);
+
+	/* trigger engine specific reset */
+	VGT_MMIO_WRITE(pdev, _REG_GEN6_GDRST, _REGBIT_GEN6_GRDOM_RENDER);
+
+#define GDRST_COUNT 0x1000
+	/* wait for reset complete */
+	for (i = 0; i < GDRST_COUNT; i++) {
+		if (!(VGT_MMIO_READ(pdev, _REG_GEN6_GDRST) &
+			_REGBIT_GEN6_GRDOM_RENDER))
+			break;
+	}
+
+	if (i == GDRST_COUNT) {
+		vgt_err("ring-%d engine reset incomplete\n", id);
+		return false;
+	}
+
+	/* restore reset context */
+	for (i = 0; i < ARRAY_NUM(rcs_reset_mmio); i++) {
+		struct reg_mask_t *r = &rcs_reset_mmio[i];
+
+		VGT_MMIO_WRITE(pdev, r->reg, r->val);
+	}
+
+	VGT_WRITE_CTL(pdev, id, 0);
+	VGT_WRITE_START(pdev, id, start);
+	VGT_WRITE_HEAD(pdev, id, head);
+	VGT_WRITE_TAIL(pdev, id, tail);
+
+	val = VGT_MMIO_READ(pdev, 0x2214);
+	val &= 0xFFFFFFFE;
+	val1 = VGT_MMIO_READ(pdev, 0x138064);
+	if (val1 & 0x3) {
+		if (val1 & 0x1)
+			val |= 0x1;
+	} else if (val1 & 0x8) {
+		val |= 0x1;
+	}
+	VGT_MMIO_WRITE(pdev, 0x2214, val);
+
+	VGT_WRITE_CTL(pdev, id, ctl);
+	VGT_POST_READ_TAIL(pdev, id);
+	VGT_POST_READ_HEAD(pdev, id);
+	VGT_POST_READ_START(pdev, id);
+	VGT_POST_READ_CTL(pdev, id);
+
+	return true;
+}
+
+static bool gen7_restore_hw_context(int id, struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	vgt_state_ring_t	*rb = &vgt->rb[id];
+	struct vgt_rsvd_ring *ring = &pdev->ring_buffer[id];
+
+	/* sync between vReg and saved context */
+	//update_context(vgt, rb->context_save_area);
+
+	/* pipeline flush */
+	vgt_ring_emit(ring, PIPE_CONTROL(5));
+	vgt_ring_emit(ring, PIPE_CONTROL_CS_STALL |
+			    PIPE_CONTROL_TLB_INVALIDATE |
+			    PIPE_CONTROL_FLUSH_ENABLE);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+
+#if 0
+	/*
+	 * we don't want to clobber the null context. so invalidate
+	 * the current context before restoring next instance
+	 */
+	vgt_ring_emit(ring, MI_LOAD_REGISTER_IMM |
+			    MI_LRI_BYTE1_DISABLE |
+			    MI_LRI_BYTE2_DISABLE |
+			    MI_LRI_BYTE3_DISABLE);
+	vgt_ring_emit(ring, _REG_CCID);
+	vgt_ring_emit(ring, 0);
+
+	/* pipeline flush */
+	vgt_ring_emit(ring, PIPE_CONTROL(5));
+	vgt_ring_emit(ring, PIPE_CONTROL_CS_STALL |
+			    PIPE_CONTROL_TLB_INVALIDATE |
+			    PIPE_CONTROL_FLUSH_ENABLE |
+			    PIPE_CONTROL_POST_SYNC_IMM |
+			    PIPE_CONTROL_POST_SYNC_GLOBAL_GTT);
+	vgt_ring_emit(ring, vgt_data_ctx_magic(pdev));
+	vgt_ring_emit(ring, ++pdev->magic);
+	vgt_ring_emit(ring, 0);
+
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_NOOP);
+
+	/* submit cmds */
+	vgt_ring_advance(ring);
+
+	if (!ring_wait_for_completion(pdev, id)) {
+		vgt_err("Invalidate CCID after NULL restore: commands unfinished\n");
+		return false;
+	}
+
+	if (VGT_MMIO_READ(pdev, _REG_CCID) != 0) {
+		vgt_err("Invalidate CCID after NULL restore: fail [%x, %x]\n",
+			VGT_MMIO_READ(pdev, _REG_CCID), 0);
+		return false;
+	}
+#endif
+
+	/* restore HW context */
+	vgt_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_DISABLE);
+	vgt_ring_emit(ring, MI_SET_CONTEXT);
+	vgt_ring_emit(ring, rb->context_save_area |
+			    MI_MM_SPACE_GTT |
+			    MI_SAVE_EXT_STATE_EN |
+			    MI_RESTORE_EXT_STATE_EN |
+			    MI_FORCE_RESTORE);
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_ENABLE);
+
+#if 0
+	vgt_ring_emit(ring, DUMMY_3D);
+	vgt_ring_emit(ring, PRIM_TRILIST);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, MI_NOOP);
+#endif
+
+	/* pipeline flush */
+	vgt_ring_emit(ring, PIPE_CONTROL(5));
+	vgt_ring_emit(ring, PIPE_CONTROL_CS_STALL |
+			    PIPE_CONTROL_TLB_INVALIDATE |
+			    PIPE_CONTROL_FLUSH_ENABLE |
+			    PIPE_CONTROL_MEDIA_STATE_CLEAR |
+			    PIPE_CONTROL_POST_SYNC_IMM |
+			    PIPE_CONTROL_POST_SYNC_GLOBAL_GTT);
+	vgt_ring_emit(ring, vgt_data_ctx_magic(pdev));
+	vgt_ring_emit(ring, ++pdev->magic);
+	vgt_ring_emit(ring, 0);
+
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_NOOP);
+
+	/* submit CMDs */
+	vgt_ring_advance(ring);
+
+	if (!ring_wait_for_completion(pdev, id)) {
+		vgt_err("restore context switch commands unfinished\n");
+		return false;
+	}
+
+#if 0
+	/* then restore current context to whatever VM expects */
+	vgt_ring_emit(ring, MI_LOAD_REGISTER_IMM);
+	vgt_ring_emit(ring, _REG_CCID);
+	vgt_ring_emit(ring, __vreg(vgt, _REG_CCID));
+
+	/* pipeline flush */
+	vgt_ring_emit(ring, PIPE_CONTROL(5));
+	vgt_ring_emit(ring, PIPE_CONTROL_CS_STALL |
+			    PIPE_CONTROL_TLB_INVALIDATE |
+			    PIPE_CONTROL_FLUSH_ENABLE |
+			    PIPE_CONTROL_POST_SYNC_IMM |
+			    PIPE_CONTROL_POST_SYNC_GLOBAL_GTT);
+	vgt_ring_emit(ring, vgt_data_ctx_magic(pdev));
+	vgt_ring_emit(ring, ++pdev->magic);
+	vgt_ring_emit(ring, 0);
+
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_NOOP);
+
+	/* submit CMDs */
+	vgt_ring_advance(ring);
+
+	if (!ring_wait_for_completion(pdev, id)) {
+		vgt_err("Restore VM CCID: commands unfinished\n");
+		return false;
+	}
+
+	if (VGT_MMIO_READ(pdev, _REG_CCID) != __vreg(vgt, _REG_CCID)) {
+		vgt_err("Restore VM CCID: fail [%x, %x]\n",
+			VGT_MMIO_READ(pdev, _REG_CCID),
+			__vreg(vgt, _REG_CCID));
+		return false;
+	}
+#else
+	if (vgt->has_context && rb->active_vm_context) {
+		vgt_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_DISABLE);
+		vgt_ring_emit(ring, MI_SET_CONTEXT);
+		vgt_ring_emit(ring, rb->active_vm_context |
+				MI_MM_SPACE_GTT |
+				MI_SAVE_EXT_STATE_EN |
+				MI_RESTORE_EXT_STATE_EN |
+				MI_FORCE_RESTORE);
+		vgt_ring_emit(ring, MI_NOOP);
+		vgt_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_ENABLE);
+
+		vgt_ring_emit(ring, DUMMY_3D);
+		vgt_ring_emit(ring, PRIM_TRILIST);
+		vgt_ring_emit(ring, 0);
+		vgt_ring_emit(ring, 0);
+		vgt_ring_emit(ring, 0);
+		vgt_ring_emit(ring, 0);
+		vgt_ring_emit(ring, 0);
+		vgt_ring_emit(ring, MI_NOOP);
+
+		vgt_ring_emit(ring, MI_STORE_DATA_IMM | MI_SDI_USE_GTT);
+		vgt_ring_emit(ring, 0);
+		vgt_ring_emit(ring, vgt_data_ctx_magic(pdev));
+		vgt_ring_emit(ring, ++pdev->magic);
+		vgt_ring_emit(ring, 0);
+		vgt_ring_emit(ring, 0);
+		vgt_ring_emit(ring, MI_NOOP);
+
+		vgt_ring_advance(ring);
+
+		if (!ring_wait_for_completion(pdev, id)) {
+			vgt_err("change to VM context switch commands unfinished\n");
+			return false;
+		}
+	}
+#endif
+	return true;
+}
+
+static void dump_regs_on_err(struct pgt_device *pdev)
+{
+	static vgt_reg_t regs[] =  {
+		0x2054,
+		0x12054,
+		0x22054,
+		0x1A054,
+		0xA098,
+		0xA09C,
+		0xA0A8,
+		0xA0AC,
+		0xA0B4,
+		0xA0B8,
+		0xA090,
+		0xA094};
+
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(regs); i++)
+		vgt_info("reg=0x%x, val=0x%x\n", regs[i],
+			VGT_MMIO_READ(pdev, regs[i]));
+}
+
+static bool gen7_ring_switch(struct pgt_device *pdev,
+		enum vgt_ring_id ring_id,
+		struct vgt_device *prev,
+		struct vgt_device *next)
+{
+	bool rc = false;
+	struct vgt_rsvd_ring *ring = &pdev->ring_buffer[ring_id];
+
+	/* STEP-a: stop the ring */
+	if (!stop_ring(pdev, ring_id)) {
+		vgt_err("Fail to stop ring (1st)\n");
+		goto out;
+	}
+	/* STEP-b: save current ring buffer structure */
+	vgt_save_ringbuffer(prev, ring_id);
+
+	if (ring->stateless) {
+		rc = true;
+		goto out;
+	}
+
+	/* STEP-c: switch to vGT ring buffer */
+	if (!vgt_setup_rsvd_ring(ring)) {
+		vgt_err("Fail to setup rsvd ring\n");
+		goto out;
+	}
+
+	start_ring(pdev, ring_id);
+
+	/* STEP-d: save HW render context for prev */
+	if (!context_ops->save_hw_context(ring_id, prev)) {
+		vgt_err("Fail to save context\n");
+		goto out;
+	}
+
+	if (render_engine_reset && !vgt_reset_engine(pdev, ring_id)) {
+		vgt_err("Fail to reset engine\n");
+		goto out;
+	}
+
+	/* STEP-e: restore HW render context for next */
+	if (!context_ops->restore_hw_context(ring_id, next)) {
+		vgt_err("Fail to restore context\n");
+		goto out;
+	}
+
+	/* STEP-f: idle and stop ring at the end of HW switch */
+	if (!idle_render_engine(pdev, ring_id)) {
+		vgt_err("fail to idle ring-%d after ctx restore\n", ring_id);
+		goto out;
+	}
+
+	if (!stop_ring(pdev, ring_id)) {
+		vgt_err("Fail to stop ring (2nd)\n");
+		goto out;
+	}
+
+	rc = true;
+out:
+	return rc;
+}
+
+
+struct vgt_render_context_ops gen7_context_ops = {
+	.init_null_context = gen7_init_null_context,
+	.save_hw_context = gen7_save_hw_context,
+	.restore_hw_context = gen7_restore_hw_context,
+	.ring_context_switch = gen7_ring_switch,
+};
+
+static struct reg_mask_t gen8_rcs_reset_mmio[] = {
+	{0x2098, 0},
+	{0x229c, 1},
+	{0x20c0, 1},
+
+	{0x24d0, 0},
+	{0x24d4, 0},
+	{0x24d8, 0},
+	{0x24dc, 0},
+
+	{0xe4f0, 0},
+	{0xe4f4, 0},
+
+	{0xe184, 0},
+	{0x7300, 0},
+	{0x7004, 1},
+	{0x7008, 1},
+
+	{0x7000, 1},
+	{0x20e4, 1},
+
+	{0x7010, 1},
+
+	{0xb118, 0},
+	{0xb100, 0},
+	{0xb110, 0},
+	{0xb10c, 0},
+
+	{0x83a4, 1},
+};
+
+static bool gen8_reset_engine(int ring_id,
+		struct vgt_device *prev, struct vgt_device *next)
+{
+	struct pgt_device *pdev = next->pdev;
+	int count = 0;
+
+	if (ring_id != RING_BUFFER_RCS)
+		return true;
+
+	for (count = 0; count < ARRAY_SIZE(gen8_rcs_reset_mmio); count++) {
+		struct reg_mask_t *r = &gen8_rcs_reset_mmio[count];
+		__vreg(prev, r->reg) = VGT_MMIO_READ(pdev, r->reg);
+	}
+#if 0
+	VGT_MMIO_WRITE(pdev, 0x20d0, (1 << 16) | (1 << 0));
+
+	for (count = 1000; count > 0; count --)
+		if (VGT_MMIO_READ(pdev, 0x20d0) & (1 << 1))
+			break;
+
+	if (!count) {
+		vgt_err("wait 0x20d0 timeout.\n");
+		return false;
+	}
+
+	VGT_MMIO_WRITE(pdev, _REG_GEN6_GDRST, _REGBIT_GEN6_GRDOM_RENDER);
+
+	for (count = 1000; count > 0; count --)
+		if (!(VGT_MMIO_READ(pdev, _REG_GEN6_GDRST) & _REGBIT_GEN6_GRDOM_RENDER))
+			break;
+
+	if (!count) {
+		vgt_err("wait gdrst timeout.\n");
+		return false;
+	}
+
+	VGT_MMIO_WRITE(pdev, _REG_RCS_IMR, __sreg(vgt_dom0, _REG_RCS_IMR));
+#endif
+	for (count = 0; count < ARRAY_SIZE(gen8_rcs_reset_mmio); count++) {
+		struct reg_mask_t *r = &gen8_rcs_reset_mmio[count];
+		vgt_reg_t v = __vreg(next, r->reg);
+		if (r->mask)
+			v |= 0xffff0000;
+
+		VGT_MMIO_WRITE(pdev, r->reg, v);
+		VGT_POST_READ(pdev, r->reg);
+	}
+
+//	reset_el_structure(pdev, ring_id);
+
+	return true;
+}
+
+static bool gen8_init_null_context(struct pgt_device *pdev, int id)
+{
+	/* disable null context right now */
+	return true;
+}
+
+static bool gen8_save_hw_context(int id, struct vgt_device *vgt)
+{
+	return true;
+}
+
+static bool gen8_restore_hw_context(int id, struct vgt_device *vgt)
+{
+	return true;
+}
+
+static bool gen8_ring_switch(struct pgt_device *pdev,
+		enum vgt_ring_id ring_id,
+		struct vgt_device *prev,
+		struct vgt_device *next)
+{
+	if (render_engine_reset && !gen8_reset_engine(ring_id, prev, next)) {
+		vgt_err("Fail to reset engine\n");
+		return false;
+	}
+
+	return true;
+}
+
+struct vgt_render_context_ops gen8_context_ops = {
+	.init_null_context = gen8_init_null_context,
+	.save_hw_context = gen8_save_hw_context,
+	.restore_hw_context = gen8_restore_hw_context,
+	.ring_context_switch = gen8_ring_switch,
+};
+
+static struct vgt_render_context_ops *context_ops;
+
+bool vgt_render_init(struct pgt_device *pdev)
+{
+	if (IS_PREBDW(pdev))
+		context_ops = &gen7_context_ops;
+	else if (pdev->enable_execlist)
+		context_ops = &gen8_context_ops;
+
+	return true;
+}
+
+bool vgt_do_render_sched(struct pgt_device *pdev)
+{
+	int threshold = 500; /* print every 500 times */
+	int cpu;
+	bool rc = true;
+
+	if (!(vgt_ctx_check(pdev) % threshold))
+		vgt_dbg(VGT_DBG_RENDER, "vGT: %lldth checks, %lld switches\n",
+			vgt_ctx_check(pdev), vgt_ctx_switch(pdev));
+	vgt_ctx_check(pdev)++;
+
+	ASSERT(!vgt_runq_is_empty(pdev));
+
+	/*
+	 * disable interrupt which is sufficient to prevent more
+	 * cmds submitted by the current owner, when dom0 is UP.
+	 * if the mmio handler for HVM is made into a thread,
+	 * simply a spinlock is enough. IRQ handler is another
+	 * race point
+	 */
+	vgt_lock_dev(pdev, cpu);
+
+	vgt_schedule(pdev);
+
+	if (ctx_switch_requested(pdev)) {
+		if ((!irq_based_ctx_switch) || vgt_rings_need_idle_notification(pdev))
+			vgt_raise_request(pdev, VGT_REQUEST_CTX_SWITCH);
+		else
+			pdev->ctx_switch_pending = true;
+	}
+
+	vgt_unlock_dev(pdev, cpu);
+	return rc;
+}
+
+bool vgt_do_render_context_switch(struct pgt_device *pdev)
+{
+	int i = 0;
+	int cpu;
+	struct vgt_device *next, *prev;
+	cycles_t t0, t1, t2;
+
+	vgt_lock_dev(pdev, cpu);
+	if (!ctx_switch_requested(pdev))
+		goto out;
+
+	ASSERT(spin_is_locked(&pdev->lock));
+	vgt_force_wake_get();
+
+	next = pdev->next_sched_vgt;
+	prev = current_render_owner(pdev);
+	ASSERT(pdev->next_sched_vgt);
+	ASSERT(next != prev);
+
+	t0 = vgt_get_cycles();
+	if (!idle_rendering_engines(pdev, &i)) {
+		int j;
+		vgt_err("vGT: (%lldth switch<%d>)...ring(%d) is busy\n",
+			vgt_ctx_switch(pdev),
+			current_render_owner(pdev)->vgt_id, i);
+		for (j = 0; j < 10; j++)
+			printk("pHEAD(%x), pTAIL(%x)\n",
+				VGT_READ_HEAD(pdev, i),
+				VGT_READ_TAIL(pdev, i));
+		goto err;
+	}
+
+	if (pdev->enable_execlist) {
+		int ring_id;
+		for (ring_id = 0; ring_id < pdev->max_engines; ++ ring_id) {
+			if (!pdev->ring_buffer[ring_id].need_switch)
+				continue;
+			if (!vgt_idle_execlist(pdev, ring_id)) {
+				vgt_dbg(VGT_DBG_EXECLIST, "rendering ring is not idle. "
+					"Ignore the context switch!\n");
+				vgt_force_wake_put();
+				goto out;
+			}
+			vgt_clear_submitted_el_record(pdev, ring_id);
+		}
+	}
+
+	vgt_dbg(VGT_DBG_RENDER, "vGT: next vgt (%d)\n", next->vgt_id);
+	
+
+	/* variable exported by debugfs */
+	context_switch_num ++;
+	t1 = vgt_get_cycles();
+	ring_idle_wait += t1 - t0;
+
+	vgt_sched_update_prev(prev, t0);
+
+	if ( prev )
+		prev->stat.allocated_cycles +=
+			(t0 - prev->stat.schedule_in_time);
+	vgt_ctx_switch(pdev)++;
+
+	/* STEP-1: manually save render context */
+	vgt_rendering_save_mmio(prev);
+
+	/* STEP-2: HW render context switch */
+	for (i=0; i < pdev->max_engines; i++) {
+		if (!pdev->ring_buffer[i].need_switch)
+			continue;
+
+		context_ops->ring_context_switch(pdev, i, prev, next);
+	}
+
+	/* STEP-3: manually restore render context */
+	vgt_rendering_restore_mmio(next);
+
+	/* STEP-4: restore ring buffer structure */
+	for (i = 0; i < pdev->max_engines; i++)
+		vgt_restore_ringbuffer(next, i);
+
+	/* STEP-5: switch PPGTT */
+	current_render_owner(pdev) = next;
+	/* ppgtt switch must be done after render owner switch */
+	if (!pdev->enable_execlist && pdev->enable_ppgtt && next->gtt.active_ppgtt_mm_bitmap)
+		vgt_ppgtt_switch(next);
+
+	/* STEP-6: ctx switch ends, and then kicks of new tail */
+	vgt_kick_off_execution(next);
+
+	/* NOTE: do NOT access MMIO after this PUT hypercall! */
+	vgt_force_wake_put();
+
+	/* request to check IRQ when ctx switch happens */
+	if (prev->force_removal ||
+		bitmap_empty(prev->enabled_rings, MAX_ENGINES)) {
+		printk("Disable render for vgt(%d) from kthread\n",
+			prev->vgt_id);
+		vgt_disable_render(prev);
+		wmb();
+		if (prev->force_removal) {
+			prev->force_removal = 0;
+			if (waitqueue_active(&pdev->destroy_wq))
+				wake_up(&pdev->destroy_wq);
+		}
+		/* no need to check if prev is to be destroyed */
+	}
+
+	next->stat.schedule_in_time = vgt_get_cycles();
+
+	vgt_sched_update_next(next);
+
+	t2 = vgt_get_cycles();
+	context_switch_cost += (t2-t1);
+out:
+	vgt_unlock_dev(pdev, cpu);
+	return true;
+err:
+	dump_regs_on_err(pdev);
+	/* TODO: any cleanup for context switch errors? */
+	vgt_err("Ring-%d: (%lldth checks %lldth switch<%d->%d>)\n",
+			i, vgt_ctx_check(pdev), vgt_ctx_switch(pdev),
+			prev->vgt_id, next->vgt_id);
+	vgt_err("FAIL on ring-%d\n", i);
+	vgt_err("cur(%d): head(%x), tail(%x), start(%x)\n",
+			current_render_owner(pdev)->vgt_id,
+			current_render_owner(pdev)->rb[i].sring.head,
+			current_render_owner(pdev)->rb[i].sring.tail,
+			current_render_owner(pdev)->rb[i].sring.start);
+	vgt_err("dom0(%d): head(%x), tail(%x), start(%x)\n",
+			vgt_dom0->vgt_id,
+			vgt_dom0->rb[i].sring.head,
+			vgt_dom0->rb[i].sring.tail,
+			vgt_dom0->rb[i].sring.start);
+	show_ring_debug(pdev, i);
+	show_ring_buffer(pdev, i, 16 * sizeof(vgt_reg_t));
+	if (!enable_reset)
+		/* crash system now, to avoid causing more confusing errors */
+		ASSERT(0);
+
+	/*
+	 * put this after the ASSERT(). When ASSERT() tries to dump more
+	 * CPU/GPU states: we want to hold the lock to prevent other
+	 * vcpus' vGT related codes at this time.
+	 */
+	vgt_force_wake_put();
+
+	vgt_unlock_dev(pdev, cpu);
+
+	return false;
+}
+
+bool ring_mmio_read_in_rb_mode(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes)
+{
+	int ring_id, rel_off;
+	vgt_ringbuffer_t	*vring;
+	struct vgt_statistics *stat = &vgt->stat;
+
+	stat->ring_mmio_rcnt++;
+
+	if ((hvm_render_owner && (vgt->vm_id != 0)) || reg_hw_access(vgt, off)){
+		unsigned long data;
+		data = VGT_MMIO_READ_BYTES(vgt->pdev, off, bytes);
+		memcpy(p_data, &data, bytes);
+		return true;
+	}
+
+	rel_off = off & ( sizeof(vgt_ringbuffer_t) - 1 );
+	ring_id = tail_to_ring_id (vgt->pdev, _tail_reg_(off) );
+	vring = &vgt->rb[ring_id].vring;
+
+	memcpy(p_data, (char *)vring + rel_off, bytes);
+	//ring_debug(vgt, ring_id);
+	return true;
+}
+
+bool ring_mmio_write_in_rb_mode(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes)
+{
+	int ring_id, rel_off;
+	vgt_state_ring_t	*rs;
+	vgt_ringbuffer_t	*vring;
+	vgt_ringbuffer_t	*sring;
+	struct vgt_tailq *tailq = NULL;
+	vgt_reg_t	oval;
+	cycles_t	t0, t1;
+	struct pgt_device *pdev = vgt->pdev;
+	struct vgt_statistics *stat = &vgt->stat;
+
+	t0 = get_cycles();
+	stat->ring_mmio_wcnt++;
+
+	vgt_dbg(VGT_DBG_RENDER, "vGT:ring_mmio_write (0x%x) with val (0x%x)\n", off, *((u32 *)p_data));
+	rel_off = off & ( sizeof(vgt_ringbuffer_t) - 1 );
+
+	ring_id = tail_to_ring_id (pdev, _tail_reg_(off) );
+	rs = &vgt->rb[ring_id];
+	vring = &rs->vring;
+	sring = &rs->sring;
+
+	if (shadow_tail_based_qos)
+		tailq = &vgt->rb_tailq[ring_id];
+
+	if (ring_id == RING_BUFFER_VECS)
+		vgt->vebox_support = 1;
+
+	oval = *(vgt_reg_t *)((char *)vring + rel_off);
+	memcpy((char *)vring + rel_off, p_data, bytes);
+
+	switch (rel_off) {
+	case RB_OFFSET_TAIL:
+		stat->ring_tail_mmio_wcnt++;
+
+		/* enable hvm tailq after the ring enabled */
+		if (shadow_tail_based_qos) {
+			if (test_bit(ring_id, vgt->enabled_rings))
+				vgt_tailq_pushback(tailq, vring->tail, 0);
+		} else
+			sring->tail = vring->tail;
+
+#if 0
+		if (shadow_tail_based_qos) {
+			if (vgt->vgt_id > 0) {
+				if (enable_hvm_tailq && !vgt->force_removal)
+					vgt_tailq_pushback(tailq, vring->tail, 0);
+			} else
+				vgt_tailq_pushback(tailq, vring->tail, 0);
+		} else
+			sring->tail = vring->tail;
+#endif
+
+		if (vring->ctl & _RING_CTL_ENABLE)
+			vgt_scan_vring(vgt, ring_id);
+
+		t1 = get_cycles();
+		stat->ring_tail_mmio_wcycles += (t1-t0);
+
+		if (shadow_tail_based_qos) {
+			if (vgt_tailq_last_stail(tailq)
+					&& !test_and_set_bit(ring_id, (void *)vgt->started_rings))
+				printk("Ring-%d starts work for vgt-%d\n",
+						ring_id, vgt->vgt_id);
+			/* When a ring is enabled, tail value
+			 * can never write to real hardware */
+			return true;
+		} else {
+			if (sring->tail &&
+					!test_and_set_bit(ring_id, (void *)vgt->started_rings))
+				printk("Ring-%d starts work for vgt-%d\n",
+						ring_id, vgt->vgt_id);
+		}
+
+		break;
+	case RB_OFFSET_HEAD:
+		//debug
+		//vring->head |= 0x200000;
+		sring->head = vring->head;
+		break;
+	case RB_OFFSET_START:
+		sring->start = mmio_g2h_gmadr(vgt, off, vring->start);
+		break;
+	case RB_OFFSET_CTL:
+		sring->ctl = vring->ctl;
+
+		if ( (oval & _RING_CTL_ENABLE) &&
+			!(vring->ctl & _RING_CTL_ENABLE) ) {
+			printk("vGT: deactivate vgt (%d) on ring (%d)\n", vgt->vgt_id, ring_id);
+			vgt_disable_ring(vgt, ring_id);
+		}
+		else if ( !(oval & _RING_CTL_ENABLE) &&
+			(vring->ctl & _RING_CTL_ENABLE) ) {
+			printk("vGT: activate vgt (%d) on ring (%d)\n", vgt->vgt_id, ring_id);
+			vgt_enable_ring(vgt, ring_id);
+			/*
+			 * We rely on dom0 to init the render engine.
+			 * So wait until dom0 enables ring for the 1st time,
+			 * and then we can initialize the null context safely
+			 */
+			if (!hvm_render_owner && !pdev->ring_buffer[ring_id].null_context) {
+				if (!context_ops->init_null_context(pdev, ring_id))
+					return false;
+			}
+
+			clear_bit(RESET_INPROGRESS, &vgt->reset_flags);
+		}
+		if (vring->ctl & _RING_CTL_ENABLE) {
+			rs->last_scan_head =
+				vring->head & RB_HEAD_OFF_MASK;
+			vgt_scan_vring(vgt, ring_id);
+		}
+		break;
+	default:
+		return false;
+		break;
+	}
+
+	/* TODO: lock with kthread? */
+	/*
+	 * FIXME: Linux VM doesn't read head register directly. Instead it relies on
+	 * automatic head reporting mechanism. Later with command parser, there's no
+	 * problem since all commands are translated and filled by command parser. for
+	 * now it's possible for dom0 to fill over than a full ring in a scheduled
+	 * quantum
+	 */
+	if (reg_hw_access(vgt, off)) {
+		if (rel_off == RB_OFFSET_TAIL && (vring->ctl & _RING_CTL_ENABLE))
+			vgt_submit_commands(vgt, ring_id);
+		else
+			VGT_MMIO_WRITE(pdev, off,
+				*(vgt_reg_t*)((char *)sring + rel_off));
+	}
+
+	//ring_debug(vgt, ring_id);
+	return true;
+}
+
+u64	ring_0_idle = 0;
+u64	ring_0_busy = 0;
+struct pgt_device *perf_pgt = NULL;
+
+void vgt_gpu_perf_sample(void)
+{
+	int	ring_id = 0;
+
+	if ( perf_pgt ) {
+		if ( ring_is_empty(perf_pgt, ring_id) )
+			ring_0_idle ++;
+		else
+			ring_0_busy ++;
+	}
+}
+EXPORT_SYMBOL_GPL(vgt_gpu_perf_sample);
+
+bool ring_uhptr_write_in_rb_mode(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes)
+{
+	int ring_id;
+	vgt_state_ring_t	*rs;
+	vgt_reg_t val = *(vgt_reg_t *)p_data;
+
+	switch (off) {
+	case _REG_RCS_UHPTR:
+		ring_id = RING_BUFFER_RCS;
+		break;
+	case _REG_VCS_UHPTR:
+		ring_id = RING_BUFFER_VCS;
+		break;
+	case _REG_BCS_UHPTR:
+		ring_id = RING_BUFFER_BCS;
+		break;
+	case _REG_VECS_UHPTR:
+		ring_id = RING_BUFFER_VECS;
+		break;
+	default:
+		ASSERT(0);
+		break;
+	}
+
+	rs = &vgt->rb[ring_id];
+
+	/* only cache the latest value */
+	if (rs->uhptr & _REGBIT_UHPTR_VALID)
+		vgt_info("VM(%d)-r%d: overwrite a valid uhptr (o:%x, n:%x)\n",
+			vgt->vm_id, ring_id, rs->uhptr, val);
+
+	rs->uhptr = val;
+	rs->uhptr_id = rs->request_id;
+	return true;
+}
diff --git a/drivers/gpu/drm/i915/vgt/sched.c b/drivers/gpu/drm/i915/vgt/sched.c
new file mode 100644
index 0000000..cc314ea
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/sched.c
@@ -0,0 +1,877 @@
+/*
+ * Render schedulers
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "vgt.h"
+
+/* Lets move context scheduler specific parameters here */
+bool timer_based_qos = true;
+
+struct vgt_hrtimer vgt_hrtimer;
+struct pgt_device *vgt_hrtimer_pdev;
+static void vgt_hrtimer_init(struct pgt_device *pdev,
+	enum hrtimer_restart (*function)(struct hrtimer *),
+	u64 period)
+{
+	struct vgt_hrtimer *hrtimer = &vgt_hrtimer;
+	hrtimer_init(&hrtimer->timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
+	hrtimer->timer.function = function;
+	hrtimer->period = period;
+	vgt_hrtimer_pdev = pdev;
+
+	hrtimer_start(&hrtimer->timer,
+			ktime_add_ns(ktime_get(), hrtimer->period),
+			HRTIMER_MODE_ABS);
+}
+
+static void vgt_hrtimer_exit(struct pgt_device *pdev)
+{
+	hrtimer_cancel(&vgt_hrtimer.timer);
+}
+
+static inline bool phys_head_catch_tail(struct pgt_device *pdev)
+{
+	int ring_id;
+	unsigned int reg_head, reg_tail;
+	vgt_reg_t head, tail;
+
+	for (ring_id = 0; ring_id < pdev->max_engines; ring_id++) {
+		reg_head = RB_HEAD(pdev, ring_id);
+		reg_tail = RB_TAIL(pdev, ring_id);
+		head = VGT_MMIO_READ(pdev, reg_head);
+		tail = VGT_MMIO_READ(pdev, reg_tail);
+		if (!RB_HEAD_TAIL_EQUAL(head, tail))
+			return false;
+	}
+
+	return true;
+}
+
+
+/* FIXME: Since it is part of "timer based scheduler",
+ * move this from vgt_context.c here and renamed from
+ * next_vgt() to tbs_next_vgt()
+ */
+static struct vgt_device *tbs_next_vgt(
+	struct list_head *head, struct vgt_device *vgt)
+{
+	struct list_head *next = &vgt->list;
+	struct vgt_device *next_vgt = NULL;
+	struct pgt_device *pdev;
+
+	if (vgt->force_removal)
+		return vgt_dom0;
+
+	pdev = vgt->pdev;
+	if (ctx_switch_requested(pdev))
+		return pdev->next_sched_vgt;
+
+	do {
+		next = next->next;
+		/* wrap the list */
+		if (next == head)
+			next = head->next;
+		next_vgt = list_entry(next, struct vgt_device, list);
+
+		if (!vgt_vrings_empty(next_vgt))
+			break;
+
+	} while (next_vgt != vgt);
+
+	return next_vgt;
+}
+
+/* safe to not use vgt_enter/vgt_exit, otherwise easily lead to deadlock */
+static enum hrtimer_restart vgt_tbs_timer_fn(struct hrtimer *data)
+{
+	struct vgt_hrtimer *hrtimer = container_of(data,
+			struct vgt_hrtimer, timer);
+	struct pgt_device *pdev = vgt_hrtimer_pdev;
+
+	ASSERT(pdev);
+
+	if (vgt_nr_in_runq(pdev) > 1) {
+		vgt_raise_request(pdev, VGT_REQUEST_SCHED);
+	}
+	hrtimer_add_expires_ns(&hrtimer->timer, hrtimer->period);
+	return HRTIMER_RESTART;
+}
+
+/* tail queue operation */
+static int init_vgt_tailq(struct vgt_tailq *tailq)
+{
+	int retval = 0;
+
+	if (!tailq)
+		return -EINVAL;
+
+	tailq->__head = 0;
+	tailq->__tail = 0;
+
+	tailq->__buf_tail = vmalloc(VGT_TAILQ_SIZE);
+	if (!tailq->__buf_tail)
+		return -ENOMEM;
+
+	tailq->__buf_cmdnr = vmalloc(VGT_TAILQ_SIZE);
+	if (!tailq->__buf_cmdnr) {
+		retval = -ENOMEM;
+		goto free_buf_tail;
+	}
+
+	tailq->__buf_tail[0] = 0;
+	tailq->__buf_cmdnr[0] = 0;
+
+	return 0;
+
+free_buf_tail:
+	vfree(tailq->__buf_tail);
+	return retval;
+}
+
+void destroy_vgt_tailq(struct vgt_tailq *tailq)
+{
+	/* FIXME: if need to check if it is empty ? */
+	tailq->__head = 0;
+	tailq->__tail = 0;
+
+	if (tailq->__buf_tail) {
+		vfree(tailq->__buf_tail);
+		tailq->__buf_tail = NULL;
+	}
+
+	if (tailq->__buf_cmdnr) {
+		vfree(tailq->__buf_cmdnr);
+		tailq->__buf_cmdnr = NULL;
+	}
+
+}
+
+static inline bool is_vgt_tailq_empty(struct vgt_tailq *tailq)
+{
+	return (tailq->__head == tailq->__tail);
+}
+
+static inline bool is_vgt_tailq_full(struct vgt_tailq *tailq)
+{
+	return (vgt_tailq_idx(tailq->__tail + 1) == tailq->__head);
+}
+
+static inline u32 vgt_tailq_cur_stail(struct vgt_tailq *tailq)
+{
+	return (tailq->__buf_tail[tailq->__head]);
+}
+
+inline u32 vgt_tailq_last_stail(struct vgt_tailq *tailq)
+{
+	return (tailq->__buf_tail[tailq->__tail]);
+}
+
+static inline int vgt_tailq_len(struct vgt_tailq *tailq)
+{
+	int len = tailq->__tail - tailq->__head;
+	if (tailq->__tail < tailq->__head)
+		len += VGT_TAILQ_MAX_ENTRIES;
+	return len;
+}
+
+static inline void vgt_tailq_popall(struct vgt_tailq *tailq)
+{
+	tailq->__head = tailq->__tail;
+}
+
+/* catch each tail-writing */
+int vgt_tailq_pushback(struct vgt_tailq *tailq, u32 tail, u32 cmdnr)
+{
+	u32 __tail;
+
+	if (is_vgt_tailq_full(tailq))
+		return -ENOSPC;
+
+	__tail = vgt_tailq_idx(tailq->__tail + 1);
+	tailq->__buf_tail[__tail] = tail;
+	tailq->__buf_cmdnr[__tail] = cmdnr;
+
+	tailq->__tail = __tail;
+	return 0;
+}
+
+/* pop several tail-writing */
+static int vgt_tailq_popfront(struct vgt_tailq *tailq, u32 nr)
+{
+	u32 __head = tailq->__head;
+
+	if (nr > vgt_tailq_len(tailq))
+		return -EACCES;
+
+	tailq->__head = vgt_tailq_idx(__head + nr);
+	return 0;
+}
+
+#define vgt_tailq_for_each_entry(idx, tailq) \
+	for (idx = vgt_tailq_idx(tailq->__head + 1); \
+		(tailq->__head != tailq->__tail) && (idx != vgt_tailq_idx(tailq->__tail + 1));\
+		idx = vgt_tailq_idx(idx + 1))
+
+/* Parameter:
+ * @tailq:	Tail queue that we walk on
+ * @cmd_nr:	Quantity of ring commands, that need to be
+ *			executed in next round context switch.
+ * @move_cnt:
+ *			to get that quantity of ring commands, how many
+ *			elements we need to pop out
+ *
+ * Return value:
+ *			number of tail-writing that we need to pop-up
+ */
+static u32 __vgt_tailq_commit_cmd(struct vgt_tailq *tailq, u32 req_cmd_nr,
+		u32 *move_cnt)
+{
+	u32 idx;
+	u32 cmd_nr = 0, loop_cnt = 0;
+	if (is_vgt_tailq_empty(tailq))
+		return 0;
+
+	vgt_tailq_for_each_entry(idx, tailq) {
+		if (cmd_nr <= req_cmd_nr) {
+			cmd_nr += tailq->__buf_cmdnr[idx];
+			loop_cnt++;
+		}
+	}
+
+	*move_cnt = loop_cnt;
+	return cmd_nr;
+}
+
+static u32 vgt_tailq_commit_num_stail(struct vgt_device *vgt,
+		int ring_id, u32 num)
+{
+	struct vgt_tailq *tailq = &vgt->rb_tailq[ring_id];
+	vgt_reg_t stail;
+	int len;
+
+	len = vgt_tailq_len(tailq);
+	if (num > len)
+		num = len;
+
+	vgt_tailq_popfront(tailq, num);
+	stail = vgt_tailq_cur_stail(tailq);
+	vgt->rb[ring_id].sring.tail = stail;
+
+	return num;
+}
+
+static u32 vgt_tailq_commit_stail(struct vgt_device *vgt,
+		int ring_id, u32 req_cmdnr)
+{
+	struct vgt_tailq *tailq = &vgt->rb_tailq[ring_id];
+	u32 hmove;
+	u32 actual_cmdnr;
+	vgt_reg_t stail;
+
+	actual_cmdnr = __vgt_tailq_commit_cmd(tailq, req_cmdnr, &hmove);
+	if (actual_cmdnr == 0)
+		return 0;
+
+	if (vgt_tailq_popfront(tailq, hmove) != 0)
+		return 0;
+
+	stail = vgt_tailq_cur_stail(tailq);
+	vgt->rb[ring_id].sring.tail = stail;
+
+	return actual_cmdnr;
+}
+
+
+
+/* FIXME: default value of CMD number */
+#define VGT_RCS_SCHED_DEFAULT_CMDNR  500
+#define VGT_BCS_SCHED_DEFAULT_CMDNR  300
+
+int vgt_init_rb_tailq(struct vgt_device *vgt)
+{
+	int retval = 0;
+	int ring_id, from;
+
+	for (ring_id = 0; ring_id < vgt->pdev->max_engines; ring_id++) {
+		retval = init_vgt_tailq(&vgt->rb_tailq[ring_id]);
+		if (retval < 0)
+			goto cleanup;
+	}
+	return 0;
+
+cleanup:
+	for (from = 0; from < ring_id; from++)
+		destroy_vgt_tailq(&vgt->rb_tailq[from]);
+
+	return retval;
+}
+
+/* rb should be disabled before call this */
+void vgt_destroy_rb_tailq(struct vgt_device *vgt)
+{
+	int ring_id;
+	for (ring_id = 0; ring_id < vgt->pdev->max_engines; ring_id++)
+		destroy_vgt_tailq(&vgt->rb_tailq[ring_id]);
+}
+
+//int rb_chk_set[] = {RING_BUFFER_RCS, RING_BUFFER_BCS, RING_BUFFER_VCS};
+bool is_vgt_rb_tailq_empty(struct vgt_device *vgt, int max_engines)
+{
+	int ring_id;
+	for (ring_id = 0; ring_id < max_engines; ring_id++) {
+		if (test_bit(ring_id, (void *)vgt->started_rings)
+				&& !is_vgt_tailq_empty(&vgt->rb_tailq[ring_id])) {
+			/* check how many tail-writings can be cached */
+			vgt_dbg(VGT_DBG_RENDER, "vGT(%d): rb(%d) tailq length(%d)\n",
+					vgt->vgt_id, ring_id,
+					vgt_tailq_len(&vgt->rb_tailq[ring_id]));
+			return false;
+		}
+	}
+
+	return true;
+}
+
+
+/* Use command(ring/batch buffer) number
+ * to estimate how many stails
+ * will be commited, this will be used soon
+ */
+bool vgt_rb_tailq_commit_stail(struct vgt_device *vgt,
+		u32 *ring_req_cmdnr)
+{
+	u32 actual_cmdnr;
+	int ring_id;
+	for (ring_id = 0; ring_id < vgt->pdev->max_engines; ring_id++)
+		if (test_bit(ring_id, (void*)vgt->enabled_rings))
+			actual_cmdnr = vgt_tailq_commit_stail(vgt,
+					ring_id,
+					ring_req_cmdnr[ring_id]);
+
+	return true;
+}
+
+bool vgt_rb_tailq_commit_num_stail(struct vgt_device *vgt,
+		u32* req)
+{
+	u32 actual_move;
+	int ring_id;
+	for (ring_id = 0; ring_id < vgt->pdev->max_engines; ring_id++)
+		if (test_bit(ring_id, (void *)vgt->enabled_rings))
+			actual_move = vgt_tailq_commit_num_stail(vgt,
+					ring_id,
+					req[ring_id]);
+	return true;
+}
+
+bool vgt_removal_req = false;
+static struct vgt_device *ondemand_sched_next(struct pgt_device *pdev)
+{
+	struct vgt_device *cur_vgt = current_render_owner(pdev);
+	struct vgt_device *next_vgt = NULL;
+	struct list_head *next = &cur_vgt->list;
+	struct list_head *head = &pdev->rendering_runq_head;
+
+	do {
+		next = next->next;
+		/* wrap the list */
+		if (next == head)
+			next = head->next;
+		next_vgt = list_entry(next, struct vgt_device, list);
+
+		if (!cur_vgt->force_removal
+				&& is_vgt_rb_tailq_empty(next_vgt, pdev->max_engines))
+			continue;
+		else
+			break;
+	} while (next_vgt != cur_vgt);
+
+	if (cur_vgt->force_removal) {
+		vgt_removal_req = true;
+		printk("force_removal(%p), next_vgt(%p)\n", cur_vgt, next_vgt);
+	}
+
+	return next_vgt;
+}
+
+/* Command based scheduling */
+static void ondemand_sched_ctx(struct pgt_device *pdev)
+{
+	struct vgt_device *cur_vgt = current_render_owner(pdev);
+	struct vgt_device *next_vgt = ondemand_sched_next(pdev);
+	/* default commit 5 tail writing at most */
+	u32 tails_per_ring[MAX_ENGINES] = {5, 5, 5, 5};
+
+	//if (is_vgt_rb_tailq_empty(next_vgt, pdev->max_engines))
+	//	return;
+
+	if (next_vgt == cur_vgt) {
+		//FIXME: request 5 stails to be committed
+		vgt_rb_tailq_commit_num_stail(next_vgt, tails_per_ring);
+		vgt_kick_off_execution(next_vgt);
+		return;
+	}
+
+	if (vgt_chk_raised_request(pdev, VGT_REQUEST_CTX_SWITCH)) {
+		printk("Warning: last request for ctx_switch not handled yet!\n");
+		/* this is a big change */
+		//return;
+	}
+
+	/* set next vgt for ctx switch */
+	vgt_rb_tailq_commit_num_stail(next_vgt, tails_per_ring);
+	pdev->next_sched_vgt = next_vgt;
+	vgt_raise_request(pdev, VGT_REQUEST_CTX_SWITCH);
+}
+
+
+//bool enable_tailq = false;
+static enum hrtimer_restart vgt_poll_rb_tail(struct hrtimer *data)
+{
+
+	unsigned long flags;
+	int active_nr;
+	struct vgt_hrtimer *hrtimer = container_of(data,
+			struct vgt_hrtimer, timer);
+	struct pgt_device *pdev = vgt_hrtimer_pdev;
+	int cpu;
+
+	ASSERT(pdev);
+
+	vgt_lock_dev_flags(pdev, cpu, flags);
+	/* TODO: if no more than 2 vgt in runqueue */
+	active_nr = vgt_nr_in_runq(pdev);
+
+	if (active_nr == 0)
+		goto reload_timer;
+
+#if 0
+	if (((active_nr = vgt_nr_in_runq(pdev)) < 2)
+			&& enable_tailq == true) {
+		enable_tailq = false;
+		// call disable tail queue
+		// down the polling frequency ?
+	} else if ((active_nr > 1)
+			&& (enable_tailq == false)) {
+		enable_tailq = true;
+		// call enable tail queue
+	}
+#endif
+
+	/* bspec said head and tail initially as 0 */
+	if (phys_head_catch_tail(pdev))
+		ondemand_sched_ctx(pdev);
+
+reload_timer:
+	vgt_unlock_dev_flags(pdev, cpu, flags);
+	/* Slow down the polling as 16 ms to prevent the starvation
+	 * of vgt_thread
+	 */
+	if (vgt_removal_req == true) {
+		vgt_removal_req = false;
+		hrtimer->period = (VGT_TAILQ_RB_POLLING_PERIOD << 3);
+	} else
+		hrtimer->period = VGT_TAILQ_RB_POLLING_PERIOD;
+
+	hrtimer_add_expires_ns(&hrtimer->timer, hrtimer->period);
+
+	return HRTIMER_RESTART;
+}
+
+void vgt_initialize_ctx_scheduler(struct pgt_device *pdev)
+{
+	ASSERT(pdev);
+	/* If configured more than one,
+	 * choose the one that has highest priority
+	 */
+	if (hvm_render_owner) {
+		timer_based_qos = false;
+		event_based_qos = false;
+		shadow_tail_based_qos = false;
+		return;
+	}
+
+	if (shadow_tail_based_qos) {
+		vgt_hrtimer_init(pdev,
+				vgt_poll_rb_tail,
+				VGT_TAILQ_RB_POLLING_PERIOD);
+		event_based_qos = false;
+		timer_based_qos = false;
+	} else if (event_based_qos)
+		timer_based_qos = false;
+
+	if (timer_based_qos) {
+		vgt_hrtimer_init(pdev,
+				vgt_tbs_timer_fn,
+				VGT_TBS_DEFAULT_PERIOD);
+	}
+}
+
+void vgt_cleanup_ctx_scheduler(struct pgt_device *pdev)
+{
+	ASSERT(pdev);
+
+	if (event_based_qos)
+		return;
+
+	if (shadow_tail_based_qos || timer_based_qos)
+		vgt_hrtimer_exit(pdev);
+
+	pdev->next_sched_vgt = NULL;
+}
+
+/* internal facilities */
+#if 0
+static int64_t vgt_dec_tslice(struct vgt_device *vgt, vgt_tslice_t tslice)
+{
+	vgt_tslice_t *remained_tslice = &vgt->sched_info.time_slice;
+	*remained_tslice -= tslice;
+	return *remained_tslice;
+}
+#endif
+
+static inline int64_t vgt_get_tslice(struct vgt_device *vgt)
+{
+	struct vgt_sched_info *sched_info = &vgt->sched_info;
+	return sched_info->time_slice;
+}
+/* end of facilities functions */
+
+static void vgt_alloc_tslice(struct vgt_device *vgt)
+{
+	/*
+	 * Further this will rely on different policies
+	 * */
+	int64_t *ts = &(vgt->sched_info.time_slice);
+	if (*ts > 0)
+		return;
+	vgt_dbg(VGT_DBG_RENDER, "vgt(%d): allocate tslice %lld\n",
+			vgt->vgt_id,
+			VGT_DEFAULT_TSLICE);
+
+	*ts = VGT_DEFAULT_TSLICE;
+#if 0
+	/* Apply impact of statistics info */
+	if (ctx_rb_empty_delay(vgt) > XXX )
+		*ts = YYY;
+	else
+		*ts = ZZZ;
+#endif
+}
+
+
+static void vgt_alloc_tslice_all(struct pgt_device *pdev)
+{
+	int i;
+	struct vgt_device *vgt;
+
+	/* FIXME: treat idle vgt differently
+	 * 1) mark its idle state (add to statistics or something else)
+	 * 2) If thought it is really idle, decrease its allocated time slice
+	 * TODO: but the vgt triggered scheduler in its read/write handler,
+	 * should not be regarded as idle.
+	 */
+	/* walk through the runqueue list */
+	for (i = 0; i < VGT_MAX_VMS; i++) {
+		vgt = pdev->device[i];
+		if (vgt)
+			vgt_alloc_tslice(vgt);
+	}
+}
+
+/* pick up next vgt */
+static struct vgt_device *vgt_sched_next(struct vgt_device *vgt,
+		struct list_head *head)
+{
+	struct vgt_device *next_vgt = NULL;
+	struct vgt_device *next_ctx_owner = vgt;
+	struct vgt_sched_info *sched_info;
+	struct list_head *next = &vgt->list;
+	/* must used signed number for comparison */
+	int64_t max_tslice = 0;
+
+	do {
+		next = next->next;
+		if (next == head)
+			next = head->next;
+		next_vgt = list_entry(next, struct vgt_device, list);
+
+		if (!vgt_vrings_empty(next_vgt)) {
+			sched_info = &next_vgt->sched_info;
+			if (sched_info->time_slice > max_tslice) {
+				max_tslice = sched_info->time_slice;
+				next_ctx_owner = next_vgt;
+			}
+		}
+	} while (next_vgt != vgt);
+
+	return next_ctx_owner;
+}
+
+void vgt_setup_countdown(struct vgt_device *vgt)
+{
+	vgt_tslice_t *start_time = &(vgt->sched_info.start_time);
+	vgt_tslice_t *end_time = &(vgt->sched_info.end_time);
+	int64_t *tslice = &(vgt->sched_info.time_slice);
+
+	*start_time = vgt_get_cycles();
+
+	ASSERT(*tslice > 0)
+	*end_time = *start_time + (vgt_tslice_t)(*tslice);
+}
+
+
+static inline void vgt_sched_dump(struct vgt_device *cur_vgt, vgt_tslice_t cur_time)
+{
+	struct pgt_device *pdev = cur_vgt->pdev;
+	struct vgt_device *vgt;
+	printk("------------------------------------------\n");
+	printk("     vgt scheduler dump vGT\n");
+	printk("------------------------------------------\n");
+	printk("....Current time (%llu)\n", cur_time);
+	printk("....Current render owner vgt(%d))\n", (current_render_owner(pdev))->vgt_id);
+	list_for_each_entry(vgt, &pdev->rendering_runq_head, list) {
+		if (vgt == cur_vgt)
+			printk("....vGT(%d) [dump caller]:\n", cur_vgt->vgt_id);
+		else
+			printk("....vGT(%d):\n", vgt->vgt_id);
+		printk("........context start time (%llu)\n", ctx_start_time(vgt));
+		printk("........context end time   (%llu)\n", ctx_end_time(vgt));
+		printk("........context Remain time slice (%lld)\n", ctx_remain_time(vgt));
+		printk("\n");
+	}
+}
+
+/* TODO: call this for each eligible checkpoint */
+int sched_next_failed = 0;
+
+void vgt_sched_ctx(struct pgt_device *pdev)
+{
+	/* start of vgt context sheduling */
+	vgt_tslice_t cur_time;
+	struct vgt_device *next_vgt;
+	struct vgt_device *cur_vgt = current_render_owner(pdev);
+	struct list_head *head = &pdev->rendering_runq_head;
+
+	/* TODO: before the first vgt was put into runqueue,
+	 * the timestamp was used as the initial value of vgt_sched_tstamp
+	 */
+	if (vgt_nr_in_runq(pdev) <= 1)
+		return;
+
+	/* For the first time vgt_sched_ctx() called */
+	if (ctx_start_time(cur_vgt) == 0) {
+		vgt_setup_countdown(cur_vgt);
+		return;
+	}
+
+	/* cycles counter will wrap in 126 years */
+	cur_time = vgt_get_cycles();
+	/* Two situations we need to consider
+	 * 1) the vgt (render_owner) used up its time slice
+	 * 2) the vgt (render_owner) become idle when its time slice left
+	 *	  how to find out this:
+	 *	  2.1) only check physical render engine
+	 *	  2.2) check vhead/vtail of all rendering engines
+	 * But now, for 2), lets just leave it TODO.
+	 * */
+	/* update remain time slice */
+	ctx_remain_time(cur_vgt) = ctx_end_time(cur_vgt) - cur_time;
+
+	/* time slice not used up */
+	if (cur_time < ctx_end_time(cur_vgt)) {
+		vgt_dbg(VGT_DBG_RENDER, "vgt(%d): cur_time(%lld), [%lld, %lld]\n",
+				cur_vgt->vgt_id,
+				cur_time,
+				ctx_start_time(cur_vgt),
+				ctx_end_time(cur_vgt)
+				);
+		return;
+	}
+
+	vgt_dbg(VGT_DBG_RENDER, "vgt(%d): tslice used up, cur_time(%lld), ctx_end_time(%lld)\n",
+			cur_vgt->vgt_id,
+			cur_time,
+			ctx_end_time(cur_vgt));
+
+	next_vgt = vgt_sched_next(cur_vgt, head);
+	if (ctx_remain_time(next_vgt) <= 0) {
+		vgt_alloc_tslice_all(pdev);
+		next_vgt = vgt_sched_next(cur_vgt, head);
+	}
+
+	if (cur_vgt != next_vgt) {
+		/* sometimes, it can be long to wait for the done of
+		 * last context switch, so let's wait for it done
+		 */
+		if (vgt_chk_raised_request(pdev, VGT_REQUEST_CTX_SWITCH)) {
+			return;
+		}
+
+		vgt_dbg(VGT_DBG_RENDER, "try to switch to vgt(%d), cur_time(%lld)\n", next_vgt->vgt_id, cur_time);
+		vgt_dbg(VGT_DBG_RENDER, "vgt(%d): rb wait(%lld) to be empty\n",
+				cur_vgt->vgt_id,
+				ctx_rb_empty_delay(cur_vgt));
+
+		/* set Global varaible next_sched_vgt for context switch */
+		pdev->next_sched_vgt = next_vgt;
+		vgt_raise_request(pdev, VGT_REQUEST_CTX_SWITCH);
+	} else {
+		/* setup countdown of cur_vgt for next round */
+		vgt_setup_countdown(next_vgt);
+	}
+}
+
+/* define which ring will be checked when trigger context scheduling */
+void __raise_ctx_sched(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+
+	if (vgt_runq_is_empty(pdev))
+		return;
+
+	/* we used to call scheduler until physical head equal to tail
+	 * but it is unnecessary, the context switch logic helps to
+	 * wait for head equal to tail
+	 */
+	vgt_sched_ctx(pdev);
+}
+
+/* cleanup for previous vgt instance */
+void vgt_sched_update_prev(struct vgt_device *vgt, cycles_t time)
+{
+	/* cancel timer to avoid including context switch time */
+	if (timer_based_qos)
+		hrtimer_cancel(&vgt_hrtimer.timer);
+
+	/* Records actual tsc when all rendering engines
+	 * are stopped */
+	if (event_based_qos) {
+		ctx_actual_end_time(current_render_owner(vgt->pdev)) = time;
+	}
+}
+
+/* prepare for next vgt instance */
+void vgt_sched_update_next(struct vgt_device *vgt)
+{
+	if (timer_based_qos)
+		hrtimer_start(&vgt_hrtimer.timer,
+			ktime_add_ns(ktime_get(), vgt_hrtimer.period),
+			HRTIMER_MODE_ABS);
+
+	/* setup countdown for next vgt context */
+	if (event_based_qos) {
+		vgt_setup_countdown(vgt);
+	}
+}
+
+void vgt_schedule(struct pgt_device *pdev)
+{
+	ASSERT(spin_is_locked(&pdev->lock));
+
+	if (vgt_nr_in_runq(pdev) < 2)
+		return;
+
+	pdev->next_sched_vgt = tbs_next_vgt(&pdev->rendering_runq_head,
+			current_render_owner(pdev));
+}
+
+
+static int calculate_budget(struct vgt_device *vgt)
+{
+#if 0
+	int budget;
+
+	budget = vgt->allocated_cmds - vgt->submitted_cmds;
+	/* call scheduler when budget is not enough */
+	if (budget <= 0) {
+		vgt_schedule(pdev);
+		if (ctx_switch_requested(pdev))
+			return;
+	}
+#endif
+
+	return MAX_CMD_BUDGET;
+}
+
+void vgt_submit_commands(struct vgt_device *vgt, int ring_id)
+{
+	int cmd_nr;
+	struct pgt_device *pdev = vgt->pdev;
+	vgt_state_ring_t	*rs = &vgt->rb[ring_id];
+	int budget;
+	uint64_t submission_id;
+	bool need_irq = rs->tail_list.cmd[rs->tail_list.tail].flags & F_CMDS_ISSUE_IRQ;
+	unsigned long ip_offset = rs->tail_list.cmd[rs->tail_list.tail].ip_offset;
+
+	/*
+	 * No commands submision when context switch is in
+	 * progress. Current owner is prevented from further
+	 * submission to ensure quantum control, and new owner
+	 * request will be recovered at end of ctx switch.
+	 */
+	if (ctx_switch_requested(pdev)) {
+		vgt_dbg(VGT_DBG_RENDER, "<%d>: Hold commands in render ctx switch (%d->%d)\n",
+			vgt->vm_id, current_render_owner(pdev)->vm_id,
+			pdev->next_sched_vgt->vm_id);
+		return;
+	}
+
+	/* kicks scheduler for non-owner write. */
+	if (!is_current_render_owner(vgt)) {
+		//vgt_schedule(pdev);
+		return;
+	}
+
+	budget = calculate_budget(vgt);
+	if (!budget)
+		return;
+
+	cmd_nr = get_submission_id(rs, budget, &submission_id);
+	/* no valid cmd queued */
+	if (cmd_nr == MAX_CMD_BUDGET) {
+		vgt_dbg(VGT_DBG_RENDER, "VM(%d): tail write w/o cmd to submit\n",
+			vgt->vm_id);
+		return;
+	}
+
+	/*
+	 * otherwise submit to GPU, even when cmd_nr is ZERO.
+	 8 this is necessary, because sometimes driver may write
+	 * old tail which must take real effect.
+	 */
+	apply_tail_list(vgt, ring_id, submission_id);
+	pdev->ring_buffer[ring_id].need_irq = need_irq;
+	pdev->ring_buffer[ring_id].ip_offset = ip_offset;
+	vgt->total_cmds += cmd_nr;
+	vgt->submitted_cmds += cmd_nr;
+}
+
+void vgt_request_force_removal(struct vgt_device *vgt)
+{
+	vgt->force_removal = 1;
+	vgt->pdev->next_sched_vgt = vgt_dom0;
+	vgt_raise_request(vgt->pdev, VGT_REQUEST_SCHED);
+	wmb();
+}
diff --git a/drivers/gpu/drm/i915/vgt/sysfs.c b/drivers/gpu/drm/i915/vgt/sysfs.c
new file mode 100644
index 0000000..1c54890
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/sysfs.c
@@ -0,0 +1,1179 @@
+/*
+ * vGT sysfs interface (the original code comes from samples/kobject-example.c)
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of Version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#include <linux/slab.h>
+#include "vgt.h"
+
+struct kobject *vgt_ctrl_kobj;
+static struct kset *vgt_kset;
+static DEFINE_MUTEX(vgt_sysfs_lock);
+
+static void vgt_kobj_release(struct kobject *kobj)
+{
+	pr_debug("kobject: (%p): %s\n", kobj, __func__);
+	/* NOTE: we do not deallocate our kobject */
+	/* see the comment before vgt_init_sysfs() */
+	//kfree(kobj);
+}
+
+static int vgt_add_state_sysfs(vgt_params_t vp);
+static int vgt_del_state_sysfs(vgt_params_t vp);
+static ssize_t vgt_create_instance_store(struct kobject *kobj, struct kobj_attribute *attr,
+			const char *buf, size_t count)
+{
+	vgt_params_t vp;
+	int param_cnt;
+	char param_str[64];
+	int rc;
+	int high_gm_sz;
+	int low_gm_sz;
+
+	/* We expect the param_str should be vmid,a,b,c (where the guest
+	* wants a MB aperture and b MB gm, and c fence registers) or -vmid
+	* (where we want to release the vgt instance).
+	*/
+	(void)sscanf(buf, "%63s", param_str);
+	param_cnt = sscanf(param_str, "%d,%d,%d,%d,%d", &vp.vm_id, &low_gm_sz,
+		&high_gm_sz, &vp.fence_sz, &vp.vgt_primary);
+	vp.aperture_sz = low_gm_sz;
+	vp.gm_sz = high_gm_sz + low_gm_sz;
+
+	if (param_cnt == 1) {
+		if (vp.vm_id >= 0)
+			return -EINVAL;
+	} else if (param_cnt == 4 || param_cnt == 5) {
+		if (!(vp.vm_id > 0 && vp.aperture_sz > 0 &&
+			vp.aperture_sz <= vp.gm_sz && vp.fence_sz > 0))
+			return -EINVAL;
+
+		if (param_cnt == 5) {
+			/* -1/0/1 means: not-specified, non-primary, primary */
+			if (vp.vgt_primary < -1 && vp.vgt_primary > 1)
+				return -EINVAL;
+		} else {
+			vp.vgt_primary = -1; /* no valid value specified. */
+		}
+	} else
+		return -EINVAL;
+
+	mutex_lock(&vgt_sysfs_lock);
+	rc = (vp.vm_id > 0) ? vgt_add_state_sysfs(vp) : vgt_del_state_sysfs(vp);
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return rc < 0 ? rc : count;
+}
+
+static ssize_t vgt_display_owner_show(struct kobject *kobj, struct kobj_attribute *attr,
+			char *buf)
+{
+	struct pgt_device *pdev = &default_device;
+	return sprintf(buf,"%d\n", current_display_owner(pdev)->vm_id);
+}
+
+static ssize_t vgt_display_owner_store(struct kobject *kobj, struct kobj_attribute *attr,
+	const char *buf, size_t count)
+{
+	int vmid;
+	if (sscanf(buf, "%d", &vmid) != 1)
+		return -EINVAL;
+
+	if (vmid != 0) {
+		vgt_warn("Cannot change display_owner to vms other than domain0!\n");
+	}
+
+	return count;
+}
+
+static ssize_t vgt_foreground_vm_show(struct kobject *kobj, struct kobj_attribute *attr,
+			char *buf)
+{
+	return sprintf(buf,"%d\n", current_foreground_vm((&default_device))->vm_id);
+}
+
+static ssize_t vgt_foreground_vm_store(struct kobject *kobj, struct kobj_attribute *attr,
+	const char *buf, size_t count)
+{
+	unsigned long flags;
+	int ret = count;
+	int vmid;
+	struct vgt_device *next_vgt;
+	struct pgt_device *pdev = &default_device;
+	int cpu;
+
+	if (sscanf(buf, "%d", &vmid) != 1)
+		return -EINVAL;
+
+	mutex_lock(&vgt_sysfs_lock);
+
+	vgt_lock_dev_flags(pdev, cpu, flags);
+
+	next_vgt = vmid_2_vgt_device(vmid);
+	if (next_vgt == NULL) {
+		printk("vGT: can not find the vgt instance of dom%d!\n", vmid);
+		ret = -ENODEV;
+		goto out;
+	}
+
+	if (current_foreground_vm(pdev) == next_vgt) {
+		goto out;
+	}
+
+	if (!__vreg(next_vgt, vgt_info_off(display_ready))) {
+		printk("VGT %d: Display is not ready.\n", vmid);
+		ret = -EAGAIN;
+		goto out;
+	}
+
+	pdev->next_foreground_vm = next_vgt;
+	vgt_raise_request(pdev, VGT_REQUEST_DPY_SWITCH);
+out:
+	vgt_unlock_dev_flags(pdev, cpu, flags);
+
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return ret;
+}
+
+static ssize_t vgt_ctx_switch_store(struct kobject *kobj, struct kobj_attribute *attr,
+			const char *buf, size_t count)
+{
+	int val;
+	bool enabled;
+
+	if (sscanf(buf, "%du", &val) != 1)
+		return -EINVAL;
+	enabled = !!val;
+	vgt_toggle_ctx_switch(enabled);
+	return count;
+}
+
+static ssize_t vgt_validate_ctx_switch_store(struct kobject *kobj,
+			struct kobj_attribute *attr, const char *buf, size_t count)
+{
+	int val;
+
+	if (sscanf(buf, "%du", &val) != 1)
+		return -EINVAL;
+	vgt_validate_ctx_switch = !!val;
+	return count;
+}
+
+static ssize_t vgt_ctx_switch_show(struct kobject *kobj, struct kobj_attribute *attr,
+			char *buf)
+{
+	return sprintf(buf, "VGT context switch: %s\n",
+			vgt_ctx_switch ? "enabled" : "disabled");
+}
+
+static ssize_t vgt_validate_ctx_switch_show(struct kobject *kobj,
+			struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "VGT check mmio restore: %s\n",
+			vgt_validate_ctx_switch ? "enabled" : "disabled");
+}
+
+static ssize_t vgt_dpy_switch_store(struct kobject *kobj, struct kobj_attribute *attr,
+			const char *buf, size_t count)
+{
+	int val;
+	if (sscanf(buf, "%d", &val) != 1)
+		return -EINVAL;
+
+//	fastpath_dpy_switch = !!val;
+	fastpath_dpy_switch = true;
+	return count;
+}
+
+static ssize_t vgt_dpy_switch_show(struct kobject *kobj, struct kobj_attribute *attr,
+			char *buf)
+{
+	return sprintf(buf, "VGT display_owner switch: using the %s\n",
+				fastpath_dpy_switch ?
+				"fast-path method. (write 0 to use the slow-path method)"
+				: "slow-path method. (write 1 to use the fast-path method)");
+}
+
+ static ssize_t vgt_available_res_show(struct kobject *kobj, struct kobj_attribute *attr,
+			char *buf)
+{
+	struct pgt_device *pdev = &default_device;
+	ssize_t buf_len;
+	int cpu;
+
+	mutex_lock(&vgt_sysfs_lock);
+	vgt_lock_dev(pdev, cpu);
+	buf_len = get_avl_vm_aperture_gm_and_fence(pdev, buf,
+			PAGE_SIZE);
+	vgt_unlock_dev(pdev, cpu);
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return buf_len;
+}
+
+
+static ssize_t vgt_virtual_event_reader(struct kobject *kobj,
+				struct kobj_attribute *attr, char *buf)
+{
+	ssize_t nbytes = sprintf(buf, "== README ==\n"
+			"This interface is only used for debugging purpose.\n"
+			"Cat an integer to trigger a virtual event.\n"
+			"\t bit 0 - bit 15: for virtual event number;\n"
+			"\t bit 16 - bit 23: for vm id;\n"
+			"Supported event list:\n"
+			"[0x%x]: %s\n"
+			"[0x%x]: %s\n"
+			"[0x%x]: %s\n",
+			RCS_AS_CONTEXT_SWITCH, vgt_irq_name[RCS_AS_CONTEXT_SWITCH],
+			VCS_AS_CONTEXT_SWITCH, vgt_irq_name[VCS_AS_CONTEXT_SWITCH],
+			BCS_AS_CONTEXT_SWITCH, vgt_irq_name[BCS_AS_CONTEXT_SWITCH]);
+	return nbytes;
+}
+
+static ssize_t vgt_virtual_event_trigger(struct kobject *kobj,
+		struct kobj_attribute *attr,
+		const char *buf, size_t count)
+{
+	int cpu;
+	enum vgt_event_type event;
+	struct vgt_device *vgt;
+	vgt_virtual_event_t v_event;
+	struct pgt_device *pdev = &default_device;
+
+	if (sscanf(buf, "%i", &v_event.dw) != 1)
+		return -EINVAL;
+
+	event = v_event.virtual_event;
+	if ((event != RCS_AS_CONTEXT_SWITCH) &&
+			(event != VCS_AS_CONTEXT_SWITCH) &&
+			(event != BCS_AS_CONTEXT_SWITCH)) {
+		return -EINVAL;
+	}
+
+	vgt = vmid_2_vgt_device(v_event.vmid);
+
+	if (!vgt)
+		return -EINVAL;
+
+	vgt_lock_dev(pdev, cpu);
+	vgt_trigger_virtual_event(vgt, event);
+	vgt_unlock_dev(pdev, cpu);
+
+	return count;
+}
+
+static struct kobj_attribute create_vgt_instance_attrs =
+	__ATTR(create_vgt_instance, 0220, NULL, vgt_create_instance_store);
+static struct kobj_attribute display_owner_ctrl_attrs =
+	__ATTR(display_owner, 0660, vgt_display_owner_show, vgt_display_owner_store);
+static struct kobj_attribute foreground_vm_ctrl_attrs =
+	__ATTR(foreground_vm, 0660, vgt_foreground_vm_show, vgt_foreground_vm_store);
+
+static struct kobj_attribute virtual_event_attrs =
+	__ATTR(virtual_event, 0660, vgt_virtual_event_reader, vgt_virtual_event_trigger);
+
+static struct kobj_attribute ctx_switch_attrs =
+	__ATTR(ctx_switch, 0660, vgt_ctx_switch_show, vgt_ctx_switch_store);
+
+static struct kobj_attribute validate_ctx_switch_attrs =
+	__ATTR(validate_ctx_switch, 0660, vgt_validate_ctx_switch_show, vgt_validate_ctx_switch_store);
+
+static struct kobj_attribute dpy_switch_attrs =
+	__ATTR(display_switch_method, 0660, vgt_dpy_switch_show, vgt_dpy_switch_store);
+
+static struct kobj_attribute available_res_attrs =
+	__ATTR(available_resource, 0440, vgt_available_res_show, NULL);
+
+static struct attribute *vgt_ctrl_attrs[] = {
+	&create_vgt_instance_attrs.attr,
+	&display_owner_ctrl_attrs.attr,
+	&foreground_vm_ctrl_attrs.attr,
+	&virtual_event_attrs.attr,
+	&ctx_switch_attrs.attr,
+	&validate_ctx_switch_attrs.attr,
+	&dpy_switch_attrs.attr,
+	&available_res_attrs.attr,
+	NULL,	/* need to NULL terminate the list of attributes */
+};
+
+#define kobj_to_port(kobj) container_of((kobj), struct gt_port, kobj)
+#define kobj_to_vgt(xkobj) container_of((xkobj), struct vgt_device, kobj)
+
+static ssize_t vgt_port_edid_show(struct file *filp, struct kobject *kobj,
+				  struct bin_attribute *attr, char *buf, loff_t off,
+				  size_t count)
+{
+	struct gt_port *port = kobj_to_port(kobj);
+	struct vgt_edid_data_t *edid;
+
+	if (off >= EDID_SIZE) {
+		return 0;
+	}
+
+	if (off + count > EDID_SIZE) {
+		count = EDID_SIZE - off;
+	}
+
+	mutex_lock(&vgt_sysfs_lock);
+
+	edid = port->edid;
+
+	if (edid && edid->data_valid) {
+		memcpy(buf, edid->edid_block + off, count);
+	} else {
+		count = 0;
+	}
+
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return count;
+}
+
+static ssize_t
+vgt_port_edid_store(struct file* filp, struct kobject *kobj,
+		    struct bin_attribute *bin_attr,
+		    char *buf, loff_t off, size_t count)
+{
+	struct gt_port *port = kobj_to_port(kobj);
+	int write_count = count;
+	char *dest;
+
+	if (!count || off < 0 || (off & 3))
+		return count;
+
+	if (off >= bin_attr->size)
+		return -EINVAL;
+
+	if (off + count > bin_attr->size)
+		write_count = bin_attr->size - off;
+
+	mutex_lock(&vgt_sysfs_lock);
+
+	if (port->cache.edid == NULL) {
+		port->cache.edid = kmalloc(sizeof(struct vgt_edid_data_t),
+			      GFP_ATOMIC);
+	}
+
+	if (port->cache.edid == NULL) {
+		mutex_unlock(&vgt_sysfs_lock);
+		return -ENOMEM;
+	}
+
+	dest = port->cache.edid->edid_block + off;
+	memcpy(dest, buf, write_count);
+	if (off + write_count == bin_attr->size &&
+		vgt_is_edid_valid(port->cache.edid->edid_block)) {
+		
+		// customize the EDID to remove extended EDID block.
+		u8 *block = port->cache.edid->edid_block;
+		if (block[0x7e]) {
+			block[0x7f] += block[0x7e];
+			block[0x7e] = 0;
+		}
+		port->cache.edid->data_valid = true;
+		port->cache.valid = true;
+	}
+
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return count;
+}
+
+static inline int ctoi(char chr)
+{
+    return (chr >= '0' && chr <= '9' ? chr - '0' :
+		(chr >= 'a' && chr <= 'f' ? chr - 'a' + 10 :
+		(chr >= 'A' && chr <= 'F' ? chr - 'A' + 10 :
+			-1)));
+}
+
+static ssize_t
+vgt_port_edid_text_store(struct kobject *kobj, struct kobj_attribute *attr,
+                        const char *buf, size_t count)
+{
+	struct gt_port *port = kobj_to_port(kobj);
+	int i;
+	char *dest;
+
+	if (count != (EDID_SIZE << 1))
+		return -EINVAL;
+
+	mutex_lock(&vgt_sysfs_lock);
+
+	if (port->cache.edid == NULL) {
+		port->cache.edid = kmalloc(sizeof(struct vgt_edid_data_t),
+			      GFP_ATOMIC);
+	}
+
+	if (port->cache.edid == NULL) {
+		return -ENOMEM;
+	}
+
+	port->cache.edid->data_valid = false;
+
+	dest = port->cache.edid->edid_block;
+
+	for (i = 0; i < count; i += 2) {
+		int hi = ctoi(buf[i]);
+		int lo  = ctoi(buf[i + 1]);
+		if (hi < 0 || lo < 0) {
+			vgt_warn("invalid injected EDID!\n");
+			break;
+		}
+
+		*dest= (hi << 4) + lo;
+		dest++;
+	}
+
+	if ((i == count) && vgt_is_edid_valid(port->cache.edid->edid_block)) {
+		// customize the EDID to remove extended EDID block.
+		u8 *block = port->cache.edid->edid_block;
+		if (block[0x7e]) {
+			block[0x7f] += block[0x7e];
+			block[0x7e] = 0;
+		}
+		port->cache.edid->data_valid = true;
+		port->cache.valid = true;
+	}
+
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return count;
+}
+
+
+static bool is_port_connected(struct gt_port *port)
+{
+	if (port && port->edid && port->edid->data_valid) {
+		return true;
+	}
+	return false;
+}
+
+static ssize_t vgt_pport_connection_show(struct kobject *kobj, struct kobj_attribute *attr,
+				   char *buf)
+{
+	struct pgt_device *pgt = &default_device;
+	ssize_t buf_len;
+	int i;
+
+        for (i = 0; i < I915_MAX_PORTS; i++) {
+                if (strcmp(VGT_PORT_NAME(i), kobj->name) == 0) {
+                        break;
+                }
+        }
+
+	if (i >= I915_MAX_PORTS) {
+		return 0;
+	}
+
+	mutex_lock(&vgt_sysfs_lock);
+
+	buf_len = sprintf(buf, "%s\n", is_port_connected(&(pgt->ports[i])) ?
+			"connected" : "disconnected");
+
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return buf_len;
+}
+
+static bool is_pport_present(struct pgt_device *pgt, struct gt_port *port)
+{
+
+	bool found = false;
+
+	switch (port->physcal_port) {
+	case PORT_A:
+		found = VGT_MMIO_READ(pgt, _REG_DDI_BUF_CTL_A) & _DDI_BUFCTL_DETECT_MASK;
+		break;
+	case PORT_B:
+		found = VGT_MMIO_READ(pgt,_REG_SFUSE_STRAP) & _REGBIT_SFUSE_STRAP_B_PRESENTED;
+		break;
+	case PORT_C:
+		found = VGT_MMIO_READ(pgt,_REG_SFUSE_STRAP) & _REGBIT_SFUSE_STRAP_C_PRESENTED;
+		break;
+	case PORT_D:
+		found = VGT_MMIO_READ(pgt,_REG_SFUSE_STRAP) & _REGBIT_SFUSE_STRAP_D_PRESENTED;
+		break;
+	case PORT_E:
+		found = true;
+		break;
+	default:
+		found = false;
+		break;
+	}
+
+	return found;
+
+}
+
+static ssize_t vgt_pport_presnece_show(struct kobject *kobj, struct kobj_attribute *attr,
+				   char *buf)
+{
+	struct pgt_device *pgt = &default_device;
+
+	ssize_t buf_len;
+	int i;
+
+        for (i = 0; i < I915_MAX_PORTS; i++) {
+                if (strcmp(VGT_PORT_NAME(i), kobj->name) == 0) {
+                        break;
+                }
+        }
+
+	if (i >= I915_MAX_PORTS) {
+		return 0;
+	}
+
+	mutex_lock(&vgt_sysfs_lock);
+
+	buf_len = sprintf(buf, "%s\n", is_pport_present(pgt, &(pgt->ports[i])) ?
+			"present" : "unpresent");
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return buf_len;
+}
+
+static ssize_t vgt_pport_connection_store(struct kobject *kobj, struct kobj_attribute *attr,
+			const char *buf, size_t count)
+{
+	bool is_connected = false;
+
+	mutex_lock(&vgt_sysfs_lock);
+	if (strncmp("connect", buf, 7) == 0) {
+			is_connected = true;
+	} else if (strncmp("disconnect", buf, 10) == 0) {
+			is_connected = false;
+	}
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return count;
+}
+
+static ssize_t vgt_vport_connection_show(struct kobject *kobj, struct kobj_attribute *attr,
+				   char *buf)
+{
+	struct gt_port *port = kobj_to_port(kobj);
+	ssize_t buf_len;
+
+	mutex_lock(&vgt_sysfs_lock);
+	
+	buf_len = sprintf(buf, "%s\n", is_port_connected(port) ? "connected" : "disconnected");
+
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return buf_len;
+}
+
+static ssize_t vgt_vport_connection_store(struct kobject *kobj, struct kobj_attribute *attr,
+                        const char *buf, size_t count)
+{
+	struct gt_port *port = kobj_to_port(kobj);
+	struct vgt_device *vgt = kobj_to_vgt(kobj->parent);
+	enum vgt_event_type event;
+	bool flush_request = false;
+	bool hotplug_request = false;
+	int cpu;
+	bool is_current_connected = is_port_connected(port);
+
+	mutex_lock(&vgt_sysfs_lock);
+	vgt_lock_dev(vgt->pdev, cpu);
+
+	if (strncmp("connect", buf, 7) == 0) {
+		vgt_info("Monitor detection: %s  is connected\n", VGT_PORT_NAME(port->physcal_port));
+		if (!(port->cache.valid && port->cache.edid &&
+				port->cache.edid->data_valid))
+			vgt_warn("Request to connect a monitor but new monitor "
+				"setting is not ready. Will be ignored\n");
+		else if (!is_current_connected) {
+			flush_request = hotplug_request = true;
+		} else if (is_current_connected &&
+			memcmp(port->edid->edid_block, port->cache.edid->edid_block, EDID_SIZE)) {
+			flush_request = hotplug_request = true;
+		}
+	} else if (strncmp("disconnect", buf, 10) == 0) {
+		vgt_info("Monitor detection: %s  is disconnected\n", VGT_PORT_NAME(port->physcal_port));
+		if (is_current_connected) {
+			if (port->cache.edid)
+				port->cache.edid->data_valid = false;
+			port->cache.valid = true;
+			flush_request = hotplug_request = true;
+		}
+	} else if (strncmp("flush", buf, 5) == 0) {
+		flush_request = true;
+	} else {
+		vgt_warn("Input string not recognized: %s\n", buf);
+	}
+
+	if (flush_request)
+		vgt_flush_port_info(vgt, port);
+
+	if (hotplug_request) {
+		enum vgt_port port_type = vgt_get_port(vgt, port);
+		switch (port_type) {
+		case PORT_A:
+			event = EVENT_MAX; break;
+		case PORT_B:
+			event = DP_B_HOTPLUG; break;
+		case PORT_C:
+			event = DP_C_HOTPLUG; break;
+		case PORT_D:
+			event = DP_D_HOTPLUG; break;
+		case PORT_E:
+			event = CRT_HOTPLUG; break;
+		default:
+			event = EVENT_MAX;
+			vgt_err("Invalid port(%s) for hotplug!\n",
+				VGT_PORT_NAME(port_type));
+		}
+		if (event != EVENT_MAX)
+			vgt_trigger_virtual_event(vgt, event);
+	}
+
+	vgt_unlock_dev(vgt->pdev, cpu);
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return count;
+}
+
+static ssize_t vgt_port_type_show(struct kobject *kobj, struct kobj_attribute *attr,
+				   char *buf)
+{
+	struct gt_port *port = kobj_to_port(kobj);
+	ssize_t buf_len;
+
+	mutex_lock(&vgt_sysfs_lock);
+	buf_len = sprintf(buf, "%s\n", VGT_PORT_TYPE_NAME(port->type));
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return buf_len;
+}
+
+static ssize_t vgt_port_type_store(struct kobject *kobj, struct kobj_attribute *attr,
+				   const char *buf, size_t count)
+{
+	struct gt_port *port = kobj_to_port(kobj);
+	int portIndex;
+
+
+	mutex_lock(&vgt_sysfs_lock);
+	if (sscanf(buf, "%d", &portIndex) != 1) {
+		mutex_unlock(&vgt_sysfs_lock);
+		return -EINVAL;
+	}
+
+	port->cache.type = VGT_CRT + portIndex;
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return count;
+}
+
+static ssize_t vgt_vport_port_override_show(struct kobject *kobj, struct kobj_attribute *attr,
+				   char *buf)
+{
+	struct gt_port *port = kobj_to_port(kobj);
+	ssize_t buf_len;
+
+	mutex_lock(&vgt_sysfs_lock);
+	buf_len = sprintf(buf, "%s\n", VGT_PORT_NAME(port->port_override));
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return buf_len;
+}
+
+static ssize_t vgt_vport_port_override_store(struct kobject *kobj, struct kobj_attribute *attr,
+                        const char *buf, size_t count)
+{
+	struct gt_port *port = kobj_to_port(kobj);
+	enum vgt_port override;
+
+	if (strncmp("PORT_A", buf, 6) == 0) {
+		override = PORT_A;
+	} else if (strncmp("PORT_B", buf, 6) == 0) {
+		override = PORT_B;
+	} else if (strncmp("PORT_C", buf, 6) == 0) {
+		override  = PORT_C;
+	} else if (strncmp("PORT_D", buf, 6) == 0) {
+		override  = PORT_D;
+	} else if (strncmp("PORT_E", buf, 6) == 0) {
+		override = PORT_E;
+	} else {
+		return -EINVAL;
+	}
+
+	mutex_lock(&vgt_sysfs_lock);
+
+	port->cache.port_override = override;
+	port->cache.valid = true;
+
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return count;
+}
+
+static ssize_t vgt_vport_pipe_show(struct kobject *kobj, struct kobj_attribute *attr,
+				   char *buf)
+{
+	struct gt_port *port_ptr = kobj_to_port(kobj);
+	struct vgt_device *vgt = kobj_to_vgt(kobj->parent);
+	enum vgt_port port;
+	ssize_t buf_len;
+	int cpu;
+
+	mutex_lock(&vgt_sysfs_lock);
+	vgt_lock_dev(vgt->pdev, cpu);
+
+	port = vgt_get_port(vgt, port_ptr);
+	if (port == PORT_A)
+		buf_len = sprintf(buf, "PIPE_EDP\n");
+	else {
+		enum vgt_pipe pipe = vgt_get_pipe_from_port(vgt, port);
+		buf_len = sprintf(buf, "%s\n", VGT_PIPE_NAME(pipe));
+	}
+
+	vgt_unlock_dev(vgt->pdev, cpu);
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return buf_len;
+}
+
+static struct kobj_attribute vport_connection_attrs =
+	__ATTR(connection, 0660, vgt_vport_connection_show, vgt_vport_connection_store);
+
+static struct kobj_attribute vport_type_attrs =
+	__ATTR(type, 0660, vgt_port_type_show, vgt_port_type_store);
+
+static struct kobj_attribute vport_port_override_attrs =
+	__ATTR(port_override, 0660, vgt_vport_port_override_show, vgt_vport_port_override_store);
+
+static struct kobj_attribute vport_pipe_attrs =
+	__ATTR(pipe, 0440, vgt_vport_pipe_show, NULL);
+
+// EDID text mode input interface for the convenience of testing
+static struct kobj_attribute vport_edid_text_attrs =
+	__ATTR(edid_text, 0660, NULL, vgt_port_edid_text_store);
+
+static struct attribute *vgt_vport_attrs[] = {
+	&vport_connection_attrs.attr,
+	&vport_type_attrs.attr,
+	&vport_port_override_attrs.attr,
+	&vport_pipe_attrs.attr,
+	&vport_edid_text_attrs.attr,
+	NULL,
+};
+
+static struct bin_attribute port_edid_attr = {
+        .attr = {
+                .name = "edid",
+                .mode = 0660
+        },
+        .size = EDID_SIZE,
+        .read = vgt_port_edid_show,
+        .write = vgt_port_edid_store,
+};
+
+static struct kobj_attribute pport_type_attrs =
+	__ATTR(type, 0660, vgt_port_type_show, vgt_port_type_store);
+
+static struct kobj_attribute pport_connection_attrs =
+	__ATTR(connection, 0660, vgt_pport_connection_show, vgt_pport_connection_store);
+
+static struct kobj_attribute pport_presence_attrs =
+	__ATTR(presence, 0440, vgt_pport_presnece_show, NULL);
+
+static struct attribute *vgt_pport_attrs[] = {
+	&pport_connection_attrs.attr,
+	&pport_type_attrs.attr,
+	&pport_presence_attrs.attr,
+	NULL,
+};
+
+/* copied code from here */
+static ssize_t kobj_attr_show(struct kobject *kobj, struct attribute *attr,
+				char *buf)
+{
+	struct kobj_attribute *kattr;
+	ssize_t ret = -EIO;
+
+	kattr = container_of(attr, struct kobj_attribute, attr);
+	if (kattr->show)
+		ret = kattr->show(kobj, kattr, buf);
+	return ret;
+}
+
+static ssize_t kobj_attr_store(struct kobject *kobj, struct attribute *attr,
+				const char *buf, size_t count)
+{
+	struct kobj_attribute *kattr;
+	ssize_t ret = -EIO;
+
+	kattr = container_of(attr, struct kobj_attribute, attr);
+	if (kattr->store)
+		ret = kattr->store(kobj, kattr, buf, count);
+	return ret;
+}
+
+const struct sysfs_ops vgt_kobj_sysfs_ops = {
+	.show	= kobj_attr_show,
+	.store	= kobj_attr_store,
+};
+
+
+/* copied code end */
+
+static ssize_t vgt_id_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)
+{
+	struct vgt_device *vgt = kobj_to_vgt(kobj);
+	return sprintf(buf, "%x\n", vgt->vgt_id);
+}
+
+static ssize_t gm_sz_show(struct kobject *kobj, struct kobj_attribute *attr,char *buf)
+{
+	struct vgt_device *vgt = kobj_to_vgt(kobj);
+	return sprintf(buf, "%016llx\n", vgt->gm_sz);
+}
+
+static ssize_t aperture_sz_show(struct kobject *kobj, struct kobj_attribute *attr,char *buf)
+{
+	struct vgt_device *vgt = kobj_to_vgt(kobj);
+	return sprintf(buf, "%016llx\n", vgt->aperture_sz);
+}
+
+static ssize_t aperture_base_show(struct kobject *kobj, struct kobj_attribute *attr,char *buf)
+{
+	struct vgt_device *vgt = kobj_to_vgt(kobj);
+	return sprintf(buf, "%016llx\n", vgt->aperture_base);
+}
+
+static ssize_t aperture_base_va_show(struct kobject *kobj, struct kobj_attribute *attr,char *buf)
+{
+	struct vgt_device *vgt = kobj_to_vgt(kobj);
+	return sprintf(buf, "%p\n", vgt->aperture_base_va);
+}
+
+static struct kobj_attribute vgt_id_attribute =
+	__ATTR_RO(vgt_id);
+
+static struct kobj_attribute gm_sz_attribute =
+	__ATTR_RO(gm_sz);
+
+static struct kobj_attribute aperture_sz_attribute =
+	__ATTR_RO(aperture_sz);
+
+static struct kobj_attribute aperture_base_attribute =
+	__ATTR_RO(aperture_base);
+
+static struct kobj_attribute aperture_base_va_attribute =
+	__ATTR_RO(aperture_base_va);
+
+/*
+ * Create a group of attributes so that we can create and destroy them all
+ * at once.
+ */
+static struct attribute *vgt_instance_attrs[] = {
+	&vgt_id_attribute.attr,
+	&gm_sz_attribute.attr,
+	&aperture_sz_attribute.attr,
+	&aperture_base_attribute.attr,
+	&aperture_base_va_attribute.attr,
+	NULL,	/* need to NULL terminate the list of attributes */
+};
+
+/*
+ * An unnamed attribute group will put all of the attributes directly in
+ * the kobject directory.  If we specify a name, a subdirectory will be
+ * created for the attributes with the directory being the name of the
+ * attribute group.
+ */
+#if 0
+static struct attribute_group attr_group = {
+	.attrs = attrs,
+};
+#endif
+
+static struct kobj_type vgt_instance_ktype = {
+	.release	= vgt_kobj_release,
+	.sysfs_ops	= &vgt_kobj_sysfs_ops,
+	.default_attrs = vgt_instance_attrs,
+};
+
+static struct kobj_type vgt_ctrl_ktype = {
+	.release	= vgt_kobj_release,
+	.sysfs_ops  = &vgt_kobj_sysfs_ops,
+	.default_attrs = vgt_ctrl_attrs,
+};
+
+static struct kobj_type vgt_vport_ktype = {
+	.release	= vgt_kobj_release,
+	.sysfs_ops	= &vgt_kobj_sysfs_ops,
+	.default_attrs	= vgt_vport_attrs,
+};
+
+static struct kobj_type vgt_pport_ktype = {
+	.release	= vgt_kobj_release,
+	.sysfs_ops	= &vgt_kobj_sysfs_ops,
+	.default_attrs	= vgt_pport_attrs,
+};
+
+static ssize_t
+igd_mmio_read(struct file *filp, struct kobject *kobj,
+		struct bin_attribute *bin_attr,
+		char *buf, loff_t off, size_t count)
+{
+	struct pgt_device *pdev = &default_device;
+	size_t init_count = count, len;
+	unsigned long data;
+	int cpu;
+
+	if (!count || off < 0 || off + count > bin_attr->size || (off & 0x3))
+		return -EINVAL;
+
+	vgt_lock_dev(pdev, cpu);
+
+	while (count > 0) {
+		len = (count > sizeof(unsigned long)) ? sizeof(unsigned long) :
+				count;
+
+		if (vgt_native_mmio_read(off, &data, len, false) != 0) {
+			vgt_unlock_dev(pdev, cpu);
+			return -EIO;
+		}
+
+		memcpy(buf, &data, len);
+		buf += len;
+		count -= len;
+	}
+
+	vgt_unlock_dev(pdev, cpu);
+
+	return init_count;
+}
+
+static ssize_t
+igd_mmio_write(struct file* filp, struct kobject *kobj,
+		struct bin_attribute *bin_attr,
+		char *buf, loff_t off, size_t count)
+{
+	struct pgt_device *pdev = &default_device;
+	size_t init_count = count, len;
+	unsigned long data;
+	int cpu;
+
+	if (!count || off < 0 || off + count > bin_attr->size || (off & 0x3))
+		return -EINVAL;
+
+	vgt_lock_dev(pdev, cpu);
+
+	while (count > 0) {
+		len = (count > sizeof(unsigned long)) ? sizeof(unsigned long) :
+				count;
+
+		memcpy(&data, buf, len);
+		if (vgt_native_mmio_read(off, &data, len, false) != 0) {
+			vgt_unlock_dev(pdev, cpu);
+			return -EIO;
+		}
+
+		buf += len;
+		count -= len;
+	}
+
+	vgt_unlock_dev(pdev, cpu);
+	return init_count;
+}
+
+static struct bin_attribute igd_mmio_attr = {
+	.attr =	{
+		.name = "igd_mmio",
+		.mode = 0660
+	},
+	.size = VGT_MMIO_SPACE_SZ,
+	.read = igd_mmio_read,
+	.write = igd_mmio_write,
+};
+
+
+static int vgt_add_state_sysfs(vgt_params_t vp)
+{
+	int retval, i;
+	struct vgt_device *vgt;
+	/*
+	* Create a simple kobject located under /sys/kernel/
+	* As this is a simple directory, no uevent will be sent to
+	* userspace.  That is why this function should not be used for
+	* any type of dynamic kobjects, where the name and number are
+	* not known ahead of time.
+	*/
+
+	ASSERT(vgt_ctrl_kobj);
+
+	/* check if such vmid has been used */
+	if (vmid_2_vgt_device(vp.vm_id))
+		return -EINVAL;
+
+	retval = create_vgt_instance(&default_device, &vgt, vp);
+
+	if (retval < 0)
+		return retval;
+
+	/* init kobject */
+	kobject_init(&vgt->kobj, &vgt_instance_ktype);
+
+	/* set it before calling the kobject core */
+	vgt->kobj.kset = vgt_kset;
+
+	/* add kobject, NULL parent indicates using kset as parent */
+	retval = kobject_add(&vgt->kobj, NULL, "vm%u", vgt->vm_id);
+	if (retval) {
+		printk(KERN_WARNING "%s: vgt kobject add error: %d\n",
+					__func__, retval);
+		kobject_put(&vgt->kobj);
+	}
+
+	for (i = 0; i < I915_MAX_PORTS; i++) {
+		retval = kobject_init_and_add(&vgt->ports[i].kobj,
+					      &vgt_vport_ktype,
+					      &vgt->kobj,
+					      "%s",
+					      VGT_PORT_NAME(i));
+
+		if (retval) {
+			printk(KERN_WARNING
+			       "%s: vgt vport kobject add error: %d\n",
+			       __func__, retval);
+			retval = -EINVAL;
+			goto kobj_fail;
+		}
+
+		retval = sysfs_create_bin_file(&vgt->ports[i].kobj,
+					    &port_edid_attr);
+		if (retval < 0) {
+			retval = -EINVAL;
+			goto kobj_fail;
+		}
+	}
+
+	if ((propagate_monitor_to_guest) && (vgt->vm_id != 0)) {
+		vgt_detect_display(vgt, -1);
+	}
+
+	return retval;
+
+kobj_fail:
+	for (; i >= 0; i++) {
+		kobject_put(&vgt->ports[i].kobj);
+	}
+	kobject_put(&vgt->kobj);
+	return retval;
+}
+
+static int vgt_del_state_sysfs(vgt_params_t vp)
+{
+	struct vgt_device *vgt;
+	int i;
+
+	vp.vm_id = -vp.vm_id;
+	vgt = vmid_2_vgt_device(vp.vm_id);
+	if (!vgt)
+		return -ENODEV;
+
+	for (i = 0; i < I915_MAX_PORTS; i++) {
+		kobject_put(&vgt->ports[i].kobj);
+	}
+
+	kobject_put(&vgt->kobj);
+
+	vgt_release_instance(vgt);
+
+	return 0;
+}
+
+int vgt_init_sysfs(struct pgt_device *pdev)
+{
+	struct pgt_device *pgt = &default_device;
+	int ret, i = 0;
+
+	vgt_kset = kset_create_and_add("vgt", NULL, kernel_kobj);
+	if (!vgt_kset) {
+		ret = -ENOMEM;
+		goto kset_fail;
+	}
+
+	vgt_ctrl_kobj = kzalloc(sizeof(struct kobject), GFP_KERNEL);
+	if (!vgt_ctrl_kobj) {
+		ret = -ENOMEM;
+		goto ctrl_fail;
+	}
+
+	vgt_ctrl_kobj->kset = vgt_kset;
+
+	ret = kobject_init_and_add(vgt_ctrl_kobj, &vgt_ctrl_ktype, NULL, "control");
+	if (ret) {
+		ret = -EINVAL;
+		goto kobj_fail;
+	}
+
+	ret = sysfs_create_bin_file(vgt_ctrl_kobj, &igd_mmio_attr);
+	if (ret < 0) {
+		ret = -EINVAL;
+		goto kobj_fail;
+	}
+
+	for (i = 0; i < I915_MAX_PORTS; i++) {
+		ret = kobject_init_and_add(&pgt->ports[i].kobj,
+					      &vgt_pport_ktype,
+					      vgt_ctrl_kobj,
+					      "%s",
+					      VGT_PORT_NAME(i));
+
+		if (ret) {
+			printk(KERN_WARNING
+			       "%s: vgt pport kobject add error: %d\n",
+			       __func__, ret);
+			ret = -EINVAL;
+			kobject_put(&pgt->ports[i].kobj);
+			goto kobj_fail;
+		}
+
+		ret = sysfs_create_bin_file(&pgt->ports[i].kobj,
+					    &port_edid_attr);
+		if (ret < 0) {
+			ret = -EINVAL;
+			goto kobj_fail;
+		}
+	}
+
+	return 0;
+
+kobj_fail:
+	for (; i > 0; i--) {
+		kobject_put(&pgt->ports[i - 1].kobj);
+	}
+	kobject_put(vgt_ctrl_kobj);
+ctrl_fail:
+	kset_unregister(vgt_kset);
+kset_fail:
+	return ret;
+}
+
+void vgt_destroy_sysfs(void)
+{
+	sysfs_remove_bin_file(vgt_ctrl_kobj, &igd_mmio_attr);
+	kobject_put(vgt_ctrl_kobj);
+	kset_unregister(vgt_kset);
+}
diff --git a/drivers/gpu/drm/i915/vgt/trace.h b/drivers/gpu/drm/i915/vgt/trace.h
new file mode 100644
index 0000000..b0f485c
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/trace.h
@@ -0,0 +1,319 @@
+/*
+ * vGT ftrace header
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of Version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#if !defined(_VGT_TRACE_H_) || defined(TRACE_HEADER_MULTI_READ)
+#define _VGT_TRACE_H_
+
+#include <linux/types.h>
+#include <linux/stringify.h>
+#include <linux/tracepoint.h>
+
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM vgt
+#define TRACE_SYSTEM_STRING __stringify(TRACE_SYSTEM)
+
+TRACE_EVENT(vgt_mmio_rw,
+		TP_PROTO(bool write, u32 vm_id, u32 offset, void *pd,
+			int bytes),
+
+		TP_ARGS(write, vm_id, offset, pd, bytes),
+
+		TP_STRUCT__entry(
+			__field(bool, write)
+			__field(u32, vm_id)
+			__field(u32, offset)
+			__field(int, bytes)
+			__field(u64, value)
+			),
+
+		TP_fast_assign(
+			__entry->write = write;
+			__entry->vm_id = vm_id;
+			__entry->offset = offset;
+			__entry->bytes = bytes;
+
+			memset(&__entry->value, 0, sizeof(u64));
+			memcpy(&__entry->value, pd, bytes);
+		),
+
+		TP_printk("VM%u %s offset 0x%x data 0x%llx byte %d\n",
+				__entry->vm_id,
+				__entry->write ? "write" : "read",
+				__entry->offset,
+				__entry->value,
+				__entry->bytes)
+);
+
+#define MAX_CMD_STR_LEN	200
+TRACE_EVENT(vgt_command,
+		TP_PROTO(u8 vm_id, u8 ring_id, u32 ip_gma, u32 *cmd_va, u32 cmd_len, bool ring_buffer_cmd),
+
+		TP_ARGS(vm_id, ring_id, ip_gma, cmd_va, cmd_len, ring_buffer_cmd),
+
+		TP_STRUCT__entry(
+			__field(u8, vm_id)
+			__field(u8, ring_id)
+			__field(int, i)
+			__array(char,tmp_buf, MAX_CMD_STR_LEN)
+			__array(char, cmd_str, MAX_CMD_STR_LEN)
+			),
+
+		TP_fast_assign(
+			__entry->vm_id = vm_id;
+			__entry->ring_id = ring_id;
+			__entry->cmd_str[0] = '\0';
+			snprintf(__entry->tmp_buf, MAX_CMD_STR_LEN, "VM(%d) Ring(%d): %s ip(%08x) ", vm_id, ring_id, ring_buffer_cmd ? "RB":"BB", ip_gma);
+			strcat(__entry->cmd_str, __entry->tmp_buf);
+			entry->i = 0;
+			while (cmd_len > 0) {
+				if (cmd_len >= 8) {
+					snprintf(__entry->tmp_buf, MAX_CMD_STR_LEN, "%08x %08x %08x %08x %08x %08x %08x %08x ",
+						cmd_va[__entry->i], cmd_va[__entry->i+1], cmd_va[__entry->i+2], cmd_va[__entry->i+3],
+						cmd_va[__entry->i+4],cmd_va[__entry->i+5],cmd_va[__entry->i+6],cmd_va[__entry->i+7]);
+					__entry->i += 8;
+					cmd_len -= 8;
+					strcat(__entry->cmd_str, __entry->tmp_buf);
+				} else if (cmd_len >= 4) {
+					snprintf(__entry->tmp_buf, MAX_CMD_STR_LEN, "%08x %08x %08x %08x ",
+						cmd_va[__entry->i], cmd_va[__entry->i+1], cmd_va[__entry->i+2], cmd_va[__entry->i+3]);
+					__entry->i += 4;
+					cmd_len -= 4;
+					strcat(__entry->cmd_str, __entry->tmp_buf);
+				} else if (cmd_len >= 2) {
+					snprintf(__entry->tmp_buf, MAX_CMD_STR_LEN, "%08x %08x ", cmd_va[__entry->i], cmd_va[__entry->i+1]);
+					__entry->i += 2;
+					cmd_len -= 2;
+					strcat(__entry->cmd_str, __entry->tmp_buf);
+				} else if (cmd_len == 1) {
+					snprintf(__entry->tmp_buf, MAX_CMD_STR_LEN, "%08x ", cmd_va[__entry->i]);
+					__entry->i += 1;
+					cmd_len -= 1;
+					strcat(__entry->cmd_str, __entry->tmp_buf);
+				}
+			}
+			strcat(__entry->cmd_str, "\n");
+		),
+
+		TP_printk("%s", __entry->cmd_str)
+);
+
+TRACE_EVENT(spt_alloc,
+		TP_PROTO(int vm_id, void *spt, int type, unsigned long mfn, unsigned long gpt_gfn),
+
+		TP_ARGS(vm_id, spt, type, mfn, gpt_gfn),
+
+		TP_STRUCT__entry(
+			__field(int, vm_id)
+			__field(void *, spt)
+			__field(int, type)
+			__field(unsigned long, mfn)
+			__field(unsigned long, gpt_gfn)
+			),
+
+		TP_fast_assign(
+			__entry->vm_id = vm_id;
+			__entry->spt = spt;
+			__entry->type = type;
+			__entry->mfn = mfn;
+			__entry->gpt_gfn = gpt_gfn;
+		),
+
+		TP_printk("VM%d [alloc] spt %p type %d mfn 0x%lx gpt_gfn 0x%lx\n",
+				__entry->vm_id,
+				__entry->spt,
+				__entry->type,
+				__entry->mfn,
+				__entry->gpt_gfn)
+);
+
+TRACE_EVENT(spt_free,
+		TP_PROTO(int vm_id, void *spt, int type),
+
+		TP_ARGS(vm_id, spt, type),
+
+		TP_STRUCT__entry(
+			__field(int, vm_id)
+			__field(void *, spt)
+			__field(int, type)
+			),
+
+		TP_fast_assign(
+			__entry->vm_id = vm_id;
+			__entry->spt = spt;
+			__entry->type = type;
+		),
+
+		TP_printk("VM%u [free] spt %p type %d\n",
+				__entry->vm_id,
+				__entry->spt,
+				__entry->type)
+);
+
+#define MAX_BUF_LEN 256
+
+TRACE_EVENT(gma_index,
+		TP_PROTO(const char *prefix, unsigned long gma, unsigned long index),
+
+		TP_ARGS(prefix, gma, index),
+
+		TP_STRUCT__entry(
+			__array(char, buf, MAX_BUF_LEN)
+		),
+
+		TP_fast_assign(
+			snprintf(__entry->buf, MAX_BUF_LEN, "%s gma 0x%lx index 0x%lx\n", prefix, gma, index);
+		),
+
+		TP_printk("%s", __entry->buf)
+);
+
+TRACE_EVENT(gma_translate,
+		TP_PROTO(int vm_id, char *type, int ring_id, int pt_level, unsigned long gma, unsigned long gpa),
+
+		TP_ARGS(vm_id, type, ring_id, pt_level, gma, gpa),
+
+		TP_STRUCT__entry(
+			__array(char, buf, MAX_BUF_LEN)
+		),
+
+		TP_fast_assign(
+			snprintf(__entry->buf, MAX_BUF_LEN, "VM%d %s ring %d pt_level %d gma 0x%lx -> gpa 0x%lx\n",
+					vm_id, type, ring_id, pt_level, gma, gpa);
+		),
+
+		TP_printk("%s", __entry->buf)
+);
+
+TRACE_EVENT(spt_refcount,
+		TP_PROTO(int vm_id, char *action, void *spt, int before, int after),
+
+		TP_ARGS(vm_id, action, spt, before, after),
+
+		TP_STRUCT__entry(
+			__array(char, buf, MAX_BUF_LEN)
+		),
+
+		TP_fast_assign(
+			snprintf(__entry->buf, MAX_BUF_LEN, "VM%d [%s] spt %p before %d -> after %d\n",
+					vm_id, action, spt, before, after);
+		),
+
+		TP_printk("%s", __entry->buf)
+);
+
+TRACE_EVENT(spt_change,
+		TP_PROTO(int vm_id, char *action, void *spt, unsigned long gfn, int type),
+
+		TP_ARGS(vm_id, action, spt, gfn, type),
+
+		TP_STRUCT__entry(
+			__array(char, buf, MAX_BUF_LEN)
+		),
+
+		TP_fast_assign(
+			snprintf(__entry->buf, MAX_BUF_LEN, "VM%d [%s] spt %p gfn 0x%lx type %d\n",
+					vm_id, action, spt, gfn, type);
+		),
+
+		TP_printk("%s", __entry->buf)
+);
+
+TRACE_EVENT(guest_pt_change,
+		TP_PROTO(int vm_id, const char *tag, void *spt, int type, u64 v, unsigned long index),
+
+		TP_ARGS(vm_id, tag, spt, type, v, index),
+
+		TP_STRUCT__entry(
+			__array(char, buf, MAX_BUF_LEN)
+		),
+
+		TP_fast_assign(
+			snprintf(__entry->buf, MAX_BUF_LEN, "VM%d [%s] spt %p type %d entry 0x%llx index 0x%lx\n",
+					vm_id, tag, spt, type, v, index);
+		),
+
+		TP_printk("%s", __entry->buf)
+);
+
+TRACE_EVENT(ctx_lifecycle,
+		TP_PROTO(int vm_id, int ring_id,
+				uint32_t guest_lrca, const char *action),
+
+		TP_ARGS(vm_id, ring_id, guest_lrca, action),
+
+		TP_STRUCT__entry(
+			__array(char, buf, MAX_BUF_LEN)
+		),
+
+		TP_fast_assign(
+			snprintf(__entry->buf, MAX_BUF_LEN,
+				"VM-%d <ring-%d>: EXECLIST Context guest lrca 0x%x - %s\n",
+				vm_id, ring_id, guest_lrca, action);
+		),
+
+		TP_printk("%s", __entry->buf)
+);
+
+TRACE_EVENT(ctx_protection,
+		TP_PROTO(int vm_id, int ring_id, uint32_t guest_lrca,
+			uint32_t page_idx, uint32_t gfn, const char *operation),
+
+		TP_ARGS(vm_id, ring_id, guest_lrca, page_idx, gfn, operation),
+
+		TP_STRUCT__entry(
+			__array(char, buf, MAX_BUF_LEN)
+		),
+
+		TP_fast_assign(
+			snprintf(__entry->buf, MAX_BUF_LEN,
+				"VM-%d <ring-%d>: EXECLIST Context guest lrca[0x%x] "
+				"page[%i] gfn[0x%x] - %s\n",
+				vm_id, ring_id, guest_lrca, page_idx, gfn, operation);
+		),
+
+		TP_printk("%s", __entry->buf)
+);
+
+TRACE_EVENT(ctx_write_trap,
+		TP_PROTO(uint64_t pa, int bytes),
+
+		TP_ARGS(pa, bytes),
+
+		TP_STRUCT__entry(
+			__field(u64, pa)
+			__field(int, bytes)
+			),
+
+		TP_fast_assign(
+			__entry->pa = pa;
+			__entry->bytes = bytes;
+		),
+
+		TP_printk("EXECLIST Context Write Protection addr: <0x%llx>, bytes %i\n",
+				__entry->pa, __entry->bytes)
+);
+
+#endif /* _VGT_TRACE_H_ */
+
+/* This part must be out of protection */
+#undef TRACE_INCLUDE_PATH
+#define TRACE_INCLUDE_PATH .
+#undef TRACE_INCLUDE_FILE
+#define TRACE_INCLUDE_FILE trace
+#include <trace/define_trace.h>
diff --git a/drivers/gpu/drm/i915/vgt/utility.c b/drivers/gpu/drm/i915/vgt/utility.c
new file mode 100644
index 0000000..7945f87
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/utility.c
@@ -0,0 +1,1309 @@
+/*
+ * Various utility helpers.
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/module.h>
+
+#include <linux/delay.h>
+
+#include "vgt.h"
+#include <drm/intel-gtt.h>
+#include <asm/cacheflush.h>
+
+bool inline is_execlist_mode(struct pgt_device *pdev, int ring_id)
+{
+	unsigned long ring_mode = RB_TAIL(pdev, ring_id) - 0x30 + 0x29c;
+
+	return VGT_MMIO_READ(pdev, ring_mode) & _REGBIT_EXECLIST_ENABLE;
+}
+
+void show_debug(struct pgt_device *pdev)
+{
+	int i, cpu;
+
+	printk("========vGT DEBUG INFO==========\n");
+	for_each_online_cpu(cpu)
+		printk("CPU[%d]: %s\n", cpu,
+			per_cpu(in_vgt, cpu) ? "in vgt" : "out of vgt");
+	printk("DE_RRMR: %x\n", VGT_MMIO_READ(pdev, _REG_DE_RRMR));
+
+	for (i = 0; i < pdev->max_engines; i++) {
+		printk("-----------ring-%d info-------------\n", i);
+		show_ring_debug(pdev, i);
+		show_ring_buffer(pdev, i, 16 * sizeof(vgt_reg_t));
+	}
+}
+
+/*
+ * Print debug registers for CP
+ *
+ * Hope to introduce a sysfs interface to dump this information on demand
+ * in the future
+ */
+void common_show_ring_debug(struct pgt_device *pdev, int ring_id)
+{
+	printk("debug registers,reg maked with <*>"
+		" may not apply to every ring):\n");
+	printk("....RING_EIR: %08x\n", VGT_MMIO_READ(pdev, RING_EIR(ring_id)));
+	printk("....RING_EMR: %08x\n", VGT_MMIO_READ(pdev, RING_EMR(ring_id)));
+	printk("....RING_ESR: %08x\n", VGT_MMIO_READ(pdev, RING_ESR(ring_id)));
+
+	if (ring_id)
+		printk("....%08x*: %08x\n", RING_REG_2064(ring_id),
+				VGT_MMIO_READ(pdev, RING_REG_2064(ring_id)));
+
+	printk("....%08x: %08x\n", RING_REG_2068(ring_id),
+		VGT_MMIO_READ(pdev, RING_REG_2068(ring_id)));
+	printk("....ACTHD(active header): %08x\n",
+			VGT_MMIO_READ(pdev, VGT_ACTHD(ring_id)));
+	printk("....UHPTR(pending header): %08x\n",
+			VGT_MMIO_READ(pdev, VGT_UHPTR(ring_id)));
+	printk("....%08x: %08x\n", RING_REG_2078(ring_id),
+		VGT_MMIO_READ(pdev, RING_REG_2078(ring_id)));
+
+	if (!ring_id) {
+		printk("....INSTPS* (parser state): %08x :\n",
+				VGT_MMIO_READ(pdev, 0x2070));
+		printk("....CSCMDOP* (instruction DWORD): %08x\n",
+				VGT_MMIO_READ(pdev, 0x220C));
+		printk("....CSCMDVLD* (command buffer valid): %08x\n",
+				VGT_MMIO_READ(pdev, 0x2210));
+	}
+
+	printk("(informative)\n");
+	printk("....INSTDONE_1(FYI): %08x\n",
+			VGT_MMIO_READ(pdev, RING_REG_206C(ring_id)));
+	if (!ring_id)
+		printk("....INSTDONE_2*: %08x\n",
+				VGT_MMIO_READ(pdev, 0x207C));
+}
+
+void legacy_show_ring_debug(struct pgt_device *pdev, int ring_id)
+{
+	int i;
+
+	for (i = 0; i < VGT_MAX_VMS; i++) {
+		struct vgt_device *vgt;
+		if (pdev->device[i]) {
+			vgt = pdev->device[i];
+			if (vgt == current_render_owner(pdev))
+				printk("VM%d(*):", vgt->vm_id);
+			else
+				printk("VM%d   :", vgt->vm_id);
+
+			printk("head(%x), tail(%x), start(%x), ctl(%x), uhptr(%x)\n",
+				vgt->rb[ring_id].sring.head,
+				vgt->rb[ring_id].sring.tail,
+				vgt->rb[ring_id].sring.start,
+				vgt->rb[ring_id].sring.ctl,
+				__vreg(vgt, VGT_UHPTR(ring_id)));
+		}
+	}
+
+	common_show_ring_debug(pdev, ring_id);
+}
+
+void execlist_show_ring_debug(struct pgt_device *pdev, int ring_id)
+{
+	int i;
+
+	for (i = 0; i < VGT_MAX_VMS; i++) {
+		struct vgt_device *vgt;
+
+		if (!pdev->device[i])
+			continue;
+
+		vgt = pdev->device[i];
+
+		if (vgt == current_render_owner(pdev))
+			printk("VM%d(*):", vgt->vm_id);
+		else
+			printk("VM%d   :", vgt->vm_id);
+	}
+
+	common_show_ring_debug(pdev, ring_id);
+}
+
+void show_ring_debug(struct pgt_device *pdev, int ring_id)
+{
+	is_execlist_mode(pdev, ring_id) ?
+		execlist_show_ring_debug(pdev, ring_id) :
+		legacy_show_ring_debug(pdev, ring_id);
+}
+
+/*
+ * Show some global register settings, if we care about bits
+ * in those registers.
+ *
+ * normally invoked from initialization phase, and mmio emulation
+ * logic
+ */
+void show_mode_settings(struct pgt_device *pdev)
+{
+	vgt_reg_t val;
+	struct vgt_device *vgt1 = default_device.device[1];
+
+	if (current_render_owner(pdev))
+		printk("Current render owner: %d\n", current_render_owner(pdev)->vgt_id);
+
+#define SHOW_MODE(reg)		\
+	do{				\
+		val = VGT_MMIO_READ(pdev, reg);	\
+		printk("vGT: "#reg"(%x): p(%x), 0(%x), 1(%x)\n",	\
+			reg, val, __sreg(vgt_dom0, reg), vgt1 ? __sreg(vgt1, reg) : 0);	\
+	} while (0);
+	SHOW_MODE(_REG_RCS_MI_MODE);
+	SHOW_MODE(_REG_VCS_MI_MODE);
+	SHOW_MODE(_REG_BCS_MI_MODE);
+
+	if (IS_IVB(pdev) || IS_HSW(pdev)) {
+		SHOW_MODE(_REG_RCS_GFX_MODE_IVB);
+		SHOW_MODE(_REG_BCS_BLT_MODE_IVB);
+		SHOW_MODE(_REG_VCS_MFX_MODE_IVB);
+		SHOW_MODE(_REG_CACHE_MODE_0_IVB);
+		SHOW_MODE(_REG_CACHE_MODE_1_IVB);
+		SHOW_MODE(_REG_GT_MODE_IVB);
+	} else if (IS_BDWGT3(pdev)) {
+		SHOW_MODE(_REG_VCS2_MI_MODE);
+		SHOW_MODE(_REG_VCS2_MFX_MODE_BDW);
+		SHOW_MODE(_REG_VCS2_INSTPM);
+	} else if (IS_SNB(pdev)) {
+		SHOW_MODE(_REG_GFX_MODE);
+		SHOW_MODE(_REG_ARB_MODE);
+		SHOW_MODE(_REG_GT_MODE);
+		SHOW_MODE(_REG_CACHE_MODE_0);
+		SHOW_MODE(_REG_CACHE_MODE_1);
+	}
+
+	SHOW_MODE(_REG_RCS_INSTPM);
+	SHOW_MODE(_REG_VCS_INSTPM);
+	SHOW_MODE(_REG_BCS_INSTPM);
+
+	SHOW_MODE(_REG_TILECTL);
+}
+
+static void show_batchbuffer(struct pgt_device *pdev, int ring_id, u64 addr,
+	int bytes, int ppgtt)
+{
+	struct vgt_device_info *info = &pdev->device_info;
+	int i;
+	char *ip_va;
+	u64 start;
+	struct vgt_device *vgt = current_render_owner(pdev);
+	uint32_t val;
+
+	if (!vgt) {
+		vgt_err("no render owner at hanging point\n");
+		return;
+	}
+
+	if (addr < bytes) {
+		bytes *= 2;
+		start = 0;
+	} else if (!ppgtt && (addr + bytes) >= info->max_gtt_gm_sz) {
+		bytes *= 2;
+		start = info->max_gtt_gm_sz - bytes;
+	} else {
+		start = addr - bytes;
+		bytes *= 2;
+	}
+
+	printk("Batch buffer contents: \n");
+	for (i = 0; i < bytes; i += 4) {
+		struct vgt_mm *mm = ppgtt ? vgt->rb[ring_id].active_ppgtt_mm :
+			vgt->gtt.ggtt_mm;
+
+		ip_va = vgt_gma_to_va(mm, start + i);
+
+		if (!(i % 32))
+			printk("\n[%08llx]:", start + i);
+
+		if (ip_va == NULL)
+			printk(" %8s", "N/A");
+		else {
+			hypervisor_read_va(vgt, ip_va, &val, sizeof(val), 0);
+			printk(" %08x", val);
+		}
+		if (start + i == addr)
+			printk("(*)");
+	}
+	printk("\n");
+}
+
+/*
+ * Given a ring buffer, print out the current data [-bytes, bytes]
+ */
+void common_show_ring_buffer(struct pgt_device *pdev, int ring_id, int bytes,
+	vgt_reg_t p_tail, vgt_reg_t p_head, vgt_reg_t p_start, vgt_reg_t p_ctl,
+	unsigned long batch_head)
+{
+	char *p_contents;
+	int i;
+	struct vgt_device *vgt = current_render_owner(pdev);
+	u32 *cur;
+	u64 ring_len, off;
+
+	printk("ring buffer(%d): head (0x%x) tail(0x%x), start(0x%x), "
+			"ctl(0x%x)\n", ring_id, p_head, p_tail, p_start, p_ctl);
+	printk("ring xxx:(%d), mi_mode idle:(%d)\n",
+		VGT_MMIO_READ(pdev, pdev->ring_xxx[ring_id]) & (1 << pdev->ring_xxx_bit[ring_id]),
+		VGT_MMIO_READ(pdev, pdev->ring_mi_mode[ring_id]) & _REGBIT_MI_RINGS_IDLE);
+
+	if (!(p_ctl & _RING_CTL_ENABLE)) {
+		printk("<NO CONTENT>\n");
+		return;
+	}
+
+	p_head &= RB_HEAD_OFF_MASK;
+	ring_len = _RING_CTL_BUF_SIZE(p_ctl);
+	p_contents = vgt_gma_to_va(vgt->gtt.ggtt_mm, p_start);
+
+#define WRAP_OFF(off, size)			\
+	({					\
+		u64 val = off;			\
+		if ((int64_t)val < 0)		\
+			val += size;	\
+		if (val >= size)		\
+			val -= size;	\
+		(val);				\
+	})
+	printk("p_contents(%lx)\n", (unsigned long)p_contents);
+	/* length should be 4 bytes aligned */
+	bytes &= ~0x3;
+	for (i = -bytes; i < bytes; i += 4) {
+		off = (p_head + i) % ring_len;
+		off = WRAP_OFF(off, ring_len);
+		/* print offset within the ring every 8 Dword */
+		if (!((i + bytes) % 32))
+			printk("\n[%08llx]:", off);
+		printk(" %08x", *((u32*)(p_contents + off)));
+		if (!i)
+			printk("(*)");
+	}
+	printk("\n");
+
+	if (IS_PREBDW(pdev))
+		off = WRAP_OFF(((int32_t)p_head) - 8, ring_len);
+	else
+		off = WRAP_OFF(((int32_t)p_head) - 12, ring_len);
+
+	cur = (u32*)(p_contents + off);
+	if ((*cur & 0xfff00000) == 0x18800000 && vgt) {
+		int ppgtt = (*cur & _CMDBIT_BB_START_IN_PPGTT);
+
+		if (ppgtt && !test_bit(ring_id, &vgt->gtt.active_ppgtt_mm_bitmap)) {
+			printk("Batch buffer in PPGTT with PPGTT disabled?\n");
+			return;
+		}
+
+		printk("Hang in (%s) batch buffer (%x)\n",
+			ppgtt ? "PPGTT" : "GTT",
+			*(cur + 1));
+
+		show_batchbuffer(pdev, ring_id,
+			batch_head,
+			bytes,
+			ppgtt);
+	}
+}
+
+void legacy_show_ring_buffer(struct pgt_device *pdev, int ring_id, int bytes)
+{
+	vgt_reg_t p_tail, p_head, p_start, p_ctl;
+
+	p_tail = VGT_MMIO_READ(pdev, RB_TAIL(pdev, ring_id));
+	p_head = VGT_MMIO_READ(pdev, RB_HEAD(pdev, ring_id));
+	p_start = VGT_MMIO_READ(pdev, RB_START(pdev, ring_id));
+	p_ctl = VGT_MMIO_READ(pdev, RB_CTL(pdev, ring_id));
+
+	common_show_ring_buffer(pdev, ring_id, bytes,
+			p_tail, p_head, p_start, p_ctl,
+			VGT_MMIO_READ(pdev, VGT_ACTHD(ring_id)));
+}
+
+unsigned long ring_id_2_current_desc_reg [] = {
+	[RING_BUFFER_RCS] = 0x4400,
+	[RING_BUFFER_VCS] = 0x4440,
+	[RING_BUFFER_VCS2] = 0x4480,
+	[RING_BUFFER_VECS] = 0x44c0,
+	[RING_BUFFER_BCS] = 0x4500,
+};
+
+void execlist_show_ring_buffer(struct pgt_device *pdev, int ring_id, int bytes)
+{
+	struct vgt_device *vgt = current_render_owner(pdev);
+	vgt_reg_t p_tail, p_head, p_start, p_ctl;
+	unsigned long reg, val;
+	u64 bb_head;
+	u32 *p;
+
+	printk("Execlist:\n");
+
+	reg = RB_TAIL(pdev, ring_id) - 0x30 + _EL_OFFSET_STATUS;
+	val = VGT_MMIO_READ(pdev, reg);
+
+	printk("....Current execlist status: %lx.\n", val);
+
+	val = VGT_MMIO_READ(pdev, ring_id_2_current_desc_reg[ring_id]);
+
+	printk("....Current element descriptor(low): %lx.\n", val);
+
+	val &= ~0xfff;
+
+	printk("....LRCA: %lx.\n", val);
+
+	if (!val)
+		return;
+
+	p = vgt_gma_to_va(vgt->gtt.ggtt_mm, val + 4096);
+	if (!p)
+		return;
+
+	if ((ring_id == RING_BUFFER_RCS && p[1] != 0x1100101b)
+		|| (ring_id != RING_BUFFER_RCS && p[1] != 0x11000015)) {
+		printk("Invalid signature: %x.\n", p[1]);
+		return;
+	}
+
+	p_head = *(p + 0x4 + 1);
+	p_tail = *(p + 0x6 + 1);
+	p_start = *(p + 0x8 + 1);
+	p_ctl = *(p + 0xa + 1);
+
+	bb_head = *(p + 0xc + 1) & 0xFFFF;
+	bb_head <<= 32;
+	bb_head |= *(p + 0xe + 1);
+	reg = RB_TAIL(pdev, ring_id) - 0x30 + 0x140;
+
+	common_show_ring_buffer(pdev, ring_id, bytes,
+			p_tail, p_head, p_start, p_ctl,
+			bb_head);
+}
+
+void show_ring_buffer(struct pgt_device *pdev, int ring_id, int bytes)
+{
+	is_execlist_mode(pdev, ring_id) ?
+		execlist_show_ring_buffer(pdev, ring_id, bytes) :
+		legacy_show_ring_buffer(pdev, ring_id, bytes);
+}
+
+void show_interrupt_regs(struct pgt_device *pdev,
+		struct seq_file *seq)
+{
+#define P(fmt, args...) \
+	do { \
+		if (!seq) \
+			vgt_info(fmt, ##args); \
+		else \
+			seq_printf(seq, fmt, ##args); \
+	}while(0)
+
+	if (IS_PREBDW(pdev)) {
+		P("vGT: DEISR is %x, DEIIR is %x, DEIMR is %x, DEIER is %x\n",
+				VGT_MMIO_READ(pdev, _REG_DEISR),
+				VGT_MMIO_READ(pdev, _REG_DEIIR),
+				VGT_MMIO_READ(pdev, _REG_DEIMR),
+				VGT_MMIO_READ(pdev, _REG_DEIER));
+		P("vGT: GTISR is %x, GTIIR is %x, GTIMR is %x, GTIER is %x\n",
+				VGT_MMIO_READ(pdev, _REG_GTISR),
+				VGT_MMIO_READ(pdev, _REG_GTIIR),
+				VGT_MMIO_READ(pdev, _REG_GTIMR),
+				VGT_MMIO_READ(pdev, _REG_GTIER));
+		P("vGT: PMISR is %x, PMIIR is %x, PMIMR is %x, PMIER is %x\n",
+				VGT_MMIO_READ(pdev, _REG_PMISR),
+				VGT_MMIO_READ(pdev, _REG_PMIIR),
+				VGT_MMIO_READ(pdev, _REG_PMIMR),
+				VGT_MMIO_READ(pdev, _REG_PMIER));
+	} else {
+		P("vGT: MASTER_IRQ: %x\n",
+			VGT_MMIO_READ(pdev, _REG_MASTER_IRQ));
+
+#define P_GROUP_WHICH(group, w) do {\
+		P("vGT: "#group"|"#w" ISR: %x IIR: %x IMR: %x IER: %x\n", \
+			VGT_MMIO_READ(pdev, _REG_##group##_ISR(w)), \
+			VGT_MMIO_READ(pdev, _REG_##group##_IIR(w)), \
+			VGT_MMIO_READ(pdev, _REG_##group##_IMR(w)), \
+			VGT_MMIO_READ(pdev, _REG_##group##_IER(w))); \
+	}while(0)
+
+#define P_GROUP(group) do {\
+		P("vGT: "#group" ISR: %x IIR: %x IMR: %x IER: %x\n", \
+			VGT_MMIO_READ(pdev, _REG_##group##_ISR), \
+			VGT_MMIO_READ(pdev, _REG_##group##_IIR), \
+			VGT_MMIO_READ(pdev, _REG_##group##_IMR), \
+			VGT_MMIO_READ(pdev, _REG_##group##_IER)); \
+	}while(0)
+
+		P_GROUP_WHICH(DE_PIPE, PIPE_A);
+		P_GROUP_WHICH(DE_PIPE, PIPE_B);
+		P_GROUP_WHICH(DE_PIPE, PIPE_C);
+
+		P_GROUP_WHICH(GT, 0);
+		P_GROUP_WHICH(GT, 1);
+		P_GROUP_WHICH(GT, 2);
+		P_GROUP_WHICH(GT, 3);
+
+		P_GROUP(DE_PORT);
+		P_GROUP(DE_MISC);
+		P_GROUP(PCU);
+	}
+
+	P("vGT: SDEISR is %x, SDEIIR is %x, SDEIMR is %x, SDEIER is %x\n",
+			VGT_MMIO_READ(pdev, _REG_SDEISR),
+			VGT_MMIO_READ(pdev, _REG_SDEIIR),
+			VGT_MMIO_READ(pdev, _REG_SDEIMR),
+			VGT_MMIO_READ(pdev, _REG_SDEIER));
+
+	P("vGT: RCS_IMR is %x, VCS_IMR is %x, BCS_IMR is %x\n",
+			VGT_MMIO_READ(pdev, _REG_RCS_IMR),
+			VGT_MMIO_READ(pdev, _REG_VCS_IMR),
+			VGT_MMIO_READ(pdev, _REG_BCS_IMR));
+	return;
+#undef P
+#undef P_GROUP
+#undef P_GROUP_WHICH
+}
+
+void show_virtual_interrupt_regs(struct vgt_device *vgt,
+		struct seq_file *seq)
+{
+#define P(fmt, args...) \
+	do { \
+		if (!seq) \
+			vgt_info(fmt, ##args); \
+		else \
+			seq_printf(seq, fmt, ##args); \
+	}while(0)
+
+	if (IS_PREBDW(vgt->pdev)) {
+		P("....vreg (deier: %x, deiir: %x, deimr: %x, deisr: %x)\n",
+				__vreg(vgt, _REG_DEIER),
+				__vreg(vgt, _REG_DEIIR),
+				__vreg(vgt, _REG_DEIMR),
+				__vreg(vgt, _REG_DEISR));
+		P("....vreg (gtier: %x, gtiir: %x, gtimr: %x, gtisr: %x)\n",
+				__vreg(vgt, _REG_GTIER),
+				__vreg(vgt, _REG_GTIIR),
+				__vreg(vgt, _REG_GTIMR),
+				__vreg(vgt, _REG_GTISR));
+		P("....vreg (pmier: %x, pmiir: %x, pmimr: %x, pmisr: %x)\n",
+				__vreg(vgt, _REG_PMIER),
+				__vreg(vgt, _REG_PMIIR),
+				__vreg(vgt, _REG_PMIMR),
+				__vreg(vgt, _REG_PMISR));
+	} else {
+		P("....vreg: MASTER_IRQ: %x\n",
+				__vreg(vgt, _REG_MASTER_IRQ));
+
+#define P_GROUP_WHICH(group, w) do {\
+		P("....vreg "#group"|"#w" ISR: %x IIR: %x IMR: %x IER: %x\n", \
+			__vreg(vgt, _REG_##group##_ISR(w)), \
+			__vreg(vgt, _REG_##group##_IIR(w)), \
+			__vreg(vgt, _REG_##group##_IMR(w)), \
+			__vreg(vgt, _REG_##group##_IER(w))); \
+	}while(0)
+
+#define P_GROUP(group) do {\
+		P("....vreg "#group" ISR: %x IIR: %x IMR: %x IER: %x\n", \
+			__vreg(vgt, _REG_##group##_ISR), \
+			__vreg(vgt, _REG_##group##_IIR), \
+			__vreg(vgt, _REG_##group##_IMR), \
+			__vreg(vgt, _REG_##group##_IER)); \
+	}while(0)
+
+		P_GROUP_WHICH(DE_PIPE, PIPE_A);
+		P_GROUP_WHICH(DE_PIPE, PIPE_B);
+		P_GROUP_WHICH(DE_PIPE, PIPE_C);
+
+		P_GROUP_WHICH(GT, 0);
+		P_GROUP_WHICH(GT, 1);
+		P_GROUP_WHICH(GT, 2);
+		P_GROUP_WHICH(GT, 3);
+
+		P_GROUP(DE_PORT);
+		P_GROUP(DE_MISC);
+		P_GROUP(PCU);
+	}
+
+	P("....vreg (sdeier: %x, sdeiir: %x, sdeimr: %x, sdeisr: %x)\n",
+			__vreg(vgt, _REG_SDEIER),
+			__vreg(vgt, _REG_SDEIIR),
+			__vreg(vgt, _REG_SDEIMR),
+			__vreg(vgt, _REG_SDEISR));
+
+	P("....vreg (rcs_imr: %x, vcs_imr: %x, bcs_imr: %x\n",
+			__vreg(vgt, _REG_RCS_IMR),
+			__vreg(vgt, _REG_VCS_IMR),
+			__vreg(vgt, _REG_BCS_IMR));
+
+	return;
+#undef P
+#undef P_GROUP
+#undef P_GROUP_WHICH
+}
+
+uint32_t pci_bar_size(struct pgt_device *pdev, unsigned int bar_off)
+{
+	unsigned long bar_s, bar_size=0;
+	struct pci_dev *dev = pdev->pdev;
+
+	pci_read_config_dword(dev, bar_off, (uint32_t *)&bar_s);
+	pci_write_config_dword(dev, bar_off, 0xFFFFFFFF);
+
+	pci_read_config_dword(dev, bar_off, (uint32_t *)&bar_size);
+	vgt_dbg(VGT_DBG_GENERIC, "read back bar_size %lx\n", bar_size);
+	bar_size &= ~0xf; /* bit 4-31 */
+	vgt_dbg(VGT_DBG_GENERIC, "read back bar_size1 %lx\n", bar_size);
+	bar_size = 1 << find_first_bit(&bar_size, BITS_PER_LONG);
+	vgt_dbg(VGT_DBG_GENERIC, "read back bar_size2 %lx\n", bar_size);
+
+	pci_write_config_dword(dev, bar_off, bar_s);
+
+#if 0
+	bar_s = pci_conf_read32( 0, vgt_bus, vgt_dev, vgt_fun, bar_off);
+	pci_conf_write32(0, vgt_bus, vgt_dev, vgt_fun, bar_off, 0xFFFFFFFF);
+
+	bar_size = pci_conf_read32(0, vgt_bus, vgt_dev, vgt_fun, bar_off);
+	bar_size &= ~0xf; /* bit 4-31 */
+	bar_size = 1 << find_first_bit(&bar_size, sizeof(bar_size));
+
+	pci_conf_write32(0, vgt_bus, vgt_dev, vgt_fun, bar_offset, bar_s);
+#endif
+	return bar_size;
+}
+
+uint64_t vgt_get_gtt_size(struct pgt_device *pdev)
+{
+	struct pci_bus *bus = pdev->pbus;
+	uint16_t gmch_ctrl;
+
+	ASSERT(!bus->number);
+
+	/* GTT size is within GMCH. */
+	pci_bus_read_config_word(bus, 0, _REG_GMCH_CONTRL, &gmch_ctrl);
+
+	if (IS_PREBDW(pdev)) {
+		gmch_ctrl = (gmch_ctrl >> 8) & 3;
+		switch (gmch_ctrl) {
+			case 1:
+			case 2:
+				return gmch_ctrl << 20;
+			default:
+				vgt_err("Invalid GTT memory size: %d\n", gmch_ctrl);
+				break;
+		}
+	} else {
+		gmch_ctrl = (gmch_ctrl >> 6) & 3;
+		if (gmch_ctrl)
+			gmch_ctrl = 1 << gmch_ctrl;
+		switch (gmch_ctrl) {
+			case 2:
+			case 4:
+			case 8:
+				return gmch_ctrl << 20;
+			default:
+				vgt_err("Invalid GTT memory size: %d\n", gmch_ctrl);
+				break;
+		}
+	}
+
+	return 0;
+}
+
+/*
+ * random GTT entry check
+ */
+void check_gtt(struct pgt_device *pdev)
+{
+	static unsigned int addr[] = {
+	0x00000000, 0x02000000, 0x04000000, 0x08000000,
+	0x0C000000, 0x0FFFF000, 0x10000000, 0x20000000,
+	0x40000000, 0x60000000, 0x7FFFF000 };
+
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(addr); i++)
+		vgt_dbg(VGT_DBG_MEM, "GMADR: 0x08%x, GTT INDEX: %x, GTT VALUE: %x\n",
+			addr[i], GTT_INDEX(pdev, addr[i]),
+			vgt_read_gtt(pdev, GTT_INDEX(pdev, addr[i])));
+}
+
+static inline u64 dma_addr_to_pte_uc(struct pgt_device *pdev, dma_addr_t addr)
+{
+	u64 v;
+
+	if (IS_BDWPLUS(pdev)) {
+		v = addr & (0x7ffffff << 12);
+	} else {
+		if (IS_HSW(pdev)) {
+			/* Haswell has new cache control bits */
+			v = addr & ~0xfff;
+			v |= (addr >> 28) & 0x7f0;
+		} else {
+			v = addr & ~0xfff;
+			v |= (addr >> 28) & 0xff0;
+			v |= (1 << 1); /* UC */
+		}
+	}
+	v |= 1;
+	return v;
+}
+
+void init_gm_space(struct pgt_device *pdev)
+{
+	struct vgt_gtt_pte_ops *ops = pdev->gtt.pte_ops;
+	unsigned long i;
+
+	/* clear all GM space, instead of only aperture */
+	for (i = 0; i < gm_pages(pdev); i++)
+		ops->set_entry(NULL, &pdev->dummy_gtt_entry, i, false, NULL);
+
+	vgt_dbg(VGT_DBG_MEM, "content at 0x0: %lx\n",
+			*(unsigned long *)((char *)phys_aperture_vbase(pdev) + 0x0));
+	vgt_dbg(VGT_DBG_MEM, "content at 0x64000: %lx\n",
+			*(unsigned long *)((char *)phys_aperture_vbase(pdev) + 0x64000));
+	vgt_dbg(VGT_DBG_MEM, "content at 0x8064000: %lx\n",
+			*(unsigned long *)((char *)phys_aperture_vbase(pdev) + 0x8064000));
+}
+
+static void vgt_free_gtt_pages(struct pgt_device *pdev)
+{
+	int i;
+	struct page *dummy_page = pdev->dummy_page;
+	struct page *(*pages)[VGT_APERTURE_PAGES] =
+		pdev->rsvd_aperture_pages;
+
+	if (pages != NULL) {
+		for (i = 0; i < VGT_APERTURE_PAGES; i++) {
+			if ((*pages)[i] == NULL)
+				continue;
+			put_page((*pages)[i]);
+			__free_page((*pages)[i]);
+		}
+		kfree(pages);
+	}
+
+	if (dummy_page != NULL) {
+		put_page(dummy_page);
+		__free_page(dummy_page);
+	}
+}
+
+void vgt_clear_gtt(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	struct vgt_gtt_pte_ops *ops = pdev->gtt.pte_ops;
+	uint32_t index;
+	uint32_t offset;
+	uint32_t num_entries;
+
+	index = vgt_visible_gm_base(vgt) >> PAGE_SHIFT;
+	num_entries = vgt_aperture_sz(vgt) >> PAGE_SHIFT;
+	for (offset = 0; offset < num_entries; offset++){
+		ops->set_entry(NULL, &pdev->dummy_gtt_entry, index+offset, false, NULL);
+	}
+
+	index = vgt_hidden_gm_base(vgt) >> PAGE_SHIFT;
+	num_entries = vgt_hidden_gm_sz(vgt) >> PAGE_SHIFT;
+	for (offset = 0; offset < num_entries; offset++){
+		ops->set_entry(NULL, &pdev->dummy_gtt_entry, index+offset, false, NULL);
+	}
+}
+
+int setup_gtt(struct pgt_device *pdev)
+{
+	struct vgt_gtt_pte_ops *ops = pdev->gtt.pte_ops;
+	struct page *dummy_page;
+	struct page *(*pages)[VGT_APERTURE_PAGES];
+	struct page *page;
+
+	int i, ret, index;
+	dma_addr_t dma_addr;
+	gtt_entry_t e;
+	u64 v;
+
+	check_gtt(pdev);
+
+	printk("vGT: clear all GTT entries.\n");
+
+	dummy_page = alloc_page(GFP_KERNEL | __GFP_ZERO | GFP_DMA32);
+	if (!dummy_page)
+		return -ENOMEM;
+	pdev->dummy_page = dummy_page;
+
+	get_page(dummy_page);
+	set_pages_uc(dummy_page, 1);
+	dma_addr = pci_map_page(pdev->pdev, dummy_page, 0, PAGE_SIZE, PCI_DMA_BIDIRECTIONAL);
+	if (pci_dma_mapping_error(pdev->pdev, dma_addr)) {
+		ret = -EINVAL;
+		goto err_out;
+	}
+
+	printk("....dummy page (0x%llx, 0x%llx)\n", page_to_phys(dummy_page), dma_addr);
+
+	/* for debug purpose */
+	memset(pfn_to_kaddr(page_to_pfn(dummy_page)), 0x77, PAGE_SIZE);
+
+	v = dma_addr_to_pte_uc(pdev, dma_addr);
+	gtt_init_entry(&e, GTT_TYPE_GGTT_PTE, pdev, v);
+	pdev->dummy_gtt_entry = e;
+
+	init_gm_space(pdev);
+
+	check_gtt(pdev);
+
+	printk("vGT: allocate vGT aperture\n");
+	/* Fill GTT range owned by vGT driver */
+
+	ASSERT(sizeof(*pages) == VGT_APERTURE_PAGES * sizeof(struct page*));
+	if ((pages = kzalloc(sizeof(*pages), GFP_KERNEL)) == NULL) {
+		ret = -ENOMEM;
+		goto err_out;
+	}
+	pdev->rsvd_aperture_pages = pages;
+
+
+	index = GTT_INDEX(pdev, aperture_2_gm(pdev, pdev->rsvd_aperture_base));
+	for (i = 0; i < VGT_APERTURE_PAGES; i++) {
+		/* need a DMA flag? */
+		page = alloc_page(GFP_KERNEL | __GFP_ZERO);
+		if (!page) {
+			vgt_dbg(VGT_DBG_MEM, "vGT: Failed to create page for setup_gtt!\n");
+			ret = -ENOMEM;
+			goto err_out;
+		}
+
+		get_page(page);
+		/* use wc instead! */
+		set_pages_uc(page, 1);
+
+		(*pages)[i] = page;
+
+		/* dom0 needs DMAR anyway */
+		dma_addr = pci_map_page(pdev->pdev, page, 0, PAGE_SIZE, PCI_DMA_BIDIRECTIONAL);
+		if (pci_dma_mapping_error(pdev->pdev, dma_addr)) {
+			printk(KERN_ERR "vGT: Failed to do pci_dma_mapping while handling %d 0x%llx\n", i, dma_addr);
+			ret = -EINVAL;
+			goto err_out;
+		}
+
+		ops->set_pfn(&e, dma_addr >> GTT_PAGE_SHIFT);
+		ops->set_entry(NULL, &e, index + i, false, NULL);
+
+		if (!(i % 1024))
+			vgt_dbg(VGT_DBG_MEM, "vGT: write GTT-%x phys: %llx, dma: %llx\n",
+				index + i, page_to_phys(page), dma_addr);
+	}
+
+	check_gtt(pdev);
+	/* any cache flush required here? */
+	return 0;
+err_out:
+	printk("vGT: error in GTT initialization\n");
+	vgt_free_gtt_pages(pdev);
+
+	return ret;
+}
+
+void free_gtt(struct pgt_device *pdev)
+{
+	intel_gtt_clear_range(0,
+		(phys_aperture_sz(pdev) - GTT_PAGE_SIZE)/PAGE_SIZE);
+
+	vgt_free_gtt_pages(pdev);
+}
+
+void vgt_save_gtt_and_fence(struct pgt_device *pdev)
+{
+	int i;
+	uint32_t *entry = pdev->saved_gtt;
+
+	ASSERT(pdev->saved_gtt);
+	vgt_info("Save GTT table...\n");
+	for (i = 0; i < gm_pages(pdev); i++)
+		*(entry + i) = vgt_read_gtt(pdev, i);
+
+	for (i = 0; i < VGT_MAX_NUM_FENCES; i++)
+		pdev->saved_fences[i] =
+			VGT_MMIO_READ64(pdev, _REG_FENCE_0_LOW + 8 * i);
+}
+
+void vgt_restore_gtt_and_fence(struct pgt_device *pdev)
+{
+	int i;
+	uint32_t *entry = pdev->saved_gtt;
+
+	ASSERT(pdev->saved_gtt);
+	vgt_info("Restore GTT table...\n");
+	for (i = 0; i < gm_pages(pdev); i++)
+		vgt_write_gtt(pdev, i, *(entry + i));
+
+	for (i = 0; i < VGT_MAX_NUM_FENCES; i++)
+		VGT_MMIO_WRITE_BYTES(pdev,
+			_REG_FENCE_0_LOW + 8 * i,
+			pdev->saved_fences[i], 8);
+}
+
+static void _hex_dump(const char *data, size_t size)
+{
+	char buf[74];
+	size_t offset;
+	int line;
+
+	for (line = 0; line < ((size + 0xF) / 0x10); line++) {
+		int byte;
+
+		memset(buf, ' ', sizeof(buf));
+		buf[73] = '\0';
+		offset = 0;
+
+		offset += snprintf(buf + offset, 74 - offset, "%07x: ", line * 0x10);
+
+		for (byte = 0; byte < 0x10; byte++) {
+			if (!(byte & 0x1)) {
+				offset += snprintf(buf + offset, 74 - offset, " ");
+			}
+
+			if (((line * 0x10) + byte) >= size) {
+				offset += snprintf(buf + offset, 74 - offset, "  ");
+			} else {
+				offset += snprintf(buf + offset, 74 - offset, "%02x",
+					       data[byte + (line * 0x10)] & 0xFF);
+			}
+		}
+		
+		offset += snprintf(buf + offset, 74 - offset, "  ");
+
+		for (byte = 0; byte < 0x10; byte++) {
+			if (data[byte + (line * 0x10)] >= 0x20 &&
+			    data[byte + (line * 0x10)] <= 0x7E) {
+				offset += snprintf(buf + offset, 74 - offset, "%c",
+				    data[byte + (line * 0x10)] & 0xFF);
+			} else {
+				offset += snprintf(buf + offset, 74 - offset, ".");
+			}
+		}
+
+		offset += snprintf(buf + offset, 74 - offset, "\n");
+		printk(buf);
+	}
+}
+
+void vgt_print_edid(struct vgt_edid_data_t *edid)
+{
+	if (edid && edid->data_valid) {
+		_hex_dump(edid->edid_block, EDID_SIZE);
+	} else {
+		printk("EDID is not available!\n");
+	}
+
+	return;
+}
+
+void vgt_print_dpcd(struct vgt_dpcd_data *dpcd)
+{
+	if (dpcd && dpcd->data_valid) {
+		_hex_dump(dpcd->data, DPCD_SIZE);
+	} else {
+		printk("DPCD is not available!\n");
+	}
+}
+
+int vgt_hvm_map_aperture (struct vgt_device *vgt, int map)
+{
+	char *cfg_space = &vgt->state.cfg_space[0];
+	uint64_t bar_s;
+	int r, nr_mfns;
+	unsigned long first_gfn, first_mfn;
+
+	if (!vgt_pci_mmio_is_enabled(vgt))
+		return 0;
+
+	/* guarantee the sequence of map -> unmap -> map -> unmap */
+	if (map == vgt->state.bar_mapped[1])
+		return 0;
+
+	cfg_space += VGT_REG_CFG_SPACE_BAR1;	/* APERTUR */
+	if (VGT_GET_BITS(*cfg_space, 2, 1) == 2){
+		/* 64 bits MMIO bar */
+		bar_s = * (uint64_t *) cfg_space;
+	} else {
+		/* 32 bits MMIO bar */
+		bar_s = * (uint32_t*) cfg_space;
+	}
+
+	first_gfn = (bar_s + vgt_aperture_offset(vgt)) >> PAGE_SHIFT;
+	first_mfn = vgt_aperture_base(vgt) >> PAGE_SHIFT;
+	if (!vgt->ballooning)
+		nr_mfns = vgt->state.bar_size[1] >> PAGE_SHIFT;
+	else
+		nr_mfns = vgt_aperture_sz(vgt) >> PAGE_SHIFT;
+
+	printk("%s: domid=%d gfn_s=0x%lx mfn_s=0x%lx nr_mfns=0x%x\n", map==0? "remove_map":"add_map",
+		vgt->vm_id, first_gfn, first_mfn, nr_mfns);
+
+	r = hypervisor_map_mfn_to_gpfn(vgt, first_gfn, first_mfn,
+		nr_mfns, map);
+
+	if (r != 0)
+		printk(KERN_ERR "vgt_hvm_map_aperture fail with %d!\n", r);
+	else
+		vgt->state.bar_mapped[1] = map;
+
+	return r;
+}
+
+/*
+ * Zap the GTTMMIO bar area for vGT trap and emulation.
+ */
+int vgt_hvm_set_trap_area(struct vgt_device *vgt, int map)
+{
+	char *cfg_space = &vgt->state.cfg_space[0];
+	uint64_t bar_s, bar_e;
+
+	if (!vgt_pci_mmio_is_enabled(vgt))
+		return 0;
+
+	cfg_space += VGT_REG_CFG_SPACE_BAR0;
+	if (VGT_GET_BITS(*cfg_space, 2, 1) == 2) {
+		/* 64 bits MMIO bar */
+		bar_s = * (uint64_t *) cfg_space;
+	} else {
+		/* 32 bits MMIO bar */
+		bar_s = * (uint32_t*) cfg_space;
+	}
+
+	bar_s &= ~0xF; /* clear the LSB 4 bits */
+	bar_e = bar_s + vgt->state.bar_size[0] - 1;
+
+	return hypervisor_set_trap_area(vgt, bar_s, bar_e, map);
+}
+
+/* EXECLIST dump functions */
+
+void dump_ctx_desc(struct vgt_device *vgt, struct ctx_desc_format *desc)
+{
+	const char *addressing_string[4] = {
+		"advanced context 64-bit no A/D",
+		"legacy context 32-bit",
+		"advanced context 64-bit A/D",
+		"legacy context 64-bit" };
+
+	printk("\nContext Descriptor ----\n");
+	printk("\t ldw = 0x%x; udw = 0x%x\n", desc->elm_low, desc->elm_high);
+	printk("\t context_id:0x%x\n", desc->context_id);
+	printk("\t valid:%d\n", desc->valid);
+	printk("\t force_pd_restore:%d\n", desc->force_pd_restore);
+	printk("\t force_restore:%d\n", desc->force_restore);
+	printk("\t addressing_mode:%d(%s)\n", desc->addressing_mode,
+			addressing_string[desc->addressing_mode]);
+	printk("\t llc_coherency:%d\n", desc->llc_coherency);
+	printk("\t fault_handling:%d\n", desc->fault_handling);
+	printk("\t privilege_access:%d\n", desc->privilege_access);
+	printk("\t lrca:0x%x\n", desc->lrca);
+}
+
+void dump_execlist_status(struct execlist_status_format *status, enum vgt_ring_id ring_id)
+{
+	printk("-------- Current EXECLIST status of ring-%d --------\n", ring_id);
+	printk("\tCurrent Context ID: 0x%x\n", status->context_id);
+	printk("\tLDW: 0x%x\n", status->ldw);
+	printk("\tEXECLIST queue full: %d\n", status->execlist_queue_full);
+	printk("\tCurrent EXECLIST index: %d\n",
+				status->current_execlist_pointer);
+	printk("\tEXECLIST write index: %d\n", status->execlist_write_pointer);
+	printk("\t  EXECLIST 0 status: %d(1 for valid)\t %d(1 for active)\n",
+			status->execlist_0_valid, status->execlist_0_active);
+	printk("\t  EXECLIST 1 status: %d(1 for valid)\t %d(1 for active)\n",
+			status->execlist_1_valid, status->execlist_1_active);
+	printk("\tActive context information:\n");
+	printk("\t    %s is active\n", status->current_active_elm_status == 0 ?
+			"no context" : (status->current_active_elm_status == 1 ?
+						"context 0" : "context 1"));
+	printk("\tLast ctx switch reason: 0x%x\n",
+				status->last_ctx_switch_reason);
+	printk("\tArbitration is: %s\n", status->arbitration_enable ?
+						"enabled" : "disabled");
+	if (status->current_active_elm_status == 2)
+		vgt_err("EXECLIST status register has invalid active context value!\n");
+}
+
+void dump_execlist_info(struct pgt_device *pdev, enum vgt_ring_id ring_id)
+{
+	uint32_t status_reg = el_ring_mmio(ring_id, _EL_OFFSET_STATUS);
+	struct execlist_status_format status;
+
+	READ_STATUS_MMIO(pdev, status_reg, status);
+	dump_execlist_status(&status, ring_id);
+}
+
+static void dump_ctx_status_buf_entry(struct vgt_device *vgt,
+			enum vgt_ring_id ring_id, unsigned int buf_entry, bool hw_status)
+{
+	struct context_status_format status;
+	uint32_t ctx_status_reg;
+
+	if (buf_entry >= CTX_STATUS_BUF_NUM)
+		return;
+
+	ctx_status_reg = el_ring_mmio(ring_id, _EL_OFFSET_STATUS_BUF);
+	ctx_status_reg += buf_entry * 8;
+
+	if (hw_status) {
+		READ_STATUS_MMIO(vgt->pdev, ctx_status_reg, status);
+	} else {
+		status.ldw = __vreg(vgt, ctx_status_reg);
+		status.udw = __vreg(vgt, ctx_status_reg + 4);
+	}
+
+	printk("    ring-%d CSB[%d]: ctx(0x%08x) val(0x%08x) <set bits: ",
+			ring_id, buf_entry, status.context_id, status.ldw);
+	if (status.idle_to_active)
+		printk("idle_to_active; ");
+	if (status.preempted)
+		printk("preemptedn; ");
+	if (status.element_switch)
+		printk("element_switch; ");
+	if (status.active_to_idle)
+		printk("active_to_idle; ");
+	if (status.context_complete)
+		printk("context_complete; ");
+	if (status.wait_on_sync_flip)
+		printk("wait_on_sync_flip; ");
+	if (status.wait_on_vblank)
+		printk("wait_on_vblank; ");
+	if (status.wait_on_semaphore)
+		printk("wait_on_semaphore; ");
+	if (status.wait_on_scanline)
+		printk("wait_on_scanline; ");
+	if (status.semaphore_wait_mode)
+		printk("semaphore_wait_mode; ");
+	if (status.display_plane)
+		printk("display_plane; ");
+	if (status.lite_restore)
+		printk("lite_restore; ");
+	printk(">\n");
+}
+
+static void dump_ctx_st_ptr(struct vgt_device *vgt, struct ctx_st_ptr_format *ptr)
+{
+	printk("Context StatusBufPtr Value: 0x%x. ", ptr->dw);
+	printk("(write_ptr: %d; ", ptr->status_buf_write_ptr);
+	printk("read_ptr: %d; ", ptr->status_buf_read_ptr);
+	printk("mask: 0x%x\n", ptr->mask);
+}
+
+void dump_ctx_status_buf(struct vgt_device *vgt,
+			enum vgt_ring_id ring_id, bool hw_status)
+{
+	uint32_t ctx_ptr_reg;
+	struct ctx_st_ptr_format ctx_st_ptr;
+	unsigned read_idx;
+	unsigned write_idx;
+	int i;
+
+	ctx_ptr_reg = el_ring_mmio(ring_id, _EL_OFFSET_STATUS_PTR);
+
+	if (hw_status)
+		ctx_st_ptr.dw = VGT_MMIO_READ(vgt->pdev, ctx_ptr_reg);
+	else
+		ctx_st_ptr.dw = __vreg(vgt, ctx_ptr_reg);
+
+	if (hw_status)
+		printk("---- Physical status Buffer of Ring %d ---- \n",
+			ring_id);
+	else
+		printk("---- Virtual status Buffer for VM-%d of Ring %d ---- \n",
+			vgt->vm_id, ring_id);
+
+	dump_ctx_st_ptr(vgt, &ctx_st_ptr);
+
+	read_idx = ctx_st_ptr.status_buf_read_ptr;
+	write_idx = ctx_st_ptr.status_buf_write_ptr;
+
+	if (read_idx == DEFAULT_INV_SR_PTR)
+		read_idx = 0;
+
+	if (write_idx == DEFAULT_INV_SR_PTR) {
+		vgt_err("No writes happened and no interesting data "
+			"in status buffer to show.\n");
+		return;
+	}
+
+	if (hw_status) {
+		/* show all contents in hw buffer */
+		read_idx = 0;
+		write_idx = CTX_STATUS_BUF_NUM - 1;
+	}
+
+	if (read_idx > write_idx)
+		write_idx += CTX_STATUS_BUF_NUM;
+
+	for (i = read_idx; i <= write_idx; ++ i)
+		dump_ctx_status_buf_entry(vgt, ring_id,
+			i % CTX_STATUS_BUF_NUM, hw_status);
+}
+
+#define DUMP_CTX_MMIO(prefix, mmio)	\
+printk("\t " #mmio ": <addr>0x%x - <val>0x%x\n", \
+	prefix->mmio.addr, prefix->mmio.val);
+
+void dump_regstate_ctx_header (struct reg_state_ctx_header *regstate)
+{
+	printk("\tlri_command:0x%x\n", regstate->lri_cmd_1);
+	DUMP_CTX_MMIO(regstate, ctx_ctrl);
+	DUMP_CTX_MMIO(regstate, ring_header);
+	DUMP_CTX_MMIO(regstate, ring_tail);
+	DUMP_CTX_MMIO(regstate, rb_start);
+	DUMP_CTX_MMIO(regstate, rb_ctrl);
+	DUMP_CTX_MMIO(regstate, bb_cur_head_UDW);
+	DUMP_CTX_MMIO(regstate, bb_cur_head_LDW);
+	DUMP_CTX_MMIO(regstate, bb_state);
+	DUMP_CTX_MMIO(regstate, second_bb_addr_UDW);
+	DUMP_CTX_MMIO(regstate, second_bb_addr_LDW);
+	DUMP_CTX_MMIO(regstate, second_bb_state);
+	DUMP_CTX_MMIO(regstate, bb_per_ctx_ptr);
+	DUMP_CTX_MMIO(regstate, rcs_indirect_ctx);
+	DUMP_CTX_MMIO(regstate, rcs_indirect_ctx_offset);
+	printk("\tlri_command2:0x%x\n", regstate->lri_cmd_2);
+	DUMP_CTX_MMIO(regstate, ctx_timestamp);
+	DUMP_CTX_MMIO(regstate, pdp3_UDW);
+	DUMP_CTX_MMIO(regstate, pdp3_LDW);
+	DUMP_CTX_MMIO(regstate, pdp2_UDW);
+	DUMP_CTX_MMIO(regstate, pdp2_LDW);
+	DUMP_CTX_MMIO(regstate, pdp1_UDW);
+	DUMP_CTX_MMIO(regstate, pdp1_LDW);
+	DUMP_CTX_MMIO(regstate, pdp0_UDW);
+	DUMP_CTX_MMIO(regstate, pdp0_LDW);
+}
+
+static inline struct reg_state_ctx_header *
+vgt_get_reg_state_from_lrca(struct vgt_device *vgt, uint32_t lrca)
+{
+	struct reg_state_ctx_header *header;
+	uint32_t state_gma = (lrca + 1) << GTT_PAGE_SHIFT;
+
+	header = (struct reg_state_ctx_header *)
+			vgt_gma_to_va(vgt->gtt.ggtt_mm, state_gma);
+	return header;
+}
+
+void dump_el_context_information(struct vgt_device *vgt,
+					struct execlist_context *el_ctx)
+{
+	struct reg_state_ctx_header *guest_state;
+	struct reg_state_ctx_header *shadow_state;
+	bool has_shadow;
+
+	if (el_ctx == NULL)
+		return;
+
+	has_shadow = vgt_require_shadow_context(vgt) && ! hvm_render_owner;
+
+	if (has_shadow)
+		guest_state = (struct reg_state_ctx_header *)
+				el_ctx->ctx_pages[1].guest_page.vaddr;
+	else
+		guest_state = vgt_get_reg_state_from_lrca(vgt,
+					el_ctx->guest_context.lrca);
+
+	printk("-- Context with guest ID 0x%x: --\n", el_ctx->guest_context.context_id);
+	printk("Guest(LRCA 0x%x) register state in context<0x%llx> is:\n",
+			el_ctx->guest_context.lrca,
+			(unsigned long long)guest_state);
+	dump_regstate_ctx_header(guest_state);
+
+	if (!has_shadow)
+		return;
+
+	shadow_state = (struct reg_state_ctx_header *)
+				el_ctx->ctx_pages[1].shadow_page.vaddr;
+
+	printk("Shadow(LRCA 0x%x) register state in context <0x%llx> is:\n",
+			el_ctx->shadow_lrca,
+			(unsigned long long)shadow_state);
+	dump_regstate_ctx_header(shadow_state);
+}
+
+void dump_all_el_contexts(struct pgt_device *pdev)
+{
+	struct vgt_device *vgt;
+	int i;
+	printk("-------- dump all shadow contexts --------\n");
+	for (i = 0; i < VGT_MAX_VMS; ++ i) {
+		struct hlist_node *n;
+		struct execlist_context *el_ctx;
+		int j;
+
+		vgt = pdev->device[i];
+		if (!vgt)
+			continue;
+		printk("-- VM(%d) --\n", vgt->vm_id);
+		hash_for_each_safe(vgt->gtt.el_ctx_hash_table, j, n, el_ctx, node) {
+			dump_el_context_information(vgt, el_ctx);
+			printk("^^^^^^^^\n");
+		}
+	}
+}
+
+static void dump_el_queue(struct vgt_device *vgt, int ring_id)
+{
+	int i;
+	printk("---- VM(%d): ring-%d EL queue ---", vgt->vm_id, ring_id);
+	printk("\thead: %d; tail: %d;\n", vgt_el_queue_head(vgt, ring_id),
+			vgt_el_queue_tail(vgt, ring_id));
+	for (i = 0; i < EL_QUEUE_SLOT_NUM; ++ i) {
+		int j;
+		struct vgt_exec_list *el_slot;
+		el_slot = &vgt_el_queue_slot(vgt, ring_id, i);
+		printk("[%d]: status: %d\n", i, el_slot->status);
+		for (j = 0; j < 2; ++ j) {
+			struct execlist_context *el_ctx;
+			el_ctx = vgt_el_queue_ctx(vgt, ring_id, i, j);
+			printk("|-ctx[%d]: ", j);
+			if (el_ctx == NULL)
+				printk("NULL\n");
+			else
+				printk("guest lrca:0x%x\n", el_ctx->guest_context.lrca);
+		}
+	}
+}
+
+void dump_el_status(struct pgt_device *pdev)
+{
+	enum vgt_ring_id ring_id;
+
+	for (ring_id = RING_BUFFER_RCS; ring_id < MAX_ENGINES; ++ ring_id) {
+		int i;
+		dump_execlist_info(pdev, ring_id);
+		dump_ctx_status_buf(vgt_dom0, ring_id, true);
+		for (i = 0; i < VGT_MAX_VMS; ++ i) {
+			struct vgt_device *vgt = pdev->device[i];
+			if (!vgt)
+				continue;
+			dump_ctx_status_buf(vgt, ring_id, false);
+			dump_el_queue(vgt, ring_id);
+		}
+	}
+}
diff --git a/drivers/gpu/drm/i915/vgt/vbios.c b/drivers/gpu/drm/i915/vgt/vbios.c
new file mode 100644
index 0000000..b7083b1
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/vbios.c
@@ -0,0 +1,293 @@
+/*
+ * vGT virtual video BIOS data block parser
+ *
+ * Copyright(c) 2011-2014 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "vgt.h"
+#include "vbios.h"
+
+#define CHILD_DEV_FAKE_SIZE sizeof(struct child_devices)
+
+static u8 child_dev_fake_dpb[CHILD_DEV_FAKE_SIZE] = {
+	0x04, 0x00, 0xd6, 0x60, 0x00, 0x10, 0x10, 0x06,
+	0x28, 0x14, 0x00, 0x20, 0x00, 0x00, 0x00, 0xd6,
+	0x07, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00, 0x00,
+	0x07, 0x10, 0x01, 0x20, 0x01, 0x00, 0x00, 0x00,
+	0x00
+};
+
+static u8 child_dev_fake_dpc[CHILD_DEV_FAKE_SIZE] = {
+	0x40, 0x00, 0xd6, 0x60, 0x00, 0x10, 0x10, 0x06,
+	0x3a, 0x14, 0x00, 0x20, 0x00, 0x00, 0x10, 0xd6,
+	0x08, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x00,
+	0x07, 0x20, 0x01, 0x20, 0x02, 0x00, 0x00, 0x00,
+	0x00
+};
+
+static u8 child_dev_fake_dpd[CHILD_DEV_FAKE_SIZE] = {
+	0x20, 0x00, 0xd6, 0x60, 0x00, 0x10, 0x10, 0x06,
+	0x4c, 0x14, 0x00, 0x20, 0x00, 0x00, 0x00, 0xd6,
+	0x09, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 0x00,
+	0x07, 0x30, 0x01, 0x20, 0x03, 0x00, 0x00, 0x00,
+	0x00,
+};
+
+static void child_dev_print(u8* bitstream, int size)
+{
+	int i;
+	for(i=0; i < size; i+=8) {
+	vgt_dbg(VGT_DBG_GENERIC,
+	"VGT_VBIOS: %02x %02x %02x %02x %02x %02x %02x %02x\n",
+		(i < size) ? bitstream[i+0] : 0xFF,
+		(i+1 < size) ? bitstream[i+1] : 0xFF,
+		(i+2 < size) ? bitstream[i+2] : 0xFF,
+		(i+3 < size) ? bitstream[i+3] : 0xFF,
+		(i+4 < size) ? bitstream[i+4] : 0xFF,
+		(i+5 < size) ? bitstream[i+5] : 0xFF,
+		(i+6 < size) ? bitstream[i+6] : 0xFF,
+		(i+7 < size) ? bitstream[i+7] : 0xFF);
+	}
+
+	vgt_dbg(VGT_DBG_GENERIC, "VGT_VBIOS: %d bytes. End of print \n", size);
+	vgt_dbg(VGT_DBG_GENERIC, "VGT_VBIOS: \n");
+}
+
+static void* get_block_by_id(struct bios_data_header *header, int id)
+{
+	void *curr = NULL;
+	int offset;
+	struct block_header *block = NULL;
+
+	ASSERT(header != NULL);
+
+	if (memcmp(header->signature, "BIOS_DATA_BLOCK", 15) != 0) {
+		/* invalid bios_data_block */
+		return NULL;
+	}
+
+	offset = header->header_size;
+
+	while(offset < header->bdb_size) {
+		block = (struct block_header*) ( ((u8*) header)+offset);
+
+		/* find block by block ID */
+		if (block->block_id == id) {
+			curr = block;
+			break;
+		}
+		else {
+			/* search for next block */
+			offset += block->block_size
+				+ sizeof(struct block_header);
+		}
+
+	}
+
+	return curr;
+}
+
+static struct child_devices*
+child_dev_found_available_slot(struct child_devices* dev, int max_child_num)
+{
+#define EFP_PORT_B_SLOT 1
+	int i;
+	struct child_devices *child_dev = NULL;
+
+	/* start from PORT_B, usually PORT_B/C/D takes slot: 1,2,3 */
+	for (i=EFP_PORT_B_SLOT; i<max_child_num; i++) {
+
+		child_dev = dev + i;
+		if (child_dev->dev_type == 0) {
+			/* Got available empty slot */
+			break;
+		}
+	}
+
+	return (i==max_child_num) ? NULL : child_dev;
+}
+
+
+static void
+child_dev_set_capability(struct child_devices* child_dev)
+{
+	if (child_dev->efp_port <= EFP_HDMI_D) {
+		/* update aux channel for HDMI B/C/D */
+		child_dev->aux_channel = (child_dev->efp_port << 4);
+		/* modify HDMI port to be DP port */
+		child_dev->efp_port += (EFP_DPORT_B - EFP_HDMI_B);
+	}
+
+	/* set DP capable bit */
+	child_dev->dev_type |= DEVTYPE_FLAG_DISPLAY_PORT;
+	child_dev->is_dp_compatible = 1;
+
+	/* set HDMI capable bit */
+	child_dev->dev_type &= (~DEVTYPE_FLAG_NOT_HDMI);
+	child_dev->is_hdmi_compatible = 1;
+
+	/* set DVI capable bit */
+	child_dev->dev_type |= DEVTYPE_FLAG_DVI;
+	child_dev->is_dvi_compatible = 1;
+	return;
+}
+
+static void
+child_dev_insert_fake_port(struct child_devices* dev, int max_num, int efp_port)
+{
+	struct child_devices *child_dev;
+
+	if((child_dev = child_dev_found_available_slot(dev, max_num)) == NULL) {
+		return;
+	}
+
+	/* already got a available slot */
+	switch(efp_port) {
+	case EFP_HDMI_B:
+	case EFP_DPORT_B:
+		memcpy(child_dev, child_dev_fake_dpb,
+			CHILD_DEV_FAKE_SIZE);
+		break;
+	case EFP_HDMI_C:
+	case EFP_DPORT_C:
+		memcpy(child_dev, child_dev_fake_dpc,
+			CHILD_DEV_FAKE_SIZE);
+		break;
+	case EFP_HDMI_D:
+	case EFP_DPORT_D:
+		memcpy(child_dev, child_dev_fake_dpd,
+			CHILD_DEV_FAKE_SIZE);
+		break;
+	default:
+		break;
+	}
+}
+
+/*
+ * We modify opregion vbios data to indicate that we support full port
+ * features: DP, HDMI, DVI
+ */
+bool vgt_prepare_vbios_general_definition(struct vgt_device *vgt)
+{
+	bool ret = true;
+	struct vbt_header *header;
+	struct bios_data_header *data_header;
+	struct vbios_general_definitions *gendef;
+	struct child_devices* child_dev;
+	int child_dev_num = 0;
+	int i;
+	bool encoder_b_found = false;
+	bool encoder_c_found = false;
+	bool encoder_d_found = false;
+
+	/* only valid for HSW */
+	if (!IS_HSW(vgt->pdev)) {
+		vgt_dbg(VGT_DBG_GENERIC, "Not HSW platform. Do nothing\n");
+		return false;
+	}
+
+	header = (struct vbt_header*) (vgt->state.opregion_va + VBIOS_OFFSET);
+
+	data_header = (struct bios_data_header*)
+		(((u8*)header) + header->bios_data_offset);
+
+	gendef = get_block_by_id(data_header, VBIOS_GENERAL_DEFINITIONS);
+	if (gendef == NULL) {
+		vgt_dbg(VGT_DBG_GENERIC,
+			"VBIOS_GENERAL_DEFINITIONS block was not found. \n");
+		return false;
+	}
+
+	child_dev_num = (gendef->block_header.block_size
+		- sizeof(*gendef)
+		+ sizeof(struct block_header))/ sizeof(struct child_devices);
+
+	vgt_dbg(VGT_DBG_GENERIC,
+		"VGT_VBIOS: block_size=%d child_dev_num=%d \n",
+		gendef->block_header.block_size, child_dev_num);
+
+	for (i=0; i<child_dev_num; i++) {
+		child_dev = gendef->dev + i;
+
+		/* print all VBT child dev structure */
+		child_dev_print((u8*)child_dev,
+			sizeof(struct child_devices));
+
+		if (child_dev->dev_type == 0) {
+			continue;
+		}
+
+		switch(child_dev->efp_port) {
+		case EFP_HDMI_B:
+			encoder_b_found = true;
+			break;
+		case EFP_HDMI_C:
+			encoder_c_found = true;
+			break;
+		case EFP_HDMI_D:
+			encoder_d_found = true;
+			break;
+		case EFP_DPORT_B:
+			encoder_b_found = true;
+			break;
+		case EFP_DPORT_C:
+			encoder_c_found = true;
+			break;
+		case EFP_DPORT_D:
+			encoder_d_found = true;
+			break;
+		case EFP_DPORT_A:
+			/* DPORT A is eDP, ignore */
+			continue;
+		default:
+			/* not port description. Skip this child_dev */
+			continue;
+		}
+
+		/* got valid PORT description */
+		child_dev_set_capability(child_dev);
+
+		vgt_dbg(VGT_DBG_GENERIC,
+		"VGT_VBIOS: child_dev modified. child_dev[%d].dev_type=%04x \n",
+			i, child_dev->dev_type);
+
+		ret = true;
+	}
+
+	if (!encoder_b_found) {
+		child_dev_insert_fake_port(gendef->dev,
+			child_dev_num, EFP_DPORT_B);
+	}
+
+	if (!encoder_c_found) {
+		child_dev_insert_fake_port(gendef->dev,
+			child_dev_num, EFP_DPORT_C);
+	}
+
+	if (!encoder_d_found) {
+		child_dev_insert_fake_port(gendef->dev,
+			child_dev_num, EFP_DPORT_D);
+	}
+
+	return ret;
+}
+
diff --git a/drivers/gpu/drm/i915/vgt/vbios.h b/drivers/gpu/drm/i915/vgt/vbios.h
new file mode 100644
index 0000000..024233b
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/vbios.h
@@ -0,0 +1,112 @@
+/*
+ * vGT virtual video BIOS data block parser
+ *
+ * Copyright(c) 2011-2014 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef _VGT_VBIOS_H_
+#define _VGT_VBIOS_H_
+
+/*
+ * video BIOS data block ID defination, each block has an ID and size.
+ * used by get_block_by_id()
+ */
+#define VBIOS_OFFSET 0x400
+#define VBIOS_GENERAL_FEATURES  1
+#define VBIOS_GENERAL_DEFINITIONS  2
+
+struct vbt_header {
+	u8 product_string[20]; /* string of "$VBT HASWELL"*/
+	u16 version;
+	u16 header_size;
+	u16 vbt_size;
+	u8  checksum;
+	u8  reserved;
+	u32 bios_data_offset;	/* bios_data beginning offset in bytes */
+	u32 aim_data_offset[4];
+};
+
+struct bios_data_header {
+	u8 signature[16];	/* signature of 'BIOS_DATA_BLOCK' */
+	u16 version;
+	u16 header_size;	/* in bytes */
+	u16 bdb_size;		/* in bytes */
+};
+
+struct block_header{
+	u8  block_id;		/* data block ID */
+	u16 block_size;		/* current data block size */
+}__attribute__((packed));	/* packed struct, not to align to 4 bytes */
+
+
+#define DEVTYPE_FLAG_DISPLAY_PORT 0x0004 /* BIT2 */
+#define DEVTYPE_FLAG_DVI 0x0010 /* BIT4 */
+#define DEVTYPE_FLAG_NOT_HDMI 0x0800 /* BIT11 */
+
+enum efp_port_type{
+	INVALID = 0,
+	EFP_HDMI_B = 1,
+	EFP_HDMI_C = 2,
+	EFP_HDMI_D = 3,
+	EFP_DPORT_B = 7,
+	EFP_DPORT_C = 8,
+	EFP_DPORT_D = 9,
+	EFP_DPORT_A = 10,
+	EFP_MIPI_A = 21,
+	EFP_MIPI_C = 23
+};
+
+/* VBIOS version >= 165 integrated EFP (HDMI/DP) structure.
+ * Valid for HSW/VLV
+ */
+struct child_devices
+{
+	u8 reserved[2];
+	u16 dev_type;
+	u8 reserved1[12];
+
+	u8 efp_port;
+	u8 reserved2[7];
+
+	/* compatibility_flag */
+	u8 is_hdmi_compatible:1;
+	u8 is_dp_compatible:1;
+	u8 is_dvi_compatible:1;
+	u8 reservedbit:5;
+	/* end of compatibility byte */
+	u8 aux_channel;
+	u8 reserved3[7];
+}__attribute__((packed));
+
+
+struct vbios_general_definitions
+{
+	struct block_header block_header;
+	u8 crt_ddc_pin; 	/* CRT DDC GPIO pin*/
+	u8 dpms;  		/* DPMS bits */
+	u16 boot_dev;		/* boot device bits */
+	u8 child_dev_size;	/* child_devices size */
+	struct child_devices dev[0]; /* a block could be many child devices */
+}__attribute__((packed));
+
+
+#endif	/* _VGT_VBIOS_H_ */
diff --git a/drivers/gpu/drm/i915/vgt/vgt-if.h b/drivers/gpu/drm/i915/vgt/vgt-if.h
new file mode 100644
index 0000000..9c9679d
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/vgt-if.h
@@ -0,0 +1,229 @@
+/*
+ * Interface between Gfx dricer and vGT enabled hypervisor
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef _VGT_IF_H
+#define _VGT_IF_H
+
+
+/* Reserve 32KB for vGT shared infor: 0x78000-0x7FFFF */
+#define VGT_PVINFO_PAGE	0x78000
+
+/* XXX: the 32KB range conflicts with PIPE_EDP_CONF: 0x7f008, so let's change
+ * to reserve only 4KB for now.
+ */
+//#define VGT_PVINFO_SIZE	0x8000
+#define VGT_PVINFO_SIZE	0x1000
+
+/*
+ * The following structure pages are defined in GEN MMIO space for virtualization.
+ * (One page for now)
+ */
+#define    VGT_MAGIC         0x4776544776544776    /* 'vGTvGTvG' */
+#define    VGT_VERSION_MAJOR 1
+#define    VGT_VERSION_MINOR 0
+
+/*
+ * The information set by the guest gfx driver, through the display_ready field
+ */
+#define    VGT_DRV_DISPLAY_NOT_READY	(0 << 0)
+#define    VGT_DRV_DISPLAY_READY	(1 << 0)	/* ready for display switch */
+#define    VGT_DRV_LEGACY_VGA_MODE	(1 << 1)	/* in the legacy VGA mode */
+
+/*
+ * guest-to-vgt notifications
+ */
+enum vgt_g2v_type {
+	VGT_G2V_DISPLAY_REFRESH,
+	VGT_G2V_SET_POINTER_SHAPE,
+	VGT_G2V_PPGTT_L3_PAGE_TABLE_CREATE,
+	VGT_G2V_PPGTT_L3_PAGE_TABLE_DESTROY,
+	VGT_G2V_PPGTT_L4_PAGE_TABLE_CREATE,
+	VGT_G2V_PPGTT_L4_PAGE_TABLE_DESTROY,
+	VGT_G2V_EXECLIST_CONTEXT_ELEMENT_CREATE,
+	VGT_G2V_EXECLIST_CONTEXT_ELEMENT_DESTROY,
+	VGT_G2V_MAX,
+};
+
+/*
+ * vgt-to-guest notifications
+ */
+enum vgt_v2g_type {
+	VGT_V2G_SET_HW_CURSOR,
+	VGT_V2G_SET_SW_CURSOR,
+	VGT_V2G_MAX,
+};
+
+struct vgt_if {
+    uint64_t  magic;      /* VGT_MAGIC */
+    uint16_t  version_major;
+    uint16_t  version_minor;
+    uint32_t  vgt_id;       /* ID of vGT instance */
+    uint32_t  rsv2[12];	    /* pad to offset 0x40 */
+    /*
+     *  Data structure to describe the balooning info of resources.
+     *  Each VM can only have one portion of continuous area for now.
+     *  (May support scattered resource in future)
+     *  (next starting from offset 0x40)
+     */
+    struct {
+        /* Aperture register balooning */
+        struct    {
+           uint32_t  my_base;
+           uint32_t  my_size;
+        } low_gmadr;		/* aperture */
+        /* GMADR register balooning */
+        struct    {
+           uint32_t  my_base;
+           uint32_t  my_size;
+        } high_gmadr;		/* non aperture */
+        /* allowed fence registers */
+        uint32_t fence_num;
+        uint32_t  rsv2[3];
+    } avail_rs;			/* available/assigned resource */
+    uint32_t  rsv3[0x200-24];   /* pad to half page */
+    /*
+     * The bottom half page is for the response from Gfx driver to hypervisor.
+     */
+    uint16_t  drv_version_major;
+    uint16_t  drv_version_minor;
+    uint32_t  display_ready;/* ready for display owner switch */
+    /*
+     * driver reported status/error code
+     *     0: if the avail_rs is sufficient to driver
+     *  Bit 2,1,0 set indicating
+     *       Insufficient low_gmadr, high_gmadr, fence resources.
+     *  Other bits are reserved.
+     */
+    uint32_t  rs_insufficient;
+    /*
+     * The driver is required to update the following field with minimal
+     * required resource size.
+     */
+    uint32_t  min_low_gmadr;
+    uint32_t  min_high_gmadr;
+    uint32_t  min_fence_num;
+
+    /*
+     * notifications between guest and vgt
+     */
+    uint32_t  g2v_notify;
+    uint32_t  v2g_notify;
+
+    /*
+     * PPGTT PTE table info
+     */
+    uint32_t  gmm_gtt_seg_base;
+    uint32_t  rsv4;
+    uint32_t  gmm_gtt_seg_size;
+    uint32_t  rsv5;
+
+    /*
+     * Cursor hotspot info
+     */
+    uint32_t  xhot;
+    uint32_t  yhot;
+
+    uint32_t pdp0_lo;
+    uint32_t pdp0_hi;
+    uint32_t pdp1_lo;
+    uint32_t pdp1_hi;
+    uint32_t pdp2_lo;
+    uint32_t pdp2_hi;
+    uint32_t pdp3_lo;
+    uint32_t pdp3_hi;
+
+    uint32_t execlist_context_descriptor_lo;
+    uint32_t execlist_context_descriptor_hi;
+
+    /*
+     * scratch space for debugging
+     */
+    uint32_t  scratch;;
+
+    uint32_t  rsv6[0x200-25];    /* pad to one page */
+};
+
+#define vgt_info_off(x)        (VGT_PVINFO_PAGE + (long)&((struct vgt_if*) NULL)->x)
+
+
+/* get the bits high:low of the data, high and low is starting from zero*/
+#define VGT_GET_BITS(data, high, low)	(((data) & ((1 << ((high) + 1)) - 1)) >> (low))
+/* get one bit of the data, bit is starting from zeor */
+#define VGT_GET_BIT(data, bit)		VGT_GET_BITS(data, bit, bit)
+
+
+struct vgt_device;
+bool vgt_emulate_write(struct vgt_device *vgt, uint64_t pa, void *p_data, int bytes);
+bool vgt_emulate_read(struct vgt_device *vgt, uint64_t pa, void *p_data, int bytes);
+bool vgt_emulate_cfg_write(struct vgt_device *vgt, unsigned int off, void *p_data, int bytes);
+bool vgt_emulate_cfg_read(struct vgt_device *vgt, unsigned int off, void *p_data, int bytes);
+
+/* save the fixed/translated guest address
+ * restore the address after the command is executed
+*/
+#define VGT_ENABLE_ADDRESS_FIX_SAVE_RESTORE
+
+enum {
+	VGT_DELAY_IRQ = 0,
+	VGT_DELAY_VBLANK_DISABLE_TIMER,
+	VGT_DELAY_HANGCHECK_TIMER,
+	VGT_DELAY_HOTPLUG_REENABLE_TIMER,
+	VGT_DELAY_EVENT_MAX,
+};
+
+extern bool vgt_check_busy(int event);
+extern void vgt_set_delayed_event_data(int event, void *data);
+
+DECLARE_PER_CPU(u8, in_vgt);
+
+extern bool vgt_handle_dom0_device_reset(void);
+
+/*
+ * in_vgt flag is used to indicate whether current code
+ * path is in vgt core module, which is key for virtual
+ * irq delivery in de-privileged dom0 framework. So use
+ * get_cpu/put_cpu here to avoid preemption, otherwise
+ * this flag loses its intention.
+ */
+static inline int vgt_enter(void)
+{
+	int cpu = get_cpu();
+
+	per_cpu(in_vgt, cpu)++;
+	return cpu;
+}
+
+extern void inject_dom0_virtual_interrupt(void *info);
+static inline void vgt_exit(int cpu)
+{
+	per_cpu(in_vgt, cpu)--;
+
+	/* check for delayed virq injection */
+	inject_dom0_virtual_interrupt(NULL);
+
+	put_cpu();
+}
+
+#endif /* _VGT_IF_H */
diff --git a/drivers/gpu/drm/i915/vgt/vgt.c b/drivers/gpu/drm/i915/vgt/vgt.c
new file mode 100644
index 0000000..2625e59
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/vgt.c
@@ -0,0 +1,1258 @@
+/*
+ * vGT module interface
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/module.h>
+#include <linux/kthread.h>
+#include <linux/freezer.h>
+
+#include "vgt.h"
+
+
+MODULE_AUTHOR("Intel Corporation");
+MODULE_DESCRIPTION("Virtual GPU device model for Intel Processor Graphics");
+MODULE_LICENSE("GPL");
+MODULE_VERSION("0.1");
+
+bool hvm_render_owner = false;
+module_param_named(hvm_render_owner, hvm_render_owner, bool, 0600);
+MODULE_PARM_DESC(hvm_render_owner, "Make HVM to be render owner after create (default: false)");
+
+bool hvm_dpy_owner = false;
+module_param_named(hvm_dpy_owner, hvm_dpy_owner, bool, 0600);
+MODULE_PARM_DESC(hvm_dpy_owner, "Deprecated option! Please use hvm_boot_foreground or hvm_display_owner!");
+
+bool hvm_display_owner = false;
+module_param_named(hvm_display_owner, hvm_display_owner, bool, 0600);
+MODULE_PARM_DESC(hvm_display_owner, "Make HVM to be display owner after create (default: false)");
+
+bool hvm_super_owner = false;
+module_param_named(hvm_super_owner, hvm_super_owner, bool, 0600);
+MODULE_PARM_DESC(hvm_super_owner, "Make HVM to be GPU owner after create (default: false)");
+
+bool hvm_boot_foreground = false;
+module_param_named(hvm_boot_foreground, hvm_boot_foreground, bool, 0600);
+MODULE_PARM_DESC(hvm_boot_foreground, "Make HVM to be foreground after create and visible on screen from booting (default: false)");
+
+bool vgt_primary = false;
+module_param_named(vgt_primary, vgt_primary, bool, 0600);
+
+bool vgt_track_nest = true;
+module_param_named(track_nest, vgt_track_nest, bool, 0600);
+
+bool vgt_delay_nest = true;
+module_param_named(delay_nest, vgt_delay_nest, bool, 0600);
+
+int vgt_debug = 0;
+module_param_named(debug, vgt_debug, int, 0600);
+
+bool vgt_enabled = true;
+module_param_named(vgt, vgt_enabled, bool, 0400);
+
+bool fastpath_dpy_switch = true;
+module_param_named(fastpath_dpy_switch, fastpath_dpy_switch, bool, 0600);
+
+bool event_based_qos = false;
+module_param_named(event_based_qos, event_based_qos, bool, 0600);
+MODULE_PARM_DESC(event_based_qos, "Use event based QoS scheduler (default: false)");
+
+bool shadow_tail_based_qos = false;
+module_param_named(shadow_tail_based_qos, shadow_tail_based_qos, bool, 0600);
+MODULE_PARM_DESC(shadow_tail_based_qos, "Use Shadow tail based QoS scheduler (default: false)");
+
+bool render_engine_reset = true;
+module_param_named(render_engine_reset, render_engine_reset, bool, 0600);
+MODULE_PARM_DESC(render_engine_reset, "Reset rendering engines before loading another VM's context");
+
+bool propagate_monitor_to_guest = true;
+module_param_named(propagate_monitor_to_guest, propagate_monitor_to_guest, bool, 0600);
+MODULE_PARM_DESC(propagate_monitor_to_guest, "Propagate monitor information to guest by XenGT, other than dom0 services to do so");
+
+bool irq_based_ctx_switch = true;
+module_param_named(irq_based_ctx_switch, irq_based_ctx_switch, bool, 0600);
+MODULE_PARM_DESC(irq_based_ctx_switch, "Use user interrupt based context switch (default: true)");
+
+int preallocated_shadow_pages = -1;
+module_param_named(preallocated_shadow_pages, preallocated_shadow_pages, int, 0600);
+MODULE_PARM_DESC(preallocated_shadow_pages, "Amount of pre-allocated shadow pages");
+
+/*
+ * FIXME: now video ring switch has weird issue. The cmd
+ * parser may enter endless loop even when head/tail is
+ * zero. earlier posting read doesn't solve the issue.
+ * so disable it for now.
+ *
+ * Dexuan: let's enable VCS switch, because on HSW, win7 gfx drver's PAVP
+ * initialization uses VCS. Without enabling this option, win7 guest's gfx
+ * driver's initializtion will hang when we create the guest for the 2nd
+ * time(VCS.TAIL is 0x70, but VCS.HEAD is always 0x30).
+ */
+int enable_video_switch = 1;
+module_param_named(enable_video_switch, enable_video_switch, int, 0600);
+
+/*
+ * On HSW, the max low/high gm sizes are 512MB/1536MB.
+ * If each VM takes 512MB GM, we can support 4VMs.
+ * By default Dom0 has 512MB GM, including 120MB low gm used by i915 and
+ * 8MB low gm used by vGT driver itself(see VGT_RSVD_APERTURE_SZ), and
+ * (512-120-8)MB high GM space used by i915.
+ * We can reduce the GM space used by Dom0 i915, but remember: Dom0
+ * render/display may not work properly without enough GM space.
+ */
+int dom0_low_gm_sz = 96;	//in MB.
+module_param_named(dom0_low_gm_sz, dom0_low_gm_sz, int, 0600);
+
+int dom0_high_gm_sz = 384;	//in MB.
+module_param_named(dom0_high_gm_sz, dom0_high_gm_sz, int, 0600);
+
+int dom0_fence_sz = 4;
+module_param_named(dom0_fence_sz, dom0_fence_sz, int, 0600);
+
+int bypass_scan_mask = 0;
+module_param_named(bypass_scan, bypass_scan_mask, int, 0600);
+
+bool bypass_dom0_addr_check = false;
+module_param_named(bypass_dom0_addr_check, bypass_dom0_addr_check, bool, 0600);
+
+bool enable_panel_fitting = true;
+module_param_named(enable_panel_fitting, enable_panel_fitting, bool, 0600);
+
+bool enable_reset = true;
+module_param_named(enable_reset, enable_reset, bool, 0600);
+
+bool vgt_lock_irq = false;
+module_param_named(vgt_lock_irq, vgt_lock_irq, bool, 0400);
+
+bool vgt_preliminary_hw_support = true;
+module_param_named(vgt_preliminary_hw_support, vgt_preliminary_hw_support, bool, 0400);
+
+int shadow_execlist_context = 0;
+module_param_named(shadow_execlist_context, shadow_execlist_context, int, 0400);
+
+/* Very frequent set/clear write protection can see wrong write trap even if
++ * write protection has been cleared. Below option is to disable the context
++ * protection between ctx submission and ctx completion. Normal context shadow
++ * will not be impacted by this option, which will have ctx write protection
++ * between ctx creation and ctx destroy.
++ */
+bool wp_submitted_ctx = false;
+module_param_named(wp_submitted_ctx, wp_submitted_ctx, bool, 0400);
+
+struct kernel_dm *vgt_pkdm __weak = NULL;
+
+static vgt_ops_t vgt_xops = {
+	.mem_read = vgt_emulate_read,
+	.mem_write = vgt_emulate_write,
+	.cfg_read = vgt_emulate_cfg_read,
+	.cfg_write = vgt_emulate_cfg_write,
+	.initialized = false,
+};
+vgt_ops_t *vgt_ops = NULL;
+
+LIST_HEAD(pgt_devices);
+struct pgt_device default_device = {
+	.bus = 0,
+	.devfn = 0x10,		/* BDF: 0:2:0 */
+};
+
+struct vgt_device *vgt_dom0;
+struct pgt_device *pdev_default = &default_device;
+struct drm_i915_private *dev_priv = NULL;
+DEFINE_PER_CPU(u8, in_vgt);
+
+uint64_t vgt_gttmmio_va(struct pgt_device *pdev, off_t reg)
+{
+	return (uint64_t)((char *)pdev->gttmmio_base_va + reg);
+}
+
+uint64_t vgt_gttmmio_pa(struct pgt_device *pdev, off_t reg)
+{
+	return (uint64_t)(pdev->gttmmio_base + reg);
+}
+
+struct pci_dev *pgt_to_pci(struct pgt_device *pdev)
+{
+	return pdev->pdev;
+}
+
+/*
+ * The thread to perform the VGT ownership switch.
+ *
+ * We need to handle race conditions from different paths around
+ * vreg/sreg/hwreg. So far there're 4 paths at least:
+ *   a) the vgt thread to conduct context switch
+ *   b) the GP handler to emulate MMIO for dom0
+ *   c) the event handler to emulate MMIO for other VMs
+ *   d) the interrupt handler to do interrupt virtualization
+ *   e) /sysfs interaction from userland program
+ *
+ * Now d) is removed from the race path, because we adopt a delayed
+ * injection mechanism. Physical interrupt handler only saves pending
+ * IIR bits, and then wake up the vgt thread. Later the vgt thread
+ * checks the pending bits to do the actual virq injection. This approach
+ * allows vgt thread to handle ownership switch cleanly.
+ *
+ * So it's possible for other 3 paths to touch vreg/sreg/hwreg:
+ *   a) the vgt thread may need to update HW updated regs into
+ *	  vreg/sreg of the prev owner
+ *   b) the GP handler and event handler always updates vreg/sreg,
+ *	  and may touch hwreg if vgt is the current owner
+ *	  and then update vreg for interrupt virtualization
+ *
+ * To simplify the lock design, we make below assumptions:
+ *   a) the vgt thread doesn't trigger GP fault itself, i.e. always
+ *	  issues hypercall to do hwreg access
+ *   b) the event handler simply notifies another kernel thread, leaving
+ *	  to that thread for actual MMIO emulation
+ *
+ * Given above assumption, no nest would happen among 4 paths, and a
+ * simple global spinlock now should be enough to protect the whole
+ * vreg/sreg/ hwreg. In the future we can futher tune this part on
+ * a necessary base.
+ */
+static int vgt_thread(void *priv)
+{
+	struct pgt_device *pdev = (struct pgt_device *)priv;
+	int ret;
+	int cpu;
+
+	//ASSERT(current_render_owner(pdev));
+	printk("vGT: start kthread for dev (%x, %x)\n", pdev->bus, pdev->devfn);
+
+	set_freezable();
+	while (!kthread_should_stop()) {
+		enum vgt_ring_id ring_id;
+		bool ctx_irq_received = false;
+		ret = wait_event_interruptible(pdev->event_wq,
+			pdev->request || freezing(current));
+
+		if (ret)
+			vgt_warn("Main thread waken up by unexpected signal!\n");
+
+		if (!pdev->request && !freezing(current)) {
+			vgt_warn("Main thread waken up by unknown reasons!\n");
+			continue;
+		}
+
+		if (freezing(current)) {
+			if (current_render_owner(pdev) == vgt_dom0) {
+				try_to_freeze();
+			}
+			else {
+				vgt_lock_dev(pdev, cpu);
+				pdev->next_sched_vgt = vgt_dom0;
+				vgt_raise_request(pdev, VGT_REQUEST_SCHED);
+				vgt_unlock_dev(pdev, cpu);
+			}
+		}
+
+		if (test_and_clear_bit(VGT_REQUEST_DEVICE_RESET,
+					(void *)&pdev->request)) {
+			vgt_reset_device(pdev);
+		}
+
+		for (ring_id = 0; ring_id < MAX_ENGINES; ++ ring_id) {
+			if (test_and_clear_bit(
+				VGT_REQUEST_CTX_EMULATION_RCS + ring_id,
+				(void *)&pdev->request)) {
+				vgt_lock_dev(pdev, cpu);
+				vgt_emulate_context_switch_event(pdev, ring_id);
+				vgt_unlock_dev(pdev, cpu);
+				ctx_irq_received = true;
+			}
+		}
+
+		if (ctx_irq_received && ctx_switch_requested(pdev)) {
+			bool all_rings_empty = true;
+			for (ring_id = 0; ring_id < MAX_ENGINES; ++ ring_id) {
+				if(!vgt_idle_execlist(pdev, ring_id)) {
+					all_rings_empty = false;
+					break;
+				}
+			}
+			if (all_rings_empty)
+				vgt_raise_request(pdev, VGT_REQUEST_CTX_SWITCH);
+		}
+
+		/* forward physical GPU events to VMs */
+		if (test_and_clear_bit(VGT_REQUEST_IRQ,
+					(void *)&pdev->request)) {
+			unsigned long flags;
+			vgt_lock_dev(pdev, cpu);
+			vgt_get_irq_lock(pdev, flags);
+			vgt_forward_events(pdev);
+			vgt_put_irq_lock(pdev, flags);
+			vgt_unlock_dev(pdev, cpu);
+		}
+
+		/* Send uevent to userspace */
+		if (test_and_clear_bit(VGT_REQUEST_UEVENT,
+					(void *)&pdev->request)) {
+			vgt_signal_uevent(pdev);
+		}
+
+		if (test_and_clear_bit(VGT_REQUEST_DPY_SWITCH,
+					(void *)&pdev->request)) {
+			vgt_lock_dev(pdev, cpu);
+			if (prepare_for_display_switch(pdev) == 0)
+				do_vgt_fast_display_switch(pdev);
+			vgt_unlock_dev(pdev, cpu);
+		}
+
+		/* Handle render engine scheduling */
+		if (vgt_ctx_switch &&
+		    test_and_clear_bit(VGT_REQUEST_SCHED,
+				(void *)&pdev->request)) {
+			if (!vgt_do_render_sched(pdev)) {
+				if (enable_reset) {
+					vgt_err("Hang in render sched, try to reset device.\n");
+
+					vgt_reset_device(pdev);
+				} else {
+					vgt_err("Hang in render sched, panic the system.\n");
+					ASSERT(0);
+				}
+			}
+		}
+
+		/* Handle render context switch */
+		if (vgt_ctx_switch &&
+		    test_and_clear_bit(VGT_REQUEST_CTX_SWITCH,
+				(void *)&pdev->request)) {
+			if (!vgt_do_render_context_switch(pdev)) {
+				if (enable_reset) {
+					vgt_err("Hang in context switch, try to reset device.\n");
+
+					vgt_reset_device(pdev);
+				} else {
+					vgt_err("Hang in context switch, panic the system.\n");
+					ASSERT(0);
+				}
+			}
+		}
+
+		if (test_and_clear_bit(VGT_REQUEST_EMUL_DPY_EVENTS,
+				(void *)&pdev->request)) {
+			vgt_lock_dev(pdev, cpu);
+			vgt_emulate_dpy_events(pdev);
+			vgt_unlock_dev(pdev, cpu);
+		}
+	}
+	return 0;
+}
+
+
+bool initial_phys_states(struct pgt_device *pdev)
+{
+	struct vgt_device_info *info = &pdev->device_info;
+	int i;
+	uint64_t	bar0, bar1;
+	struct pci_dev *dev = pdev->pdev;
+
+	vgt_dbg(VGT_DBG_GENERIC, "VGT: Initial_phys_states\n");
+
+	pdev->gtt_size = vgt_get_gtt_size(pdev);
+	gm_sz(pdev) = pdev->gtt_size >> info->gtt_entry_size_shift << GTT_PAGE_SHIFT;
+
+	ASSERT(gm_sz(pdev) <= info->max_gtt_gm_sz);
+
+	pdev->saved_gtt = vzalloc(pdev->gtt_size);
+	if (!pdev->saved_gtt)
+		return false;
+
+	for (i=0; i<VGT_CFG_SPACE_SZ; i+=4)
+		pci_read_config_dword(dev, i,
+				(uint32_t *)&pdev->initial_cfg_space[i]);
+
+	for (i=0; i<VGT_CFG_SPACE_SZ; i+=4) {
+		if (!(i % 16))
+			vgt_dbg(VGT_DBG_GENERIC, "\n[%2x]: ", i);
+
+		vgt_dbg(VGT_DBG_GENERIC, "%02x %02x %02x %02x ",
+			*((uint32_t *)&pdev->initial_cfg_space[i]) & 0xff,
+			(*((uint32_t *)&pdev->initial_cfg_space[i]) & 0xff00) >> 8,
+			(*((uint32_t *)&pdev->initial_cfg_space[i]) & 0xff0000) >> 16,
+			(*((uint32_t *)&pdev->initial_cfg_space[i]) & 0xff000000) >> 24);
+	}
+	for (i=0; i < 3; i++) {
+		pdev->bar_size[i] = pci_bar_size(pdev, VGT_REG_CFG_SPACE_BAR0 + 8*i);
+		printk("bar-%d size: %x\n", i, pdev->bar_size[i]);
+	}
+
+	bar0 = *(uint64_t *)&pdev->initial_cfg_space[VGT_REG_CFG_SPACE_BAR0];
+	bar1 = *(uint64_t *)&pdev->initial_cfg_space[VGT_REG_CFG_SPACE_BAR1];
+	printk("bar0: 0x%llx, Bar1: 0x%llx\n", bar0, bar1);
+
+	ASSERT ((bar0 & 7) == 4);
+	/* memory, 64 bits bar0 */
+	pdev->gttmmio_base = bar0 & ~0xf;
+	pdev->mmio_size = VGT_MMIO_SPACE_SZ;
+	pdev->reg_num = pdev->mmio_size/REG_SIZE;
+	printk("mmio size: %x, gtt size: %llx\n", pdev->mmio_size,
+		pdev->gtt_size);
+	ASSERT(pdev->mmio_size + pdev->gtt_size <= pdev->bar_size[0]);
+
+	ASSERT ((bar1 & 7) == 4);
+	/* memory, 64 bits bar */
+	pdev->gmadr_base = bar1 & ~0xf;
+	printk("gttmmio: 0x%llx, gmadr: 0x%llx\n", pdev->gttmmio_base, pdev->gmadr_base);
+
+	pdev->gttmmio_base_va = ioremap(pdev->gttmmio_base, pdev->bar_size[0]);
+	if (pdev->gttmmio_base_va == NULL) {
+		WARN_ONCE(1, "insufficient memory for ioremap!\n");
+		return false;
+	}
+	printk("gttmmio_base_va: 0x%llx\n", (uint64_t)pdev->gttmmio_base_va);
+
+	/*
+	 * From now on, the vgt driver can invoke the
+	 * VGT_MMIO_READ()/VGT_MMIO_WRITE()hypercalls, and any access to the
+	 * 4MB MMIO of the GEN device is trapped into the vgt driver.
+	 */
+
+	// TODO: runtime sanity check warning...
+	pdev->gmadr_va = ioremap (pdev->gmadr_base, pdev->bar_size[1]);
+	if ( pdev->gmadr_va == NULL ) {
+		iounmap(pdev->gttmmio_base_va);
+		printk("Insufficient memory for ioremap2\n");
+		return false;
+	}
+	printk("gmadr_va: 0x%llx\n", (uint64_t)pdev->gmadr_va);
+
+	vgt_initial_mmio_setup(pdev);
+	vgt_initial_opregion_setup(pdev);
+
+	/* FIXME: GMBUS2 has an in-use bit as the hw semaphore, and we should recover
+	 * it after the snapshot. Remove this workaround after GMBUS virtualization
+	 */
+	{
+		u32 val = VGT_MMIO_READ(pdev, 0xc5108);
+		pdev->initial_mmio_state[REG_INDEX(0xc5108)] &= ~0x8000;
+		printk("vGT: GMBUS2 init value: %x, %x\n", pdev->initial_mmio_state[REG_INDEX(0xc5108)], val);
+		VGT_MMIO_WRITE(pdev, 0xc5108, val | 0x8000);
+	}
+
+	for (i = 0; i < MAX_ENGINES; ++ i) {
+		pdev->el_read_ptr[i] = DEFAULT_INV_SR_PTR;
+		pdev->el_cache_write_ptr[i] = DEFAULT_INV_SR_PTR;
+	}
+
+	return true;
+}
+
+static bool vgt_set_device_type(struct pgt_device *pdev)
+{
+	if (_is_sandybridge(pdev->pdev->device)) {
+		pdev->gen_dev_type = IGD_SNB;
+		vgt_info("Detected Sandybridge\n");
+		return true;
+	}
+
+	if (_is_ivybridge(pdev->pdev->device)) {
+		pdev->gen_dev_type = IGD_IVB;
+		vgt_info("Detected Ivybridge\n");
+		return true;
+	}
+
+	if (_is_haswell(pdev->pdev->device)) {
+		pdev->gen_dev_type = IGD_HSW;
+		vgt_info("Detected Haswell\n");
+		return true;
+	}
+
+	if (_is_broadwell(pdev->pdev->device)) {
+		pdev->gen_dev_type = IGD_BDW;
+		vgt_info("Detected Broadwell\n");
+		return true;
+	}
+
+	vgt_err("Unknown chip 0x%x\n", pdev->pdev->device);
+	return false;
+}
+
+static bool vgt_initialize_device_info(struct pgt_device *pdev)
+{
+	struct vgt_device_info *info = &pdev->device_info;
+
+	if (!vgt_set_device_type(pdev))
+		return false;
+
+	if (!IS_HSW(pdev) && !IS_BDW(pdev)) {
+		vgt_err("Unsupported gen_dev_type(%s)!\n",
+			IS_IVB(pdev) ?
+			"IVB" : "SNB(or unknown GEN types)");
+		return false;
+	}
+
+	if (IS_BDW(pdev) && !vgt_preliminary_hw_support) {
+		vgt_err("VGT haven't fully supported preliminary platform: broadwell.\n");
+		return false;
+	}
+
+	if (IS_HSW(pdev)) {
+		info->gen = MKGEN(7, 5, 0);
+		info->max_gtt_gm_sz = (1UL << 31);	/* 2G */
+		/*
+		 * The layout of BAR0 in PreBDW:
+		 * |< - MMIO 2MB ->|<- MAX GTT 2MB ->|
+		 *
+		 * GTT offset in BAR0 starts from 2MB to 4MB
+		 */
+		info->gtt_start_offset = (1UL << 21);
+		info->max_gtt_size = (1UL << 22);
+		info->gtt_entry_size = 4;
+		info->gtt_entry_size_shift = 2;
+		info->gmadr_bytes_in_cmd = 4;
+	} else if (IS_BDW(pdev)) {
+		info->gen = MKGEN(8, 0, ((pdev->pdev->device >> 4) & 0xf) + 1);
+		info->max_gtt_gm_sz = (1UL << 32);
+		/*
+		 * The layout of BAR0 in BDW:
+		 * |< - MMIO 2MB ->|<- Reserved 6MB ->|<- MAX GTT 8MB->|
+		 *
+		 * GTT offset in BAR0 starts from 8MB to 16MB, and
+		 * Whatever GTT size is configured in BIOS,
+		 * the size of BAR0 is always 16MB. The actual configured
+		 * GTT size can be found in GMCH_CTRL.
+		 */
+		info->gtt_start_offset = (1UL << 23);
+		info->max_gtt_size = (1UL << 23);
+		info->gtt_entry_size = 8;
+		info->gtt_entry_size_shift = 3;
+		info->gmadr_bytes_in_cmd = 8;
+	}
+
+	printk("GEN device info:\n");
+	printk("	major: %u minor: %u rev: %u\n", GEN_MAJOR(info->gen),
+			GEN_MINOR(info->gen), GEN_REV(info->gen));
+	printk("	max_gtt_gm_sz: %llx\n", info->max_gtt_gm_sz);
+	printk("	gtt_start_offset: %x\n", info->gtt_start_offset);
+	printk("	max_gtt_size: %x\n", info->max_gtt_size);
+	printk("	gtt_size_entry: %x\n", info->gtt_entry_size);
+	printk("	gtt_entry_size_shift: %x.\n", info->gtt_entry_size_shift);
+
+	return true;
+}
+
+static bool vgt_initialize_platform(struct pgt_device *pdev)
+{
+	/* check PPGTT enabling. */
+	if (IS_IVB(pdev) || IS_HSW(pdev) || IS_BDW(pdev))
+		pdev->enable_ppgtt = 1;
+
+	/* execlist depends on ppgtt */
+	if (pdev->enable_ppgtt) {
+		if (IS_BDWPLUS(pdev))
+			pdev->enable_execlist = 1;
+	}
+
+	pdev->max_engines = 3;
+	pdev->ring_mmio_base[RING_BUFFER_RCS] = _REG_RCS_TAIL;
+	pdev->ring_mmio_base[RING_BUFFER_VCS] = _REG_VCS_TAIL;
+	pdev->ring_mmio_base[RING_BUFFER_BCS] = _REG_BCS_TAIL;
+
+	pdev->ring_mi_mode[RING_BUFFER_RCS] = _REG_RCS_MI_MODE;
+	pdev->ring_mi_mode[RING_BUFFER_VCS] = _REG_VCS_MI_MODE;
+	pdev->ring_mi_mode[RING_BUFFER_BCS] = _REG_BCS_MI_MODE;
+
+	pdev->ring_xxx[RING_BUFFER_RCS] = 0x2050;
+	pdev->ring_xxx[RING_BUFFER_VCS] = 0x12050;
+	pdev->ring_xxx[RING_BUFFER_BCS] = 0x22050;
+	pdev->ring_xxx_bit[RING_BUFFER_RCS] = 3;
+	pdev->ring_xxx_bit[RING_BUFFER_VCS] = 3;
+	pdev->ring_xxx_bit[RING_BUFFER_BCS] = 3;
+	/* this check is broken on SNB */
+	pdev->ring_xxx_valid = 0;
+
+	pdev->gtt.pte_ops = &gen7_gtt_pte_ops;
+	pdev->gtt.gma_ops = &gen7_gtt_gma_ops;
+	pdev->gtt.mm_alloc_page_table = gen7_mm_alloc_page_table;
+	pdev->gtt.mm_free_page_table = gen7_mm_free_page_table;
+
+	if (IS_HSW(pdev)) {
+		pdev->max_engines = 4;
+		pdev->ring_mmio_base[RING_BUFFER_VECS] = _REG_VECS_TAIL;
+		pdev->ring_mi_mode[RING_BUFFER_VECS] = _REG_VECS_MI_MODE;
+		pdev->ring_xxx[RING_BUFFER_RCS] = 0x8000;
+		pdev->ring_xxx[RING_BUFFER_VCS] = 0x8000;
+		pdev->ring_xxx[RING_BUFFER_BCS] = 0x8000;
+		pdev->ring_xxx[RING_BUFFER_VECS] = 0x8008;
+		pdev->ring_xxx_bit[RING_BUFFER_RCS] = 0;
+		pdev->ring_xxx_bit[RING_BUFFER_VCS] = 1;
+		pdev->ring_xxx_bit[RING_BUFFER_BCS] = 2;
+		pdev->ring_xxx_bit[RING_BUFFER_VECS] = 10;
+		pdev->ring_xxx_valid = 1;
+
+		if (preallocated_shadow_pages == -1)
+			preallocated_shadow_pages = 512;
+	} else if (IS_BDW(pdev)) {
+		pdev->max_engines = 4;
+		pdev->ring_mmio_base[RING_BUFFER_VECS] = _REG_VECS_TAIL;
+		pdev->ring_mi_mode[RING_BUFFER_VECS] = _REG_VECS_MI_MODE;
+		pdev->ring_xxx_valid = 0;
+
+		/*
+		 * Add GT3 VCS2 ring for BDW GT3
+		 */
+		if (IS_BDWGT3(pdev)) {
+			pdev->max_engines = 5;
+			pdev->ring_mmio_base[RING_BUFFER_VCS2] = _REG_VCS2_TAIL;
+			pdev->ring_mi_mode[RING_BUFFER_VCS2] = _REG_VCS2_MI_MODE;
+			pdev->ring_xxx[RING_BUFFER_VCS2] = 0x8008;
+			pdev->ring_xxx_bit[RING_BUFFER_VCS2] = 0;
+		}
+
+		pdev->gtt.pte_ops = &gen8_gtt_pte_ops;
+		pdev->gtt.gma_ops = &gen8_gtt_gma_ops;
+		pdev->gtt.mm_alloc_page_table = gen8_mm_alloc_page_table;
+		pdev->gtt.mm_free_page_table = gen8_mm_free_page_table;
+
+		if (preallocated_shadow_pages == -1)
+			preallocated_shadow_pages = 8192;
+	} else {
+		vgt_err("Unsupported platform.\n");
+		return false;
+	}
+
+	return true;
+}
+
+static bool vgt_initialize_pgt_device(struct pci_dev *dev, struct pgt_device *pdev)
+{
+	int i;
+
+	pdev->pdev = dev;
+	pdev->pbus = dev->bus;
+
+	if (!vgt_initialize_device_info(pdev)) {
+		vgt_err("failed to initalize device info.\n");
+		return false;
+	}
+
+	if (!vgt_initialize_platform(pdev)) {
+		vgt_err("failed to initialize platform\n");
+		return false;
+	}
+
+	INIT_LIST_HEAD(&pdev->rendering_runq_head);
+	INIT_LIST_HEAD(&pdev->rendering_idleq_head);
+
+	bitmap_zero(pdev->dpy_emul_request, VGT_MAX_VMS);
+
+	/* initialize ports */
+	memset(pdev->ports, 0, sizeof(struct gt_port) * I915_MAX_PORTS);
+	for (i = 0; i < I915_MAX_PORTS; i ++) {
+		pdev->ports[i].type = VGT_PORT_MAX;
+		pdev->ports[i].cache.type = VGT_PORT_MAX;
+		pdev->ports[i].port_override = i;
+		pdev->ports[i].physcal_port = i;
+	}
+
+	if (!initial_phys_states(pdev)) {
+		printk("vGT: failed to initialize physical state\n");
+		return false;
+	}
+
+	pdev->reg_info = vzalloc (pdev->reg_num * sizeof(reg_info_t));
+	if (!pdev->reg_info) {
+		printk("vGT: failed to allocate reg_info\n");
+		return false;
+	}
+
+	initialize_gm_fence_allocation_bitmaps(pdev);
+
+	vgt_setup_reg_info(pdev);
+	vgt_post_setup_mmio_hooks(pdev);
+	if (vgt_irq_init(pdev) != 0) {
+		printk("vGT: failed to initialize irq\n");
+		return false;
+	}
+
+	vgt_init_reserved_aperture(pdev);
+
+	for (i = 0; i < pdev->max_engines; i++)
+		vgt_ring_init(pdev, i);
+
+	perf_pgt = pdev;
+	return true;
+}
+
+/*
+ * Initialize the vgt driver.
+ *  return 0: success
+ *	-1: error
+ */
+static int vgt_initialize(struct pci_dev *dev)
+{
+	struct pgt_device *pdev = &default_device;
+	struct task_struct *p_thread;
+	vgt_params_t vp;
+
+	spin_lock_init(&pdev->lock);
+
+	if (!vgt_initialize_pgt_device(dev, pdev))
+		return -EINVAL;
+
+	if (vgt_cmd_parser_init(pdev) < 0)
+		goto err;
+
+	mutex_init(&pdev->hpd_work.hpd_mutex);
+	INIT_WORK(&pdev->hpd_work.work, vgt_hotplug_udev_notify_func);
+	
+	/* create debugfs interface */
+	if (!vgt_init_debugfs(pdev)) {
+		printk("vGT:failed to create debugfs\n");
+		goto err;
+	}
+
+	/* init all mmio_device */
+	vgt_init_mmio_device(pdev);
+
+	/* create domain 0 instance */
+	vp.vm_id = 0;
+	vp.aperture_sz = dom0_low_gm_sz;
+	vp.gm_sz = dom0_low_gm_sz + dom0_high_gm_sz;
+	vp.fence_sz = dom0_fence_sz;
+	vp.vgt_primary = 1; /* this isn't actually used for dom0 */
+	if (create_vgt_instance(pdev, &vgt_dom0, vp) < 0)
+		goto err;
+
+	reset_cached_interrupt_registers(pdev);
+
+	vgt_dbg(VGT_DBG_GENERIC, "create dom0 instance succeeds\n");
+
+	//show_mode_settings(pdev);
+
+	if (setup_gtt(pdev))
+		goto err;
+
+	vgt_ops = &vgt_xops;
+	vgt_ops->initialized = true;
+
+	if (!hvm_render_owner)
+		current_render_owner(pdev) = vgt_dom0;
+	else
+		vgt_ctx_switch = 0;
+
+	current_foreground_vm(pdev) = vgt_dom0;
+	if (!hvm_display_owner) {
+		current_display_owner(pdev) = vgt_dom0;
+	}
+
+	if (hvm_super_owner) {
+		ASSERT(hvm_render_owner);
+		ASSERT(hvm_display_owner);
+		ASSERT(hvm_boot_foreground);
+	} else {
+		current_config_owner(pdev) = vgt_dom0;
+	}
+
+	pdev->ctx_check = 0;
+	pdev->ctx_switch = 0;
+	pdev->magic = 0;
+
+	init_waitqueue_head(&pdev->event_wq);
+	init_waitqueue_head(&pdev->destroy_wq);
+
+	pdev->device_reset_flags = 0;
+
+	p_thread = kthread_run(vgt_thread, pdev, "vgt_main");
+	if (!p_thread) {
+		goto err;
+	}
+	pdev->p_thread = p_thread;
+	//show_debug(pdev, 0);
+
+	vgt_render_init(pdev);
+
+	vgt_initialize_ctx_scheduler(pdev);
+
+	list_add(&pdev->list, &pgt_devices);
+
+	vgt_init_sysfs(pdev);
+
+	vgt_init_fb_notify();
+
+	printk("vgt_initialize succeeds.\n");
+
+	/* FIXME
+	 * always enable forcewake. It was found that forcewake
+	 * operation is one of the stability issue for running
+	 * windows guest. Before having a decent fix, we will
+	 * always enable force wake for Broadwell.
+	 */
+	if (IS_BDW(pdev))
+		vgt_force_wake_get();
+
+	return 0;
+err:
+	printk("vgt_initialize failed.\n");
+	vgt_destroy();
+	return -1;
+}
+
+void vgt_destroy(void)
+{
+	struct list_head *pos, *next;
+	struct vgt_device *vgt;
+	struct pgt_device *pdev = &default_device;
+	int i;
+
+	vgt_cleanup_mmio_dev(pdev);
+
+	perf_pgt = NULL;
+	list_del(&pdev->list);
+
+	vgt_cleanup_ctx_scheduler(pdev);
+
+	/* do we need the thread actually stopped? */
+	kthread_stop(pdev->p_thread);
+
+	vgt_irq_exit(pdev);
+
+	/* Deactive all VGTs */
+	while ( !list_empty(&pdev->rendering_runq_head) ) {
+		list_for_each (pos, &pdev->rendering_runq_head) {
+			vgt = list_entry (pos, struct vgt_device, list);
+			vgt_disable_render(vgt);
+		}
+	};
+
+	/* Destruct all vgt_debugfs */
+	vgt_release_debugfs();
+
+	vgt_destroy_sysfs();
+
+	if (pdev->saved_gtt)
+		vfree(pdev->saved_gtt);
+	free_gtt(pdev);
+
+	if (pdev->gmadr_va)
+		iounmap(pdev->gmadr_va);
+	if (pdev->opregion_va)
+		iounmap(pdev->opregion_va);
+
+	while ( !list_empty(&pdev->rendering_idleq_head)) {
+		for (pos = pdev->rendering_idleq_head.next;
+			pos != &pdev->rendering_idleq_head; pos = next) {
+			next = pos->next;
+			vgt = list_entry (pos, struct vgt_device, list);
+			vgt_release_instance(vgt);
+		}
+	}
+	vgt_clear_mmio_table();
+	vfree(pdev->reg_info);
+	vfree(pdev->initial_mmio_state);
+
+	for (i = 0; i < I915_MAX_PORTS; ++ i) {
+		if (pdev->ports[i].edid) {
+			kfree(pdev->ports[i].edid);
+			pdev->ports[i].edid = NULL;
+		}
+
+		if (pdev->ports[i].dpcd) {
+			kfree(pdev->ports[i].dpcd);
+			pdev->ports[i].dpcd = NULL;
+		}
+
+		if (pdev->ports[i].cache.edid) {
+			kfree(pdev->ports[i].cache.edid);
+			pdev->ports[i].cache.edid = NULL;
+		}
+	}
+
+	vgt_cmd_parser_exit();
+}
+
+int vgt_suspend(struct pci_dev *pdev)
+{
+	struct pgt_device *node, *pgt = NULL;
+
+	if (!hypervisor_check_host() || !vgt_enabled)
+		return 0;
+
+	if (list_empty(&pgt_devices)) {
+		printk("vGT: no valid pgt_device registered at suspend\n");
+		return 0;
+	}
+
+	list_for_each_entry(node, &pgt_devices, list) {
+		if (node->pdev == pdev) {
+			pgt = node;
+			break;
+		}
+	}
+
+	if (!pgt) {
+		printk("vGT: no matching pgt_device at suspend\n");
+		return 0;
+	}
+
+	vgt_host_irq_sync();
+	vgt_info("Suspending vGT driver...\n");
+
+	/* TODO: check vGT instance state */
+	/* ... */
+
+	pgt->saved_rrmr = VGT_MMIO_READ(pgt, _REG_DE_RRMR);
+	pgt->saved_shotplug_ctl = VGT_MMIO_READ(pgt, _REG_SHOTPLUG_CTL);
+
+	/* save GTT and FENCE information */
+	vgt_save_gtt_and_fence(pgt);
+
+	vgt_reset_ppgtt(vgt_dom0, 0xff);
+
+	return 0;
+}
+
+int vgt_resume(struct pci_dev *pdev)
+{
+	struct pgt_device *node, *pgt = NULL;
+
+	if (!hypervisor_check_host() || !vgt_enabled)
+		return 0;
+
+
+	if (list_empty(&pgt_devices)) {
+		printk("vGT: no valid pgt_device registered at resume\n");
+		return 0;
+	}
+
+	list_for_each_entry(node, &pgt_devices, list) {
+		if (node->pdev == pdev) {
+			pgt = node;
+			break;
+		}
+	}
+
+	if (!pgt) {
+		printk("vGT: no matching pgt_device at resume\n");
+		return 0;
+	}
+
+	vgt_info("Resuming vGT driver...\n");
+
+	/* restore GTT table and FENCE regs */
+	vgt_restore_gtt_and_fence(pgt);
+
+	VGT_MMIO_WRITE(pgt, _REG_DE_RRMR, pgt->saved_rrmr);
+	VGT_MMIO_WRITE(pgt, _REG_SHOTPLUG_CTL, pgt->saved_shotplug_ctl);
+
+	/* redo the MMIO snapshot */
+	vgt_initial_mmio_setup(pgt);
+
+	/* XXX: need redo the PCI config space snapshot too? */
+
+	/*
+	 * TODO: need a better place to sync vmmio state
+	 * for now, force override dom0's vmmio only. other
+	 * VMs are supposed to be paused.
+	 */
+	state_sreg_init(vgt_dom0);
+	state_vreg_init(vgt_dom0);
+
+	/* TODO, GMBUS inuse bit? */
+
+	spin_lock(&pgt->lock);
+
+	recalculate_and_update_imr(pgt, _REG_DEIMR);
+	recalculate_and_update_imr(pgt, _REG_GTIMR);
+	recalculate_and_update_imr(pgt, _REG_PMIMR);
+	recalculate_and_update_imr(pgt, _REG_SDEIMR);
+
+	recalculate_and_update_imr(pgt, _REG_RCS_IMR);
+	recalculate_and_update_imr(pgt, _REG_BCS_IMR);
+	recalculate_and_update_imr(pgt, _REG_VCS_IMR);
+
+	if (IS_HSW(pgt))
+		recalculate_and_update_imr(pgt, _REG_VECS_IMR);
+
+	recalculate_and_update_ier(pgt, _REG_GTIER);
+	recalculate_and_update_ier(pgt, _REG_PMIER);
+	recalculate_and_update_ier(pgt, _REG_SDEIER);
+
+	if (pgt->enable_execlist) {
+		enum vgt_ring_id ring_id;
+		for (ring_id = 0; ring_id < MAX_ENGINES; ++ ring_id)
+			reset_el_structure(pgt, ring_id);
+	}
+
+	spin_unlock(&pgt->lock);
+
+	return 0;
+}
+
+static void do_device_reset(struct pgt_device *pdev)
+{
+	struct drm_device *drm_dev = pci_get_drvdata(pdev->pdev);
+	vgt_reg_t head, tail, start, ctl;
+	vgt_reg_t ier, imr, iir, isr;
+	int i;
+
+	vgt_info("Request DOM0 to reset device.\n");
+
+	ASSERT(drm_dev);
+
+	set_bit(WAIT_RESET, &vgt_dom0->reset_flags);
+
+	i915_handle_error(drm_dev, true, "VGT device reset");
+
+	i915_wait_error_work_complete(drm_dev);
+
+	/*
+	 * User may set i915.reset=0 in kernel command line, which will
+	 * disable the reset logic of i915, without that logics we can
+	 * do nothing, so we panic here and let user remove that parameters.
+	 */
+	if (test_bit(WAIT_RESET, &vgt_dom0->reset_flags)) {
+		vgt_err("DOM0 GPU reset didn't happen?.\n");
+		vgt_err("Maybe you set i915.reset=0 in kernel command line? Panic the system.\n");
+		ASSERT(0);
+	}
+
+	vgt_info("GPU ring status:\n");
+
+	for (i = 0; i < pdev->max_engines; i++) {
+		head = VGT_READ_HEAD(pdev, i);
+		tail = VGT_READ_TAIL(pdev, i);
+		start = VGT_READ_START(pdev, i);
+		ctl = VGT_READ_CTL(pdev, i);
+
+		vgt_info("RING %d: H: %x T: %x S: %x C: %x.\n",
+				i, head, tail, start, ctl);
+
+		if (pdev->enable_execlist)
+			reset_el_structure(pdev, i);
+	}
+
+	ier = VGT_MMIO_READ(pdev, _REG_DEIER);
+	iir = VGT_MMIO_READ(pdev, _REG_DEIIR);
+	imr = VGT_MMIO_READ(pdev, _REG_DEIMR);
+	isr = VGT_MMIO_READ(pdev, _REG_DEISR);
+
+	vgt_info("DE: ier: %x iir: %x imr: %x isr: %x.\n",
+			ier, iir, imr, isr);
+
+	vgt_info("Finish.\n");
+
+	return;
+}
+
+bool vgt_handle_dom0_device_reset(void)
+{
+	struct pgt_device *pdev = &default_device;
+	struct drm_device *drm_dev;
+
+	unsigned long flags;
+	int cpu;
+
+	int id;
+	bool rc;
+
+	if (!hypervisor_check_host() || !vgt_enabled)
+		return false;
+
+	vgt_info("DOM0 hangcheck timer request reset device.\n");
+
+	drm_dev = pci_get_drvdata(pdev->pdev);
+	ASSERT(drm_dev);
+
+	vgt_lock_dev_flags(pdev, cpu, flags);
+	rc = idle_rendering_engines(pdev, &id);
+	vgt_unlock_dev_flags(pdev, cpu, flags);
+
+	if (!rc) {
+		vgt_info("Really hung, request to reset device.\n");
+		vgt_raise_request(pdev, VGT_REQUEST_DEVICE_RESET);
+	} else {
+		vgt_info("Not really hung, continue DOM0 reset sequence.\n");
+		i915_handle_error(drm_dev, true, "VGT DOM0 device reset");
+	}
+
+	return true;
+}
+
+int vgt_reset_device(struct pgt_device *pdev)
+{
+	struct vgt_device *vgt;
+	struct list_head *pos, *n;
+	unsigned long ier;
+	unsigned long flags;
+	int i;
+
+	if (get_seconds() - vgt_dom0->last_reset_time < 6) {
+		vgt_err("Try to reset device too fast.\n");
+		return -EAGAIN;
+	}
+
+	if (test_and_set_bit(RESET_INPROGRESS,
+				&pdev->device_reset_flags)) {
+		vgt_err("Another device reset has been already running.\n");
+		return -EBUSY;
+	}
+
+	vgt_info("Stop VGT context switch.\n");
+
+	vgt_cleanup_ctx_scheduler(pdev);
+
+	current_render_owner(pdev) = vgt_dom0;
+
+	current_foreground_vm(pdev) = vgt_dom0;
+
+	spin_lock_irqsave(&pdev->lock, flags);
+
+	list_for_each_safe(pos, n, &pdev->rendering_runq_head) {
+		vgt = list_entry(pos, struct vgt_device, list);
+
+		if (vgt->vm_id) {
+			for (i = 0; i < pdev->max_engines; i++) {
+				if (test_bit(i, (void *)vgt->enabled_rings)) {
+					vgt_info("VM %d: disable ring %d\n", vgt->vm_id, i);
+
+					vgt_disable_ring(vgt, i);
+
+					set_bit(i, &vgt->enabled_rings_before_reset);
+				}
+			}
+
+			set_bit(WAIT_RESET, &vgt->reset_flags);
+		}
+	}
+
+	spin_unlock_irqrestore(&pdev->lock, flags);
+
+	vgt_info("Disable master interrupt.\n");
+
+	vgt_get_irq_lock(pdev, flags);
+
+	VGT_MMIO_WRITE(pdev, _REG_DEIER,
+			VGT_MMIO_READ(pdev, _REG_DEIER) & ~_REGBIT_MASTER_INTERRUPT);
+
+	vgt_put_irq_lock(pdev, flags);
+
+	do_device_reset(pdev);
+
+	vgt_info("Restart VGT context switch.\n");
+
+	vgt_initialize_ctx_scheduler(pdev);
+
+	clear_bit(RESET_INPROGRESS, &pdev->device_reset_flags);
+
+	spin_lock_irqsave(&pdev->lock, flags);
+	vgt_get_irq_lock(pdev, flags);
+
+	reset_cached_interrupt_registers(pdev);
+
+	ier = vgt_recalculate_ier(pdev, _REG_DEIER);
+	VGT_MMIO_WRITE(pdev, _REG_DEIER, ier);
+
+	vgt_put_irq_lock(pdev, flags);
+
+	spin_unlock_irqrestore(&pdev->lock, flags);
+
+	vgt_info("Enable master interrupt, DEIER: %lx\n", ier);
+
+	return 0;
+}
+
+bool vgt_check_host(void)
+{
+	if (!vgt_enabled)
+		return false;
+
+	if (!vgt_pkdm)
+		return false;
+
+	if (!hypervisor_check_host())
+		return false;
+
+	return true;
+}
+
+bool i915_start_vgt(struct pci_dev *pdev)
+{
+	if (!vgt_check_host())
+		return false;
+
+	if (vgt_xops.initialized) {
+		vgt_info("VGT has been intialized?\n");
+		return false;
+	}
+
+	return vgt_initialize(pdev) == 0;
+}
+
+static void vgt_param_check(void)
+{
+	/* TODO: hvm_display/render_owner are broken */
+	if (hvm_super_owner) {
+		hvm_display_owner = true;
+		hvm_render_owner = true;
+		hvm_boot_foreground = true;
+	}
+
+	if (hvm_display_owner) {
+		hvm_boot_foreground = true;
+	}
+
+	if (hvm_dpy_owner) {
+		vgt_warn("hvm_dpy_owner is deprecated option! "
+			 "Please use hvm_boot_foreground or hvm_display_owner instead!\n");
+	}
+
+	/* see the comment where dom0_low_gm_sz is defined */
+	if (dom0_low_gm_sz > 512 - 64)
+		dom0_low_gm_sz = 512 - 64;
+
+	if (dom0_low_gm_sz + dom0_high_gm_sz > 2048)
+		dom0_high_gm_sz = 2048 - dom0_low_gm_sz;
+
+	if (dom0_fence_sz > VGT_MAX_NUM_FENCES)
+		dom0_fence_sz = VGT_MAX_NUM_FENCES;
+}
+
+static int __init vgt_init_module(void)
+{
+	if (!hypervisor_check_host())
+		return 0;
+
+	vgt_param_check();
+
+	vgt_klog_init();
+
+	return 0;
+}
+module_init(vgt_init_module);
+
+static void __exit vgt_exit_module(void)
+{
+	if (!hypervisor_check_host())
+		return;
+
+	// fill other exit works here
+	vgt_destroy();
+	vgt_klog_cleanup();
+	return;
+}
+module_exit(vgt_exit_module);
diff --git a/drivers/gpu/drm/i915/vgt/vgt.h b/drivers/gpu/drm/i915/vgt/vgt.h
new file mode 100644
index 0000000..0b688c7
--- /dev/null
+++ b/drivers/gpu/drm/i915/vgt/vgt.h
@@ -0,0 +1,3077 @@
+/*
+ * vGT core headers
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef _VGT_DRV_H_
+#define _VGT_DRV_H_
+
+#include <linux/hrtimer.h>
+#include <linux/interrupt.h>
+#include <linux/sched.h>
+#include <linux/wait.h>
+#include <linux/semaphore.h>
+#include <linux/cdev.h>
+#include <linux/hashtable.h>
+#include <linux/pci.h>
+#include <linux/mempool.h>
+#include <drm/drmP.h>
+
+#include "vgt-if.h"
+#include "host.h"
+
+typedef uint32_t vgt_reg_t;
+
+#include "reg.h"
+#include "devtable.h"
+#include "edid.h"
+#include "cmd_parser.h"
+#include "hypercall.h"
+#include "execlists.h"
+
+struct pgt_device;
+struct vgt_device;
+extern struct vgt_device *vgt_dom0;
+extern struct pgt_device *perf_pgt;
+extern struct list_head pgt_devices;
+extern struct pgt_device default_device;
+extern void show_ring_buffer(struct pgt_device *pdev, int ring_id, int bytes);
+extern void show_mode_settings(struct pgt_device *pdev);
+extern void show_ring_debug(struct pgt_device *pdev, int ring_id);
+extern void show_debug(struct pgt_device *pdev);
+void show_virtual_interrupt_regs(struct vgt_device *vgt, struct seq_file *seq);
+extern void show_interrupt_regs(struct pgt_device *pdev, struct seq_file *seq);
+
+extern bool ignore_hvm_forcewake_req;
+extern bool hvm_render_owner;
+extern bool hvm_display_owner;
+extern bool hvm_super_owner;
+extern bool hvm_boot_foreground;
+extern bool vgt_primary;
+extern bool vgt_track_nest;
+extern bool vgt_delay_nest;
+extern int vgt_debug;
+extern bool vgt_enabled;
+extern bool fastpath_dpy_switch;
+extern bool shadow_tail_based_qos;
+extern bool event_based_qos;
+extern int enable_video_switch;
+extern int dom0_low_gm_sz;
+extern int dom0_high_gm_sz;
+extern int dom0_fence_sz;
+extern int bypass_scan_mask;
+extern bool bypass_dom0_addr_check;
+extern bool render_engine_reset;
+extern bool enable_panel_fitting;
+extern bool enable_reset;
+extern bool vgt_lock_irq;
+extern int shadow_execlist_context;
+extern bool wp_submitted_ctx;
+extern bool propagate_monitor_to_guest;
+extern bool irq_based_ctx_switch;
+extern int preallocated_shadow_pages;
+
+enum vgt_event_type {
+	// GT
+	RCS_MI_USER_INTERRUPT = 0,
+	RCS_DEBUG,
+	RCS_MMIO_SYNC_FLUSH,
+	RCS_CMD_STREAMER_ERR,
+	RCS_PIPE_CONTROL,
+	RCS_L3_PARITY_ERR,		/* IVB */
+	RCS_WATCHDOG_EXCEEDED,
+	RCS_PAGE_DIRECTORY_FAULT,
+	RCS_AS_CONTEXT_SWITCH,
+	RCS_MONITOR_BUFF_HALF_FULL,	/* IVB */
+
+	VCS_MI_USER_INTERRUPT,
+	VCS_MMIO_SYNC_FLUSH,
+	VCS_CMD_STREAMER_ERR,
+	VCS_MI_FLUSH_DW,
+	VCS_WATCHDOG_EXCEEDED,
+	VCS_PAGE_DIRECTORY_FAULT,
+	VCS_AS_CONTEXT_SWITCH,
+
+	VCS2_MI_USER_INTERRUPT,
+	VCS2_MI_FLUSH_DW,
+	VCS2_AS_CONTEXT_SWITCH,
+
+	BCS_MI_USER_INTERRUPT,
+	BCS_MMIO_SYNC_FLUSH,
+	BCS_CMD_STREAMER_ERR,
+	BCS_MI_FLUSH_DW,
+	BCS_PAGE_DIRECTORY_FAULT,
+	BCS_AS_CONTEXT_SWITCH,
+
+	VECS_MI_USER_INTERRUPT,
+	VECS_MI_FLUSH_DW,
+	VECS_AS_CONTEXT_SWITCH,
+
+	// DISPLAY
+	PIPE_A_FIFO_UNDERRUN,	/* This is an active high level for the duration of the Pipe A FIFO underrun */
+	PIPE_B_FIFO_UNDERRUN,	/* This is an active high level for the duration of the Pipe B FIFO underrun */
+	PIPE_A_CRC_ERR,	/* This is an active high pulse on the Pipe A CRC error */
+	PIPE_B_CRC_ERR,	/* This is an active high pulse on the Pipe B CRC error */
+	PIPE_A_CRC_DONE,	/* This is an active high pulse on the Pipe A CRC done */
+	PIPE_B_CRC_DONE,	/* This is an active high pulse on the Pipe B CRC done */
+	PIPE_A_ODD_FIELD,	/* This is an active high level for the duration of the Pipe A interlaced odd field */
+	PIPE_B_ODD_FIELD,	/* This is an active high level for the duration of the Pipe B interlaced odd field */
+	PIPE_A_EVEN_FIELD,	/* This is an active high level for the duration of the Pipe A interlaced even field */
+	PIPE_B_EVEN_FIELD,	/* This is an active high level for the duration of the Pipe B interlaced even field */
+	PIPE_A_LINE_COMPARE,	/* This is an active high level for the duration of the selected Pipe A scan lines */
+	PIPE_B_LINE_COMPARE,	/* This is an active high level for the duration of the selected Pipe B scan lines */
+	PIPE_C_LINE_COMPARE,	/* This is an active high level for the duration of the selected Pipe C scan lines */
+	PIPE_A_VBLANK,	/* This is an active high level for the duration of the Pipe A vertical blank */
+	PIPE_B_VBLANK,	/* This is an active high level for the duration of the Pipe B vertical blank */
+	PIPE_C_VBLANK,	/* This is an active high level for the duration of the Pipe C vertical blank */
+	PIPE_A_VSYNC,	/* This is an active high level for the duration of the Pipe A vertical sync */
+	PIPE_B_VSYNC,	/* This is an active high level for the duration of the Pipe B vertical sync */
+	PIPE_C_VSYNC,	/* This is an active high level for the duration of the Pipe C vertical sync */
+	PRIMARY_A_FLIP_DONE,	/* This is an active high pulse when a primary plane A flip is done */
+	PRIMARY_B_FLIP_DONE,	/* This is an active high pulse when a primary plane B flip is done */
+	PRIMARY_C_FLIP_DONE,	/* This is an active high pulse when a primary plane C flip is done */
+	SPRITE_A_FLIP_DONE,	/* This is an active high pulse when a sprite plane A flip is done */
+	SPRITE_B_FLIP_DONE,	/* This is an active high pulse when a sprite plane B flip is done */
+	SPRITE_C_FLIP_DONE,	/* This is an active high pulse when a sprite plane C flip is done */
+
+	DPST_PHASE_IN,	// This is an active high pulse on the DPST phase in event
+	DPST_HISTOGRAM,	// This is an active high pulse on the AUX A done event.
+	GSE,
+	DP_A_HOTPLUG,
+	AUX_CHANNEL_A,	// This is an active high pulse on the AUX A done event.
+	PERF_COUNTER,	// This is an active high pulse when the performance counter reaches the threshold value programmed in the Performance Counter Source register
+	POISON,		// This is an active high pulse on receiving the poison message
+	GTT_FAULT,	// This is an active high level while either of the GTT Fault Status register bits are set
+	ERROR_INTERRUPT_COMBINED,
+
+	// PM
+	GV_DOWN_INTERVAL,
+	GV_UP_INTERVAL,
+	RP_DOWN_THRESHOLD,
+	RP_UP_THRESHOLD,
+	FREQ_DOWNWARD_TIMEOUT_RC6,
+	PCU_THERMAL,
+	PCU_PCODE2DRIVER_MAILBOX,
+
+	// PCH
+	FDI_RX_INTERRUPTS_TRANSCODER_A,	// This is an active high level while any of the FDI_RX_ISR bits are set for transcoder A
+	AUDIO_CP_CHANGE_TRANSCODER_A,	// This is an active high level while any of the FDI_RX_ISR bits are set for transcoder A
+	AUDIO_CP_REQUEST_TRANSCODER_A,	// This is an active high level indicating content protection is requested by audio azalia verb programming for transcoder A
+	FDI_RX_INTERRUPTS_TRANSCODER_B,
+	AUDIO_CP_CHANGE_TRANSCODER_B,
+	AUDIO_CP_REQUEST_TRANSCODER_B,
+	FDI_RX_INTERRUPTS_TRANSCODER_C,
+	AUDIO_CP_CHANGE_TRANSCODER_C,
+	AUDIO_CP_REQUEST_TRANSCODER_C,
+	ERR_AND_DBG,
+	GMBUS,
+	SDVO_B_HOTPLUG,
+	CRT_HOTPLUG,
+	DP_B_HOTPLUG,
+	DP_C_HOTPLUG,
+	DP_D_HOTPLUG,
+	AUX_CHENNEL_B,
+	AUX_CHENNEL_C,
+	AUX_CHENNEL_D,
+	AUDIO_POWER_STATE_CHANGE_B,
+	AUDIO_POWER_STATE_CHANGE_C,
+	AUDIO_POWER_STATE_CHANGE_D,
+
+	RCS_IRQ,
+	BCS_IRQ,
+	VCS_IRQ,
+	VCS2_IRQ,
+	PM_IRQ,
+	VECS_IRQ,
+	DE_PIPE_A_IRQ,
+	DE_PIPE_B_IRQ,
+	DE_PIPE_C_IRQ,
+	DE_PORT_IRQ,
+	DE_MISC_IRQ,
+	PCH_IRQ,
+	PCU_IRQ,
+
+	EVENT_RESERVED,
+	EVENT_MAX,
+};
+
+
+enum transcoder {
+	TRANSCODER_A = 0,
+	TRANSCODER_B,
+	TRANSCODER_C,
+	TRANSCODER_EDP = 0xF,
+};
+
+#define vgt_dbg(component, fmt, s...)	\
+	do { if (vgt_debug & component) printk(KERN_DEBUG "vGT debug:(%s:%d) " fmt, __FUNCTION__, __LINE__, ##s); } while (0)
+
+#define VGT_DBG_GENERIC		(1<<0)
+#define VGT_DBG_DPY		(1<<1)
+#define VGT_DBG_MEM		(1<<2)
+#define VGT_DBG_RENDER		(1<<3)
+#define VGT_DBG_CMD		(1<<4)
+#define VGT_DBG_IRQ		(1<<5)
+#define VGT_DBG_EDID		(1<<6)
+#define VGT_DBG_EXECLIST	(1<<7)
+#define VGT_DBG_ALL		(0xffff)
+
+/*
+ * Define registers of a ring buffer per hardware register layout.
+ */
+typedef struct {
+	vgt_reg_t tail;
+	vgt_reg_t head;
+	vgt_reg_t start;
+	vgt_reg_t ctl;
+} vgt_ringbuffer_t;
+
+#define SIZE_1KB		(1024UL)
+#define SIZE_1MB		(1024UL*1024UL)
+#define SIZE_PAGE		(4 * SIZE_1KB)
+
+#define VGT_RSVD_RING_SIZE	(16 * SIZE_1KB)
+struct vgt_rsvd_ring {
+	struct pgt_device *pdev;
+	void *virtual_start;
+	int start;
+	uint64_t null_context;
+	uint64_t indirect_state;
+	int id;
+
+	u32 head;
+	u32 tail;
+	int size;
+	/* whether the engine requires special context switch */
+	bool	stateless;
+	/* whether the engine requires context switch */
+	bool	need_switch;
+	/* whether the engine end with user interrupt instruction */
+	bool	need_irq;
+	/* memory offset of the user interrupt instruction */
+	u32	ip_offset;
+};
+
+#define _tail_reg_(ring_reg_off)	\
+		(ring_reg_off & ~(sizeof(vgt_ringbuffer_t)-1))
+
+#define _vgt_mmio_va(pdev, x)		((uint64_t)((char*)pdev->gttmmio_base_va+x))	/* PA to VA */
+#define _vgt_mmio_pa(pdev, x)		(pdev->gttmmio_base+x)			/* PA to VA */
+
+#define VGT_RING_TIMEOUT	500	/* in ms */
+#define VGT_VBLANK_TIMEOUT	50	/* in ms */
+
+/* Maximum VMs supported by vGT. Actual number is device specific */
+#define VGT_MAX_VMS			4
+#define VGT_RSVD_APERTURE_SZ		(32*SIZE_1MB)	/* reserve 8MB for vGT itself */
+
+#define GTT_PAGE_SHIFT		12
+#define GTT_PAGE_SIZE		(1UL << GTT_PAGE_SHIFT)
+#define GTT_PAGE_MASK		(~(GTT_PAGE_SIZE-1))
+#define GTT_PAE_MASK		((1UL <<12) - (1UL << 4)) /* bit 11:4 */
+
+/*
+ * The maximum GM size supported by VGT GM resource allocator.
+ */
+#define VGT_MAX_GM_SIZE			(1UL << 32)
+#define VGT_GM_BITMAP_BITS		(VGT_MAX_GM_SIZE/SIZE_1MB)
+#define VGT_MAX_NUM_FENCES		32
+#define VGT_FENCE_BITMAP_BITS	VGT_MAX_NUM_FENCES
+#define VGT_FENCE_REGION_SIZE	(VGT_MAX_NUM_FENCES*8)
+#define VGT_RSVD_APERTURE_BITMAP_BITS (VGT_RSVD_APERTURE_SZ / GTT_PAGE_SIZE)
+#define VGT_APERTURE_PAGES	(VGT_RSVD_APERTURE_SZ >> GTT_PAGE_SHIFT)
+
+//#define SZ_CONTEXT_AREA_PER_RING	4096
+#define SZ_CONTEXT_AREA_PER_RING	(4096*64)	/* use 256 KB for now */
+#define SZ_INDIRECT_STATE		(4096)		/* use 4KB for now */
+#define VGT_APERTURE_PER_INSTANCE_SZ		(4*SIZE_1KB)	/* 4KB per instance (?) */
+#define VGT_ID_ALLOC_BITMAP		((1UL << VGT_MAX_VMS) - 1)
+
+#define REG_SIZE			sizeof(vgt_reg_t)		/* size of gReg/sReg[0] */
+#define REG_INDEX(reg)		((reg) / REG_SIZE)
+#define VGT_MMIO_SPACE_SZ	(2*SIZE_1MB)
+#define VGT_CFG_SPACE_SZ	256
+#define VGT_BAR_NUM		4
+typedef struct {
+	uint64_t	mmio_base_gpa;	/* base guest physical address of the MMIO registers */
+	vgt_reg_t	*vReg;		/* guest view of the register state */
+	vgt_reg_t	*sReg;		/* Shadow (used by hardware) state of the register */
+	uint8_t	cfg_space[VGT_CFG_SPACE_SZ];
+	bool	bar_mapped[VGT_BAR_NUM];
+	uint64_t	gt_mmio_base;	/* bar0/GTTMMIO */
+	uint64_t	aperture_base;	/* bar1: guest aperture base */
+//	uint64_t	gt_gmadr_base;	/* bar1/GMADR */
+
+	uint32_t	bar_size[VGT_BAR_NUM];	/* 0: GTTMMIO, 1: GMADR, 2: PIO bar size */
+
+	/* OpRegion state */
+	void		*opregion_va;
+	uint64_t	opregion_gfn[VGT_OPREGION_PAGES];
+} vgt_state_t;
+
+typedef struct {
+	vgt_reg_t base;
+	vgt_reg_t cache_ctl;
+	vgt_reg_t mode;
+} vgt_ring_ppgtt_t;
+
+#define __vreg(vgt, off) (*(vgt_reg_t *)((char *)vgt->state.vReg + off))
+#define __vreg8(vgt, off) (*(char *)((char *)vgt->state.vReg + off))
+#define __vreg16(vgt, off) (*(uint16_t *)((char *)vgt->state.vReg + off))
+#define __sreg(vgt, off) (*(vgt_reg_t *)((char *)vgt->state.sReg + off))
+#define __sreg8(vgt, off) (*(char *)((char *)vgt->state.sReg + off))
+#define __vreg64(vgt, off) (*(unsigned long *)((char *)vgt->state.vReg + off))
+#define __sreg64(vgt, off) (*(unsigned long *)((char *)vgt->state.sReg + off))
+#define vgt_vreg(vgt, off)	((vgt_reg_t *)((char *)vgt->state.vReg + off))
+#define vgt_sreg(vgt, off)	((vgt_reg_t *)((char *)vgt->state.sReg + off))
+
+#define RB_DWORDS_TO_SAVE	32
+typedef	uint32_t	rb_dword;
+
+struct execlist_context;
+enum EL_SLOT_STATUS {
+	EL_EMPTY	= 0,
+	EL_PENDING,
+	EL_SUBMITTED
+};
+
+struct vgt_exec_list {
+	enum EL_SLOT_STATUS status;
+	struct execlist_context *el_ctxs[2];
+};
+
+struct vgt_elsp_store {
+	uint32_t count;
+	uint32_t element[4];
+};
+
+#define EL_QUEUE_SLOT_NUM 3
+
+struct vgt_mm;
+
+typedef struct {
+	vgt_ringbuffer_t	vring;		/* guest view ring */
+	vgt_ringbuffer_t	sring;		/* shadow ring */
+	/* In aperture, partitioned & 4KB aligned. */
+	/* 64KB alignment requirement for walkaround. */
+	uint64_t	context_save_area;	/* VGT default context space */
+	uint32_t	active_vm_context;
+	/* ppgtt info */
+	vgt_ring_ppgtt_t	vring_ppgtt_info; /* guest view */
+	vgt_ring_ppgtt_t	sring_ppgtt_info; /* shadow info */
+	u8 has_ppgtt_base_set : 1;	/* Is PP dir base set? */
+	u8 has_ppgtt_mode_enabled : 1;	/* Is ring's mode reg PPGTT enable set? */
+	u8 has_execlist_enabled : 1;
+	struct vgt_mm *active_ppgtt_mm;
+	int ppgtt_root_pointer_type;
+	int ppgtt_page_table_level;
+
+	struct cmd_general_info	patch_list;
+	struct cmd_general_info	handler_list;
+	struct cmd_general_info	tail_list;
+
+	uint64_t cmd_nr;
+	vgt_reg_t	last_scan_head;
+	uint64_t request_id;
+
+	vgt_reg_t uhptr;
+	uint64_t uhptr_id;
+	int el_slots_head;
+	int el_slots_tail;
+	struct vgt_exec_list execlist_slots[EL_QUEUE_SLOT_NUM];
+	struct vgt_elsp_store elsp_store;
+	int csb_write_ptr;
+	bool check_uninitialized_context;
+} vgt_state_ring_t;
+
+#define vgt_el_queue_head(vgt, ring_id) \
+	((vgt)->rb[ring_id].el_slots_head)
+#define vgt_el_queue_tail(vgt, ring_id) \
+	((vgt)->rb[ring_id].el_slots_tail)
+#define vgt_el_queue_slot(vgt, ring_id, slot_idx) \
+	((vgt)->rb[ring_id].execlist_slots[slot_idx])
+#define vgt_el_queue_ctx(vgt, ring_id, slot_idx, ctx_idx) \
+	((vgt)->rb[ring_id].execlist_slots[slot_idx].el_ctxs[ctx_idx])
+
+struct vgt_device;
+typedef bool (*vgt_mmio_read)(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes);
+typedef bool (*vgt_mmio_write)(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes);
+
+struct vgt_mmio_entry {
+	struct hlist_node hlist;
+	unsigned int base;
+	unsigned int align_bytes;
+	vgt_mmio_read	read;
+	vgt_mmio_write	write;
+};
+
+#define	VGT_HASH_BITS	6
+
+/*
+ * Ring ID definition.
+ */
+enum vgt_ring_id {
+	RING_BUFFER_RCS = 0,
+	RING_BUFFER_VCS,
+	RING_BUFFER_BCS,
+	RING_BUFFER_VECS,
+	RING_BUFFER_VCS2,
+	MAX_ENGINES
+};
+
+typedef enum {
+	GTT_TYPE_INVALID = -1,
+
+	GTT_TYPE_GGTT_PTE,
+
+	GTT_TYPE_PPGTT_PTE_4K_ENTRY,
+	GTT_TYPE_PPGTT_PTE_2M_ENTRY,
+	GTT_TYPE_PPGTT_PTE_1G_ENTRY,
+
+	GTT_TYPE_PPGTT_PTE_ENTRY,
+
+	GTT_TYPE_PPGTT_PDE_ENTRY,
+	GTT_TYPE_PPGTT_PDP_ENTRY,
+	GTT_TYPE_PPGTT_PML4_ENTRY,
+
+	GTT_TYPE_PPGTT_ROOT_ENTRY,
+
+	GTT_TYPE_PPGTT_ROOT_L3_ENTRY,
+	GTT_TYPE_PPGTT_ROOT_L4_ENTRY,
+
+	GTT_TYPE_PPGTT_ENTRY,
+
+	GTT_TYPE_PPGTT_PTE_PT,
+	GTT_TYPE_PPGTT_PDE_PT,
+	GTT_TYPE_PPGTT_PDP_PT,
+	GTT_TYPE_PPGTT_PML4_PT,
+
+	GTT_TYPE_MAX,
+}gtt_type_t;
+
+#define gtt_type_is_entry(type) \
+	(type > GTT_TYPE_INVALID && type < GTT_TYPE_PPGTT_ENTRY \
+	 && type != GTT_TYPE_PPGTT_PTE_ENTRY \
+	 && type != GTT_TYPE_PPGTT_ROOT_ENTRY)
+
+#define gtt_type_is_pt(type) \
+	(type >= GTT_TYPE_PPGTT_PTE_PT && type < GTT_TYPE_MAX)
+
+#define gtt_type_is_pte_pt(type) \
+	(type == GTT_TYPE_PPGTT_PTE_PT)
+
+#define gtt_type_is_root_pointer(type) \
+	(gtt_type_is_entry(type) && type > GTT_TYPE_PPGTT_ROOT_ENTRY)
+
+typedef struct {
+	union {
+		u32 val32[2];
+		u64 val64;
+	};
+	gtt_type_t type;
+	struct pgt_device *pdev;
+}gtt_entry_t;
+
+#define gtt_init_entry(e, t, p, v) do { \
+	(e)->type = t; \
+	(e)->pdev = p; \
+	memcpy(&(e)->val64, &v, sizeof(v)); \
+}while(0)
+
+struct vgt_gtt_pte_ops {
+	gtt_entry_t *(*get_entry)(void *pt, gtt_entry_t *e, unsigned long index,
+			bool hypervisor_access, struct vgt_device *vgt);
+	gtt_entry_t *(*set_entry)(void *pt, gtt_entry_t *e, unsigned long index,
+			bool hypervisor_access, struct vgt_device *vgt);
+	bool (*test_present)(gtt_entry_t *e);
+	void (*clear_present)(gtt_entry_t *e);
+	bool (*test_pse)(gtt_entry_t *e);
+	void (*set_pfn)(gtt_entry_t *e, unsigned long pfn);
+	unsigned long (*get_pfn)(gtt_entry_t *e);
+};
+
+struct vgt_gtt_gma_ops {
+	unsigned long (*gma_to_ggtt_pte_index)(unsigned long gma);
+	unsigned long (*gma_to_pte_index)(unsigned long gma);
+	unsigned long (*gma_to_pde_index)(unsigned long gma);
+	unsigned long (*gma_to_l3_pdp_index)(unsigned long gma);
+	unsigned long (*gma_to_l4_pdp_index)(unsigned long gma);
+	unsigned long (*gma_to_pml4_index)(unsigned long gma);
+};
+
+extern struct vgt_gtt_pte_ops gen7_gtt_pte_ops;
+extern struct vgt_gtt_pte_ops gen8_gtt_pte_ops;
+extern struct vgt_gtt_gma_ops gen7_gtt_gma_ops;
+extern struct vgt_gtt_gma_ops gen8_gtt_gma_ops;
+
+typedef struct {
+	void *vaddr;
+	struct page *page;
+	gtt_type_t type;
+	struct hlist_node node;
+	unsigned long mfn;
+}shadow_page_t;
+
+typedef enum {
+	VGT_MM_GGTT = 0,
+	VGT_MM_PPGTT,
+} vgt_mm_type_t;
+
+struct vgt_mm {
+	vgt_mm_type_t type;
+	bool initialized;
+	bool shadowed;
+
+	gtt_type_t page_table_entry_type;
+	u32 page_table_entry_size;
+	u32 page_table_entry_cnt;
+	void *virtual_page_table;
+	void *shadow_page_table;
+
+	int page_table_level;
+	bool has_shadow_page_table;
+	u32 pde_base_index;
+
+	struct list_head list;
+	atomic_t refcount;
+	struct vgt_device *vgt;
+};
+
+extern gtt_entry_t *vgt_mm_get_entry(struct vgt_mm *mm,
+                void *page_table, gtt_entry_t *e,
+                unsigned long index);
+
+extern gtt_entry_t *vgt_mm_set_entry(struct vgt_mm *mm,
+                void *page_table, gtt_entry_t *e,
+                unsigned long index);
+
+#define ggtt_get_guest_entry(mm, e, index) \
+	(mm->vgt->vm_id == 0) ? \
+	vgt_mm_get_entry(mm, NULL, e, index) : \
+	vgt_mm_get_entry(mm, mm->virtual_page_table, e, index)
+
+#define ggtt_set_guest_entry(mm, e, index) \
+	vgt_mm_set_entry(mm, mm->virtual_page_table, e, index)
+
+#define ggtt_get_shadow_entry(mm, e, index) \
+	vgt_mm_get_entry(mm, mm->shadow_page_table, e, index)
+
+#define ggtt_set_shadow_entry(mm, e, index) \
+	vgt_mm_set_entry(mm, mm->shadow_page_table, e, index)
+
+#define ppgtt_get_guest_root_entry(mm, e, index) \
+	vgt_mm_get_entry(mm, mm->virtual_page_table, e, index)
+
+#define ppgtt_set_guest_root_entry(mm, e, index) \
+	vgt_mm_set_entry(mm, mm->virtual_page_table, e, index)
+
+#define ppgtt_get_shadow_root_entry(mm, e, index) \
+	vgt_mm_get_entry(mm, mm->shadow_page_table, e, index)
+
+#define ppgtt_set_shadow_root_entry(mm, e, index) \
+	vgt_mm_set_entry(mm, mm->shadow_page_table, e, index)
+
+extern struct vgt_mm *vgt_create_mm(struct vgt_device *vgt,
+		vgt_mm_type_t mm_type, gtt_type_t page_table_entry_type,
+		void *virtual_page_table, int page_table_level,
+		u32 pde_base_index);
+extern void vgt_destroy_mm(struct vgt_mm *mm);
+
+extern bool gen7_mm_alloc_page_table(struct vgt_mm *mm);
+extern void gen7_mm_free_page_table(struct vgt_mm *mm);
+extern bool gen8_mm_alloc_page_table(struct vgt_mm *mm);
+extern void gen8_mm_free_page_table(struct vgt_mm *mm);
+
+struct vgt_vgtt_info {
+	struct vgt_mm *ggtt_mm;
+	unsigned long active_ppgtt_mm_bitmap;
+	struct list_head mm_list_head;
+	mempool_t *mempool;
+	DECLARE_HASHTABLE(shadow_page_hash_table, VGT_HASH_BITS);
+	DECLARE_HASHTABLE(guest_page_hash_table, VGT_HASH_BITS);
+	DECLARE_HASHTABLE(el_ctx_hash_table, VGT_HASH_BITS);
+	atomic_t n_write_protected_guest_page;
+};
+
+extern bool vgt_init_vgtt(struct vgt_device *vgt);
+extern void vgt_clean_vgtt(struct vgt_device *vgt);
+
+extern bool vgt_expand_shadow_page_mempool(struct vgt_device *vgt);
+
+extern bool vgt_g2v_create_ppgtt_mm(struct vgt_device *vgt, int page_table_level);
+extern bool vgt_g2v_destroy_ppgtt_mm(struct vgt_device *vgt, int page_table_level);
+
+extern struct vgt_mm *gen8_find_ppgtt_mm(struct vgt_device *vgt,
+                int page_table_level, void *root_entry);
+
+typedef bool guest_page_handler_t(void *gp, uint64_t pa, void *p_data, int bytes);
+
+struct guest_page {
+	struct hlist_node node;
+	int writeprotection;
+	unsigned long gfn;
+	void *vaddr;
+	guest_page_handler_t *handler;
+	void *data;
+};
+typedef struct guest_page guest_page_t;
+
+typedef struct {
+	shadow_page_t shadow_page;
+	guest_page_t guest_page;
+	gtt_type_t guest_page_type;
+	atomic_t refcount;
+	struct vgt_device *vgt;
+} ppgtt_spt_t;
+
+extern bool vgt_init_guest_page(struct vgt_device *vgt, guest_page_t *guest_page,
+		unsigned long gfn, guest_page_handler_t handler, void *data);
+extern void vgt_clean_guest_page(struct vgt_device *vgt, guest_page_t *guest_page);
+extern bool vgt_set_guest_page_writeprotection(struct vgt_device *vgt,
+		guest_page_t *guest_page);
+extern bool vgt_clear_guest_page_writeprotection(struct vgt_device *vgt,
+		guest_page_t *guest_page);
+extern guest_page_t *vgt_find_guest_page(struct vgt_device *vgt, unsigned long gfn);
+
+extern bool gen7_ppgtt_mm_setup(struct vgt_device *vgt, int ring_id);
+
+/* shadow context */
+
+struct shadow_ctx_page {
+	guest_page_t guest_page;
+	shadow_page_t shadow_page;
+	struct vgt_device *vgt;
+};
+
+struct execlist_context {
+	struct ctx_desc_format guest_context;
+	uint32_t shadow_lrca;
+	uint32_t error_reported;
+	enum vgt_ring_id ring_id;
+	/* below are some per-ringbuffer data. Since with execlist,
+	 * each context has its own ring buffer, here we store the
+	 * data and store them into vgt->rb[ring_id] before a
+	 * context is submitted. We will have better handling later.
+	 */
+	vgt_reg_t last_scan_head;
+	uint64_t request_id;
+	//uint64_t cmd_nr;
+	//vgt_reg_t uhptr;
+	//uint64_t uhptr_id;
+
+	struct vgt_mm *ppgtt_mm;
+	struct shadow_ctx_page ctx_pages[MAX_EXECLIST_CTX_PAGES];
+	/* used for lazy context shadowing optimization */
+	gtt_entry_t shadow_entry_backup[MAX_EXECLIST_CTX_PAGES];
+
+	struct hlist_node node;
+};
+
+extern enum vgt_pipe surf_used_pipe;
+
+struct pgt_device;
+
+struct vgt_render_context_ops {
+	bool (*init_null_context)(struct pgt_device *pdev, int id);
+	bool (*save_hw_context)(int id, struct vgt_device *vgt);
+	bool (*restore_hw_context)(int id, struct vgt_device *vgt);
+	bool (*ring_context_switch)(struct pgt_device *pdev,
+				enum vgt_ring_id ring_id,
+				struct vgt_device *prev,
+				struct vgt_device *next);
+};
+
+extern bool vgt_render_init(struct pgt_device *pdev);
+extern bool idle_rendering_engines(struct pgt_device *pdev, int *id);
+extern bool idle_render_engine(struct pgt_device *pdev, int id);
+extern bool vgt_do_render_context_switch(struct pgt_device *pdev);
+extern bool vgt_do_render_sched(struct pgt_device *pdev);
+extern void vgt_destroy(void);
+extern void vgt_destroy_debugfs(struct vgt_device *vgt);
+extern void vgt_release_debugfs(void);
+extern bool vgt_register_mmio_handler(unsigned int start, int bytes,
+	vgt_mmio_read read, vgt_mmio_write write);
+extern void vgt_clear_mmio_table(void);
+
+extern bool need_scan_attached_ports;
+extern bool vgt_reinitialize_mode(struct vgt_device *cur_vgt,
+		struct vgt_device *next_vgt);
+extern int vgt_hvm_info_init(struct vgt_device *vgt);
+extern int vgt_hvm_opregion_init(struct vgt_device *vgt, uint32_t gpa);
+extern void vgt_hvm_info_deinit(struct vgt_device *vgt);
+extern bool vgt_prepare_vbios_general_definition(struct vgt_device *vgt);
+extern void vgt_check_pending_context_switch(struct vgt_device *vgt);
+
+struct vgt_irq_virt_state;
+
+struct vgt_statistics {
+	u64	schedule_in_time;	/* TSC time when it is last scheduled in */
+	u64	allocated_cycles;
+	u64	used_cycles;
+	u64	irq_num;
+	u64	events[EVENT_MAX];
+
+	/* actually this is the number of pending
+	* interrutps, check this in vgt_check_pending_events,
+	* one injection can deliver more than one events
+	*/
+	u64	pending_events;
+	u64	last_propagation;
+	u64	last_blocked_propagation;
+	u64	last_injection;
+
+	/* mmio statistics */
+	u64	gtt_mmio_rcnt;
+	u64	gtt_mmio_wcnt;
+	u64	gtt_mmio_wcycles;
+	u64	gtt_mmio_rcycles;
+	u64	mmio_rcnt;
+	u64	mmio_wcnt;
+	u64	mmio_wcycles;
+	u64	mmio_rcycles;
+	u64	ring_mmio_rcnt;
+	u64	ring_mmio_wcnt;
+	u64	ring_tail_mmio_wcnt;
+	u64	ring_tail_mmio_wcycles;
+	u64	vring_scan_cnt;
+	u64	vring_scan_cycles;
+	u64	ppgtt_wp_cnt;
+	u64	ppgtt_wp_cycles;
+	u64	skip_bb_cnt;
+};
+
+/* per-VM structure */
+typedef cycles_t vgt_tslice_t;
+struct vgt_sched_info {
+	vgt_tslice_t start_time;
+	vgt_tslice_t end_time;
+	vgt_tslice_t actual_end_time;
+	vgt_tslice_t rb_empty_delay;	/* cost for "wait rendering engines empty */
+
+	int32_t priority;
+	int32_t weight;
+	int64_t time_slice;
+	/* more properties and policies should be added in*/
+};
+
+#define VGT_TBS_DEFAULT_PERIOD (15 * 1000000) /* 15 ms */
+
+struct vgt_hrtimer {
+	struct hrtimer timer;
+	u64 period;
+};
+
+#define VGT_TAILQ_RB_POLLING_PERIOD (2 * 1000000)
+#define VGT_TAILQ_SIZE (SIZE_1MB)
+#define VGT_TAILQ_MAX_ENTRIES ((VGT_TAILQ_SIZE)/sizeof(u32))
+#define VGT_TAILQ_IDX_MASK (VGT_TAILQ_MAX_ENTRIES - 1)
+/* Maximum number of tail can be cached is (VGT_TAILQ_MAX_ENTRIES - 1) */
+struct vgt_tailq {
+	u32 __head;
+	u32 __tail;
+	u32 *__buf_tail;  /* buffer to save tail value caught by tail-write */
+	u32 *__buf_cmdnr; /* buffer to save cmd nr for each tail-write */
+};
+#define vgt_tailq_idx(idx) ((idx) & VGT_TAILQ_IDX_MASK)
+
+/* DPCD start */
+#define DPCD_SIZE	0x700
+
+struct vgt_dpcd_data {
+	bool data_valid;
+	u8 data[DPCD_SIZE];
+};
+
+enum dpcd_index {
+	DPCD_DPA = 0,
+	DPCD_DPB,
+	DPCD_DPC,
+	DPCD_DPD,
+	DPCD_MAX
+};
+
+/* DPCD addresses */
+#define DPCD_REV			0x000
+#define DPCD_MAX_LINK_RATE			0x001
+#define DPCD_MAX_LANE_COUNT			0x002
+
+#define DPCD_TRAINING_PATTERN_SET	0x102
+#define	DPCD_SINK_COUNT			0x200
+#define DPCD_LANE0_1_STATUS		0x202
+#define DPCD_LANE2_3_STATUS		0x203
+#define DPCD_LANE_ALIGN_STATUS_UPDATED	0x204
+#define DPCD_SINK_STATUS		0x205
+
+/* link training */
+#define DPCD_TRAINING_PATTERN_SET_MASK	0x03
+#define DPCD_LINK_TRAINING_DISABLED	0x00
+#define DPCD_TRAINING_PATTERN_1		0x01
+#define DPCD_TRAINING_PATTERN_2		0x02
+
+#define DPCD_CP_READY_MASK		(1 << 6)
+
+/* lane status */
+#define DPCD_LANES_CR_DONE		0x11
+#define DPCD_LANES_EQ_DONE		0x22
+#define DPCD_SYMBOL_LOCKED		0x44
+
+#define DPCD_INTERLANE_ALIGN_DONE	0x01
+
+#define DPCD_SINK_IN_SYNC		0x03
+
+/* DPCD end */
+
+#define SBI_REG_MAX	20
+
+struct sbi_register {
+	unsigned int offset;
+	vgt_reg_t value;
+};
+
+struct sbi_registers {
+	int number;
+	struct sbi_register registers[SBI_REG_MAX];
+};
+
+struct port_cache {
+	bool valid;
+	struct vgt_edid_data_t	*edid;	/* per display EDID information */
+	enum vgt_port		port_override;
+	enum vgt_port_type	type;
+};
+
+struct gt_port {
+	struct kobject  	kobj;
+
+	struct vgt_edid_data_t	*edid;	/* per display EDID information */
+	struct vgt_dpcd_data	*dpcd;	/* per display DPCD information */
+	enum vgt_port_type	type;
+	enum vgt_port		port_override;
+	struct port_cache	cache; /* the temporary updated information */
+	enum vgt_port physcal_port;
+};
+
+struct vgt_device {
+	enum vgt_pipe pipe_mapping[I915_MAX_PIPES];
+	int vgt_id;		/* 0 is always for dom0 */
+	int vm_id;		/* domain ID per hypervisor */
+	struct pgt_device *pdev;	/* the pgt device where the GT device registered. */
+	struct list_head	list;	/* FIXME: used for context switch ?? */
+	vgt_state_t	state;		/* MMIO state except ring buffers */
+	vgt_state_ring_t	rb[MAX_ENGINES];	/* ring buffer state */
+
+	struct gt_port		ports[I915_MAX_PORTS]; /* one port per PIPE */
+	struct vgt_i2c_edid_t	vgt_i2c_edid;	/* i2c bus state emulaton for reading EDID */
+
+	uint64_t	aperture_base;
+	void		*aperture_base_va;
+	uint64_t	aperture_sz;
+	uint64_t	gm_sz;
+	uint64_t	aperture_offset;	/* address fix for visible GM */
+	uint64_t	hidden_gm_offset;	/* address fix for invisible GM */
+	int			fence_base;
+	int			fence_sz;
+
+
+	/* TODO: move to hvm_info  */
+	unsigned long low_mem_max_gpfn;	/* the max gpfn of the <4G memory */
+	void *hvm_info;
+
+	vgt_reg_t	saved_wakeup;		/* disable PM before switching */
+
+	struct kobject kobj;
+	struct vgt_statistics	stat;		/* statistics info */
+
+	DECLARE_BITMAP(enabled_rings, MAX_ENGINES);
+	DECLARE_BITMAP(started_rings, MAX_ENGINES);
+	struct vgt_vgtt_info gtt;
+
+	/* embedded context scheduler information */
+	struct vgt_sched_info sched_info;
+
+	/* Tail Queue (used to cache tail-writingt) */
+	struct vgt_tailq rb_tailq[MAX_ENGINES];
+
+	uint8_t	ballooning:1; /* VM supports ballooning */
+	uint8_t	force_removal:1; /* force removal from the render run queue */
+	/* Temporary flag for VEBOX guest driver support.
+	 * Linux VM will have official VEBOX support until kernel 3.9.
+	 * Windows driver already enables VEBOX support now.
+	 * So in order to determine whether VM has turned on VEBOX on HSW, this
+	 * flag is used. Will remove in future when VM drivers all have VEBOX
+	 * support. */
+	uint8_t vebox_support:1;
+	uint8_t has_context:1;
+	/*
+	 * Have HVM been visible from boot time?
+	 * Used when hvm_boot_foreground mode is enabled.
+	 */
+	uint8_t hvm_boot_foreground_visible:1;
+	uint8_t warn_untrack:1;
+	uint8_t bypass_addr_check:1;
+
+	atomic_t crashing;
+
+	uint64_t total_cmds;		/* total CMDs since VM is started */
+	uint64_t submitted_cmds;	/* CMDs submitted in current slice */
+	uint64_t allocated_cmds;	/* CMDs allocated in current slice */
+
+	uint32_t frmcount_delta[I915_MAX_PIPES]; /* used for vblank virtualization*/
+
+	struct sbi_registers sbi_regs;
+
+	unsigned long reset_flags;
+	unsigned long enabled_rings_before_reset;
+	unsigned long last_reset_time;
+};
+
+enum vgt_owner_type {
+	VGT_OT_NONE = 0,		// No owner type
+	VGT_OT_RENDER,			// the owner directly operating all render buffers (render/blit/video)
+	VGT_OT_DISPLAY,			// the owner having its content directly shown on one or several displays
+	VGT_OT_CONFIG,			// the owner is always dom0 (PM, workarounds, etc.)
+	VGT_OT_MAX,
+};
+
+/* owner type of the reg, up to 16 owner type */
+#define VGT_REG_OWNER		(0xF)
+/*
+ * TODO:
+ * Allows pReg access from any VM but w/o save/restore,
+ * since we don't know the actual bit detail or virtualization
+ * policy yet. the examples include many workaround registers.
+ * regs marked with this flag should be cleared before final
+ * release, since this way is unsafe.
+ */
+#define VGT_REG_PASSTHROUGH	(1 << 4)
+/* reg contains address, requiring fix */
+#define VGT_REG_ADDR_FIX	(1 << 5)
+/* Status bit updated from HW */
+#define VGT_REG_HW_STATUS	(1 << 6)
+/* Virtualized */
+#define VGT_REG_VIRT		(1 << 7)
+/* Mode ctl registers with high 16 bits as the mask bits */
+#define VGT_REG_MODE_CTL	(1 << 8)
+/* VMs have different settings on this reg */
+#define VGT_REG_NEED_SWITCH	(1 << 9)
+/* This reg has been tracked in vgt_base_reg_info */
+#define VGT_REG_TRACKED		(1 << 10)
+/* This reg has been accessed by a VM */
+#define VGT_REG_ACCESSED	(1 << 11)
+/* This reg is saved/restored at context switch time */
+#define VGT_REG_SAVED		(1 << 12)
+/* Policies not impacted by the superowner mode */
+#define VGT_REG_STICKY		(1 << 13)
+/* Accessed through GPU commands */
+#define VGT_REG_CMD_ACCESS	(1 << 14)
+/* index into another auxillary table. Maximum 256 entries now */
+#define VGT_REG_INDEX_SHIFT	16
+#define VGT_REG_INDEX_MASK	(0xFFFF << VGT_REG_INDEX_SHIFT)
+typedef u32 reg_info_t;
+
+#define VGT_AUX_TABLE_NUM	256
+/* suppose a reg won't set both bits */
+typedef union {
+	struct {
+		vgt_reg_t mask;
+	} mode_ctl;
+	struct {
+		vgt_reg_t mask;
+		uint32_t  size;
+	} addr_fix;
+} vgt_aux_entry_t;
+
+struct vgt_irq_host_state;
+#define VGT_VBIOS_PAGES 16
+
+/* PLUG_OUT must equal to PLUG_IN + 1
+ * hot plug handler code has such assumption. Actually it might
+ * be OK to send HOTPLUG only, not necessarily differ IN aond
+ * OUT.
+ */
+enum vgt_uevent_type {
+	CRT_HOTPLUG_IN = 0,
+	CRT_HOTPLUG_OUT,
+	PORT_A_HOTPLUG_IN,
+	PORT_A_HOTPLUG_OUT,
+	PORT_B_HOTPLUG_IN,
+	PORT_B_HOTPLUG_OUT,
+	PORT_C_HOTPLUG_IN,
+	PORT_C_HOTPLUG_OUT,
+	PORT_D_HOTPLUG_IN,
+	PORT_D_HOTPLUG_OUT,
+	VGT_ENABLE_VGA,
+	VGT_DISABLE_VGA,
+	VGT_DISPLAY_READY,
+	VGT_DISPLAY_UNREADY,
+	VGT_DETECT_PORT_A,
+	VGT_DETECT_PORT_B,
+	VGT_DETECT_PORT_C,
+	VGT_DETECT_PORT_D,
+	VGT_DETECT_PORT_E,
+	UEVENT_MAX
+};
+
+#define HOTPLUG_VMID_FOR_ALL_VMS	0xff
+
+#define VGT_MAX_UEVENT_VARS 20
+struct vgt_uevent_info {
+	char *uevent_name;
+	int vm_id;
+	enum kobject_action action;
+	char *env_var_table[VGT_MAX_UEVENT_VARS];
+	bool (*vgt_uevent_handler)(enum vgt_uevent_type event,
+				struct vgt_uevent_info *uevent_entry,
+				struct pgt_device *dev);
+};
+
+void vgt_set_uevent(struct vgt_device *vgt, enum vgt_uevent_type uevent);
+
+enum vgt_trace_type {
+	VGT_TRACE_READ,
+	VGT_TRACE_WRITE
+};
+
+typedef union {
+	uint32_t cmd;
+	struct {
+		uint32_t action : 1;
+		uint32_t port_sel: 3;
+		uint32_t rsvd_4_7 : 4;
+		uint32_t vmid : 8;
+		uint32_t rsvd_16_31 : 16;
+	};
+} vgt_hotplug_cmd_t;
+
+typedef union {
+	uint32_t dw;
+	struct {
+		uint32_t virtual_event: 16;
+		uint32_t vmid : 8;
+		uint32_t rsvd_24_31 : 8;
+	};
+} vgt_virtual_event_t;
+
+struct hotplug_work {
+	struct work_struct work;
+	DECLARE_BITMAP(hotplug_uevent, UEVENT_MAX);
+	struct mutex hpd_mutex;
+};
+
+enum vgt_output_type {
+	VGT_OUTPUT_ANALOG = 0,
+	VGT_OUTPUT_DISPLAYPORT,
+	VGT_OUTPUT_EDP,
+	VGT_OUTPUT_LVDS,
+	VGT_OUTPUT_HDMI,
+	VGT_OUTPUT_MAX
+};
+
+struct pgt_statistics {
+	u64	irq_num;
+	u64	last_pirq;
+	u64	last_virq;
+	u64	pirq_cycles;
+	u64	virq_cycles;
+	u64	irq_delay_cycles;
+	u64	events[EVENT_MAX];
+};
+
+#define PCI_BDF2(b,df)  ((((b) & 0xff) << 8) | ((df) & 0xff))
+
+struct vgt_mmio_dev;
+
+enum {
+	RESET_INPROGRESS = 0,
+	WAIT_RESET,
+};
+
+#define device_is_reseting(pdev) \
+	test_bit(RESET_INPROGRESS, &pdev->device_reset_flags)
+
+#define MKGEN(major, minor, rev) \
+	((major << 16) | (minor << 8) | (rev))
+
+#define GEN_MAJOR(gen) ((gen >> 16) & 0xff)
+#define GEN_MINOR(gen) ((gen >> 8) & 0xff)
+#define GEN_REV(gen) ((gen) & 0xff)
+
+/* Describe the limitation of HW.*/
+struct vgt_device_info {
+	u32 gen;
+	u64 max_gtt_gm_sz;
+	u32 gtt_start_offset;
+	u32 max_gtt_size;
+	u32 gtt_entry_size;
+	u32 gtt_entry_size_shift;
+	u32 gmadr_bytes_in_cmd;
+};
+
+struct vgt_gtt_info {
+	struct vgt_gtt_pte_ops *pte_ops;
+	struct vgt_gtt_gma_ops *gma_ops;
+	bool (*mm_alloc_page_table)(struct vgt_mm *mm);
+	void (*mm_free_page_table)(struct vgt_mm *mm);
+};
+
+/* per-device structure */
+struct pgt_device {
+	struct list_head	list; /* list node for 'pgt_devices' */
+
+	struct vgt_device_info device_info;
+
+	struct pci_bus *pbus;	/* parent bus of the device */
+	struct pci_dev *pdev;	/* the gfx device bound to */
+	int bus;		/* parent bus number */
+	int devfn;		/* device function number */
+
+	struct task_struct *p_thread;
+	wait_queue_head_t event_wq;
+	wait_queue_head_t destroy_wq;
+
+	unsigned long device_reset_flags;
+
+	uint32_t request;
+
+	uint64_t ctx_check;	/* the number of checked count in vgt thread */
+	uint64_t ctx_switch;	/* the number of context switch count in vgt thread */
+	uint32_t magic;		/* the magic number for checking the completion of context switch */
+
+	vgt_reg_t *initial_mmio_state;	/* copy from physical at start */
+	uint8_t initial_cfg_space[VGT_CFG_SPACE_SZ];	/* copy from physical at start */
+	uint32_t bar_size[VGT_BAR_NUM];
+	uint64_t total_gm_sz;	/* size of available GM space, e.g 2M GTT is 2GB */
+
+	uint64_t gttmmio_base;	/* base of GTT and MMIO */
+	void *gttmmio_base_va;	/* virtual base of GTT and MMIO */
+	uint64_t gmadr_base;	/* base of GMADR */
+	void *gmadr_va;		/* virtual base of GMADR */
+	u32 mmio_size;
+	u64 gtt_size;
+	int reg_num;
+	uint32_t *saved_gtt;
+	uint64_t saved_fences[VGT_MAX_NUM_FENCES];
+
+	uint32_t saved_rrmr;
+	uint32_t saved_shotplug_ctl;
+
+	int max_engines;	/* supported max engines */
+	u32 ring_mmio_base[MAX_ENGINES];
+	u32 ring_mi_mode[MAX_ENGINES];
+	u32 ring_xxx[MAX_ENGINES];
+	u8 ring_xxx_bit[MAX_ENGINES];
+	u8 ring_xxx_valid;
+
+	struct vgt_gtt_info gtt;
+
+	struct gt_port ports[I915_MAX_PORTS];
+
+	 /* 1 bit corresponds to 1MB in the GM space */
+	DECLARE_BITMAP(gm_bitmap, VGT_GM_BITMAP_BITS);
+
+	/* 1 bit corresponds to 1 fence register */
+	DECLARE_BITMAP(fence_bitmap, VGT_FENCE_BITMAP_BITS);
+
+	/* 1 bit corresponds to 1 PAGE(4K) in aperture */
+	DECLARE_BITMAP(rsvd_aperture_bitmap, VGT_RSVD_APERTURE_BITMAP_BITS);
+
+	struct page *dummy_page;
+	struct page *(*rsvd_aperture_pages)[VGT_APERTURE_PAGES];
+	gtt_entry_t dummy_gtt_entry;
+
+	uint64_t rsvd_aperture_sz;
+	uint64_t rsvd_aperture_base;
+	uint64_t scratch_page;		/* page used for data written from GPU */
+
+	struct vgt_device *device[VGT_MAX_VMS];	/* a list of running VMs */
+	struct vgt_device *owner[VGT_OT_MAX];	/* owner list of different engines */
+	struct vgt_device *foreground_vm;		/* current visible domain on display. */
+	struct vgt_device *next_sched_vgt;
+	struct vgt_device *next_foreground_vm;
+	struct list_head rendering_runq_head; /* reuse this for context scheduler */
+	struct list_head rendering_idleq_head; /* reuse this for context scheduler */
+	spinlock_t lock;
+	spinlock_t irq_lock;
+
+	reg_info_t *reg_info;	/* virtualization policy for a given reg */
+	struct vgt_irq_host_state *irq_hstate;
+
+	DECLARE_BITMAP(dpy_emul_request, VGT_MAX_VMS);
+
+	u8 gen_dev_type;
+
+	u8 enable_ppgtt : 1;
+	u8 in_ctx_switch : 1;
+	u8 enable_execlist : 1;
+
+	vgt_aux_entry_t vgt_aux_table[VGT_AUX_TABLE_NUM];
+	int at_index;
+
+	struct pgt_statistics stat;
+
+	struct vgt_mmio_dev *mmio_dev;
+
+	struct vgt_rsvd_ring ring_buffer[MAX_ENGINES]; /* vGT ring buffer */
+
+	uint32_t opregion_pa;
+	void *opregion_va;
+
+	bool dom0_irq_pending;
+	unsigned long dom0_ipi_irq_injecting;
+	int dom0_irq_cpu;
+
+	struct hotplug_work hpd_work;
+
+	bool ctx_switch_pending;
+
+	uint32_t el_cache_write_ptr[MAX_ENGINES];
+	uint32_t el_read_ptr[MAX_ENGINES];
+};
+
+/*
+ * MI_STORE_DATA is used widely for synchronization between GPU and driver,
+ * which suppports the destination in either a specific hardware status
+ * page, or any other aperture pages mapped to main memory. We don't want
+ * to switch the hardware status page from the VM, so adopt the latter form
+ * with a scratch page created as the destination with layout defined as
+ * below:
+ */
+#define VGT_DATA_CTX_MAGIC	0x0	/* the magic number used in the context switch */
+#define vgt_data_ctx_magic(d)		(d->scratch_page + VGT_DATA_CTX_MAGIC)
+
+#define vgt_get_owner(d, t)		(d->owner[t])
+#define current_render_owner(d)		(vgt_get_owner(d, VGT_OT_RENDER))
+#define current_display_owner(d)	(vgt_get_owner(d, VGT_OT_DISPLAY))
+#define current_foreground_vm(d)	(d->foreground_vm)
+#define current_config_owner(d)		(vgt_get_owner(d, VGT_OT_CONFIG))
+#define is_current_render_owner(vgt)	(vgt && vgt == current_render_owner(vgt->pdev))
+#define is_current_display_owner(vgt)	(vgt && vgt == current_display_owner(vgt->pdev))
+#define is_current_config_owner(vgt)	(vgt && vgt == current_config_owner(vgt->pdev))
+#define ctx_switch_requested(d)		\
+	(d->next_sched_vgt &&		\
+	 (d->next_sched_vgt != current_render_owner(d)))
+#define vgt_ctx_check(d)		(d->ctx_check)
+#define vgt_ctx_switch(d)		(d->ctx_switch)
+#define vgt_has_edp_enabled(vgt, pipe)							\
+		(vgt && ((pipe) >= PIPE_A) && ((pipe) < I915_MAX_PIPES) &&		\
+		(__vreg((vgt), _REG_PIPE_EDP_CONF) & _REGBIT_PIPE_ENABLE) &&		\
+		(pipe == get_edp_input(__vreg(vgt, _REG_TRANS_DDI_FUNC_CTL_EDP))))
+#define vgt_has_pipe_enabled(vgt, pipe)				\
+		(vgt && ((pipe) >= PIPE_A) && ((pipe) < I915_MAX_PIPES) &&	\
+		((__vreg((vgt), VGT_PIPECONF(pipe)) & _REGBIT_PIPE_ENABLE) ||	\
+			vgt_has_edp_enabled(vgt, pipe)))
+#define pdev_has_pipe_enabled(pdev, pipe)					\
+		(pdev && ((pipe) >= PIPE_A) && ((pipe) < I915_MAX_PIPES) &&	\
+		((__vreg(current_display_owner(pdev),				\
+			VGT_PIPECONF(pipe)) & _REGBIT_PIPE_ENABLE) ||		\
+			vgt_has_edp_enabled(current_display_owner(pdev), pipe)))
+#define dpy_is_valid_port(port)							\
+		(((port) >= PORT_A) && ((port) < I915_MAX_PORTS))
+
+#define dpy_has_monitor_on_port(vgt, port)					\
+		(vgt && dpy_is_valid_port(port) &&				\
+		vgt->ports[port].edid && vgt->ports[port].edid->data_valid)
+
+#define dpy_port_is_dp(vgt, port)						\
+		((vgt) && dpy_is_valid_port(port)				\
+		&& ((vgt->ports[port].type == VGT_DP_A) ||			\
+		    (vgt->ports[port].type == VGT_DP_B) ||			\
+		    (vgt->ports[port].type == VGT_DP_C) ||			\
+		    (vgt->ports[port].type == VGT_DP_D)))
+
+extern int prepare_for_display_switch(struct pgt_device *pdev);
+extern void do_vgt_fast_display_switch(struct pgt_device *pdev);
+
+#define reg_addr_fix(pdev, reg)		(pdev->reg_info[REG_INDEX(reg)] & VGT_REG_ADDR_FIX)
+#define reg_hw_status(pdev, reg)	(pdev->reg_info[REG_INDEX(reg)] & VGT_REG_HW_STATUS)
+#define reg_virt(pdev, reg)		(pdev->reg_info[REG_INDEX(reg)] & VGT_REG_VIRT)
+#define reg_mode_ctl(pdev, reg)		(pdev->reg_info[REG_INDEX(reg)] & VGT_REG_MODE_CTL)
+#define reg_passthrough(pdev, reg)	(pdev->reg_info[REG_INDEX(reg)] & VGT_REG_PASSTHROUGH)
+#define reg_need_switch(pdev, reg)	(pdev->reg_info[REG_INDEX(reg)] & VGT_REG_NEED_SWITCH)
+#define reg_is_tracked(pdev, reg)	(pdev->reg_info[REG_INDEX(reg)] & VGT_REG_TRACKED)
+#define reg_is_accessed(pdev, reg)	(pdev->reg_info[REG_INDEX(reg)] & VGT_REG_ACCESSED)
+#define reg_is_saved(pdev, reg)		(pdev->reg_info[REG_INDEX(reg)] & VGT_REG_SAVED)
+#define reg_is_sticky(pdev, reg)		(pdev->reg_info[REG_INDEX(reg)] & VGT_REG_STICKY)
+#define reg_get_owner(pdev, reg)	(pdev->reg_info[REG_INDEX(reg)] & VGT_REG_OWNER)
+#define reg_is_render(pdev, reg)	(reg_get_owner(pdev, reg) == VGT_OT_RENDER)
+#define reg_is_config(pdev, reg)	(reg_get_owner(pdev, reg) == VGT_OT_CONFIG)
+#define reg_invalid(pdev, reg)		(!pdev->reg_info[REG_INDEX(reg)])
+#define reg_aux_index(pdev, reg)	\
+	((pdev->reg_info[REG_INDEX(reg)] & VGT_REG_INDEX_MASK) >> VGT_REG_INDEX_SHIFT)
+#define reg_has_aux_info(pdev, reg)	(reg_mode_ctl(pdev, reg) | reg_addr_fix(pdev, reg))
+#define reg_aux_mode_mask(pdev, reg)	\
+	(pdev->vgt_aux_table[reg_aux_index(pdev, reg)].mode_ctl.mask)
+#define reg_aux_addr_mask(pdev, reg)	\
+	(pdev->vgt_aux_table[reg_aux_index(pdev, reg)].addr_fix.mask)
+#define reg_aux_addr_size(pdev, reg)	\
+	(pdev->vgt_aux_table[reg_aux_index(pdev, reg)].addr_fix.size)
+
+#define el_read_ptr(pdev, ring_id) ((pdev)->el_read_ptr[ring_id])
+#define el_write_ptr(pdev, ring_id) ((pdev)->el_cache_write_ptr[ring_id])
+
+/*
+ * Kernel BUG() doesn't work, because bust_spinlocks try to unblank screen
+ * which may call into i915 and thus cause undesired more errors on the
+ * screen
+ */
+static inline void vgt_panic(void)
+{
+	struct pgt_device *pdev = &default_device;
+
+	show_debug(pdev);
+
+	dump_stack();
+	printk("________end of stack dump_________\n");
+	panic("FATAL VGT ERROR\n");
+}
+#define ASSERT(x)							\
+	do {								\
+		if (!(x)) {						\
+			printk("Assert at %s line %d\n",		\
+				__FILE__, __LINE__);			\
+			vgt_panic();					\
+		}							\
+	} while (0);
+#define ASSERT_NUM(x, y)						\
+	do {								\
+		if (!(x)) {						\
+			printk("Assert at %s line %d para 0x%llx\n",	\
+				__FILE__, __LINE__, (u64)y);		\
+			vgt_panic();					\
+		}							\
+	} while (0);
+
+static inline void reg_set_hw_status(struct pgt_device *pdev, vgt_reg_t reg)
+{
+	ASSERT_NUM(!reg_is_tracked(pdev, reg), reg);
+	pdev->reg_info[REG_INDEX(reg)] |= VGT_REG_HW_STATUS;
+}
+
+static inline void reg_set_virt(struct pgt_device *pdev, vgt_reg_t reg)
+{
+	ASSERT_NUM(!reg_is_tracked(pdev, reg), reg);
+	pdev->reg_info[REG_INDEX(reg)] |= VGT_REG_VIRT;
+}
+
+/* mask bits for addr fix */
+static inline void reg_set_addr_fix(struct pgt_device *pdev,
+	vgt_reg_t reg, vgt_reg_t mask)
+{
+	ASSERT(!reg_has_aux_info(pdev, reg));
+	ASSERT(pdev->at_index <= VGT_AUX_TABLE_NUM - 1);
+	ASSERT_NUM(!reg_is_tracked(pdev, reg), reg);
+
+	pdev->vgt_aux_table[pdev->at_index].addr_fix.mask = mask;
+	pdev->reg_info[REG_INDEX(reg)] |= VGT_REG_ADDR_FIX |
+		(pdev->at_index << VGT_REG_INDEX_SHIFT);
+	pdev->at_index++;
+}
+
+/* mask bits for mode mask */
+static inline void reg_set_mode_ctl(struct pgt_device *pdev,
+	vgt_reg_t reg)
+{
+	ASSERT(!reg_has_aux_info(pdev, reg));
+	ASSERT(pdev->at_index <= VGT_AUX_TABLE_NUM - 1);
+	ASSERT_NUM(!reg_is_tracked(pdev, reg), reg);
+
+	pdev->reg_info[REG_INDEX(reg)] |= VGT_REG_MODE_CTL |
+		(pdev->at_index << VGT_REG_INDEX_SHIFT);
+	pdev->at_index++;
+}
+
+/* if the type is invalid, we assume dom0 always has the permission */
+static inline bool reg_is_owner(struct vgt_device *vgt, vgt_reg_t reg)
+{
+	enum vgt_owner_type type;
+
+	type = vgt->pdev->reg_info[REG_INDEX(reg)] & VGT_REG_OWNER;
+	return vgt == vgt_get_owner(vgt->pdev, type);
+}
+
+static inline void reg_set_owner(struct pgt_device *pdev,
+	vgt_reg_t reg, enum vgt_owner_type type)
+{
+	ASSERT_NUM(!reg_is_tracked(pdev, reg), reg);
+	pdev->reg_info[REG_INDEX(reg)] |= type & VGT_REG_OWNER;
+}
+
+static inline void reg_change_owner(struct pgt_device *pdev,
+	vgt_reg_t reg, enum vgt_owner_type type)
+{
+	ASSERT_NUM(reg_is_tracked(pdev, reg), reg);
+	pdev->reg_info[REG_INDEX(reg)] &= ~VGT_REG_OWNER;
+	pdev->reg_info[REG_INDEX(reg)] |= type & VGT_REG_OWNER;
+	if ((type != VGT_OT_NONE) && (type != VGT_OT_MAX))
+		pdev->reg_info[REG_INDEX(reg)] &= ~VGT_REG_VIRT;
+}
+
+static inline void reg_set_passthrough(struct pgt_device *pdev,
+	vgt_reg_t reg)
+{
+	ASSERT_NUM(!reg_is_tracked(pdev, reg), reg);
+	pdev->reg_info[REG_INDEX(reg)] |= VGT_REG_PASSTHROUGH;
+}
+
+static inline void reg_set_tracked(struct pgt_device *pdev,
+	vgt_reg_t reg)
+{
+	ASSERT_NUM(!reg_is_tracked(pdev, reg), reg);
+	pdev->reg_info[REG_INDEX(reg)] |= VGT_REG_TRACKED;
+}
+
+static inline void reg_set_accessed(struct pgt_device *pdev,
+	vgt_reg_t reg)
+{
+	pdev->reg_info[REG_INDEX(reg)] |= VGT_REG_ACCESSED;
+}
+
+static inline void reg_set_saved(struct pgt_device *pdev,
+	vgt_reg_t reg)
+{
+	pdev->reg_info[REG_INDEX(reg)] |= VGT_REG_SAVED;
+}
+
+static inline void reg_set_sticky(struct pgt_device *pdev,
+	vgt_reg_t reg)
+{
+	pdev->reg_info[REG_INDEX(reg)] |= VGT_REG_STICKY;
+}
+
+static inline void reg_set_cmd_access(struct pgt_device *pdev,
+	vgt_reg_t reg)
+{
+	pdev->reg_info[REG_INDEX(reg)] |= VGT_REG_CMD_ACCESS;
+	reg_set_accessed(pdev, reg);
+}
+
+static inline void reg_update_handlers(struct pgt_device *pdev,
+	vgt_reg_t reg, int size, vgt_mmio_read read, vgt_mmio_write write)
+{
+	ASSERT_NUM(reg_is_tracked(pdev, reg), reg);
+	/* TODO search attr table to update fields there */
+	vgt_register_mmio_handler(reg, size, read, write);
+}
+
+/* request types to wake up main thread */
+#define VGT_REQUEST_IRQ		0	/* a new irq pending from device */
+#define VGT_REQUEST_UEVENT	1
+#define VGT_REQUEST_CTX_SWITCH	2	/* immediate reschedule(context switch) requested */
+#define VGT_REQUEST_EMUL_DPY_EVENTS	3
+#define VGT_REQUEST_DPY_SWITCH	4	/* immediate reschedule(display switch) requested */
+#define VGT_REQUEST_DEVICE_RESET 5
+#define VGT_REQUEST_SCHED	6
+#define VGT_REQUEST_CTX_EMULATION_RCS	7 /* Emulate context switch irq of Gen8 */
+#define VGT_REQUEST_CTX_EMULATION_VCS	8 /* Emulate context switch irq of Gen8 */
+#define VGT_REQUEST_CTX_EMULATION_BCS	9 /* Emulate context switch irq of Gen8 */
+#define VGT_REQUEST_CTX_EMULATION_VECS	10 /* Emulate context switch irq of Gen8 */
+#define VGT_REQUEST_CTX_EMULATION_VCS2	11 /* Emulate context switch irq of Gen8 */
+
+static inline void vgt_raise_request(struct pgt_device *pdev, uint32_t flag)
+{
+	set_bit(flag, (void *)&pdev->request);
+	if (waitqueue_active(&pdev->event_wq))
+		wake_up(&pdev->event_wq);
+}
+
+static inline bool vgt_chk_raised_request(struct pgt_device *pdev, uint32_t flag)
+{
+	return !!(test_bit(flag, (void *)&pdev->request));
+}
+
+/* check whether a reg access should happen on real hw */
+static inline bool reg_hw_access(struct vgt_device *vgt, unsigned int reg)
+{
+	struct pgt_device *pdev = vgt->pdev;
+
+	/*
+	 * In superowner mode, all registers, except those explicitly marked
+	 * as sticky, are virtualized to Dom0 while passthrough to the 1st
+	 * HVM.
+	 */
+	if (hvm_super_owner && !reg_is_sticky(pdev, reg)) {
+		if (vgt->vgt_id)
+			return true;
+		else
+			return false;
+	}
+
+	/* allows access from any VM. dangerous!!! */
+	if (reg_passthrough(pdev, reg))
+		return true;
+
+	/* normal phase of passthrough registers if vgt is the owner */
+	if (reg_is_owner(vgt, reg))
+		return true;
+
+	//ASSERT(reg_virt(pdev, reg));
+	return false;
+}
+
+#define IGD_INVALID	0
+#define IGD_SNB		1
+#define IGD_IVB		2
+#define IGD_HSW		3
+#define IGD_BDW		4
+#define IGD_MAX		IGD_BDW
+
+#define IS_SNB(pdev)	((pdev)->gen_dev_type == IGD_SNB)
+#define IS_IVB(pdev)	((pdev)->gen_dev_type == IGD_IVB)
+#define IS_HSW(pdev)	((pdev)->gen_dev_type == IGD_HSW)
+#define IS_BDW(pdev)	((pdev)->gen_dev_type == IGD_BDW)
+
+#define IS_PREBDW(pdev) (IS_SNB(pdev) || IS_IVB(pdev) || IS_HSW(pdev))
+#define IS_BDWPLUS(pdev) (IS_BDW(pdev))
+#define IS_BDWGT3(pdev) (IS_BDW(pdev) && (GEN_REV(pdev->device_info.gen) == 3))
+
+#define D_SNB	(1 << 0)
+#define D_IVB	(1 << 1)
+#define D_HSW	(1 << 2)
+#define D_BDW	(1 << 3)
+
+#define D_GEN8PLUS	(D_BDW)
+#define D_GEN75PLUS	(D_HSW | D_BDW)
+#define D_GEN7PLUS	(D_IVB | D_HSW | D_BDW)
+
+#define D_BDW_PLUS	(D_BDW)
+#define D_HSW_PLUS	(D_HSW | D_BDW)
+#define D_IVB_PLUS	(D_IVB | D_HSW | D_BDW)
+
+#define D_PRE_BDW	(D_SNB | D_IVB | D_HSW)
+
+#define D_ALL		(D_SNB | D_IVB | D_HSW | D_BDW)
+
+typedef struct {
+	u32			reg;
+	int			size;
+	u32			flags;
+	vgt_reg_t		addr_mask;
+	int			device;
+	vgt_mmio_read		read;
+	vgt_mmio_write		write;
+} reg_attr_t;
+
+typedef struct {
+	u32			reg;
+	int			size;
+} reg_list_t;
+
+/*
+ * Comments copied from i915 driver - i915_reg.h :
+ * Haswell does have the CXT_SIZE register however it does not appear to be
+ * valid. Now, docs explain in dwords what is in the context object. The full
+ * size is 70720 bytes, however, the power context and execlist context will
+ * never be saved (power context is stored elsewhere, and execlists don't work
+ * on HSW) - so the final size is 66944 bytes, which rounds to 17 pages.
+ */
+#define HSW_CXT_TOTAL_SIZE		(17 * PAGE_SIZE)
+
+typedef struct {
+	vgt_reg_t   reg;
+	u32			size;
+	int			device;
+} reg_addr_sz_t;
+
+static inline unsigned int vgt_gen_dev_type(struct pgt_device *pdev)
+{
+	if (IS_SNB(pdev))
+		return D_SNB;
+	if (IS_IVB(pdev))
+		return D_IVB;
+	if (IS_HSW(pdev))
+		return D_HSW;
+	if (IS_BDW(pdev))
+		return D_BDW;
+	WARN_ONCE(1, KERN_ERR "vGT: unknown GEN type!\n");
+	return 0;
+}
+
+static inline bool vgt_match_device_attr(struct pgt_device *pdev, reg_attr_t *attr)
+{
+	return attr->device & vgt_gen_dev_type(pdev);
+}
+
+static inline enum vgt_port vgt_get_port(struct vgt_device *vgt, struct gt_port *port_ptr)
+{
+	enum vgt_port port_type;
+
+	if (!vgt || !port_ptr)
+		return I915_MAX_PORTS;
+
+	for (port_type = PORT_A; port_type < I915_MAX_PORTS; ++ port_type)
+		if (port_ptr == &vgt->ports[port_type])
+			break;
+
+	return port_type;
+}
+
+static inline enum vgt_pipe vgt_get_pipe_from_port(struct vgt_device *vgt,
+						enum vgt_port port)
+{
+	enum vgt_pipe pipe;
+
+	if (port == I915_MAX_PORTS)
+		return I915_MAX_PIPES;
+
+	ASSERT (port != PORT_A);
+
+	for (pipe = PIPE_A; pipe < I915_MAX_PIPES; ++ pipe) {
+		vgt_reg_t ddi_func_ctl;
+		vgt_reg_t ddi_port_info;
+
+		ddi_func_ctl  = __vreg(vgt, _VGT_TRANS_DDI_FUNC_CTL(pipe));
+
+		if (!(ddi_func_ctl & _REGBIT_TRANS_DDI_FUNC_ENABLE))
+			continue;
+
+		ddi_port_info = (ddi_func_ctl & _REGBIT_TRANS_DDI_PORT_MASK) >>
+					_TRANS_DDI_PORT_SHIFT;
+		if (ddi_port_info == port) {
+			// pipe has the port setting same as input
+			break;
+		}
+	}
+
+	return pipe;
+}
+
+static inline int tail_to_ring_id(struct pgt_device *pdev, unsigned int tail_off)
+{
+	int i;
+
+	for (i = 0; i < pdev->max_engines; i++) {
+		if ( pdev->ring_mmio_base[i] == tail_off )
+			return i;
+	}
+	printk("Wrong tail register %s\n", __FUNCTION__);
+	ASSERT(0);
+	return 0;
+}
+
+/*
+ * Below are some wrappers for commonly used policy flags.
+ * Add on demand to feed your requirement
+ */
+/* virtualized */
+#define F_VIRT			VGT_OT_NONE | VGT_REG_VIRT
+
+/*
+ * config context (global setting, pm, workaround, etc.)
+ * 	- config owner access pReg
+ *      - non-config owner access vReg
+ * (dom0 is the unique config owner)
+ */
+#define F_DOM0			VGT_OT_CONFIG
+
+/*
+ * render context
+ *	- render owner access pReg
+ *	- non-render owner access vReg
+ */
+#define F_RDR			VGT_OT_RENDER
+/* render context, require address fix */
+#define F_RDR_ADRFIX		F_RDR | VGT_REG_ADDR_FIX
+/* render context, status updated by hw */
+#define F_RDR_HWSTS		F_RDR | VGT_REG_HW_STATUS
+/* render context, mode register (high 16 bits as write mask) */
+#define F_RDR_MODE		F_RDR | VGT_REG_MODE_CTL
+/*
+ * display context
+ *	- display owner access pReg
+ *	- non-display owner access vReg
+ */
+#define F_DPY			VGT_OT_DISPLAY
+/* display context, require address fix */
+#define F_DPY_ADRFIX		F_DPY | VGT_REG_ADDR_FIX
+/* display context, require address fix, status updated by hw */
+#define F_DPY_HWSTS_ADRFIX	F_DPY_ADRFIX | VGT_REG_HW_STATUS
+
+/*
+ * passthrough reg (DANGEROUS!)
+ *	- any VM directly access pReg
+ *	- no save/restore
+ *	- dangerous as a workaround only
+ */
+#define F_PT			VGT_OT_NONE | VGT_REG_PASSTHROUGH
+
+extern int vgt_ctx_switch;
+extern bool vgt_validate_ctx_switch;
+extern bool fastpath_dpy_switch;
+extern void vgt_toggle_ctx_switch(bool enable);
+extern void vgt_kick_off_ringbuffers(struct vgt_device *vgt);
+extern void vgt_kick_off_execution(struct vgt_device *vgt);
+extern void vgt_setup_reg_info(struct pgt_device *pdev);
+extern bool vgt_post_setup_mmio_hooks(struct pgt_device *pdev);
+extern bool vgt_initial_mmio_setup (struct pgt_device *pdev);
+extern void vgt_initial_opregion_setup(struct pgt_device *pdev);
+extern void state_vreg_init(struct vgt_device *vgt);
+extern void state_sreg_init(struct vgt_device *vgt);
+
+/* definitions for physical aperture/GM space */
+#define phys_aperture_sz(pdev)		(pdev->bar_size[1])
+#define phys_aperture_pages(pdev)	(phys_aperture_sz(pdev) >> GTT_PAGE_SHIFT)
+#define phys_aperture_base(pdev)	(pdev->gmadr_base)
+#define phys_aperture_vbase(pdev)	(pdev->gmadr_va)
+
+#define gm_sz(pdev)			(pdev->total_gm_sz)
+#define gm_base(pdev)			(0ULL)
+#define gm_pages(pdev)			(gm_sz(pdev) >> GTT_PAGE_SHIFT)
+#define hidden_gm_base(pdev)		(phys_aperture_sz(pdev))
+
+#define aperture_2_gm(pdev, addr)	(addr - phys_aperture_base(pdev))
+#define v_aperture(pdev, addr)		(phys_aperture_vbase(pdev) + (addr))
+
+#define vm_aperture_sz(pdev)		(pdev->vm_aperture_sz)
+#define vm_gm_sz(pdev)			(pdev->vm_gm_sz)
+#define vm_gm_hidden_sz(pdev)		(vm_gm_sz(pdev) - vm_aperture_sz(pdev))
+
+static inline uint64_t vgt_mmio_bar_base(struct vgt_device *vgt)
+{
+	char *cfg_space = &vgt->state.cfg_space[0];
+	return *(uint64_t *)(cfg_space + VGT_REG_CFG_SPACE_BAR0);
+}
+
+
+/*
+ * Aperture/GM virtualization
+ *
+ * NOTE: the below description says dom0's aperture starts at a non-zero place,
+ * this is only true if you enable the dom0's kernel parameter
+ * dom0_aperture_starts_at_128MB: now by default dom0's aperture starts at 0 of
+ * the GM space since dom0 is the first vm to request for GM space.
+ *
+ * GM is split into two parts: the 1st part visible to CPU through an aperture
+ * window mapping, and the 2nd part only accessible from GPU. The virtualization
+ * policy is like below:
+ *
+ *                | VM1 | VM2 | DOM0| RSVD|    VM1   |    VM2   |
+ *                ------------------------------------------------
+ * Aperture Space |/////|\\\\\|xxxxx|ooooo|                     v
+ * (Dev2_BAR)     v                       v                     v
+ *                v                       v                     v
+ * GM space       v   (visibale part)     v   (invisible part)  v
+ * (start from 0) |/////|\\\\\|xxxxx|ooooo|//////////|\\\\\\\\\\|
+ *                ^     ^                 ^          ^
+ *                |     |  _______________|          |
+ *                |     | /          ________________|
+ * VM1 GM space   |     |/          /
+ * (start from 0) |/////|//////////|
+ */
+
+/* definitions for vgt's aperture/gm space */
+#define vgt_aperture_base(vgt)		(vgt->aperture_base)
+#define vgt_aperture_vbase(vgt)		(vgt->aperture_base_va)
+#define vgt_aperture_offset(vgt)	(vgt->aperture_offset)
+#define vgt_hidden_gm_offset(vgt)	(vgt->hidden_gm_offset)
+#define vgt_aperture_sz(vgt)		(vgt->aperture_sz)
+#define vgt_gm_sz(vgt)			(vgt->gm_sz)
+#define vgt_hidden_gm_sz(vgt)		(vgt_gm_sz(vgt) - vgt_aperture_sz(vgt))
+
+#define vgt_aperture_end(vgt)		\
+	(vgt_aperture_base(vgt) + vgt_aperture_sz(vgt) - 1)
+#define vgt_visible_gm_base(vgt)	\
+	(gm_base(vgt->pdev) + vgt_aperture_offset(vgt))
+#define vgt_visible_gm_end(vgt)		\
+	(vgt_visible_gm_base(vgt) + vgt_aperture_sz(vgt) - 1)
+#define vgt_hidden_gm_base(vgt)	\
+	(gm_base(vgt->pdev) + vgt_hidden_gm_offset(vgt))
+#define vgt_hidden_gm_end(vgt)		\
+	(vgt_hidden_gm_base(vgt) + vgt_hidden_gm_sz(vgt) - 1)
+
+/*
+ * the view of the aperture/gm space from the VM's p.o.v
+ *
+ * when the VM supports ballooning, this view is the same as the
+ * view of vGT driver.
+ *
+ * when the VM does not support ballooning, this view starts from
+ * GM space ZERO
+ */
+#define vgt_guest_aperture_base(vgt)	\
+	(vgt->ballooning ?		\
+		(*((u32*)&vgt->state.cfg_space[VGT_REG_CFG_SPACE_BAR1]) & ~0xf) + vgt_aperture_offset(vgt) :	\
+		(*((u32*)&vgt->state.cfg_space[VGT_REG_CFG_SPACE_BAR1]) & ~0xf))
+#define vgt_guest_aperture_end(vgt)	\
+	(vgt_guest_aperture_base(vgt) + vgt_aperture_sz(vgt) - 1)
+#define vgt_guest_visible_gm_base(vgt)	\
+	(vgt->ballooning ? vgt_visible_gm_base(vgt) : gm_base(vgt->pdev))
+#define vgt_guest_visible_gm_end(vgt)	\
+	(vgt_guest_visible_gm_base(vgt) + vgt_aperture_sz(vgt) - 1)
+#define vgt_guest_hidden_gm_base(vgt)	\
+	(vgt->ballooning ?		\
+		vgt_hidden_gm_base(vgt) :	\
+		vgt_guest_visible_gm_end(vgt) + 1)
+#define vgt_guest_hidden_gm_end(vgt)	\
+	(vgt_guest_hidden_gm_base(vgt) + vgt_hidden_gm_sz(vgt) - 1)
+
+#if 0
+/* These unused functions are for non-ballooning case. */
+/* translate a guest aperture address to host aperture address */
+static inline uint64_t g2h_aperture(struct vgt_device *vgt, uint64_t g_addr)
+{
+	uint64_t offset;
+
+	ASSERT_NUM((g_addr >= vgt_guest_aperture_base(vgt)) &&
+		(g_addr <= vgt_guest_aperture_end(vgt)), g_addr);
+
+	offset = g_addr - vgt_guest_aperture_base(vgt);
+	return vgt_aperture_base(vgt) + offset;
+}
+
+/* translate a host aperture address to guest aperture address */
+static inline uint64_t h2g_aperture(struct vgt_device *vgt, uint64_t h_addr)
+{
+	uint64_t offset;
+
+	ASSERT_NUM((h_addr >= vgt_aperture_base(vgt)) &&
+		(h_addr <= vgt_aperture_end(vgt)), h_addr);
+
+	offset = h_addr - vgt_aperture_base(vgt);
+	return vgt_guest_aperture_base(vgt) + offset;
+}
+#endif
+
+/* check whether a guest GM address is within the CPU visible range */
+static inline bool g_gm_is_visible(struct vgt_device *vgt, uint64_t g_addr)
+{
+	return (g_addr >= vgt_guest_visible_gm_base(vgt)) &&
+		(g_addr <= vgt_guest_visible_gm_end(vgt));
+}
+
+/* check whether a guest GM address is out of the CPU visible range */
+static inline bool g_gm_is_hidden(struct vgt_device *vgt, uint64_t g_addr)
+{
+	return (g_addr >= vgt_guest_hidden_gm_base(vgt)) &&
+		(g_addr <= vgt_guest_hidden_gm_end(vgt));
+}
+
+static inline bool g_gm_is_reserved(struct vgt_device *vgt, uint64_t g_addr)
+{
+	uint64_t rsvd_gm_base = aperture_2_gm(vgt->pdev,
+					vgt->pdev->rsvd_aperture_base);
+
+	return ((g_addr >= rsvd_gm_base) &&
+		(g_addr < (rsvd_gm_base + vgt->pdev->rsvd_aperture_sz)));
+}
+
+static inline bool g_gm_is_valid(struct vgt_device *vgt, uint64_t g_addr)
+{
+	if (vgt->bypass_addr_check)
+		return false;
+
+	return g_gm_is_visible(vgt, g_addr) || g_gm_is_hidden(vgt, g_addr);
+}
+
+/* check whether a host GM address is within the CPU visible range */
+static inline bool h_gm_is_visible(struct vgt_device *vgt, uint64_t h_addr)
+{
+	if (vgt->bypass_addr_check)
+		return true;
+
+	return (h_addr >= vgt_visible_gm_base(vgt)) &&
+		(h_addr <= vgt_visible_gm_end(vgt));
+}
+
+/* check whether a host GM address is out of the CPU visible range */
+static inline bool h_gm_is_hidden(struct vgt_device *vgt, uint64_t h_addr)
+{
+	if (vgt->bypass_addr_check)
+		return true;
+
+	return (h_addr >= vgt_hidden_gm_base(vgt)) &&
+		(h_addr <= vgt_hidden_gm_end(vgt));
+}
+
+static inline bool h_gm_is_valid(struct vgt_device *vgt, uint64_t h_addr)
+{
+	return h_gm_is_visible(vgt, h_addr) || h_gm_is_hidden(vgt, h_addr);
+}
+
+/* for a guest GM address, return the offset within the CPU visible range */
+static inline uint64_t g_gm_visible_offset(struct vgt_device *vgt, uint64_t g_addr)
+{
+	return g_addr - vgt_guest_visible_gm_base(vgt);
+}
+
+/* for a guest GM address, return the offset within the hidden range */
+static inline uint64_t g_gm_hidden_offset(struct vgt_device *vgt, uint64_t g_addr)
+{
+	return g_addr - vgt_guest_hidden_gm_base(vgt);
+}
+
+/* for a host GM address, return the offset within the CPU visible range */
+static inline uint64_t h_gm_visible_offset(struct vgt_device *vgt, uint64_t h_addr)
+{
+	return h_addr - vgt_visible_gm_base(vgt);
+}
+
+/* for a host GM address, return the offset within the hidden range */
+static inline uint64_t h_gm_hidden_offset(struct vgt_device *vgt, uint64_t h_addr)
+{
+	return h_addr - vgt_hidden_gm_base(vgt);
+}
+
+/* validate a gm address and related range size, translate it to host gm address */
+static inline int g2h_gm_range(struct vgt_device *vgt, uint64_t *addr, uint32_t size)
+{
+	ASSERT(addr);
+
+	if (vgt->bypass_addr_check)
+		return 0;
+
+	if ((!g_gm_is_valid(vgt, *addr)) || (size && !g_gm_is_valid(vgt, *addr + size - 1))) {
+		vgt_err("VM(%d): invalid address range: g_addr(0x%llx), size(0x%x)\n",
+			vgt->vm_id, *addr, size);
+		return -EACCES;
+	}
+
+	if (g_gm_is_visible(vgt, *addr))	/* aperture */
+		*addr = vgt_visible_gm_base(vgt) +
+			g_gm_visible_offset(vgt, *addr);
+	else	/* hidden GM space */
+		*addr = vgt_hidden_gm_base(vgt) +
+			g_gm_hidden_offset(vgt, *addr);
+	return 0;
+}
+
+/* translate a guest gm address to host gm address */
+static inline int g2h_gm(struct vgt_device *vgt, uint64_t *addr)
+{
+	return g2h_gm_range(vgt, addr, 4);
+}
+
+/* translate a host gm address to guest gm address */
+static inline uint64_t h2g_gm(struct vgt_device *vgt, uint64_t h_addr)
+{
+	uint64_t g_addr;
+
+	if (vgt->bypass_addr_check)
+		return h_addr;
+
+	ASSERT_NUM(h_gm_is_valid(vgt, h_addr), h_addr);
+
+	if (h_gm_is_visible(vgt, h_addr))
+		g_addr = vgt_guest_visible_gm_base(vgt) +
+			h_gm_visible_offset(vgt, h_addr);
+	else
+		g_addr = vgt_guest_hidden_gm_base(vgt) +
+			h_gm_hidden_offset(vgt, h_addr);
+
+	return g_addr;
+}
+
+extern unsigned long rsvd_aperture_alloc(struct pgt_device *pdev,
+		unsigned long size);
+extern void rsvd_aperture_free(struct pgt_device *pdev, unsigned long start,
+		unsigned long size);
+
+#if 0
+/* This unused function is for non-ballooning case. */
+/*
+ * check whether a structure pointed by MMIO, or an instruction filled in
+ * the command buffer, may cross the visible and invisible boundary. That
+ * should be avoid since physically two parts are not contiguous
+ */
+static inline bool check_g_gm_cross_boundary(struct vgt_device *vgt,
+	uint64_t g_start, uint64_t size)
+{
+	if (vgt->bypass_addr_check)
+		return false;
+
+	if (!vgt_hidden_gm_offset(vgt))
+		return false;
+
+	return g_gm_is_visible(vgt, g_start) &&
+		g_gm_is_hidden(vgt, g_start + size - 1);
+}
+#endif
+
+#define reg_is_mmio(pdev, reg)	\
+	(reg >= 0 && reg < pdev->mmio_size)
+#define reg_is_gtt(pdev, reg)	\
+	(reg >= pdev->device_info.gtt_start_offset \
+	&& reg < pdev->device_info.gtt_start_offset + pdev->gtt_size)
+
+#define GTT_INDEX(pdev, addr)		\
+	((u32)((addr - gm_base(pdev)) >> GTT_PAGE_SHIFT))
+
+static inline uint32_t g2h_gtt_index(struct vgt_device *vgt, uint32_t g_index)
+{
+	uint64_t addr = g_index << GTT_PAGE_SHIFT;
+
+	g2h_gm(vgt, &addr);
+
+	return (uint32_t)(addr >> GTT_PAGE_SHIFT);
+}
+
+static inline uint32_t h2g_gtt_index(struct vgt_device *vgt, uint32_t h_index)
+{
+	uint64_t h_addr = h_index << GTT_PAGE_SHIFT;
+
+	return (uint32_t)(h2g_gm(vgt, h_addr) >> GTT_PAGE_SHIFT);
+}
+
+static inline void __REG_WRITE(struct pgt_device *pdev,
+	unsigned long reg, unsigned long val, int bytes)
+{
+	int ret;
+
+	/*
+	 * TODO: a simple mechanism to capture registers being
+	 * saved/restored at render/display context switch time.
+	 * It's not accurate, since vGT's normal mmio access
+	 * within that window also falls here. But suppose that
+	 * set is small for now.
+	 *
+	 * In the future let's wrap interface like vgt_restore_vreg
+	 * for accurate tracking purpose.
+	 */
+	if (pdev->in_ctx_switch)
+		reg_set_saved(pdev, reg);
+	ret = vgt_native_mmio_write(reg, &val, bytes, false);
+}
+
+static inline unsigned long __REG_READ(struct pgt_device *pdev,
+	unsigned long reg, int bytes)
+{
+	unsigned long data = 0;
+	int ret;
+
+	if (pdev->in_ctx_switch)
+		reg_set_saved(pdev, reg);
+	ret = vgt_native_mmio_read(reg, &data, bytes, false);
+	return data;
+}
+
+#define VGT_MMIO_READ_BYTES(pdev, mmio_offset, bytes)	\
+		__REG_READ(pdev, mmio_offset, bytes)
+
+#define VGT_MMIO_WRITE_BYTES(pdev, mmio_offset, val, bytes)	\
+		__REG_WRITE(pdev, mmio_offset, val, bytes)
+
+#define VGT_MMIO_WRITE(pdev, mmio_offset, val)	\
+		VGT_MMIO_WRITE_BYTES(pdev, mmio_offset, (unsigned long)val, REG_SIZE)
+
+#define VGT_MMIO_READ(pdev, mmio_offset)		\
+		((vgt_reg_t)VGT_MMIO_READ_BYTES(pdev, mmio_offset, REG_SIZE))
+
+#define VGT_MMIO_WRITE64(pdev, mmio_offset, val)	\
+		__REG_WRITE(pdev, mmio_offset, val, 8)
+
+#define VGT_MMIO_READ64(pdev, mmio_offset)		\
+		__REG_READ(pdev, mmio_offset, 8)
+
+#define VGT_REG_IS_ALIGNED(reg, bytes) (!((reg)&((bytes)-1)))
+#define VGT_REG_ALIGN(reg, bytes) ((reg) & ~((bytes)-1))
+
+#define vgt_restore_vreg(vgt, off)		\
+	VGT_MMIO_WRITE(vgt->pdev, off, __vreg(vgt, off))
+
+#define ARRAY_NUM(x)		(sizeof(x) / sizeof(x[0]))
+
+/* context scheduler */
+#define CYCLES_PER_USEC	0x10c7ull
+#define VGT_DEFAULT_TSLICE (4 * 1000 * CYCLES_PER_USEC)
+#define ctx_start_time(vgt) ((vgt)->sched_info.start_time)
+#define ctx_end_time(vgt) ((vgt)->sched_info.end_time)
+#define ctx_remain_time(vgt) ((vgt)->sched_info.time_slice)
+#define ctx_actual_end_time(vgt) ((vgt)->sched_info.actual_end_time)
+#define ctx_rb_empty_delay(vgt) ((vgt)->sched_info.rb_empty_delay)
+
+#define vgt_get_cycles() ({		\
+	cycles_t __ret;				\
+	rdtsc_barrier();			\
+	__ret = get_cycles();		\
+	rdtsc_barrier();			\
+	__ret;						\
+	})
+
+#define RB_HEAD_TAIL_EQUAL(head, tail) \
+	(((head) & RB_HEAD_OFF_MASK) == ((tail) & RB_TAIL_OFF_MASK))
+
+extern bool event_based_qos;
+extern bool vgt_vrings_empty(struct vgt_device *vgt);
+
+/* context scheduler facilities functions */
+static inline bool vgt_runq_is_empty(struct pgt_device *pdev)
+{
+	return (list_empty(&pdev->rendering_runq_head));
+}
+
+static inline void vgt_runq_insert(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	list_add(&vgt->list, &pdev->rendering_runq_head);
+}
+
+static inline void vgt_runq_remove(struct vgt_device *vgt)
+{
+	list_del(&vgt->list);
+}
+
+static inline void vgt_idleq_insert(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	list_add(&vgt->list, &pdev->rendering_idleq_head);
+}
+
+static inline void vgt_idleq_remove(struct vgt_device *vgt)
+{
+	list_del(&vgt->list);
+}
+
+static inline int vgt_nr_in_runq(struct pgt_device *pdev)
+{
+	int count = 0;
+	struct list_head *pos;
+	list_for_each(pos, &pdev->rendering_runq_head)
+		count++;
+	return count;
+}
+
+static inline void vgt_init_sched_info(struct vgt_device *vgt)
+{
+	ctx_remain_time(vgt) = VGT_DEFAULT_TSLICE;
+	ctx_start_time(vgt) = 0;
+	ctx_end_time(vgt) = 0;
+	ctx_actual_end_time(vgt) = 0;
+	ctx_rb_empty_delay(vgt) = 0;
+}
+
+/* main context scheduling process */
+extern void vgt_sched_ctx(struct pgt_device *pdev);
+extern void vgt_setup_countdown(struct vgt_device *vgt);
+extern void vgt_initialize_ctx_scheduler(struct pgt_device *pdev);
+extern void vgt_cleanup_ctx_scheduler(struct pgt_device *pdev);
+
+extern void __raise_ctx_sched(struct vgt_device *vgt);
+#define raise_ctx_sched(vgt) \
+	if (event_based_qos)	\
+		__raise_ctx_sched((vgt))
+
+extern bool shadow_tail_based_qos;
+int vgt_init_rb_tailq(struct vgt_device *vgt);
+void vgt_destroy_rb_tailq(struct vgt_device *vgt);
+int vgt_tailq_pushback(struct vgt_tailq *tailq, u32 tail, u32 cmdnr);
+u32 vgt_tailq_last_stail(struct vgt_tailq *tailq);
+/*
+ *
+ * Activate a VGT instance to render runqueue.
+ */
+static inline void vgt_enable_render(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	ASSERT(spin_is_locked(&pdev->lock));
+	if (bitmap_empty(vgt->enabled_rings, MAX_ENGINES))
+		printk("vGT-%d: Enable render but no ring is enabled yet\n",
+			vgt->vgt_id);
+	/* remove from idle queue */
+	list_del(&vgt->list);
+	/* add to run queue */
+	list_add(&vgt->list, &pdev->rendering_runq_head);
+	printk("vGT-%d: add to render run queue!\n", vgt->vgt_id);
+}
+
+/* now we scheduler all render rings together */
+/* whenever there is a ring enabled, the render(context switch ?) are enabled */
+static inline void vgt_enable_ring(struct vgt_device *vgt, int ring_id)
+{
+	int enable = bitmap_empty(vgt->enabled_rings, MAX_ENGINES);
+
+	set_bit(ring_id, (void *)vgt->enabled_rings);
+	if (enable)
+		vgt_enable_render(vgt);
+}
+
+/*
+ * Remove a VGT instance from render runqueue.
+ */
+static inline void vgt_disable_render(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	ASSERT(spin_is_locked(&pdev->lock));
+	if (!bitmap_empty(vgt->enabled_rings, MAX_ENGINES))
+		printk("vGT-%d: disable render with enabled rings\n",
+			vgt->vgt_id);
+	/* remove from run queue */
+	list_del(&vgt->list);
+	/* add to idle queue */
+	list_add(&vgt->list, &pdev->rendering_idleq_head);
+	printk("vGT-%d: remove from render run queue!\n", vgt->vgt_id);
+}
+
+static inline void vgt_disable_ring(struct vgt_device *vgt, int ring_id)
+{
+	struct pgt_device *pdev = vgt->pdev;
+
+	clear_bit(ring_id, (void *)vgt->started_rings);
+
+	/* multiple disables */
+	if (!test_and_clear_bit(ring_id, (void *)vgt->enabled_rings)) {
+		printk("vGT-%d: disable a disabled ring (%d)\n",
+			vgt->vgt_id, ring_id);
+		return;
+	}
+
+	/* request to remove from runqueue if all rings are disabled */
+	if (bitmap_empty(vgt->enabled_rings, MAX_ENGINES)) {
+		ASSERT(spin_is_locked(&pdev->lock));
+		if (current_render_owner(pdev) == vgt) {
+			pdev->next_sched_vgt = vgt_dom0;
+			vgt_raise_request(pdev, VGT_REQUEST_SCHED);
+		} else
+			vgt_disable_render(vgt);
+	}
+}
+
+static inline uint32_t vgt_ring_id_to_EL_base(enum vgt_ring_id ring_id)
+{
+	uint32_t base = 0;
+
+	switch (ring_id) {
+	case RING_BUFFER_RCS:
+		base = _EL_BASE_RCS;
+		break;
+	case RING_BUFFER_VCS:
+		base = _EL_BASE_VCS;
+		break;
+	case RING_BUFFER_VECS:
+		base = _EL_BASE_VECS;
+		break;
+	case RING_BUFFER_VCS2:
+		base = _EL_BASE_VCS2;
+		break;
+	case RING_BUFFER_BCS:
+		base = _EL_BASE_BCS;
+		break;
+	default:
+		BUG();
+	}
+	return base;
+}
+
+#define el_ring_mmio(ring_id, offset_to_base) \
+(vgt_ring_id_to_EL_base((ring_id)) + (offset_to_base))
+
+static inline enum vgt_event_type vgt_ring_id_to_ctx_event(enum vgt_ring_id ring_id)
+{
+	enum vgt_event_type event;
+
+	switch (ring_id) {
+	case RING_BUFFER_RCS:
+		event = RCS_AS_CONTEXT_SWITCH;
+		break;
+	case RING_BUFFER_VCS:
+		event = VCS_AS_CONTEXT_SWITCH;
+		break;
+	case RING_BUFFER_VECS:
+		event = VECS_AS_CONTEXT_SWITCH;
+		break;
+	case RING_BUFFER_VCS2:
+		event = VCS2_AS_CONTEXT_SWITCH;
+		break;
+	case RING_BUFFER_BCS:
+		event = BCS_AS_CONTEXT_SWITCH;
+		break;
+	default:
+		BUG();
+	}
+	return event;
+}
+
+static inline bool is_ring_empty(struct pgt_device *pdev, int ring_id)
+{
+	if (pdev->enable_execlist) {
+		struct execlist_status_format status;
+		uint32_t status_reg = vgt_ring_id_to_EL_base(ring_id)
+						+ _EL_OFFSET_STATUS;
+		status.ldw = VGT_MMIO_READ(pdev, status_reg);
+		status.udw = VGT_MMIO_READ(pdev, status_reg + 4);
+		return ((status.execlist_0_active == 0) &&
+				(status.execlist_1_active == 0));
+	} else {
+		vgt_reg_t head = VGT_MMIO_READ(pdev, RB_HEAD(pdev, ring_id));
+		vgt_reg_t tail = VGT_MMIO_READ(pdev, RB_TAIL(pdev, ring_id));
+
+		head &= RB_HEAD_OFF_MASK;
+		/*
+		 * PRM said bit2-20 for head count, but bit3-20 for tail count:
+		 * this means: HW increases HEAD by 4, and SW must increase TAIL
+		 * by 8(SW must add padding of MI_NOOP if necessary).
+		 */
+		tail &= RB_TAIL_OFF_MASK;
+		return (head == tail);
+	}
+}
+
+#define VGT_POST_READ(pdev, reg)		\
+	do {					\
+		vgt_reg_t val;			\
+		val = VGT_MMIO_READ(pdev, reg);	\
+	} while (0)
+
+#define VGT_READ_CTL(pdev, id)	VGT_MMIO_READ(pdev, RB_CTL(pdev, id))
+#define VGT_WRITE_CTL(pdev, id, val) VGT_MMIO_WRITE(pdev, RB_CTL(pdev, id), val)
+#define VGT_POST_READ_CTL(pdev, id)	VGT_POST_READ(pdev, RB_CTL(pdev,id))
+
+#define VGT_READ_HEAD(pdev, id)	VGT_MMIO_READ(pdev, RB_HEAD(pdev, id))
+#define VGT_WRITE_HEAD(pdev, id, val) VGT_MMIO_WRITE(pdev, RB_HEAD(pdev, id), val)
+#define VGT_POST_READ_HEAD(pdev, id)	VGT_POST_READ(pdev, RB_HEAD(pdev,id))
+
+#define VGT_READ_TAIL(pdev, id)	VGT_MMIO_READ(pdev, RB_TAIL(pdev, id))
+#define VGT_WRITE_TAIL(pdev, id, val) VGT_MMIO_WRITE(pdev, RB_TAIL(pdev, id), val)
+#define VGT_POST_READ_TAIL(pdev, id)	VGT_POST_READ(pdev, RB_TAIL(pdev,id))
+
+#define VGT_READ_START(pdev, id) VGT_MMIO_READ(pdev, RB_START(pdev, id))
+#define VGT_WRITE_START(pdev, id, val) VGT_MMIO_WRITE(pdev, RB_START(pdev, id), val)
+#define VGT_POST_READ_START(pdev, id)	VGT_POST_READ(pdev, RB_START(pdev,id))
+
+static inline bool is_ring_enabled (struct pgt_device *pdev, int ring_id)
+{
+	return (VGT_MMIO_READ(pdev, RB_CTL(pdev, ring_id)) & 1);	/* bit 0: enable/disable RB */
+}
+extern void vgt_ring_init(struct pgt_device *pdev, int id);
+
+static inline u32 vgt_read_gtt(struct pgt_device *pdev, u32 index)
+{
+	struct vgt_device_info *info = &pdev->device_info;
+	unsigned int off = index << info->gtt_entry_size_shift;
+	u32 ret = 0;
+
+	vgt_native_gtt_read(off, &ret, sizeof(ret));
+	return ret;
+}
+
+static inline void vgt_write_gtt(struct pgt_device *pdev, u32 index, u32 val)
+{
+	struct vgt_device_info *info = &pdev->device_info;
+	unsigned int off = index << info->gtt_entry_size_shift;
+
+	vgt_native_gtt_write(off, &val, sizeof(val));
+}
+
+static inline u64 vgt_read_gtt64(struct pgt_device *pdev, u32 index)
+{
+	struct vgt_device_info *info = &pdev->device_info;
+	unsigned int off = index << info->gtt_entry_size_shift;
+	u64 ret = 0;
+
+	vgt_native_gtt_read(off, &ret, sizeof(ret));
+	return ret;
+}
+
+static inline void vgt_write_gtt64(struct pgt_device *pdev, u32 index, u64 val)
+{
+	struct vgt_device_info *info = &pdev->device_info;
+	unsigned int off = index << info->gtt_entry_size_shift;
+
+	vgt_native_gtt_write(off, &val, sizeof(val));
+}
+
+static inline void vgt_pci_bar_write_32(struct vgt_device *vgt, uint32_t bar_offset, uint32_t val)
+{
+	uint32_t* cfg_reg;
+
+	/* BAR offset should be 32 bits algiend */
+	cfg_reg = (uint32_t*)&vgt->state.cfg_space[bar_offset & ~3];
+
+	/* only write the bits 31-4, leave the 3-0 bits unchanged, as they are read-only */
+	*cfg_reg = (val & 0xFFFFFFF0) | (*cfg_reg & 0xF);
+}
+
+static inline int vgt_pci_mmio_is_enabled(struct vgt_device *vgt)
+{
+	return vgt->state.cfg_space[VGT_REG_CFG_COMMAND] &
+		_REGBIT_CFG_COMMAND_MEMORY;
+}
+
+#define VGT_DPY_EMUL_PERIOD	16000000	// 16 ms for now
+
+struct vgt_irq_host_state;
+typedef void (*vgt_event_phys_handler_t)(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event);
+typedef void (*vgt_event_virt_handler_t)(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event, struct vgt_device *vgt);
+
+struct vgt_irq_ops {
+	void (*init_irq) (struct vgt_irq_host_state *hstate);
+	irqreturn_t (*irq_handler) (struct vgt_irq_host_state *hstate);
+	void (*check_pending_irq) (struct vgt_device *vgt);
+	void (*disable_irq) (struct vgt_irq_host_state *hstate);
+	void (*enable_irq) (struct vgt_irq_host_state *hstate);
+};
+
+/* the list of physical interrupt control register groups */
+enum vgt_irq_type {
+	IRQ_INFO_GT,
+	IRQ_INFO_DPY,
+	IRQ_INFO_PCH,
+	IRQ_INFO_PM,
+
+	IRQ_INFO_MASTER,
+	IRQ_INFO_GT0,
+	IRQ_INFO_GT1,
+	IRQ_INFO_GT2,
+	IRQ_INFO_GT3,
+	IRQ_INFO_DE_PIPE_A,
+	IRQ_INFO_DE_PIPE_B,
+	IRQ_INFO_DE_PIPE_C,
+	IRQ_INFO_DE_PORT,
+	IRQ_INFO_DE_MISC,
+	IRQ_INFO_AUD,
+	IRQ_INFO_PCU,
+
+	IRQ_INFO_MAX,
+};
+
+#define VGT_IRQ_BITWIDTH	32
+/* device specific interrupt bit definitions */
+struct vgt_irq_info {
+	char *name;
+	int reg_base;
+	enum vgt_event_type bit_to_event[VGT_IRQ_BITWIDTH];
+	unsigned long warned;
+	unsigned long default_enabled_events;
+	int group;
+	DECLARE_BITMAP(downstream_irq_bitmap, VGT_IRQ_BITWIDTH);
+	bool has_upstream_irq;
+};
+
+#define	EVENT_FW_ALL 0	/* event forwarded to all instances */
+#define	EVENT_FW_DOM0 1	/* event forwarded to dom0 only */
+#define	EVENT_FW_NONE 2	/* no forward */
+
+/* the handoff state from p-event to v-event */
+union vgt_event_state {
+	/* common state for bit based status */
+	vgt_reg_t val;
+
+	/* command stream error */
+	struct {
+		int eir_reg;
+		vgt_reg_t eir_val;
+	} cmd_err;
+};
+
+/* per-event information */
+struct vgt_event_info {
+	/* device specific info */
+	int			bit;	/* map to register bit */
+	union vgt_event_state	state;	/* handoff state*/
+	struct vgt_irq_info	*info;	/* register info */
+
+	/* device neutral info */
+	int			policy;	/* forwarding policy */
+	vgt_event_phys_handler_t	p_handler;	/* for p_event */
+	vgt_event_virt_handler_t	v_handler;	/* for v_event */
+};
+
+struct vgt_emul_timer {
+	struct hrtimer timer;
+	u64 period;
+};
+
+#define REGBIT_INTERRUPT_PIPE_MASK    0x1f
+
+struct vgt_irq_map {
+	int up_irq_group;
+	int up_irq_bit;
+	int down_irq_group;
+	u32 down_irq_bitmask;
+};
+
+/* structure containing device specific IRQ state */
+struct vgt_irq_host_state {
+	struct pgt_device *pdev;
+	struct vgt_irq_ops *ops;
+	int i915_irq;
+	int pirq;
+	struct vgt_irq_info	*info[IRQ_INFO_MAX];
+	DECLARE_BITMAP(irq_info_bitmap, IRQ_INFO_MAX);
+	struct vgt_event_info	events[EVENT_MAX];
+	DECLARE_BITMAP(pending_events, EVENT_MAX);
+	struct vgt_emul_timer dpy_timer;
+	u32  pipe_mask;
+	bool installed;
+	struct vgt_irq_map *irq_map;
+};
+
+#define vgt_get_event_phys_handler(h, e)	(h->events[e].p_handler)
+#define vgt_get_event_virt_handler(h, e)	(h->events[e].v_handler)
+#define vgt_set_event_val(h, e, v)	(h->events[e].state.val = v)
+#define vgt_get_event_val(h, e)		(h->events[e].state.val)
+#define vgt_get_event_policy(h, e)	(h->events[e].policy)
+#define vgt_get_irq_info(h, e)		(h->events[e].info)
+#define vgt_get_irq_ops(p)		(p->irq_hstate->ops)
+
+/* common offset among interrupt control registers */
+#define regbase_to_isr(base)	(base)
+#define regbase_to_imr(base)	(base + 0x4)
+#define regbase_to_iir(base)	(base + 0x8)
+#define regbase_to_ier(base)	(base + 0xC)
+
+#define iir_to_regbase(iir)    (iir - 0x8)
+#define ier_to_regbase(ier)    (ier - 0xC)
+
+static inline void vgt_clear_all_vreg_bit(struct pgt_device *pdev, unsigned int value, unsigned int offset)
+{
+	struct vgt_device *vgt;
+	vgt_reg_t vreg_data;
+	unsigned int i;
+
+	offset &= ~0x3;
+	for (i = 0; i < VGT_MAX_VMS; i++) {
+		vgt = pdev->device[i];
+		if (vgt) {
+			vreg_data = __vreg(vgt, offset) & (~value);
+			__vreg(vgt, offset) = vreg_data;
+		}
+	}
+}
+
+static inline void vgt_set_all_vreg_bit(struct pgt_device *pdev, unsigned int value, unsigned int offset)
+ {
+	struct vgt_device *vgt;
+	vgt_reg_t vreg_data;
+	unsigned int i;
+
+	offset &= ~0x3;
+	for (i = 0; i < VGT_MAX_VMS; i++) {
+		vgt = pdev->device[i];
+		if (vgt) {
+			vreg_data = __vreg(vgt, offset) | value;
+			__vreg(vgt, offset) = vreg_data;
+		}
+	}
+}
+
+/* wrappers for criticl section in vgt */
+#define vgt_lock_dev(pdev, cpu) {		\
+	if (likely(vgt_track_nest))		\
+		cpu = vgt_enter();		\
+	else					\
+		cpu = 0;			\
+	if (vgt_lock_irq)			\
+		spin_lock_irq(&pdev->lock);	\
+	else					\
+		spin_lock(&pdev->lock);		\
+}
+
+#define vgt_unlock_dev(pdev, cpu) {		\
+	if (vgt_lock_irq)			\
+		spin_unlock_irq(&pdev->lock);	\
+	else					\
+		spin_unlock(&pdev->lock);	\
+	if (likely(vgt_track_nest))		\
+		vgt_exit(cpu);			\
+	else					\
+		cpu = 0;			\
+}
+
+#define vgt_lock_dev_flags(pdev, cpu, flags) {	\
+	flags = 0;				\
+	if (likely(vgt_track_nest))		\
+		cpu = vgt_enter();		\
+	else					\
+		cpu = 0;			\
+	if (vgt_lock_irq)			\
+		spin_lock_irqsave(&pdev->lock, flags);	\
+	else					\
+		spin_lock(&pdev->lock);		\
+}
+
+#define vgt_unlock_dev_flags(pdev, cpu, flags) {	\
+	if (vgt_lock_irq)				\
+		spin_unlock_irqrestore(&pdev->lock, flags); \
+	else						\
+		spin_unlock(&pdev->lock);		\
+	if (likely(vgt_track_nest))			\
+		vgt_exit(cpu);				\
+	else						\
+		cpu = 0;				\
+}
+
+#define vgt_get_irq_lock(pdev, flags) {		\
+	spin_lock_irqsave(&pdev->irq_lock, flags);	\
+}
+
+#define vgt_put_irq_lock(pdev, flags) {		\
+	spin_unlock_irqrestore(&pdev->irq_lock, flags);	\
+}
+
+void vgt_reset_virtual_states(struct vgt_device *vgt, unsigned long ring_bitmap);
+void vgt_reset_ppgtt(struct vgt_device *vgt, unsigned long ring_bitmap);
+
+enum vgt_pipe get_edp_input(uint32_t wr_data);
+void vgt_forward_events(struct pgt_device *pdev);
+void vgt_emulate_dpy_events(struct pgt_device *pdev);
+bool vgt_manage_emul_dpy_events(struct pgt_device *pdev);
+void vgt_update_frmcount(struct vgt_device *vgt, enum vgt_pipe pipe);
+void vgt_calculate_frmcount_delta(struct vgt_device *vgt, enum vgt_pipe pipe);
+void *vgt_install_irq(struct pci_dev *pdev, struct drm_device *dev);
+int vgt_irq_init(struct pgt_device *pgt);
+void vgt_irq_exit(struct pgt_device *pgt);
+
+void vgt_inject_flip_done(struct vgt_device *vgt, enum vgt_pipe pipe);
+
+void vgt_trigger_virtual_event(struct vgt_device *vgt,
+	enum vgt_event_type event);
+
+void vgt_trigger_display_hot_plug(struct pgt_device *dev, vgt_hotplug_cmd_t hotplug_cmd);
+
+void vgt_signal_uevent(struct pgt_device *dev);
+void vgt_hotplug_udev_notify_func(struct work_struct *work);
+
+u32 vgt_recalculate_ier(struct pgt_device *pdev, unsigned int reg);
+u32 vgt_recalculate_mask_bits(struct pgt_device *pdev, unsigned int reg);
+
+void recalculate_and_update_imr(struct pgt_device *pdev, vgt_reg_t reg);
+void recalculate_and_update_ier(struct pgt_device *pdev, vgt_reg_t reg);
+
+bool vgt_reg_master_irq_handler(struct vgt_device *vgt,
+	unsigned int reg, void *p_data, unsigned int bytes);
+bool vgt_reg_imr_handler(struct vgt_device *vgt,
+	unsigned int reg, void *p_data, unsigned int bytes);
+bool vgt_reg_ier_handler(struct vgt_device *vgt,
+	unsigned int reg, void *p_data, unsigned int bytes);
+bool vgt_reg_iir_handler(struct vgt_device *vgt, unsigned int reg,
+	void *p_data, unsigned int bytes);
+bool vgt_reg_isr_write(struct vgt_device *vgt, unsigned int reg,
+	void *p_data, unsigned int bytes);
+bool vgt_reg_isr_read(struct vgt_device *vgt, unsigned int reg,
+	void *p_data, unsigned int bytes);
+void vgt_reg_watchdog_handler(struct vgt_device *state,
+	uint32_t reg, uint32_t val, bool write, ...);
+extern char *vgt_irq_name[EVENT_MAX];
+
+typedef struct {
+	int vm_id;
+	int aperture_sz; /* in MB */
+	int gm_sz;	/* in MB */
+	int fence_sz;
+
+	int vgt_primary; /* 0/1: config the vgt device as secondary/primary VGA,
+						-1: means the ioemu doesn't supply a value */
+} vgt_params_t;
+
+ssize_t get_avl_vm_aperture_gm_and_fence(struct pgt_device *pdev, char *buf,
+		ssize_t buf_sz);
+vgt_reg_t mmio_g2h_gmadr(struct vgt_device *vgt, unsigned long reg, vgt_reg_t g_value);
+vgt_reg_t mmio_h2g_gmadr(struct vgt_device *vgt, unsigned long reg, vgt_reg_t h_value);
+unsigned long rsvd_aperture_alloc(struct pgt_device *pdev, unsigned long size);
+void rsvd_aperture_free(struct pgt_device *pdev, unsigned long start, unsigned long size);
+int allocate_vm_aperture_gm_and_fence(struct vgt_device *vgt, vgt_params_t vp);
+void free_vm_aperture_gm_and_fence(struct vgt_device *vgt);
+int alloc_vm_rsvd_aperture(struct vgt_device *vgt);
+void free_vm_rsvd_aperture(struct vgt_device *vgt);
+void initialize_gm_fence_allocation_bitmaps(struct pgt_device *pdev);
+void vgt_init_reserved_aperture(struct pgt_device *pdev);
+bool vgt_map_plane_reg(struct vgt_device *vgt, unsigned int reg, unsigned int *p_real_offset);
+
+unsigned int vgt_pa_to_mmio_offset(struct vgt_device *vgt, uint64_t pa);
+
+static inline void vgt_set_pipe_mapping(struct vgt_device *vgt,
+	unsigned int v_pipe, unsigned int p_pipe)
+{
+	/* p_pipe == I915_MAX_PIPES means an invalid p_pipe */
+	if (v_pipe < I915_MAX_PIPES && p_pipe <= I915_MAX_PIPES) {
+		vgt->pipe_mapping[v_pipe] = p_pipe;
+	}
+	else {
+		vgt_err("v_pipe=%d, p_pipe=%d!\n", v_pipe, p_pipe);
+		WARN_ON(1);
+	}
+}
+
+bool rebuild_pipe_mapping(struct vgt_device *vgt, unsigned int reg, uint32_t new_data, uint32_t old_data);
+bool update_pipe_mapping(struct vgt_device *vgt, unsigned int physical_reg, uint32_t physical_wr_data);
+
+#include <drm/drmP.h>
+
+extern void *i915_drm_to_pgt(struct drm_device *dev);
+extern void vgt_schedule_host_isr(struct drm_device *dev);
+
+extern void i915_handle_error(struct drm_device *dev, bool wedged,
+		       const char *fmt, ...);
+
+extern int i915_wait_error_work_complete(struct drm_device *dev);
+
+int vgt_reset_device(struct pgt_device *pgt);
+void reset_cached_interrupt_registers(struct pgt_device *pdev);
+
+int create_vgt_instance(struct pgt_device *pdev, struct vgt_device **ptr_vgt, vgt_params_t vp);
+void vgt_release_instance(struct vgt_device *vgt);
+int vgt_init_sysfs(struct pgt_device *pdev);
+void vgt_destroy_sysfs(void);
+extern void vgt_clear_port(struct vgt_device *vgt, int index);
+void vgt_update_monitor_status(struct vgt_device *vgt);
+void vgt_detect_display(struct vgt_device *vgt, int index);
+void vgt_dpy_init_modes(vgt_reg_t *mmio_array);
+
+bool default_mmio_read(struct vgt_device *vgt, unsigned int offset,	void *p_data, unsigned int bytes);
+bool default_mmio_write(struct vgt_device *vgt, unsigned int offset, void *p_data, unsigned int bytes);
+bool default_passthrough_mmio_read(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes);
+
+bool ring_mmio_write_in_rb_mode(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes);
+
+bool ring_mmio_read_in_rb_mode(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes);
+
+bool ring_uhptr_write_in_rb_mode(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes);
+
+bool set_panel_fitting(struct vgt_device *vgt, enum vgt_pipe pipe);
+void vgt_set_power_well(struct vgt_device *vgt, bool enable);
+void vgt_flush_port_info(struct vgt_device *vgt, struct gt_port *port);
+
+extern bool gtt_emulate_read(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes);
+
+extern bool gtt_emulate_write(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes);
+
+#define INVALID_ADDR (~0UL)
+
+extern void* vgt_gma_to_va(struct vgt_mm *mm, unsigned long gma);
+
+
+#define INVALID_MFN	(~0UL)
+
+extern void vgt_ppgtt_switch(struct vgt_device *vgt);
+extern int ring_ppgtt_mode(struct vgt_device *vgt, int ring_id, u32 off, u32 mode);
+
+extern struct dentry *vgt_init_debugfs(struct pgt_device *pdev);
+extern int vgt_create_debugfs(struct vgt_device *vgt);
+
+/* command parser interface */
+#define MAX_CMD_BUDGET  0x7fffffff
+extern int vgt_cmd_parser_init(struct pgt_device *pdev);
+extern void vgt_cmd_parser_exit(void);
+extern int vgt_scan_vring(struct vgt_device *vgt, int ring_id);
+extern void vgt_init_cmd_info(vgt_state_ring_t *rs);
+extern void apply_tail_list(struct vgt_device *vgt, int ring_id,
+	uint64_t submission_id);
+extern int get_submission_id(vgt_state_ring_t *rs, int budget, uint64_t *submission_id);
+
+extern void vgt_submit_commands(struct vgt_device *vgt, int ring_id);
+extern void vgt_sched_update_prev(struct vgt_device *vgt, cycles_t time);
+extern void vgt_sched_update_next(struct vgt_device *vgt);
+extern void vgt_schedule(struct pgt_device *pdev);
+extern void vgt_request_force_removal(struct vgt_device *vgt);
+
+/* klog facility for buck printk */
+extern int vgt_klog_init(void);
+extern void vgt_klog_cleanup(void);
+extern void klog_printk(const char *fmt, ...);
+
+typedef struct {
+	char *node_name;
+	u64 *stat;
+} debug_statistics_t;
+
+extern u64 context_switch_cost;
+extern u64 context_switch_num;
+extern u64 ring_idle_wait;
+extern u64 ring_0_idle;
+extern u64 ring_0_busy;
+extern u64 vm_pending_irq[VGT_MAX_VMS];
+
+struct vgt_port_output_struct {
+	unsigned int ctrl_reg;
+	vgt_reg_t enable_bitmask;
+	vgt_reg_t select_bitmask;
+	enum vgt_output_type output_type;
+};
+
+struct vgt_mmio_dev {
+	int devid_major;
+	char *dev_name;
+	struct class *class;
+	struct cdev cdev;
+	struct device *devnode[VGT_MAX_VMS];
+};
+#define VGT_MMIO_DEV_NAME "vgt_mmio"
+int vgt_init_mmio_device(struct pgt_device *pdev);
+void vgt_cleanup_mmio_dev(struct pgt_device *pdev);
+int vgt_create_mmio_dev(struct vgt_device *vgt);
+void vgt_destroy_mmio_dev(struct vgt_device *vgt);
+
+/* invoked likely in irq disabled condition */
+#define wait_for_atomic(COND, MS) ({					\
+	unsigned long cnt = MS*100;					\
+	int ret__ = 0;							\
+	while (!(COND)) {						\
+		if (!(--cnt)) {						\
+			ret__ = -ETIMEDOUT;				\
+			break;						\
+		}							\
+		udelay(10);						\
+	}								\
+	ret__;								\
+})
+
+extern reg_attr_t vgt_base_reg_info[];
+extern reg_addr_sz_t vgt_reg_addr_sz[];
+extern int vgt_get_base_reg_num(void);
+extern int vgt_get_reg_addr_sz_num(void);
+reg_list_t *vgt_get_sticky_regs(struct pgt_device *pdev);
+extern int vgt_get_sticky_reg_num(struct pgt_device *pdev);
+
+bool vgt_hvm_write_cfg_space(struct vgt_device *vgt,
+       uint64_t addr, unsigned int bytes, unsigned long val);
+bool vgt_hvm_read_cfg_space(struct vgt_device *vgt,
+       uint64_t addr, unsigned int bytes, unsigned long *val);
+
+int vgt_hvm_opregion_map(struct vgt_device *vgt, int map);
+int vgt_hvm_set_trap_area(struct vgt_device *vgt, int map);
+int vgt_hvm_map_aperture (struct vgt_device *vgt, int map);
+int setup_gtt(struct pgt_device *pdev);
+void check_gtt(struct pgt_device *pdev);
+void free_gtt(struct pgt_device *pdev);
+void vgt_clear_gtt(struct vgt_device *vgt);
+void vgt_save_gtt_and_fence(struct pgt_device *pdev);
+void vgt_restore_gtt_and_fence(struct pgt_device *pdev);
+uint64_t vgt_get_gtt_size(struct pgt_device *pdev);
+uint32_t pci_bar_size(struct pgt_device *pdev, unsigned int bar_off);
+struct vgt_device *vmid_2_vgt_device(int vmid);
+extern void vgt_print_edid(struct vgt_edid_data_t *edid);
+extern void vgt_print_dpcd(struct vgt_dpcd_data *dpcd);
+int vgt_fb_notifier_call_chain(unsigned long val, void *data);
+void vgt_init_fb_notify(void);
+void vgt_dom0_ready(struct vgt_device *vgt);
+
+
+
+struct dump_buffer {
+	char *buffer;
+	int buf_len;
+	int buf_size;
+};
+
+int create_dump_buffer(struct dump_buffer *buf, int buf_size);
+void destroy_dump_buffer(struct dump_buffer *buf);
+void dump_string(struct dump_buffer *buf, const char *fmt, ...);
+
+bool vgt_ppgtt_update_shadow_ppgtt_for_ctx(struct vgt_device *vgt, struct execlist_context *el_ctx);
+bool vgt_handle_guest_write_rootp_in_context(struct execlist_context *el_ctx, int idx);
+gtt_entry_t *vgt_get_entry(void *pt, gtt_entry_t *e, unsigned long index);
+void execlist_ctx_table_destroy(struct vgt_device *vgt);
+
+void dump_ctx_desc(struct vgt_device *vgt, struct ctx_desc_format *desc);
+void dump_execlist_status(struct execlist_status_format *status, enum vgt_ring_id ring_id);
+void dump_execlist_info(struct pgt_device *pdev, enum vgt_ring_id ring_id);
+void dump_ctx_status_buf(struct vgt_device *vgt, enum vgt_ring_id ring_id, bool hw_status);
+void dump_regstate_ctx_header (struct reg_state_ctx_header *regstate);
+void dump_el_context_information(struct vgt_device *vgt, struct execlist_context *el_ctx);
+void dump_all_el_contexts(struct pgt_device *pdev);
+void dump_el_status(struct pgt_device *pdev);
+
+void vgt_clear_submitted_el_record(struct pgt_device *pdev, enum vgt_ring_id ring_id);
+void vgt_emulate_context_switch_event(struct pgt_device *pdev, enum vgt_ring_id ring_id);
+void vgt_submit_execlist(struct vgt_device *vgt, enum vgt_ring_id ring_id);
+void vgt_kick_off_execlists(struct vgt_device *vgt);
+bool vgt_idle_execlist(struct pgt_device *pdev, enum vgt_ring_id ring_id);
+struct execlist_context * execlist_context_find(struct vgt_device *vgt, uint32_t guest_lrca);
+
+bool vgt_g2v_execlist_context_create(struct vgt_device *vgt);
+bool vgt_g2v_execlist_context_destroy(struct vgt_device *vgt);
+
+bool vgt_batch_ELSP_write(struct vgt_device *vgt, int ring_id);
+
+static inline void reset_el_structure(struct pgt_device *pdev,
+				enum vgt_ring_id ring_id)
+{
+	el_read_ptr(pdev, ring_id) = DEFAULT_INV_SR_PTR;
+	el_write_ptr(pdev, ring_id) = DEFAULT_INV_SR_PTR;
+	vgt_clear_submitted_el_record(pdev, ring_id);
+	/* reset read ptr in MMIO as well */
+	VGT_MMIO_WRITE(pdev, el_ring_mmio(ring_id, _EL_OFFSET_STATUS_PTR),
+			((_CTXBUF_READ_PTR_MASK << 16) |
+			(DEFAULT_INV_SR_PTR << _CTXBUF_READ_PTR_SHIFT)));
+
+}
+
+extern struct kernel_dm *vgt_pkdm;
+
+static inline unsigned long hypervisor_g2m_pfn(struct vgt_device *vgt,
+	unsigned long g_pfn)
+{
+	return vgt_pkdm->g2m_pfn(vgt->vm_id, g_pfn);
+}
+
+static inline int hypervisor_pause_domain(struct vgt_device *vgt)
+{
+	return vgt_pkdm->pause_domain(vgt->vm_id);
+}
+
+static inline int hypervisor_shutdown_domain(struct vgt_device *vgt)
+{
+	return vgt_pkdm->shutdown_domain(vgt->vm_id);
+}
+
+static inline int hypervisor_map_mfn_to_gpfn(struct vgt_device *vgt,
+	unsigned long gpfn, unsigned long mfn, int nr, int map)
+{
+	if (vgt_pkdm && vgt_pkdm->map_mfn_to_gpfn)
+		return vgt_pkdm->map_mfn_to_gpfn(vgt->vm_id, gpfn, mfn, nr, map);
+
+	return 0;
+}
+
+static inline int hypervisor_set_trap_area(struct vgt_device *vgt,
+	uint64_t start, uint64_t end, bool map)
+{
+	return vgt_pkdm->set_trap_area(vgt, start, end, map);
+}
+
+static inline int hypervisor_set_wp_pages(struct vgt_device *vgt, guest_page_t *p)
+{
+	return vgt_pkdm->set_wp_pages(vgt, p);
+}
+
+static inline int hypervisor_unset_wp_pages(struct vgt_device *vgt, guest_page_t *p)
+{
+	return vgt_pkdm->unset_wp_pages(vgt, p);
+}
+
+static inline int hypervisor_check_host(void)
+{
+	return vgt_pkdm->check_host();
+}
+
+static inline int hypervisor_virt_to_mfn(void *addr)
+{
+	return vgt_pkdm->from_virt_to_mfn(addr);
+}
+
+static inline void *hypervisor_mfn_to_virt(int mfn)
+{
+	return vgt_pkdm->from_mfn_to_virt(mfn);
+}
+
+static inline void hypervisor_inject_msi(struct vgt_device *vgt)
+{
+#define MSI_CAP_OFFSET 0x90	/* FIXME. need to get from cfg emulation */
+#define MSI_CAP_CONTROL (MSI_CAP_OFFSET + 2)
+#define MSI_CAP_ADDRESS (MSI_CAP_OFFSET + 4)
+#define MSI_CAP_DATA	(MSI_CAP_OFFSET + 8)
+#define MSI_CAP_EN 0x1
+
+	char *cfg_space = &vgt->state.cfg_space[0];
+	u16 control = *(u16 *)(cfg_space + MSI_CAP_CONTROL);
+	u32 addr = *(u32 *)(cfg_space + MSI_CAP_ADDRESS);
+	u16 data = *(u16 *)(cfg_space + MSI_CAP_DATA);
+	int r;
+
+	/* Do not generate MSI if MSIEN is disable */
+	if (!(control & MSI_CAP_EN))
+		return;
+
+	/* FIXME: currently only handle one MSI format */
+	ASSERT_NUM(!(control & 0xfffe), control);
+
+	vgt_dbg(VGT_DBG_IRQ, "vGT: VM(%d): hvm injections. address (%x) data(%x)!\n",
+			vgt->vm_id, addr, data);
+	r = vgt_pkdm->inject_msi(vgt->vm_id, addr, data);
+	if (r < 0)
+		vgt_err("vGT(%d): failed to inject vmsi\n", vgt->vgt_id);
+}
+
+static inline int hypervisor_hvm_init(struct vgt_device *vgt)
+{
+	if (vgt_pkdm && vgt_pkdm->hvm_init)
+		return vgt_pkdm->hvm_init(vgt);
+
+	return 0;
+}
+
+static inline void hypervisor_hvm_exit(struct vgt_device *vgt)
+{
+	if (vgt_pkdm && vgt_pkdm->hvm_exit)
+		vgt_pkdm->hvm_exit(vgt);
+}
+
+static inline void *hypervisor_gpa_to_va(struct vgt_device *vgt, unsigned long gpa)
+{
+	if (!vgt->vm_id)
+		return (char *)hypervisor_mfn_to_virt(gpa >> PAGE_SHIFT) + offset_in_page(gpa);
+
+	return vgt_pkdm->gpa_to_va(vgt, gpa);
+}
+
+static inline bool hypervisor_read_va(struct vgt_device *vgt, void *va,
+		void *val, int len, int atomic)
+{
+	bool ret;
+
+	if (!vgt->vm_id) {
+		memcpy(val, va, len);
+		return true;
+	}
+
+	ret = vgt_pkdm->read_va(vgt, va, val, len, atomic);
+	if (unlikely(!ret))
+		vgt_err("VM(%d): read va failed, va: 0x%p, atomic : %s\n", vgt->vm_id,
+				va, atomic ? "yes" : "no");
+
+	return ret;
+}
+
+static inline bool hypervisor_write_va(struct vgt_device *vgt, void *va,
+		void *val, int len, int atomic)
+{
+	bool ret;
+
+	if (!vgt->vm_id) {
+		memcpy(va, val, len);
+		return true;
+	}
+
+	ret = vgt_pkdm->write_va(vgt, va, val, len, atomic);
+	if (unlikely(!ret))
+		vgt_err("VM(%d): write va failed, va: 0x%p, atomic : %s\n", vgt->vm_id,
+				va, atomic ? "yes" : "no");
+
+	return ret;
+}
+
+
+#define ASSERT_VM(x, vgt)						\
+	do {								\
+		if (!(x)) {						\
+			printk("Assert at %s line %d\n",		\
+				__FILE__, __LINE__);			\
+			if (atomic_cmpxchg(&(vgt)->crashing, 0, 1))	\
+				break;					\
+			vgt_warn("Killing VM%d\n", (vgt)->vm_id);	\
+			if (!hypervisor_pause_domain((vgt)))		\
+				hypervisor_shutdown_domain((vgt));	\
+		}							\
+	} while (0)
+
+
+#endif	/* _VGT_DRV_H_ */
diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 982f732..18112bc 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -309,6 +309,22 @@ __oprofile_add_ext_sample(unsigned long pc, struct pt_regs * const regs,
 	unsigned long backtrace = oprofile_backtrace_depth;
 
 	/*
+	 * GPU sampling.
+	 */
+	if ( !pv_info.paravirt_enabled ) {
+		/* Running in native or HVM guest */
+		extern void gpu_perf_sample(void);
+		gpu_perf_sample();
+	}
+#ifdef  CONFIG_XEN_DOM0
+	else {
+		/* Xen Domain0 */
+		extern void vgt_gpu_perf_sample(void);
+		vgt_gpu_perf_sample();
+	}
+#endif
+
+	/*
 	 * if log_sample() fail we can't backtrace since we lost the
 	 * source of this event
 	 */
diff --git a/drivers/pci/quirks.c b/drivers/pci/quirks.c
index 1705dea..993a012 100644
--- a/drivers/pci/quirks.c
+++ b/drivers/pci/quirks.c
@@ -2933,6 +2933,12 @@ static void fixup_debug_report(struct pci_dev *dev, ktime_t calltime,
 	}
 }
 
+#if 0
+   /*
+    * TODO: Temporary disable the GEN register access before
+    * vgt driver is fully initialized to trap-and-emulate.
+    * Need to revisit with on-demand dom0 GEN MMIO trapping.
+    */
 /*
  * Some BIOS implementations leave the Intel GPU interrupts enabled,
  * even though no one is handling them (f.e. i915 driver is never loaded).
@@ -2967,6 +2973,7 @@ static void disable_igfx_irq(struct pci_dev *dev)
 DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_INTEL, 0x0102, disable_igfx_irq);
 DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_INTEL, 0x010a, disable_igfx_irq);
 DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_INTEL, 0x0152, disable_igfx_irq);
+#endif
 
 /*
  * Some devices may pass our check in pci_intx_mask_supported if
diff --git a/drivers/xen/Kconfig b/drivers/xen/Kconfig
index 94ff724..bc1303e 100644
--- a/drivers/xen/Kconfig
+++ b/drivers/xen/Kconfig
@@ -246,4 +246,9 @@ config XEN_EFI
 	def_bool y
 	depends on X86_64 && EFI
 
+config XENGT
+	bool "Xen Dom0 support for i915 vgt device model"
+	depends on XEN_DOM0 && I915_VGT
+	default y
+
 endmenu
diff --git a/drivers/xen/Makefile b/drivers/xen/Makefile
index 57d91f4..2328410 100644
--- a/drivers/xen/Makefile
+++ b/drivers/xen/Makefile
@@ -13,12 +13,16 @@ CFLAGS_features.o			:= $(nostackp)
 
 CFLAGS_efi.o				+= -fshort-wchar
 
+VGT			:= drivers/gpu/drm/i915/vgt
+CFLAGS_xengt.o		+= -Wall -Werror -I$(VGT)
+
 dom0-$(CONFIG_PCI) += pci.o
 dom0-$(CONFIG_USB_SUPPORT) += dbgp.o
 dom0-$(CONFIG_ACPI) += acpi.o $(xen-pad-y)
 xen-pad-$(CONFIG_X86) += xen-acpi-pad.o
 dom0-$(CONFIG_X86) += pcpu.o
 obj-$(CONFIG_XEN_DOM0)			+= $(dom0-y)
+
 obj-$(CONFIG_BLOCK)			+= biomerge.o
 obj-$(CONFIG_XEN_XENCOMM)		+= xencomm.o
 obj-$(CONFIG_XEN_BALLOON)		+= xen-balloon.o
@@ -39,6 +43,7 @@ obj-$(CONFIG_XEN_ACPI_HOTPLUG_MEMORY)	+= xen-acpi-memhotplug.o
 obj-$(CONFIG_XEN_ACPI_HOTPLUG_CPU)	+= xen-acpi-cpuhotplug.o
 obj-$(CONFIG_XEN_ACPI_PROCESSOR)	+= xen-acpi-processor.o
 obj-$(CONFIG_XEN_EFI)			+= efi.o
+obj-$(CONFIG_XENGT)			+= xengt.o
 xen-evtchn-y				:= evtchn.o
 xen-gntdev-y				:= gntdev.o
 xen-gntalloc-y				:= gntalloc.o
diff --git a/drivers/xen/xengt.c b/drivers/xen/xengt.c
new file mode 100644
index 0000000..24d2f3e
--- /dev/null
+++ b/drivers/xen/xengt.c
@@ -0,0 +1,1095 @@
+/*
+ * Interfaces coupled to Xen
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of Version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+/*
+ * NOTE:
+ * This file contains hypervisor specific interactions to
+ * implement the concept of mediated pass-through framework.
+ * What this file provides is actually a general abstraction
+ * of in-kernel device model, which is not vgt specific.
+ *
+ * Now temporarily in vgt code. long-term this should be
+ * in hypervisor (xen/kvm) specific directory
+ */
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/types.h>
+#include <linux/kthread.h>
+#include <linux/time.h>
+#include <linux/freezer.h>
+#include <linux/wait.h>
+#include <linux/sched.h>
+
+#include <asm/xen/hypercall.h>
+#include <asm/xen/page.h>
+#include <xen/xen-ops.h>
+#include <xen/events.h>
+#include <xen/interface/hvm/params.h>
+#include <xen/interface/hvm/ioreq.h>
+#include <xen/interface/hvm/hvm_op.h>
+#include <xen/interface/memory.h>
+#include <xen/interface/platform.h>
+#include <xen/interface/vcpu.h>
+#include <xen/interface/domctl.h>
+
+#include "vgt.h"
+
+MODULE_AUTHOR("Intel Corporation");
+MODULE_DESCRIPTION("XenGT mediated passthrough driver");
+MODULE_LICENSE("GPL");
+MODULE_VERSION("0.1");
+
+#define MAX_HVM_VCPUS_SUPPORTED 128
+struct vgt_hvm_info {
+	/* iopage_vma->addr is just iopage. We need iopage_vma on VM destroy */
+	shared_iopage_t *iopage;
+	struct vm_struct *iopage_vma;
+	int *evtchn_irq; /* the event channle irqs to handle HVM io request
+				index is vcpu id */
+
+	DECLARE_BITMAP(ioreq_pending, MAX_HVM_VCPUS_SUPPORTED);
+	wait_queue_head_t io_event_wq;
+	struct task_struct *emulation_thread;
+
+	int nr_vcpu;
+
+	ioservid_t iosrv_id;    /* io-request server id */
+
+#define VMEM_1MB		(1ULL << 20)	/* the size of the first 1MB */
+#define VMEM_BUCK_SHIFT		20
+#define VMEM_BUCK_SIZE		(1ULL << VMEM_BUCK_SHIFT)
+#define VMEM_BUCK_MASK		(~(VMEM_BUCK_SIZE - 1))
+	uint64_t vmem_sz;
+	/* for the 1st 1MB memory of HVM: each vm_struct means one 4K-page */
+	struct vm_struct **vmem_vma_low_1mb;
+	/* for >1MB memory of HVM: each vm_struct means 1MB */
+	struct vm_struct **vmem_vma;
+	/* for >1MB memory of HVM: each vm_struct means 4KB */
+	struct vm_struct **vmem_vma_4k;
+};
+
+/* Translate from VM's guest pfn to machine pfn */
+static unsigned long xen_g2m_pfn(int vm_id, unsigned long g_pfn)
+{
+	struct xen_get_mfn_from_pfn pfn_arg;
+	int rc;
+	unsigned long pfn_list[1];
+
+	pfn_list[0] = g_pfn;
+
+	set_xen_guest_handle(pfn_arg.pfn_list, pfn_list);
+	pfn_arg.nr_pfns = 1;
+	pfn_arg.domid = vm_id;
+
+	rc = HYPERVISOR_memory_op(XENMEM_get_mfn_from_pfn, &pfn_arg);
+	if(rc < 0){
+		printk("failed to get mfn for gpfn(0x%lx)\n, errno=%d\n", g_pfn, rc);
+		return INVALID_MFN;
+	}
+
+	return pfn_list[0];
+}
+
+static int xen_get_max_gpfn(int vm_id)
+{
+	domid_t dom_id = vm_id;
+	int max_gpfn = HYPERVISOR_memory_op(XENMEM_maximum_gpfn, &dom_id);
+	BUG_ON(max_gpfn < 0);
+	return max_gpfn;
+}
+
+static int xen_pause_domain(int vm_id)
+{
+	int rc;
+	struct xen_domctl domctl;
+
+	domctl.domain = vm_id;
+	domctl.cmd = XEN_DOMCTL_pausedomain;
+	domctl.interface_version = XEN_DOMCTL_INTERFACE_VERSION;
+
+	rc = HYPERVISOR_domctl(&domctl);
+	if (rc != 0)
+		printk("HYPERVISOR_domctl pausedomain fail with %d!\n", rc);
+
+	return rc;
+}
+
+static int xen_shutdown_domain(int vm_id)
+{
+	int rc;
+	struct sched_remote_shutdown r;
+
+	r.reason = SHUTDOWN_crash;
+	r.domain_id = vm_id;
+	rc = HYPERVISOR_sched_op(SCHEDOP_remote_shutdown, &r);
+	if (rc != 0)
+		printk("HYPERVISOR_sched_op failed: %d\n", rc);
+	return rc;
+}
+
+static int xen_domain_iomem_permission(uint32_t domain_id, uint64_t first_mfn,
+                               uint64_t nr_mfns, uint8_t allow_access)
+{
+	struct xen_domctl arg;
+	int rc;
+
+	arg.domain = domain_id;
+	arg.cmd = XEN_DOMCTL_iomem_permission;
+	arg.interface_version = XEN_DOMCTL_INTERFACE_VERSION;
+	arg.u.iomem_permission.first_mfn = first_mfn;
+	arg.u.iomem_permission.nr_mfns = nr_mfns;
+	arg.u.iomem_permission.allow_access = allow_access;
+	rc = HYPERVISOR_domctl(&arg);
+
+	return rc;
+}
+
+static int xen_hvm_memory_mapping(int vm_id, uint64_t first_gfn, uint64_t first_mfn,
+				  uint32_t nr_mfns, uint32_t add_mapping)
+{
+	struct xen_domctl arg;
+	int rc;
+
+	if (add_mapping) {
+		rc = xen_domain_iomem_permission(vm_id, first_mfn, nr_mfns, 1);
+	        if (rc < 0) {
+			printk(KERN_ERR "xen_domain_iomem_permission failed: %d\n", rc);
+	        	return rc;
+		}
+	}
+
+	arg.domain = vm_id;
+	arg.cmd = XEN_DOMCTL_memory_mapping;
+	arg.interface_version = XEN_DOMCTL_INTERFACE_VERSION;
+	arg.u.memory_mapping.first_gfn = first_gfn;
+	arg.u.memory_mapping.first_mfn = first_mfn;
+	arg.u.memory_mapping.nr_mfns = nr_mfns;
+	arg.u.memory_mapping.add_mapping = add_mapping;
+
+	rc = HYPERVISOR_domctl(&arg);
+	if (rc < 0) {
+		printk(KERN_ERR "HYPERVISOR_domctl failed: %d\n", rc);
+		return rc;
+	}
+
+	if (!add_mapping) {
+		rc = xen_domain_iomem_permission(vm_id, first_mfn, nr_mfns, 0);
+	        if (rc < 0) {
+			printk(KERN_ERR "xen_domain_iomem_permission failed: %d\n", rc);
+			return rc;
+		}
+	}
+
+	return rc;
+}
+
+static int xen_map_mfn_to_gpfn(int vm_id, unsigned long gpfn,
+	unsigned long mfn, int nr, int map)
+{
+	int rc;
+	rc = xen_hvm_memory_mapping(vm_id, gpfn, mfn, nr,
+			map ? DPCI_ADD_MAPPING : DPCI_REMOVE_MAPPING);
+	if (rc != 0)
+		printk("xen_hvm_memory_mapping failed: %d\n", rc);
+	return rc;
+}
+
+static int xen_get_nr_vcpu(int vm_id)
+{
+	struct xen_domctl arg;
+	int rc;
+
+	arg.domain = vm_id;
+	arg.cmd = XEN_DOMCTL_getdomaininfo;
+	arg.interface_version = XEN_DOMCTL_INTERFACE_VERSION;
+
+	rc = HYPERVISOR_domctl(&arg);
+	if (rc<0){
+		printk(KERN_ERR "HYPERVISOR_domctl fail ret=%d\n",rc);
+		/* assume it is UP */
+		return 1;
+	}
+
+	return arg.u.getdomaininfo.max_vcpu_id + 1;
+}
+
+static int hvm_create_iorequest_server(struct vgt_device *vgt)
+{
+	struct vgt_hvm_info *info = vgt->hvm_info;
+	struct xen_hvm_create_ioreq_server arg;
+	int r;
+
+	arg.domid = vgt->vm_id;
+	arg.handle_bufioreq = 0;
+	r = HYPERVISOR_hvm_op(HVMOP_create_ioreq_server, &arg);
+	if (r < 0) {
+		printk(KERN_ERR "Cannot create io-requset server: %d!\n", r);
+		return r;
+	}
+	info->iosrv_id = arg.id;
+
+	return r;
+}
+
+static int hvm_toggle_iorequest_server(struct vgt_device *vgt, bool enable)
+{
+	struct vgt_hvm_info *info = vgt->hvm_info;
+	struct xen_hvm_set_ioreq_server_state arg;
+	int r;
+
+	arg.domid = vgt->vm_id;
+	arg.id = info->iosrv_id;
+	arg.enabled = enable;
+	r = HYPERVISOR_hvm_op(HVMOP_set_ioreq_server_state, &arg);
+	if (r < 0) {
+		printk(KERN_ERR "Cannot %s io-request server: %d!\n",
+			enable ? "enable" : "disbale",  r);
+		return r;
+	}
+
+       return r;
+}
+
+static int hvm_get_ioreq_pfn(struct vgt_device *vgt, uint64_t *value)
+{
+	struct vgt_hvm_info *info = vgt->hvm_info;
+	struct xen_hvm_get_ioreq_server_info arg;
+	int r;
+
+	arg.domid = vgt->vm_id;
+	arg.id = info->iosrv_id;
+	r = HYPERVISOR_hvm_op(HVMOP_get_ioreq_server_info, &arg);
+	if (r < 0) {
+		printk(KERN_ERR "Cannot get ioreq pfn: %d!\n", r);
+		return r;
+	}
+	*value = arg.ioreq_pfn;
+	return r;
+}
+
+static int hvm_destroy_iorequest_server(struct vgt_device *vgt)
+{
+	struct vgt_hvm_info *info = vgt->hvm_info;
+	struct xen_hvm_destroy_ioreq_server arg;
+	int r;
+
+	arg.domid = vgt->vm_id;
+	arg.id = info->iosrv_id;
+	r = HYPERVISOR_hvm_op(HVMOP_destroy_ioreq_server, &arg);
+	if (r < 0) {
+		printk(KERN_ERR "Cannot destroy io-request server(%d): %d!\n",
+			info->iosrv_id, r);
+		return r;
+	}
+	info->iosrv_id = 0;
+
+	return r;
+}
+
+static int hvm_map_io_range_to_ioreq_server(struct vgt_device *vgt,
+	int is_mmio, uint64_t start, uint64_t end, int map)
+{
+	struct vgt_hvm_info *info = vgt->hvm_info;
+	xen_hvm_io_range_t arg;
+	int rc;
+
+	arg.domid = vgt->vm_id;
+	arg.id = info->iosrv_id;
+	arg.type = is_mmio ? HVMOP_IO_RANGE_MEMORY : HVMOP_IO_RANGE_PORT;
+	arg.start = start;
+	arg.end = end;
+
+	if (map)
+		rc = HYPERVISOR_hvm_op(HVMOP_map_io_range_to_ioreq_server, &arg);
+	else
+		rc = HYPERVISOR_hvm_op(HVMOP_unmap_io_range_from_ioreq_server, &arg);
+
+	return rc;
+}
+
+static int hvm_map_pcidev_to_ioreq_server(struct vgt_device *vgt, uint64_t sbdf)
+{
+	struct vgt_hvm_info *info = vgt->hvm_info;
+	xen_hvm_io_range_t arg;
+	int rc;
+
+	arg.domid = vgt->vm_id;
+	arg.id = info->iosrv_id;
+	arg.type = HVMOP_IO_RANGE_PCI;
+	arg.start = arg.end = sbdf;
+	rc = HYPERVISOR_hvm_op(HVMOP_map_io_range_to_ioreq_server, &arg);
+	if (rc < 0) {
+		printk(KERN_ERR "Cannot map pci_dev to ioreq_server: %d!\n", rc);
+		return rc;
+	}
+
+	return rc;
+}
+
+static int hvm_set_mem_type(struct vgt_device *vgt,
+	uint16_t mem_type, uint64_t first_pfn, uint64_t nr)
+{
+	xen_hvm_set_mem_type_t args;
+	int rc;
+
+	args.domid = vgt->vm_id;
+	args.hvmmem_type = mem_type;
+	args.first_pfn = first_pfn;
+	args.nr = 1;
+	rc = HYPERVISOR_hvm_op(HVMOP_set_mem_type, &args);
+
+	return rc;
+}
+
+static int hvm_wp_page_to_ioreq_server(struct vgt_device *vgt, unsigned long page, int set)
+{
+	int rc = 0;
+	uint64_t start, end;
+	uint16_t mem_type;
+
+	start = page << PAGE_SHIFT;
+	end = ((page + 1) << PAGE_SHIFT) - 1;
+
+	rc = hvm_map_io_range_to_ioreq_server(vgt, 1, start, end, set);
+	if (rc < 0) {
+		printk(KERN_ERR "Failed to %s page 0x%lx to ioreq_server: %d!\n",
+			set ? "map":"unmap", page , rc);
+		return rc;
+	}
+
+	mem_type = set ? HVMMEM_mmio_write_dm : HVMMEM_ram_rw;
+	rc = hvm_set_mem_type(vgt, mem_type, page, 1);
+	if (rc < 0) {
+		printk(KERN_ERR "Failed to set mem type of page 0x%lx to %s!\n", page,
+			set ? "HVMMEM_mmio_write_dm":"HVMMEM_ram_rw");
+		return rc;
+	}
+	return rc;
+}
+
+static int xen_set_trap_area(struct vgt_device *vgt, uint64_t start, uint64_t end, bool map)
+{
+	if (!vgt_pci_mmio_is_enabled(vgt))
+		return 0;
+
+	return hvm_map_io_range_to_ioreq_server(vgt, 1, start, end, map);
+}
+
+static struct vm_struct *xen_map_iopage(struct vgt_device *vgt)
+{
+	uint64_t ioreq_pfn;
+	int rc;
+
+	rc = hvm_create_iorequest_server(vgt);
+	if (rc < 0)
+		return NULL;
+	rc = hvm_get_ioreq_pfn(vgt, &ioreq_pfn);
+	if (rc < 0) {
+		hvm_destroy_iorequest_server(vgt);
+		return NULL;
+	}
+
+	return xen_remap_domain_mfn_range_in_kernel(ioreq_pfn, 1, vgt->vm_id);
+}
+
+static bool xen_set_guest_page_writeprotection(struct vgt_device *vgt,
+		guest_page_t *guest_page)
+{
+	int r;
+
+	if (guest_page->writeprotection)
+		return true;
+
+	r = hvm_wp_page_to_ioreq_server(vgt, guest_page->gfn, 1);
+	if (r) {
+		vgt_err("fail to set write protection.\n");
+		return false;
+	}
+
+	guest_page->writeprotection = true;
+
+	atomic_inc(&vgt->gtt.n_write_protected_guest_page);
+
+	return true;
+}
+
+static bool xen_clear_guest_page_writeprotection(struct vgt_device *vgt,
+		guest_page_t *guest_page)
+{
+	int r;
+
+	if (!guest_page->writeprotection)
+		return true;
+
+	r = hvm_wp_page_to_ioreq_server(vgt, guest_page->gfn, 0);
+	if (r) {
+		vgt_err("fail to clear write protection.\n");
+		return false;
+	}
+
+	guest_page->writeprotection = false;
+
+	atomic_dec(&vgt->gtt.n_write_protected_guest_page);
+
+	return true;
+}
+
+static int xen_check_host(void)
+{
+	return xen_initial_domain();
+}
+
+static int xen_virt_to_mfn(void *addr)
+{
+	return virt_to_mfn(addr);
+}
+
+static void *xen_mfn_to_virt(int mfn)
+{
+	return mfn_to_virt(mfn);
+}
+
+static int xen_inject_msi(int vm_id, u32 addr_lo, u16 data)
+{
+	struct xen_hvm_inject_msi info = {
+		.domid	= vm_id,
+		.addr	= addr_lo, /* only low addr used */
+		.data	= data,
+	};
+
+	return HYPERVISOR_hvm_op(HVMOP_inject_msi, &info);
+}
+
+static int vgt_hvm_vmem_init(struct vgt_device *vgt)
+{
+	unsigned long i, j, gpfn, count;
+	unsigned long nr_low_1mb_bkt, nr_high_bkt, nr_high_4k_bkt;
+	struct vgt_hvm_info *info = vgt->hvm_info;
+
+	if (!vgt->vm_id)
+		return 0;
+
+	ASSERT(info->vmem_vma == NULL && info->vmem_vma_low_1mb == NULL);
+
+	info->vmem_sz = xen_get_max_gpfn(vgt->vm_id) + 1;
+	info->vmem_sz <<= PAGE_SHIFT;
+
+	/* warn on non-1MB-aligned memory layout of HVM */
+	if (info->vmem_sz & ~VMEM_BUCK_MASK)
+		vgt_warn("VM%d: vmem_sz=0x%llx!\n", vgt->vm_id, info->vmem_sz);
+
+	nr_low_1mb_bkt = VMEM_1MB >> PAGE_SHIFT;
+	nr_high_bkt = (info->vmem_sz >> VMEM_BUCK_SHIFT);
+	nr_high_4k_bkt = (info->vmem_sz >> PAGE_SHIFT);
+
+	info->vmem_vma_low_1mb =
+		kmalloc(sizeof(*info->vmem_vma) * nr_low_1mb_bkt, GFP_KERNEL);
+	info->vmem_vma =
+		kmalloc(sizeof(*info->vmem_vma) * nr_high_bkt, GFP_KERNEL);
+	info->vmem_vma_4k =
+		vzalloc(sizeof(*info->vmem_vma) * nr_high_4k_bkt);
+
+	if (info->vmem_vma_low_1mb == NULL || info->vmem_vma == NULL ||
+		info->vmem_vma_4k == NULL) {
+		vgt_err("Insufficient memory for vmem_vma, vmem_sz=0x%llx\n",
+				info->vmem_sz );
+		goto err;
+	}
+
+	/* map the low 1MB memory */
+	for (i = 0; i < nr_low_1mb_bkt; i++) {
+		info->vmem_vma_low_1mb[i] =
+			xen_remap_domain_mfn_range_in_kernel(i, 1, vgt->vm_id);
+
+		if (info->vmem_vma_low_1mb[i] != NULL)
+			continue;
+
+		/* Don't warn on [0xa0000, 0x100000): a known non-RAM hole */
+		if (i < (0xa0000 >> PAGE_SHIFT))
+			vgt_dbg(VGT_DBG_GENERIC, "vGT: VM%d: can't map GPFN %ld!\n",
+				vgt->vm_id, i);
+	}
+
+	printk("start vmem_map\n");
+	count = 0;
+	/* map the >1MB memory */
+	for (i = 1; i < nr_high_bkt; i++) {
+		gpfn = i << (VMEM_BUCK_SHIFT - PAGE_SHIFT);
+		info->vmem_vma[i] = xen_remap_domain_mfn_range_in_kernel(
+				gpfn, VMEM_BUCK_SIZE >> PAGE_SHIFT, vgt->vm_id);
+
+		if (info->vmem_vma[i] != NULL)
+			continue;
+
+
+		/* for <4G GPFNs: skip the hole after low_mem_max_gpfn */
+		if (gpfn < (1 << (32 - PAGE_SHIFT)) &&
+			vgt->low_mem_max_gpfn != 0 &&
+			gpfn > vgt->low_mem_max_gpfn)
+			continue;
+
+		for (j = gpfn;
+		     j < ((i + 1) << (VMEM_BUCK_SHIFT - PAGE_SHIFT));
+		     j++) {
+			info->vmem_vma_4k[j] = xen_remap_domain_mfn_range_in_kernel(j, 1, vgt->vm_id);
+
+			if (info->vmem_vma_4k[j]) {
+				count++;
+				vgt_dbg(VGT_DBG_GENERIC, "map 4k gpa (%lx)\n", j << PAGE_SHIFT);
+			}
+		}
+
+		/* To reduce the number of err messages(some of them, due to
+		 * the MMIO hole, are spurious and harmless) we only print a
+		 * message if it's at every 64MB boundary or >4GB memory.
+		 */
+		if ((i % 64 == 0) || (i >= (1ULL << (32 - VMEM_BUCK_SHIFT))))
+			vgt_dbg(VGT_DBG_GENERIC, "vGT: VM%d: can't map %ldKB\n",
+				vgt->vm_id, i);
+	}
+	printk("end vmem_map (%ld 4k mappings)\n", count);
+
+	return 0;
+err:
+	kfree(info->vmem_vma);
+	kfree(info->vmem_vma_low_1mb);
+	vfree(info->vmem_vma_4k);
+	info->vmem_vma = info->vmem_vma_low_1mb = info->vmem_vma_4k = NULL;
+	return -ENOMEM;
+}
+
+static void vgt_vmem_destroy(struct vgt_device *vgt)
+{
+	int i, j;
+	unsigned long nr_low_1mb_bkt, nr_high_bkt, nr_high_bkt_4k;
+	struct vgt_hvm_info *info = vgt->hvm_info;
+
+	if (vgt->vm_id == 0)
+		return;
+
+	/*
+	 * Maybe the VM hasn't accessed GEN MMIO(e.g., still in the legacy VGA
+	 * mode), so no mapping is created yet.
+	 */
+	if (info->vmem_vma == NULL && info->vmem_vma_low_1mb == NULL)
+		return;
+
+	ASSERT(info->vmem_vma != NULL && info->vmem_vma_low_1mb != NULL);
+
+	nr_low_1mb_bkt = VMEM_1MB >> PAGE_SHIFT;
+	nr_high_bkt = (info->vmem_sz >> VMEM_BUCK_SHIFT);
+	nr_high_bkt_4k = (info->vmem_sz >> PAGE_SHIFT);
+
+	for (i = 0; i < nr_low_1mb_bkt; i++) {
+		if (info->vmem_vma_low_1mb[i] == NULL)
+			continue;
+		xen_unmap_domain_mfn_range_in_kernel(info->vmem_vma_low_1mb[i],
+				1, vgt->vm_id);
+	}
+
+	for (i = 1; i < nr_high_bkt; i++) {
+		if (info->vmem_vma[i] == NULL) {
+			for (j = (i << (VMEM_BUCK_SHIFT - PAGE_SHIFT));
+			     j < ((i + 1) << (VMEM_BUCK_SHIFT - PAGE_SHIFT));
+			     j++) {
+				if (info->vmem_vma_4k[j] == NULL)
+					continue;
+				xen_unmap_domain_mfn_range_in_kernel(
+					info->vmem_vma_4k[j], 1, vgt->vm_id);
+			}
+			continue;
+		}
+		xen_unmap_domain_mfn_range_in_kernel(
+			info->vmem_vma[i], VMEM_BUCK_SIZE >> PAGE_SHIFT,
+			vgt->vm_id);
+	}
+
+	kfree(info->vmem_vma);
+	kfree(info->vmem_vma_low_1mb);
+	vfree(info->vmem_vma_4k);
+}
+
+static int _hvm_mmio_emulation(struct vgt_device *vgt, struct ioreq *req)
+{
+	int i, sign;
+	void *gva;
+	unsigned long gpa;
+	uint64_t base = vgt_mmio_bar_base(vgt);
+	uint64_t tmp;
+	int pvinfo_page;
+	struct vgt_hvm_info *info = vgt->hvm_info;
+
+	if (info->vmem_vma == NULL) {
+		tmp = vgt_pa_to_mmio_offset(vgt, req->addr);
+		pvinfo_page = (tmp >= VGT_PVINFO_PAGE
+				&& tmp < (VGT_PVINFO_PAGE + VGT_PVINFO_SIZE));
+		/*
+		 * hvmloader will read PVINFO to identify if HVM is in VGT
+		 * or VTD. So we don't trigger HVM mapping logic here.
+		 */
+		if (!pvinfo_page && vgt_hvm_vmem_init(vgt) < 0) {
+			vgt_err("can not map the memory of VM%d!!!\n", vgt->vm_id);
+			ASSERT_VM(info->vmem_vma != NULL, vgt);
+			return -EINVAL;
+		}
+	}
+
+	sign = req->df ? -1 : 1;
+
+	if (req->dir == IOREQ_READ) {
+		/* MMIO READ */
+		if (!req->data_is_ptr) {
+			if (req->count != 1)
+				goto err_ioreq_count;
+
+			//vgt_dbg(VGT_DBG_GENERIC,"HVM_MMIO_read: target register (%lx).\n",
+			//	(unsigned long)req->addr);
+			if (!vgt_emulate_read(vgt, req->addr, &req->data, req->size))
+				return -EINVAL;
+		}
+		else {
+			if ((req->addr + sign * req->count * req->size < base)
+			   || (req->addr + sign * req->count * req->size >=
+				base + vgt->state.bar_size[0]))
+				goto err_ioreq_range;
+			//vgt_dbg(VGT_DBG_GENERIC,"HVM_MMIO_read: rep %d target memory %lx, slow!\n",
+			//	req->count, (unsigned long)req->addr);
+
+			for (i = 0; i < req->count; i++) {
+				if (!vgt_emulate_read(vgt, req->addr + sign * i * req->size,
+					&tmp, req->size))
+					return -EINVAL;
+				gpa = req->data + sign * i * req->size;
+				gva = hypervisor_gpa_to_va(vgt, gpa);
+				if (gva) {
+					if (!IS_SNB(vgt->pdev))
+						memcpy(gva, &tmp, req->size);
+					else {
+						// On the SNB laptop, writing tmp to gva can
+						//cause bug 119. So let's do the writing only on HSW for now.
+						vgt_err("vGT: disable support of string copy instruction on SNB, gpa: 0x%lx\n", gpa);
+					}
+				} else
+					vgt_err("VM %d is trying to store mmio data block to invalid gpa: 0x%lx.\n", vgt->vm_id, gpa);
+			}
+		}
+	}
+	else { /* MMIO Write */
+		if (!req->data_is_ptr) {
+			if (req->count != 1)
+				goto err_ioreq_count;
+			//vgt_dbg(VGT_DBG_GENERIC,"HVM_MMIO_write: target register (%lx).\n", (unsigned long)req->addr);
+			if (!vgt_emulate_write(vgt, req->addr, &req->data, req->size))
+				return -EINVAL;
+		}
+		else {
+			if ((req->addr + sign * req->count * req->size < base)
+			    || (req->addr + sign * req->count * req->size >=
+				base + vgt->state.bar_size[0]))
+				goto err_ioreq_range;
+			//vgt_dbg(VGT_DBG_GENERIC,"HVM_MMIO_write: rep %d target memory %lx, slow!\n",
+			//	req->count, (unsigned long)req->addr);
+
+			for (i = 0; i < req->count; i++) {
+				gpa = req->data + sign * i * req->size;
+				gva = hypervisor_gpa_to_va(vgt, gpa);
+				if (gva != NULL)
+					memcpy(&tmp, gva, req->size);
+				else {
+					tmp = 0;
+					vgt_dbg(VGT_DBG_GENERIC, "vGT: can not read gpa = 0x%lx!!!\n", gpa);
+				}
+				if (!vgt_emulate_write(vgt, req->addr + sign * i * req->size, &tmp, req->size))
+					return -EINVAL;
+			}
+		}
+	}
+
+	return 0;
+
+err_ioreq_count:
+	vgt_err("VM(%d): Unexpected %s request count(%d)\n",
+		vgt->vm_id, req->dir == IOREQ_READ ? "read" : "write",
+		req->count);
+	return -EINVAL;
+
+err_ioreq_range:
+	vgt_err("VM(%d): Invalid %s request addr end(%016llx)\n",
+		vgt->vm_id, req->dir == IOREQ_READ ? "read" : "write",
+		req->addr + sign * req->count * req->size);
+	return -ERANGE;
+}
+
+static int _hvm_pio_emulation(struct vgt_device *vgt, struct ioreq *ioreq)
+{
+	int sign;
+
+	sign = ioreq->df ? -1 : 1;
+
+	if (ioreq->dir == IOREQ_READ) {
+		/* PIO READ */
+		if (!ioreq->data_is_ptr) {
+			if(!vgt_hvm_read_cfg_space(vgt,
+				ioreq->addr,
+				ioreq->size,
+				(unsigned long*)&ioreq->data))
+				return -EINVAL;
+		} else {
+			vgt_dbg(VGT_DBG_GENERIC,"VGT: _hvm_pio_emulation read data_ptr %lx\n",
+			(long)ioreq->data);
+			goto err_data_ptr;
+		}
+	} else {
+		/* PIO WRITE */
+		if (!ioreq->data_is_ptr) {
+			if (!vgt_hvm_write_cfg_space(vgt,
+				ioreq->addr,
+				ioreq->size,
+				(unsigned long)ioreq->data))
+				return -EINVAL;
+		} else {
+			vgt_dbg(VGT_DBG_GENERIC,"VGT: _hvm_pio_emulation write data_ptr %lx\n",
+			(long)ioreq->data);
+			goto err_data_ptr;
+		}
+	}
+	return 0;
+err_data_ptr:
+	/* The data pointer of emulation is guest physical address
+	 * so far, which goes to Qemu emulation, but hard for
+	 * vGT driver which doesn't know gpn_2_mfn translation.
+	 * We may ask hypervisor to use mfn for vGT driver.
+	 * We mark it as unsupported in case guest really it.
+	 */
+	vgt_err("VM(%d): Unsupported %s data_ptr(%lx)\n",
+		vgt->vm_id, ioreq->dir == IOREQ_READ ? "read" : "write",
+		(long)ioreq->data);
+	return -EINVAL;
+}
+
+static int vgt_hvm_do_ioreq(struct vgt_device *vgt, struct ioreq *ioreq)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	uint64_t bdf = PCI_BDF2(pdev->pbus->number, pdev->devfn);
+
+	/* When using ioreq-server, sometimes an event channal
+	 * notification is received with invalid ioreq. Don't
+	 * know the root cause. Put the workaround here.
+	 */
+	if (ioreq->state == STATE_IOREQ_NONE)
+		return 0;
+
+	if (ioreq->type == IOREQ_TYPE_INVALIDATE)
+		return 0;
+
+	switch (ioreq->type) {
+		case IOREQ_TYPE_PCI_CONFIG:
+		/* High 32 bit of ioreq->addr is bdf */
+		if ((ioreq->addr >> 32) != bdf) {
+			printk(KERN_ERR "vGT: Unexpected PCI Dev %lx emulation\n",
+				(unsigned long) (ioreq->addr>>32));
+				return -EINVAL;
+			} else
+				return _hvm_pio_emulation(vgt, ioreq);
+			break;
+		case IOREQ_TYPE_COPY:	/* MMIO */
+			return _hvm_mmio_emulation(vgt, ioreq);
+			break;
+		default:
+			printk(KERN_ERR "vGT: Unknown ioreq type %x addr %llx size %u state %u\n",
+				ioreq->type, ioreq->addr, ioreq->size, ioreq->state);
+			return -EINVAL;
+	}
+
+	return 0;
+}
+
+static struct ioreq *vgt_get_hvm_ioreq(struct vgt_device *vgt, int vcpu)
+{
+	struct vgt_hvm_info *info = vgt->hvm_info;
+	return &(info->iopage->vcpu_ioreq[vcpu]);
+}
+
+static int vgt_emulation_thread(void *priv)
+{
+	struct vgt_device *vgt = (struct vgt_device *)priv;
+	struct vgt_hvm_info *info = vgt->hvm_info;
+
+	int vcpu;
+	int nr_vcpus = info->nr_vcpu;
+
+	struct ioreq *ioreq;
+	int irq, ret;
+
+	vgt_info("start kthread for VM%d\n", vgt->vm_id);
+
+	ASSERT(info->nr_vcpu <= MAX_HVM_VCPUS_SUPPORTED);
+
+	set_freezable();
+	while (1) {
+		ret = wait_event_freezable(info->io_event_wq,
+			kthread_should_stop() ||
+			bitmap_weight(info->ioreq_pending, nr_vcpus));
+		if (ret)
+			vgt_warn("Emulation thread(%d) waken up"
+				 "by unexpected signal!\n", vgt->vm_id);
+
+		if (kthread_should_stop())
+			return 0;
+
+		for (vcpu = 0; vcpu < nr_vcpus; vcpu++) {
+			if (!test_and_clear_bit(vcpu, info->ioreq_pending))
+				continue;
+
+			ioreq = vgt_get_hvm_ioreq(vgt, vcpu);
+
+			if (vgt_hvm_do_ioreq(vgt, ioreq) ||
+					!vgt_expand_shadow_page_mempool(vgt)) {
+				hypervisor_pause_domain(vgt);
+				hypervisor_shutdown_domain(vgt);
+			}
+
+			if (vgt->force_removal)
+				wait_event(vgt->pdev->destroy_wq, !vgt->force_removal);
+
+			ioreq->state = STATE_IORESP_READY;
+
+			irq = info->evtchn_irq[vcpu];
+			notify_remote_via_irq(irq);
+		}
+	}
+
+	BUG(); /* It's actually impossible to reach here */
+	return 0;
+}
+
+static inline void vgt_raise_emulation_request(struct vgt_device *vgt,
+	int vcpu)
+{
+	struct vgt_hvm_info *info = vgt->hvm_info;
+	set_bit(vcpu, info->ioreq_pending);
+	if (waitqueue_active(&info->io_event_wq))
+		wake_up(&info->io_event_wq);
+}
+
+static irqreturn_t vgt_hvm_io_req_handler(int irq, void* dev)
+{
+	struct vgt_device *vgt;
+	struct vgt_hvm_info *info;
+	int vcpu;
+
+	vgt = (struct vgt_device *)dev;
+	info = vgt->hvm_info;
+
+	for(vcpu=0; vcpu < info->nr_vcpu; vcpu++){
+		if(info->evtchn_irq[vcpu] == irq)
+			break;
+	}
+	if (vcpu == info->nr_vcpu){
+		/*opps, irq is not the registered one*/
+		vgt_info("Received a IOREQ w/o vcpu target\n");
+		vgt_info("Possible a false request from event binding\n");
+		return IRQ_NONE;
+	}
+
+	vgt_raise_emulation_request(vgt, vcpu);
+
+	return IRQ_HANDLED;
+}
+
+static void xen_hvm_exit(struct vgt_device *vgt)
+{
+	struct vgt_hvm_info *info;
+	int vcpu;
+
+	info = vgt->hvm_info;
+
+	if (info == NULL)
+		return;
+
+	if (info->iosrv_id != 0)
+		hvm_destroy_iorequest_server(vgt);
+
+	if (info->emulation_thread != NULL)
+		kthread_stop(info->emulation_thread);
+
+	if (!info->nr_vcpu || info->evtchn_irq == NULL)
+		goto out1;
+
+	for (vcpu = 0; vcpu < info->nr_vcpu; vcpu++){
+		if(info->evtchn_irq[vcpu] >= 0)
+			unbind_from_irqhandler(info->evtchn_irq[vcpu], vgt);
+	}
+
+	if (info->iopage_vma != NULL)
+		xen_unmap_domain_mfn_range_in_kernel(info->iopage_vma, 1, vgt->vm_id);
+
+	kfree(info->evtchn_irq);
+
+out1:
+	vgt_vmem_destroy(vgt);
+	kfree(info);
+}
+
+static int xen_hvm_init(struct vgt_device *vgt)
+{
+	struct vgt_hvm_info *info;
+	int vcpu, irq, rc = 0;
+	struct task_struct *thread;
+	struct pgt_device *pdev = vgt->pdev;
+
+	info = kzalloc(sizeof(struct vgt_hvm_info), GFP_KERNEL);
+	if (info == NULL)
+		return -ENOMEM;
+
+	vgt->hvm_info = info;
+
+	info->iopage_vma = xen_map_iopage(vgt);
+	if (info->iopage_vma == NULL) {
+		printk(KERN_ERR "Failed to map HVM I/O page for VM%d\n", vgt->vm_id);
+		rc = -EFAULT;
+		goto err;
+	}
+	info->iopage = info->iopage_vma->addr;
+
+	init_waitqueue_head(&info->io_event_wq);
+
+	info->nr_vcpu = xen_get_nr_vcpu(vgt->vm_id);
+	ASSERT(info->nr_vcpu > 0);
+	ASSERT(info->nr_vcpu <= MAX_HVM_VCPUS_SUPPORTED);
+
+	info->evtchn_irq = kmalloc(info->nr_vcpu * sizeof(int), GFP_KERNEL);
+	if (info->evtchn_irq == NULL){
+		rc = -ENOMEM;
+		goto err;
+	}
+	for( vcpu = 0; vcpu < info->nr_vcpu; vcpu++ )
+		info->evtchn_irq[vcpu] = -1;
+
+	rc = hvm_map_pcidev_to_ioreq_server(vgt, PCI_BDF2(pdev->pbus->number, pdev->devfn));
+	if (rc < 0)
+		goto err;
+	rc = hvm_toggle_iorequest_server(vgt, 1);
+	if (rc < 0)
+		goto err;
+
+	for (vcpu = 0; vcpu < info->nr_vcpu; vcpu++){
+		irq = bind_interdomain_evtchn_to_irqhandler( vgt->vm_id,
+				info->iopage->vcpu_ioreq[vcpu].vp_eport,
+				vgt_hvm_io_req_handler, 0,
+				"vgt", vgt );
+		if ( irq < 0 ){
+			rc = irq;
+			printk(KERN_ERR "Failed to bind event channle for vgt HVM IO handler, rc=%d\n", rc);
+			goto err;
+		}
+		info->evtchn_irq[vcpu] = irq;
+	}
+
+	thread = kthread_run(vgt_emulation_thread, vgt,
+			"vgt_emulation:%d", vgt->vm_id);
+	if(IS_ERR(thread))
+		goto err;
+	info->emulation_thread = thread;
+
+	return 0;
+
+err:
+	xen_hvm_exit(vgt);
+	return rc;
+}
+
+static void *xen_gpa_to_va(struct vgt_device *vgt, unsigned long gpa)
+{
+	unsigned long buck_index, buck_4k_index;
+	struct vgt_hvm_info *info = vgt->hvm_info;
+
+	if (!vgt->vm_id)
+		return (char*)hypervisor_mfn_to_virt(gpa>>PAGE_SHIFT) + (gpa & (PAGE_SIZE-1));
+	/*
+	 * At the beginning of _hvm_mmio_emulation(), we already initialize
+	 * info->vmem_vma and info->vmem_vma_low_1mb.
+	 */
+	ASSERT(info->vmem_vma != NULL && info->vmem_vma_low_1mb != NULL);
+
+	/* handle the low 1MB memory */
+	if (gpa < VMEM_1MB) {
+		buck_index = gpa >> PAGE_SHIFT;
+		if (!info->vmem_vma_low_1mb[buck_index])
+			return NULL;
+
+		return (char*)(info->vmem_vma_low_1mb[buck_index]->addr) +
+			(gpa & ~PAGE_MASK);
+
+	}
+
+	/* handle the >1MB memory */
+	buck_index = gpa >> VMEM_BUCK_SHIFT;
+
+	if (!info->vmem_vma[buck_index]) {
+		buck_4k_index = gpa >> PAGE_SHIFT;
+		if (!info->vmem_vma_4k[buck_4k_index]) {
+			if (buck_4k_index > vgt->low_mem_max_gpfn)
+				vgt_err("vGT failed to map gpa=0x%lx?\n", gpa);
+			return NULL;
+		}
+
+		return (char*)(info->vmem_vma_4k[buck_4k_index]->addr) +
+			(gpa & ~PAGE_MASK);
+	}
+
+	return (char*)(info->vmem_vma[buck_index]->addr) +
+		(gpa & (VMEM_BUCK_SIZE -1));
+}
+
+static bool xen_read_va(struct vgt_device *vgt, void *va, void *val,
+		int len, int atomic)
+{
+	memcpy(val, va, len);
+
+	return true;
+}
+
+static bool xen_write_va(struct vgt_device *vgt, void *va, void *val,
+		int len, int atomic)
+{
+	memcpy(va, val, len);
+	return true;
+}
+
+static struct kernel_dm xen_kdm = {
+	.g2m_pfn = xen_g2m_pfn,
+	.pause_domain = xen_pause_domain,
+	.shutdown_domain = xen_shutdown_domain,
+	.map_mfn_to_gpfn = xen_map_mfn_to_gpfn,
+	.set_trap_area = xen_set_trap_area,
+	.set_wp_pages = xen_set_guest_page_writeprotection,
+	.unset_wp_pages = xen_clear_guest_page_writeprotection,
+	.check_host = xen_check_host,
+	.from_virt_to_mfn = xen_virt_to_mfn,
+	.from_mfn_to_virt = xen_mfn_to_virt,
+	.inject_msi = xen_inject_msi,
+	.hvm_init = xen_hvm_init,
+	.hvm_exit = xen_hvm_exit,
+	.gpa_to_va = xen_gpa_to_va,
+	.read_va = xen_read_va,
+	.write_va = xen_write_va,
+};
+
+struct kernel_dm *vgt_pkdm = &xen_kdm;
diff --git a/include/acpi/acoutput.h b/include/acpi/acoutput.h
index 4f52ea7..64a9397 100644
--- a/include/acpi/acoutput.h
+++ b/include/acpi/acoutput.h
@@ -44,6 +44,9 @@
 #ifndef __ACOUTPUT_H__
 #define __ACOUTPUT_H__
 
+#ifndef ACPI_NO_ERROR_MESSAGES
+#define ACPI_NO_ERROR_MESSAGES
+#endif
 /*
  * Debug levels and component IDs. These are used to control the
  * granularity of the output of the ACPI_DEBUG_PRINT macro -- on a
diff --git a/include/drm/intel-gtt.h b/include/drm/intel-gtt.h
index b08bdad..771d82d 100644
--- a/include/drm/intel-gtt.h
+++ b/include/drm/intel-gtt.h
@@ -6,6 +6,8 @@
 void intel_gtt_get(size_t *gtt_total, size_t *stolen_size,
 		   phys_addr_t *mappable_base, unsigned long *mappable_end);
 
+struct agp_bridge_data;
+
 int intel_gmch_probe(struct pci_dev *bridge_pdev, struct pci_dev *gpu_pdev,
 		     struct agp_bridge_data *bridge);
 void intel_gmch_remove(void);
diff --git a/include/linux/pci.h b/include/linux/pci.h
index 3451f17..2461cc1 100644
--- a/include/linux/pci.h
+++ b/include/linux/pci.h
@@ -1905,4 +1905,8 @@ static inline struct eeh_dev *pci_dev_to_eeh_dev(struct pci_dev *pdev)
  */
 struct pci_dev *pci_find_upstream_pcie_bridge(struct pci_dev *pdev);
 
+/* VGT device definition */
+#define VGT_BUS_ID	0
+#define VGT_DEVFN	0x10	/* B:D:F = 0:2:0 */
+
 #endif /* LINUX_PCI_H */
diff --git a/include/uapi/drm/i915_drm.h b/include/uapi/drm/i915_drm.h
index 2502622..32d0ea0 100644
--- a/include/uapi/drm/i915_drm.h
+++ b/include/uapi/drm/i915_drm.h
@@ -224,6 +224,7 @@ typedef struct _drm_i915_sarea {
 #define DRM_I915_REG_READ		0x31
 #define DRM_I915_GET_RESET_STATS	0x32
 #define DRM_I915_GEM_USERPTR		0x33
+#define DRM_I915_GEM_VGTBUFFER          0x34
 
 #define DRM_IOCTL_I915_INIT		DRM_IOW( DRM_COMMAND_BASE + DRM_I915_INIT, drm_i915_init_t)
 #define DRM_IOCTL_I915_FLUSH		DRM_IO ( DRM_COMMAND_BASE + DRM_I915_FLUSH)
@@ -275,6 +276,7 @@ typedef struct _drm_i915_sarea {
 #define DRM_IOCTL_I915_REG_READ			DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_REG_READ, struct drm_i915_reg_read)
 #define DRM_IOCTL_I915_GET_RESET_STATS		DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_GET_RESET_STATS, struct drm_i915_reset_stats)
 #define DRM_IOCTL_I915_GEM_USERPTR			DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_GEM_USERPTR, struct drm_i915_gem_userptr)
+#define DRM_IOCTL_I915_GEM_VGTBUFFER		DRM_IOWR(DRM_COMMAND_BASE + DRM_I915_GEM_VGTBUFFER, struct drm_i915_gem_vgtbuffer)
 
 /* Allow drivers to submit batchbuffers directly to hardware, relying
  * on the security mechanisms provided by hardware.
@@ -1073,4 +1075,40 @@ struct drm_i915_gem_userptr {
 	__u32 handle;
 };
 
+struct drm_i915_gem_vgtbuffer {
+        __u32 vmid;
+	__u32 plane_id;
+#define I915_VGT_PLANE_PRIMARY 1
+#define I915_VGT_PLANE_SPRITE 2
+#define I915_VGT_PLANE_CURSOR 3
+	__u32 pipe_id;
+	__u32 phys_pipe_id;
+	__u8  enabled;
+	__u8  tiled;
+	__u32 bpp;
+	__u32 hw_format;
+	__u32 drm_format;
+	__u32 start;
+	__u32 x_pos;
+	__u32 y_pos;
+	__u32 x_offset;
+	__u32 y_offset;
+	__u32 size;
+	__u32 width;
+	__u32 height;
+	__u32 stride;
+	__u64 user_ptr;
+	__u32 user_size;
+	__u32 flags;
+#define I915_VGTBUFFER_READ_ONLY (1<<0)
+#define I915_VGTBUFFER_QUERY_ONLY (1<<1)
+#define I915_VGTBUFFER_UNSYNCHRONIZED 0x80000000
+	/**
+	 * Returned handle for the object.
+	 *
+	 * Object handles are nonzero.
+	 */
+	__u32 handle;
+};
+
 #endif /* _UAPI_I915_DRM_H_ */
diff --git a/include/uapi/linux/connector.h b/include/uapi/linux/connector.h
index 4cb2835..e0d969e 100644
--- a/include/uapi/linux/connector.h
+++ b/include/uapi/linux/connector.h
@@ -46,9 +46,9 @@
 #define CN_KVP_VAL			0x1	/* queries from the kernel */
 #define CN_VSS_IDX			0xA     /* HyperV VSS */
 #define CN_VSS_VAL			0x1     /* queries from the kernel */
+#define CN_IDX_VGT			0xB	/* VGT */
 
-
-#define CN_NETLINK_USERS		11	/* Highest index + 1 */
+#define CN_NETLINK_USERS		12	/* Highest index + 1 */
 
 /*
  * Maximum connector's message size.
diff --git a/include/xen/interface/domctl.h b/include/xen/interface/domctl.h
index 286ee32..70c45a9 100644
--- a/include/xen/interface/domctl.h
+++ b/include/xen/interface/domctl.h
@@ -32,7 +32,7 @@
 #include "grant_table.h"
 //#include "hvm/save.h"
 
-#define XEN_DOMCTL_INTERFACE_VERSION 0x0000000b
+#define XEN_DOMCTL_INTERFACE_VERSION 0x0000000a
 
 #if 1
 /*
@@ -852,6 +852,21 @@ struct xen_domctl_cacheflush {
 };
 DEFINE_GUEST_HANDLE_STRUCT(xen_domctl_cacheflush);
 
+struct vgt_io_trap_info {
+        uint64_t s;
+        uint64_t e;
+};
+
+#define MAX_VGT_IO_TRAP_INFO 4
+
+struct xen_domctl_vgt_io_trap {
+        uint32_t n_pio;
+        struct vgt_io_trap_info pio[MAX_VGT_IO_TRAP_INFO];
+
+        uint32_t n_mmio;
+        struct vgt_io_trap_info mmio[MAX_VGT_IO_TRAP_INFO];
+};
+
 #if defined(__i386__) || defined(__x86_64__)
 struct xen_domctl_vcpu_msr {
     uint32_t         index;
@@ -1073,6 +1088,7 @@ struct xen_domctl {
         struct xen_domctl_cacheflush        cacheflush;
         struct xen_domctl_gdbsx_pauseunp_vcpu gdbsx_pauseunp_vcpu;
         struct xen_domctl_gdbsx_domstatus   gdbsx_domstatus;
+	struct xen_domctl_vgt_io_trap       vgt_io_trap;
         uint8_t                             pad[128];
     } u __attribute__((aligned(8)));
 };
diff --git a/include/xen/interface/hvm/hvm_op.h b/include/xen/interface/hvm/hvm_op.h
index 83aeb64..c6e2a24 100644
--- a/include/xen/interface/hvm/hvm_op.h
+++ b/include/xen/interface/hvm/hvm_op.h
@@ -21,6 +21,8 @@
 #ifndef __XEN_PUBLIC_HVM_HVM_OP_H__
 #define __XEN_PUBLIC_HVM_HVM_OP_H__
 
+#include "../event_channel.h"
+
 /* Get/set subcommands: the second argument of the hypercall is a
  * pointer to a xen_hvm_param struct. */
 #define HVMOP_set_param           0
@@ -97,6 +99,8 @@ struct xen_hvm_set_mem_type {
     /* First pfn. */
     aligned_u64 first_pfn;
 };
+typedef struct xen_hvm_set_mem_type xen_hvm_set_mem_type_t;
+DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_set_mem_type_t);
 
 /* Hint from PV drivers for pagetable destruction. */
 #define HVMOP_pagetable_dying       9
@@ -108,12 +112,6 @@ struct xen_hvm_pagetable_dying {
 };
 typedef struct xen_hvm_pagetable_dying xen_hvm_pagetable_dying_t;
 DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_pagetable_dying_t);
- 
-enum hvmmem_type_t {
-    HVMMEM_ram_rw,             /* Normal read/write guest RAM */
-    HVMMEM_ram_ro,             /* Read-only; writes are discarded */
-    HVMMEM_mmio_dm,            /* Reads and write go to the device model */
-};
 
 #define HVMOP_get_mem_type    15
 /* Return hvmmem_type_t for the specified pfn. */
@@ -138,6 +136,159 @@ struct xen_hvm_inject_msi {
     /* Address (0xfeexxxxx) */
     uint64_t  addr;
 };
-DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_inject_msi);
+typedef struct xen_hvm_inject_msi xen_hvm_inject_msi_t;
+DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_inject_msi_t);
+
+enum hvmmem_type_t {
+    HVMMEM_ram_rw,             /* Normal read/write guest RAM */
+    HVMMEM_ram_ro,             /* Read-only; writes are discarded */
+    HVMMEM_mmio_dm,            /* Reads and write go to the device model */
+    HVMMEM_mmio_write_dm       /* Read-only; writes go to the device model */
+};
+
+#define HVMOP_vgt_wp_pages         27  /* writeprotection to guest pages */
+#define MAX_WP_BATCH_PAGES         128
+struct xen_hvm_vgt_wp_pages {
+	uint16_t domid;
+	uint16_t set;            /* 1: set WP, 0: remove WP */
+	uint16_t nr_pages;
+	unsigned long  wp_pages[MAX_WP_BATCH_PAGES];
+};
+typedef struct xen_hvm_vgt_wp_pages xen_hvm_vgt_wp_pages_t;
+
+/*
+ * IOREQ Servers
+ *
+ * The interface between an I/O emulator an Xen is called an IOREQ Server.
+ * A domain supports a single 'legacy' IOREQ Server which is instantiated if
+ * parameter...
+ *
+ * HVM_PARAM_IOREQ_PFN is read (to get the gmfn containing the synchronous
+ * ioreq structures), or...
+ * HVM_PARAM_BUFIOREQ_PFN is read (to get the gmfn containing the buffered
+ * ioreq ring), or...
+ * HVM_PARAM_BUFIOREQ_EVTCHN is read (to get the event channel that Xen uses
+ * to request buffered I/O emulation).
+ *
+ * The following hypercalls facilitate the creation of IOREQ Servers for
+ * 'secondary' emulators which are invoked to implement port I/O, memory, or
+ * PCI config space ranges which they explicitly register.
+ */
+typedef uint16_t ioservid_t;
+
+/*
+ * HVMOP_create_ioreq_server: Instantiate a new IOREQ Server for a secondary
+ *                            emulator servicing domain <domid>.
+ *
+ * The <id> handed back is unique for <domid>. If <handle_bufioreq> is zero
+ * the buffered ioreq ring will not be allocated and hence all emulation
+ * requestes to this server will be synchronous.
+ */
+#define HVMOP_create_ioreq_server 17
+struct xen_hvm_create_ioreq_server {
+    domid_t domid;           /* IN - domain to be serviced */
+    uint8_t handle_bufioreq; /* IN - should server handle buffered ioreqs */
+    ioservid_t id;           /* OUT - server id */
+};
+typedef struct xen_hvm_create_ioreq_server xen_hvm_create_ioreq_server_t;
+DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_create_ioreq_server_t);
+
+/*
+ * HVMOP_get_ioreq_server_info: Get all the information necessary to access
+ *                              IOREQ Server <id>.
+ *
+ * The emulator needs to map the synchronous ioreq structures and buffered
+ * ioreq ring (if it exists) that Xen uses to request emulation. These are
+ * hosted in domain <domid>'s gmfns <ioreq_pfn> and <bufioreq_pfn>
+ * respectively. In addition, if the IOREQ Server is handling buffered
+ * emulation requests, the emulator needs to bind to event channel
+ * <bufioreq_port> to listen for them. (The event channels used for
+ * synchronous emulation requests are specified in the per-CPU ioreq
+ * structures in <ioreq_pfn>).
+ * If the IOREQ Server is not handling buffered emulation requests then the
+ * values handed back in <bufioreq_pfn> and <bufioreq_port> will both be 0.
+ */
+#define HVMOP_get_ioreq_server_info 18
+struct xen_hvm_get_ioreq_server_info {
+    domid_t domid;                 /* IN - domain to be serviced */
+    ioservid_t id;                 /* IN - server id */
+    evtchn_port_t bufioreq_port;   /* OUT - buffered ioreq port */
+    uint64_t ioreq_pfn;    /* OUT - sync ioreq pfn */
+    uint64_t bufioreq_pfn; /* OUT - buffered ioreq pfn */
+};
+typedef struct xen_hvm_get_ioreq_server_info xen_hvm_get_ioreq_server_info_t;
+DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_get_ioreq_server_info_t);
+
+/*
+ * HVM_map_io_range_to_ioreq_server: Register an I/O range of domain <domid>
+ *                                   for emulation by the client of IOREQ
+ *                                   Server <id>
+ * HVM_unmap_io_range_from_ioreq_server: Deregister an I/O range of <domid>
+ *                                       for emulation by the client of IOREQ
+ *                                       Server <id>
+ *
+ * There are three types of I/O that can be emulated: port I/O, memory accesses
+ * and PCI config space accesses. The <type> field denotes which type of range
+ * the <start> and <end> (inclusive) fields are specifying.
+ * PCI config space ranges are specified by segment/bus/device/function values
+ * which should be encoded using the HVMOP_PCI_SBDF helper macro below.
+ *
+ * NOTE: unless an emulation request falls entirely within a range mapped
+ * by a secondary emulator, it will not be passed to that emulator.
+ */
+#define HVMOP_map_io_range_to_ioreq_server 19
+#define HVMOP_unmap_io_range_from_ioreq_server 20
+struct xen_hvm_io_range {
+    domid_t domid;               /* IN - domain to be serviced */
+    ioservid_t id;               /* IN - server id */
+    uint32_t type;               /* IN - type of range */
+# define HVMOP_IO_RANGE_PORT   0 /* I/O port range */
+# define HVMOP_IO_RANGE_MEMORY 1 /* MMIO range */
+# define HVMOP_IO_RANGE_PCI    2 /* PCI segment/bus/dev/func range */
+    uint64_t start, end; /* IN - inclusive start and end of range */
+};
+typedef struct xen_hvm_io_range xen_hvm_io_range_t;
+DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_io_range_t);
+
+#define HVMOP_PCI_SBDF(s,b,d,f)                 \
+       ((((s) & 0xffff) << 16) |                   \
+        (((b) & 0xff) << 8) |                      \
+        (((d) & 0x1f) << 3) |                      \
+        ((f) & 0x07))
+
+/*
+ * HVMOP_destroy_ioreq_server: Destroy the IOREQ Server <id> servicing domain
+ *                             <domid>.
+ *
+ * Any registered I/O ranges will be automatically deregistered.
+ */
+#define HVMOP_destroy_ioreq_server 21
+struct xen_hvm_destroy_ioreq_server {
+    domid_t domid; /* IN - domain to be serviced */
+    ioservid_t id; /* IN - server id */
+};
+typedef struct xen_hvm_destroy_ioreq_server xen_hvm_destroy_ioreq_server_t;
+DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_destroy_ioreq_server_t);
+
+
+/*
+ * HVMOP_set_ioreq_server_state: Enable or disable the IOREQ Server <id> servicing
+ *                               domain <domid>.
+ *
+ * The IOREQ Server will not be passed any emulation requests until it is in the
+ * enabled state.
+ * Note that the contents of the ioreq_pfn and bufioreq_fn (see
+ * HVMOP_get_ioreq_server_info) are not meaningful until the IOREQ Server is in
+ * the enabled state.
+ */
+#define HVMOP_set_ioreq_server_state 22
+struct xen_hvm_set_ioreq_server_state {
+    domid_t domid;   /* IN - domain to be serviced */
+    ioservid_t id;   /* IN - server id */
+    uint8_t enabled; /* IN - enabled? */
+};
+typedef struct xen_hvm_set_ioreq_server_state xen_hvm_set_ioreq_server_state_t;
+DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_set_ioreq_server_state_t);
+
 
 #endif /* __XEN_PUBLIC_HVM_HVM_OP_H__ */
diff --git a/include/xen/interface/hvm/ioreq.h b/include/xen/interface/hvm/ioreq.h
new file mode 100644
index 0000000..6bbf4e4
--- /dev/null
+++ b/include/xen/interface/hvm/ioreq.h
@@ -0,0 +1,132 @@
+/*
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+#ifndef _IOREQ_H_
+#define _IOREQ_H_
+
+#define IOREQ_READ      1
+#define IOREQ_WRITE     0
+
+#define STATE_IOREQ_NONE        0
+#define STATE_IOREQ_READY       1
+#define STATE_IOREQ_INPROCESS   2
+#define STATE_IORESP_READY      3
+
+#define IOREQ_TYPE_PIO          0 /* pio */
+#define IOREQ_TYPE_COPY         1 /* mmio ops */
+#define IOREQ_TYPE_PCI_CONFIG   2
+#define IOREQ_TYPE_TIMEOFFSET   7
+#define IOREQ_TYPE_INVALIDATE   8 /* mapcache */
+
+/*
+ * VMExit dispatcher should cooperate with instruction decoder to
+ * prepare this structure and notify service OS and DM by sending
+ * virq
+ */
+struct ioreq {
+    uint64_t addr;          /* physical address */
+    uint64_t data;          /* data (or paddr of data) */
+    uint32_t count;         /* for rep prefixes */
+    uint32_t size;          /* size in bytes */
+    uint32_t vp_eport;      /* evtchn for notifications to/from device model */
+    uint16_t _pad0;
+    uint8_t state:4;
+    uint8_t data_is_ptr:1;  /* if 1, data above is the guest paddr
+                             * of the real data to use. */
+    uint8_t dir:1;          /* 1=read, 0=write */
+    uint8_t df:1;
+    uint8_t _pad1:1;
+    uint8_t type;           /* I/O type */
+};
+typedef struct ioreq ioreq_t;
+
+struct shared_iopage {
+    struct ioreq vcpu_ioreq[1];
+};
+typedef struct shared_iopage shared_iopage_t;
+
+struct buf_ioreq {
+    uint8_t  type;   /* I/O type                    */
+    uint8_t  pad:1;
+    uint8_t  dir:1;  /* 1=read, 0=write             */
+    uint8_t  size:2; /* 0=>1, 1=>2, 2=>4, 3=>8. If 8, use two buf_ioreqs */
+    uint32_t addr:20;/* physical address            */
+    uint32_t data;   /* data                        */
+};
+typedef struct buf_ioreq buf_ioreq_t;
+
+#define IOREQ_BUFFER_SLOT_NUM     511 /* 8 bytes each, plus 2 4-byte indexes */
+struct buffered_iopage {
+    unsigned int read_pointer;
+    unsigned int write_pointer;
+    buf_ioreq_t buf_ioreq[IOREQ_BUFFER_SLOT_NUM];
+}; /* NB. Size of this structure must be no greater than one page. */
+typedef struct buffered_iopage buffered_iopage_t;
+
+#if defined(__ia64__)
+struct pio_buffer {
+    uint32_t page_offset;
+    uint32_t pointer;
+    uint32_t data_end;
+    uint32_t buf_size;
+    void *opaque;
+};
+
+#define PIO_BUFFER_IDE_PRIMARY   0 /* I/O port = 0x1F0 */
+#define PIO_BUFFER_IDE_SECONDARY 1 /* I/O port = 0x170 */
+#define PIO_BUFFER_ENTRY_NUM     2
+struct buffered_piopage {
+    struct pio_buffer pio[PIO_BUFFER_ENTRY_NUM];
+    uint8_t buffer[1];
+};
+#endif /* defined(__ia64__) */
+
+/*
+ * ACPI Control/Event register locations. Location is controlled by a
+ * version number in HVM_PARAM_ACPI_IOPORTS_LOCATION.
+ */
+
+/* Version 0 (default): Traditional Xen locations. */
+#define ACPI_PM1A_EVT_BLK_ADDRESS_V0 0x1f40
+#define ACPI_PM1A_CNT_BLK_ADDRESS_V0 (ACPI_PM1A_EVT_BLK_ADDRESS_V0 + 0x04)
+#define ACPI_PM_TMR_BLK_ADDRESS_V0   (ACPI_PM1A_EVT_BLK_ADDRESS_V0 + 0x08)
+#define ACPI_GPE0_BLK_ADDRESS_V0     (ACPI_PM_TMR_BLK_ADDRESS_V0 + 0x20)
+#define ACPI_GPE0_BLK_LEN_V0         0x08
+
+/* Version 1: Locations preferred by modern Qemu. */
+#define ACPI_PM1A_EVT_BLK_ADDRESS_V1 0xb000
+#define ACPI_PM1A_CNT_BLK_ADDRESS_V1 (ACPI_PM1A_EVT_BLK_ADDRESS_V1 + 0x04)
+#define ACPI_PM_TMR_BLK_ADDRESS_V1   (ACPI_PM1A_EVT_BLK_ADDRESS_V1 + 0x08)
+#define ACPI_GPE0_BLK_ADDRESS_V1     0xafe0
+#define ACPI_GPE0_BLK_LEN_V1         0x04
+
+/* Compatibility definitions for the default location (version 0). */
+#define ACPI_PM1A_EVT_BLK_ADDRESS    ACPI_PM1A_EVT_BLK_ADDRESS_V0
+#define ACPI_PM1A_CNT_BLK_ADDRESS    ACPI_PM1A_CNT_BLK_ADDRESS_V0
+#define ACPI_PM_TMR_BLK_ADDRESS      ACPI_PM_TMR_BLK_ADDRESS_V0
+#define ACPI_GPE0_BLK_ADDRESS        ACPI_GPE0_BLK_ADDRESS_V0
+#define ACPI_GPE0_BLK_LEN            ACPI_GPE0_BLK_LEN_V0
+
+
+#endif /* _IOREQ_H_ */
+
+/*
+ * Local variables:
+ * mode: C
+ * c-set-style: "BSD"
+ * c-basic-offset: 4
+ * tab-width: 4
+ * indent-tabs-mode: nil
+ * End:
+ */
diff --git a/include/xen/interface/memory.h b/include/xen/interface/memory.h
index b8ac3b3..7e093ea 100644
--- a/include/xen/interface/memory.h
+++ b/include/xen/interface/memory.h
@@ -112,6 +112,11 @@ DEFINE_GUEST_HANDLE_STRUCT(xen_memory_exchange);
 #define XENMEM_maximum_reservation  4
 
 /*
+ * Returns the maximum GPFN in use by the guest, or -ve errcode on failure.
+ */
+#define XENMEM_maximum_gpfn         14
+
+/*
  * Returns a list of MFN bases of 2MB extents comprising the machine_to_phys
  * mapping table. Architectures which do not have a m2p table do not implement
  * this command.
@@ -241,6 +246,27 @@ DEFINE_GUEST_HANDLE_STRUCT(xen_memory_map);
  */
 #define XENMEM_machine_memory_map   10
 
+/*
+ * Translate the given guest PFNs to MFNs
+ */
+#define XENMEM_get_mfn_from_pfn    25
+struct xen_get_mfn_from_pfn {
+    /*
+     * Pointer to buffer to fill with list of pfn.
+     * for IN, it contains the guest PFN that need to translated
+     * for OUT, it contains the translated MFN. or INVALID_MFN if no valid translation
+     */
+    GUEST_HANDLE(ulong) pfn_list;
+
+    /*
+     * IN: Size of the pfn_array.
+     */
+    unsigned int nr_pfns;
+
+    /* IN: which domain */
+    domid_t domid;
+};
+DEFINE_GUEST_HANDLE_STRUCT(xen_get_mfn_from_pfn);
 
 /*
  * Prevent the balloon driver from changing the memory reservation
diff --git a/include/xen/interface/vcpu.h b/include/xen/interface/vcpu.h
index 87e6f8a..182267f 100644
--- a/include/xen/interface/vcpu.h
+++ b/include/xen/interface/vcpu.h
@@ -170,4 +170,49 @@ struct vcpu_register_vcpu_info {
 };
 DEFINE_GUEST_HANDLE_STRUCT(vcpu_register_vcpu_info);
 
+
+/* Request an I/O emulation for the specified VCPU. */
+#define VCPUOP_request_io_emulation       14
+#define PV_IOREQ_READ      1
+#define PV_IOREQ_WRITE     0
+
+#define PV_IOREQ_TYPE_PIO          0 /* pio */
+#define PV_IOREQ_TYPE_COPY         1 /* mmio ops */
+
+struct vcpu_emul_ioreq {
+    uint64_t      addr;           /* physical address */
+    uint64_t      data;           /* data (or paddr of data) */
+    uint64_t      count;          /* for rep prefixes */
+    uint32_t      size;           /* size in bytes */
+    uint16_t      _pad0;
+    uint8_t       state:4;
+    uint8_t       data_is_ptr:1;  /* if 1, data above is the guest paddr
+                                   * of the real data to use. */
+    uint8_t       dir:1;          /* 1=read, 0=write */
+    uint8_t       df:1;
+    uint8_t       _pad1:1;
+    uint8_t       type;           /* I/O type */
+};
+DEFINE_GUEST_HANDLE_STRUCT(vcpu_emul_ioreq);
+
+#define VCPUOP_get_sysdata           16
+/* sub operations */
+#define VCPUOP_sysdata_get_segment   0
+#define VCPUOP_sysdata_read	     1
+struct vcpu_sysdata_request {
+    uint64_t      op_type;
+    union {
+	struct {
+	    uint32_t     selector;
+            uint32_t     pad1;
+	    uint64_t     xdt_desc[2];
+	};
+	struct {
+	    uint64_t     src_addr;	/* linear address */
+            uint64_t     sys_data;
+            uint32_t     bytes;
+	};
+    };
+};
+
 #endif /* __XEN_PUBLIC_VCPU_H__ */
diff --git a/include/xen/interface/xen.h b/include/xen/interface/xen.h
index 5caab46..460a037 100644
--- a/include/xen/interface/xen.h
+++ b/include/xen/interface/xen.h
@@ -83,6 +83,7 @@
 #define VIRQ_DEBUGGER   6  /* (DOM0) A domain has paused for debugging.   */
 #define VIRQ_XENOPROF   7  /* V. XenOprofile interrupt: new sample available */
 #define VIRQ_PCPU_STATE 9  /* (DOM0) PCPU state changed                   */
+#define VIRQ_VGT_GFX	15 /* (DOM0) Used for graphics interrupt          */
 
 /* Architecture-specific VIRQ definitions. */
 #define VIRQ_ARCH_0    16
diff --git a/include/xen/xen-ops.h b/include/xen/xen-ops.h
index f17377f..a69909a 100644
--- a/include/xen/xen-ops.h
+++ b/include/xen/xen-ops.h
@@ -81,4 +81,8 @@ static inline void xen_preemptible_hcall_end(void)
 
 #endif /* CONFIG_PREEMPT */
 
+struct vm_struct * xen_remap_domain_mfn_range_in_kernel(unsigned long mfn,
+        int nr, unsigned domid);
+void xen_unmap_domain_mfn_range_in_kernel(struct vm_struct *area, int nr,
+		unsigned domid);
 #endif /* INCLUDE_XEN_OPS_H */
diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
index df04ed0..bcbdda5 100644
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@ -244,6 +244,7 @@ config BOOTPARAM_SOFTLOCKUP_PANIC_VALUE
 
 config PANIC_ON_OOPS
 	bool "Panic on Oops"
+	default y
 	help
 	  Say Y here to enable the kernel to panic when it oopses. This
 	  has the same effect as setting oops=panic on the kernel command
diff --git a/tools/vgt/Makefile b/tools/vgt/Makefile
new file mode 100644
index 0000000..8ca04b9
--- /dev/null
+++ b/tools/vgt/Makefile
@@ -0,0 +1,6 @@
+all: klog
+
+klog: klog.c
+	$(CC) -o klog klog.c -lpthread
+clean:
+	/bin/rm -rf klog *.o
diff --git a/tools/vgt/README b/tools/vgt/README
new file mode 100644
index 0000000..ac7707f
--- /dev/null
+++ b/tools/vgt/README
@@ -0,0 +1,20 @@
+klog
+----
+
+klog is the userspace app to collect kernel log generated by klog_printk().
+
+To build it, simply:
+
+# make
+
+To use it, you first need to make sure debugfs is mounted in /sys/kernel/debug:
+
+# mount -t debugfs debugfs /sys/kernel/debug
+
+then run klog to start the kernel log collecting:
+
+# ./klog
+# ^C to stop logging
+
+the kernel log will be saved in per-CPU file
+./cpu0 ./cup1 ...
diff --git a/tools/vgt/klog.c b/tools/vgt/klog.c
new file mode 100644
index 0000000..c6497ba
--- /dev/null
+++ b/tools/vgt/klog.c
@@ -0,0 +1,524 @@
+/*
+ * klog - log klog trace data
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * Copyright (C) 2005 - Tom Zanussi (zanussi@us.ibm.com), IBM Corp
+ *
+ * Usage:
+ *
+ * mount -t debugfs debugfs /debug
+ * insmod ./klog-mod.ko
+ * ./klog [-b subbuf-size -n n_subbufs]
+ *
+ * captured output will appear in ./cpu0...cpuN-1
+ *
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <unistd.h>
+#include <string.h>
+#include <signal.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <errno.h>
+#include <sys/mman.h>
+#include <sys/poll.h>
+#include <pthread.h>
+
+/* name of directory containing relay files */
+char *app_dirname = "/sys/kernel/debug/klog";
+/* base name of per-cpu relay files (e.g. /debug/klog/cpu0, cpu1, ...) */
+char *percpu_basename = "cpu";
+/* base name of per-cpu output files (e.g. ./cpu0, cpu1, ...) */
+char *percpu_out_basename = "cpu";
+
+/* maximum number of CPUs we can handle - change if more */
+#define NR_CPUS 256
+
+/* internal variables */
+static size_t subbuf_size = 524288; /* 512K */
+static size_t n_subbufs = 8;
+static unsigned int ncpus;
+static int processing;
+static pthread_mutex_t processing_mutex;
+
+/* per-cpu internal variables */
+static int relay_file[NR_CPUS];
+static int out_file[NR_CPUS];
+static char *relay_buffer[NR_CPUS];
+static pthread_t reader[NR_CPUS];
+
+/* control files */
+static int produced_file[NR_CPUS];
+static int consumed_file[NR_CPUS];
+
+/* per-cpu buffer info */
+static struct buf_status
+{
+	size_t produced;
+	size_t consumed;
+	size_t max_backlog; /* max # sub-buffers ready at one time */
+} status[NR_CPUS];
+
+static void usage(void)
+{
+	fprintf(stderr, "klog [-b subbuf_size -n n_subbufs]\n");
+	exit(1);
+}
+
+/* Boilerplate code below here */
+
+/**
+ *	process_subbufs - write ready subbufs to disk
+ */
+static int process_subbufs(unsigned int cpu)
+{
+	size_t i, start_subbuf, end_subbuf, subbuf_idx, subbufs_consumed = 0;
+	size_t subbufs_ready = status[cpu].produced - status[cpu].consumed;
+	char *subbuf_ptr;
+	size_t padding;
+	int len;
+
+	start_subbuf = status[cpu].consumed % n_subbufs;
+	end_subbuf = start_subbuf + subbufs_ready;
+	for (i = start_subbuf; i < end_subbuf; i++) {
+		subbuf_idx = i % n_subbufs;
+		subbuf_ptr = relay_buffer[cpu] + subbuf_idx * subbuf_size;
+		padding = *((size_t *)subbuf_ptr);
+		subbuf_ptr += sizeof(padding);
+		len = (subbuf_size - sizeof(padding)) - padding;
+		if (write(out_file[cpu], subbuf_ptr, len) < 0) {
+			printf("Couldn't write to output file for cpu %d, exiting: errcode = %d: %s\n", cpu, errno, strerror(errno));
+			exit(1);
+		}
+		subbufs_consumed++;
+	}
+
+	return subbufs_consumed;
+}
+
+/**
+ *	check_buffer - check for and read any available sub-buffers in a buffer
+ */
+static void check_buffer(unsigned cpu)
+{
+	size_t subbufs_consumed;
+
+	lseek(produced_file[cpu], 0, SEEK_SET);
+	if (read(produced_file[cpu], &status[cpu].produced,
+		 sizeof(status[cpu].produced)) < 0) {
+		printf("Couldn't read from consumed file for cpu %d, exiting: errcode = %d: %s\n", cpu, errno, strerror(errno));
+		exit(1);
+	}
+
+	subbufs_consumed = process_subbufs(cpu);
+	if (subbufs_consumed) {
+		if (subbufs_consumed == n_subbufs)
+			fprintf(stderr, "cpu %d buffer full.  Consider using a larger buffer size.\n", cpu);
+		if (subbufs_consumed > status[cpu].max_backlog)
+			status[cpu].max_backlog = subbufs_consumed;
+		status[cpu].consumed += subbufs_consumed;
+		if (write(consumed_file[cpu], &subbufs_consumed,
+			  sizeof(subbufs_consumed)) < 0) {
+			printf("Couldn't write to consumed file for cpu %d, exiting: errcode = %d: %s\n", cpu, errno, strerror(errno));
+			exit(1);
+		}
+	}
+}
+
+/**
+ *	reader_thread - per-cpu channel buffer reader
+ */
+static void *reader_thread(void *data)
+{
+	int rc;
+	unsigned long cpu = (unsigned long)data;
+	struct pollfd pollfd;
+
+	do {
+		pollfd.fd = relay_file[cpu];
+		pollfd.events = POLLIN;
+		rc = poll(&pollfd, 1, -1);
+		if (rc < 0) {
+			if (errno != EINTR) {
+				printf("poll error: %s\n",strerror(errno));
+				exit(1);
+			}
+			printf("poll warning: %s\n",strerror(errno));
+			rc = 0;
+		}
+		pthread_setcancelstate(PTHREAD_CANCEL_DISABLE, NULL);
+		pthread_mutex_lock(&processing_mutex);
+		processing++;
+		pthread_mutex_unlock(&processing_mutex);
+		check_buffer(cpu);
+		pthread_mutex_lock(&processing_mutex);
+		processing--;
+		pthread_mutex_unlock(&processing_mutex);
+		pthread_setcancelstate(PTHREAD_CANCEL_ENABLE, NULL);
+	} while (1);
+}
+
+/**
+ *	control_read - read a control file and return the value read
+ */
+static size_t control_read(const char *dirname,
+			   const char *filename)
+{
+	char tmp[4096];
+	int fd;
+
+	sprintf(tmp, "%s/%s", dirname, filename);
+	fd = open(tmp, O_RDONLY);
+	if (fd < 0) {
+		printf("Couldn't open control file %s\n", tmp);
+		exit(1);
+	}
+
+	if (read(fd, tmp, sizeof(tmp)) < 0) {
+		printf("Couldn't read control file %s: errcode = %d: %s\n",
+		       tmp, errno, strerror(errno));
+		close(fd);
+		exit(1);
+	}
+
+	close(fd);
+
+	return atoi(tmp);
+}
+
+/**
+ *	control_read - write a value to a control file
+ */
+static void control_write(const char *dirname,
+			  const char *filename,
+			  size_t val)
+{
+	char tmp[4096];
+	int fd;
+
+	sprintf(tmp, "%s/%s", dirname, filename);
+	fd = open(tmp, O_RDWR);
+	if (fd < 0) {
+		printf("Couldn't open control file %s\n", tmp);
+		exit(1);
+	}
+
+	sprintf(tmp, "%zu", val);
+
+	if (write(fd, tmp, strlen(tmp)) < 0) {
+		printf("Couldn't write control file %s: errcode = %d: %s\n",
+		       tmp, errno, strerror(errno));
+		close(fd);
+		exit(1);
+	}
+
+	close(fd);
+}
+
+static void summarize(void)
+{
+	int i;
+	size_t dropped;
+
+	printf("summary:\n");
+	for (i = 0; i < ncpus; i++) {
+		printf("  cpu %u:\n", i);
+		printf("    %zu sub-buffers processed\n",
+		       status[i].consumed);
+		printf("    %zu max backlog\n", status[i].max_backlog);
+		printf("    data stored in file ./cpu%d\n", i);
+	}
+
+	dropped = control_read(app_dirname, "dropped");
+	if (dropped)
+		printf("\n    %zu dropped events.\n", dropped);
+}
+
+/**
+ *      create_percpu_threads - create per-cpu threads
+ */
+static int create_percpu_threads(void)
+{
+	unsigned long i;
+
+	for (i = 0; i < ncpus; i++) {
+		/* create a thread for each per-cpu buffer */
+		if (pthread_create(&reader[i], NULL, reader_thread,
+				   (void *)i) < 0) {
+			printf("Couldn't create thread\n");
+			control_write(app_dirname, "enabled", 0);
+			control_write(app_dirname, "create", 0);
+			return -1;
+		}
+	}
+
+	return 0;
+}
+
+/**
+ *      kill_percpu_threads - kill per-cpu threads 0->n-1
+ *      @n: number of threads to kill
+ *
+ *      Returns number of threads killed.
+ */
+static int kill_percpu_threads(int n)
+{
+        int i, killed = 0, err;
+
+        for (i = 0; i < n; i++) {
+                if ((err = pthread_cancel(reader[i])) == 0)
+			killed++;
+		else
+			fprintf(stderr, "WARNING: couldn't kill per-cpu thread %d, err = %d\n", i, err);
+        }
+
+        if (killed != n)
+                fprintf(stderr, "WARNING: couldn't kill all per-cpu threads:  %d killed, %d total\n", killed, n);
+
+        return killed;
+}
+
+/**
+ *	close_control_files - open per-cpu produced/consumed control files
+ */
+static void close_control_files(void)
+{
+	int i;
+
+	for (i = 0; i < ncpus; i++) {
+		if (produced_file[i] > 0)
+			close(produced_file[i]);
+		if (consumed_file[i] > 0)
+			close(consumed_file[i]);
+	}
+}
+
+/**
+ *	open_control_files - open per-cpu produced/consumed control files
+ */
+static int open_control_files(const char *dirname, const char *basename)
+{
+	int i;
+	char tmp[4096];
+
+	for (i = 0; i < ncpus; i++) {
+		sprintf(tmp, "%s/%s%d.produced", dirname, basename, i);
+		produced_file[i] = open(tmp, O_RDONLY);
+		if (produced_file[i] < 0) {
+			printf("Couldn't open control file %s\n", tmp);
+			goto fail;
+		}
+	}
+
+	for (i = 0; i < ncpus; i++) {
+		sprintf(tmp, "%s/%s%d.consumed", dirname, basename, i);
+		consumed_file[i] = open(tmp, O_RDWR);
+		if (consumed_file[i] < 0) {
+			printf("Couldn't open control file %s\n", tmp);
+			goto fail;
+		}
+	}
+
+	return 0;
+fail:
+	close_control_files();
+	return -1;
+}
+
+/**
+ *	open_cpu_files - open and mmap buffer and create output file for a cpu
+ */
+static int open_cpu_files(int cpu, const char *dirname, const char *basename,
+			  const char *out_basename)
+{
+	size_t total_bufsize;
+	char tmp[4096];
+
+	memset(&status[cpu], 0, sizeof(struct buf_status));
+
+	sprintf(tmp, "%s/%s%d", dirname, basename, cpu);
+	relay_file[cpu] = open(tmp, O_RDONLY | O_NONBLOCK);
+	if (relay_file[cpu] < 0) {
+		printf("Couldn't open relay file %s: errcode = %s\n",
+		       tmp, strerror(errno));
+		return -1;
+	}
+
+	sprintf(tmp, "%s%d", out_basename, cpu);
+	if((out_file[cpu] = open(tmp, O_CREAT | O_RDWR | O_TRUNC, S_IRUSR |
+				 S_IWUSR | S_IRGRP | S_IROTH)) < 0) {
+		printf("Couldn't open output file %s: errcode = %s\n",
+		       tmp, strerror(errno));
+		close(relay_file[cpu]);
+		return -1;
+	}
+
+	total_bufsize = subbuf_size * n_subbufs;
+	relay_buffer[cpu] = mmap(NULL, total_bufsize, PROT_READ,
+				 MAP_PRIVATE | MAP_POPULATE, relay_file[cpu],
+				 0);
+	if(relay_buffer[cpu] == MAP_FAILED)
+	{
+		printf("Couldn't mmap relay file, total_bufsize (%ld) = subbuf_size (%ld) * n_subbufs(%ld), error = %s \n", total_bufsize, subbuf_size, n_subbufs, strerror(errno));
+		close(relay_file[cpu]);
+		close(out_file[cpu]);
+		return -1;
+	}
+
+	return 0;
+}
+
+/**
+ *	close_cpu_files - close and munmap buffer and open output file for cpu
+ */
+static void close_cpu_files(int cpu)
+{
+	size_t total_bufsize = subbuf_size * n_subbufs;
+
+	munmap(relay_buffer[cpu], total_bufsize);
+	close(relay_file[cpu]);
+	close(out_file[cpu]);
+}
+
+static void close_app_files(void)
+{
+	int i;
+
+	for (i = 0; i < ncpus; i++)
+		close_cpu_files(i);
+}
+
+static int open_app_files(void)
+{
+	int i;
+
+	for (i = 0; i < ncpus; i++) {
+		if (open_cpu_files(i, app_dirname, percpu_basename,
+				   percpu_out_basename) < 0) {
+			control_write(app_dirname, "enabled", 0);
+			control_write(app_dirname, "create", 0);
+			return -1;
+		}
+	}
+
+	return 0;
+}
+
+int main(int argc, char **argv)
+{
+	extern char *optarg;
+	extern int optopt;
+	int i, c, signal;
+	size_t opt_subbuf_size = 0;
+	size_t opt_n_subbufs = 0;
+	sigset_t signals;
+
+	pthread_mutex_init(&processing_mutex, NULL);
+
+	sigemptyset(&signals);
+	sigaddset(&signals, SIGINT);
+	sigaddset(&signals, SIGTERM);
+	pthread_sigmask(SIG_BLOCK, &signals, NULL);
+
+	while ((c = getopt(argc, argv, "b:n:")) != -1) {
+		switch (c) {
+		case 'b':
+			opt_subbuf_size = (unsigned)atoi(optarg);
+			if (!opt_subbuf_size)
+				usage();
+			break;
+		case 'n':
+			opt_n_subbufs = (unsigned)atoi(optarg);
+			if (!opt_n_subbufs)
+				usage();
+			break;
+		case '?':
+			printf("Unknown option -%c\n", optopt);
+			usage();
+			break;
+		default:
+			break;
+		}
+	}
+
+	if ((opt_n_subbufs && !opt_subbuf_size) ||
+	    (!opt_n_subbufs && opt_subbuf_size))
+		usage();
+
+	if (opt_n_subbufs && opt_n_subbufs) {
+		subbuf_size = opt_subbuf_size;
+		n_subbufs = opt_n_subbufs;
+	}
+
+	ncpus = sysconf(_SC_NPROCESSORS_ONLN);
+
+	control_write(app_dirname, "subbuf_size", subbuf_size);
+	control_write(app_dirname, "n_subbufs", n_subbufs);
+	/* disable logging in case we exited badly in a previous run */
+	control_write(app_dirname, "enabled", 0);
+	fprintf(stderr, "control_write: create\n");
+
+	control_write(app_dirname, "create", 1);
+
+	if (open_app_files())
+		return -1;
+
+	if (open_control_files(app_dirname, percpu_basename)) {
+		close_app_files();
+		return -1;
+	}
+
+	if (create_percpu_threads()) {
+		close_control_files();
+		close_app_files();
+		return -1;
+	}
+
+	control_write(app_dirname, "enabled", 1);
+
+	printf("Creating channel with %lu sub-buffers of size %lu.\n",
+	       n_subbufs, subbuf_size);
+	printf("Logging... Press Control-C to stop.\n");
+
+	sigemptyset(&signals);
+	sigaddset(&signals, SIGINT);
+	sigaddset(&signals, SIGTERM);
+
+	while (sigwait(&signals, &signal) == 0) {
+		switch(signal) {
+		case SIGINT:
+		case SIGTERM:
+			control_write(app_dirname, "enabled", 0);
+			kill_percpu_threads(ncpus);
+			while(1) {
+				pthread_mutex_lock(&processing_mutex);
+				if (!processing) {
+					pthread_mutex_unlock(&processing_mutex);
+					break;
+				}
+				pthread_mutex_unlock(&processing_mutex);
+			}
+			for (i = 0; i < ncpus; i++)
+				check_buffer(i);
+			summarize();
+			close_control_files();
+			close_app_files();
+			control_write(app_dirname, "create", 0);
+			exit(0);
+		}
+	}
+}
diff --git a/tools/vgt/vgt_perf b/tools/vgt/vgt_perf
new file mode 100644
index 0000000..9c080de
--- /dev/null
+++ b/tools/vgt/vgt_perf
@@ -0,0 +1,560 @@
+#!/usr/bin/python
+
+#Copyright (c) 2013, Intel Corporation.
+#
+#This program is free software; you can redistribute it and/or modify it
+#under the terms and conditions of the GNU General Public License,
+#version 2, as published by the Free Software Foundation.
+#
+#This program is distributed in the hope it will be useful, but WITHOUT
+#ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+#FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+#more details.
+#
+#You should have received a copy of the GNU General Public License along with
+#this program; if not, write to the Free Software Foundation, Inc., 
+#51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+import os, sys, re
+import time
+import shlex, subprocess
+import fileinput
+from optparse import OptionParser
+from subprocess import *
+
+parser = OptionParser()
+parser.add_option("-i", "--vmid", dest="vmid", type="int",
+		   help="Specify the instance id to be sampled")
+parser.add_option("-t", "--timeout", dest="timeout", type="int",
+		   help="Timeout in seconds")
+parser.add_option("-v", "--verbose", dest="verbose",
+		   help="pring status message", action="store_true")
+
+(options, args) = parser.parse_args()
+if options.verbose:
+	print (options, args)
+
+#from stackoverflow
+class Tee(object):
+	def __init__(self, *files):
+		self.files = files
+	def write(self, obj):
+		for f in self.files:
+			f.write(obj)
+
+tag = long(time.time())
+e = {}
+e["log_top"] = "/tmp/xengt.top.%ld" % tag
+e["log_xentop"] = '/tmp/xengt.xentop.%ld' % tag
+e["log_gpu"] = '/tmp/xengt.gpu.%ld' % tag
+e["log_file"] = '/tmp/xengt.log.%ld' % tag
+e["log_vm"] = '/tmp/xengt.vm.%ld' % tag
+
+# output to both console and logfile
+e["logf"] = open(e["log_file"], "w")
+e["old_stdout"] = sys.stdout
+sys.stdout = Tee(sys.stdout, e["logf"])
+
+print "TAG: %ld (logfile: %s)" % (tag, e["log_file"])
+
+e["timeout"] = 360 
+if options.timeout:
+	e["timeout"] = options.timeout
+print "Timeout: %d" % e["timeout"]
+
+def err_exit(e, msg):
+	print "Clean up environment on error (%s)" % msg
+	if "p_top" in e:
+		e["p_top"].terminate()
+	if "top_file" in e:	
+		e["top_file"].close()
+	if "p_xentop" in e:
+		e["p_xentop"].terminate()
+	if "xentop_file" in e:
+		e["xentop_file"].close()
+	if "p_gpu" in e:
+		e["p_gpu"].terminate()
+	if "gpu_file" in e:
+		e["gpu_file"].close()
+	sys.stdout = e["old_stdout"]
+	e["logf"].close()
+	sys.exit()
+
+# check environment
+path_vgt = "/sys/kernel/debug/vgt"
+path_gpu = "/sys/kernel/debug/dri/0/i915_cur_delayinfo"
+e["sample_top"] = True
+e["sample_vm"] = True
+e["sample_gpu"] = True
+e["sample_mmio"] = True
+
+if os.path.exists(path_vgt):
+	print "Running in XenGT environment..."
+elif os.path.exists(path_gpu):
+	print "Running in Native or VM environment..."
+	e["sample_vm"] = False
+	e["sample_mmio"] = False
+else:
+	print "Running in VT-d environment"
+	e["sample_gpu"] = False
+	e["sample_mmio"] = False
+
+cpu_num = 0
+cpu_mhz = ""
+for line in fileinput.input("/proc/cpuinfo"):
+	m = re.search("^processor[ \t]*:", line)
+	if m:
+		cpu_num += 1
+		continue
+
+	if not cpu_mhz:
+		m = re.search("cpu MHz[ \t].: (?P<freq>[0-9\.]*)", line)
+		if m:
+			cpu_mhz = m.group("freq")
+			continue
+
+if cpu_num == 0 or not cpu_mhz:
+	err_exit(e, "Failed to get cpu num(%d) and cpu_mhz(%s))" % (cpu_num, cpu_mhz))
+
+e["cpu_num"] = cpu_num
+e["cpu_mhz"] = cpu_mhz
+e["cpu_freq"] = long(float(cpu_mhz) * 1000000)
+
+print "Detecting %d cpus (%sMHz)" % (cpu_num, cpu_mhz)
+
+e["dom_info"] = {}
+if e["sample_vm"]:
+	os.system("xl list > %s" % e["log_vm"])
+	for line in fileinput.input(e["log_vm"]):
+		if line.find("VCPUs") != -1:
+			continue
+
+		m = re.search("(?P<name>[^ ^\t]+)[ \t]*(?P<id>[0-9]+)", line)
+		if not m:
+			err_exit(e, "Confusing VM info: %s" % line)
+
+		e["dom_info"][int(m.group("id"))] = m.group("name")
+
+e["vmid"] = -1
+if options.vmid:
+	e["vmid"] = int(options.vmid)
+
+def read_gen_perf_stat(node):
+	fi = open(path_vgt + '/' + node, "r")
+	s = fi.read()
+	fi.close()
+	return long(s)
+
+def get_gen_stat(gs):
+	gs['context_switch_cycles'] = read_gen_perf_stat('context_switch_cycles')
+	gs['context_switch_num'] = read_gen_perf_stat('context_switch_num')
+	gs['ring_idle_wait'] = read_gen_perf_stat('ring_idle_wait')
+
+def read_vm_perf_stat(vmid, node):
+	fi = open(path_vgt + ('/vm%d' % vmid) + '/perf/' + node, "r")
+	s = fi.read()
+	fi.close()
+	return long(s)
+
+state_nodes = {
+	"Allocated GPU cycles" : {
+		"node"	: "allocated_",
+		"count" : 0,
+		"cycles": 1,
+	},
+	"GTT reads" : {
+		"node"	: "gtt_mmio_r",
+		"count" : 1,
+		"cycles": 1,
+	},
+	"GTT writes" : {
+		"node"	: "gtt_mmio_w",
+		"count" : 1,
+		"cycles": 1,
+	},
+	"PPGTT writes" : {
+		"node"	: "ppgtt_wp_",
+		"count" : 1,
+		"cycles": 1,
+	},
+	"MMIO reads" : {
+		"node"	: "mmio_r",
+		"count" : 1,
+		"cycles": 1,
+	},
+	"MMIO writes" : {
+		"node"	: "mmio_w",
+		"count" : 1,
+		"cycles": 1,
+	},
+	"Ring MMIO reads" : {
+		"node"	: "ring_mmio_r",
+		"count" : 1,
+		"cycles": 0,
+	},
+	"Ring MMIO writes" : {
+		"node"	: "ring_mmio_w",
+		"count" : 1,
+		"cycles": 0,
+	},
+	"Ring tail writes" : {
+		"node"	: "ring_tail_mmio_w",
+		"count" : 1,
+		"cycles": 1,
+	},
+	"CMD scans" : {
+		"node"	: "vring_scan_",
+		"count" : 1,
+		"cycles": 1,
+	},
+	"GP faults" : {
+		"node"	: "vgt_gp_",
+		"count" : 1,
+		"cycles": 1,
+	},
+	"Skipped batch buffers" : {
+		"node"	: "skip_bb_",
+		"count"	: 1,
+		"cycles": 0,
+	},
+	#"PM accesses" : {
+	#	"node" 	: "mmio_pm_",
+	#	"count"	: 1,
+	#	"cycles": 0,
+	#},
+	#"IRQ accesses" : {
+	#	"node" 	: "mmio_irq_",
+	#	"count"	: 1,
+	#	"cycles": 0,
+	#},
+}
+
+def get_vm_stat(vs, vmid):
+	for key in state_nodes.keys():
+		node = state_nodes[key]
+		count = node["node"] + 'cnt'
+		cycles = node["node"] + 'cycles'
+		if node["count"]:
+			vs[count] = read_vm_perf_stat(vmid, count)
+		if node["cycles"]:
+			vs[cycles] = read_vm_perf_stat(vmid, cycles)
+		else:
+			vs[cycles] = 0
+	vs['total_cmds'] = read_vm_perf_stat(vmid, 'total_cmds')
+
+def collect_vm_stat(e):
+	print "Collecting vgt statistics..."
+	s = {}
+	s['general'] = {}
+	get_gen_stat(s['general'])
+
+	for path in os.listdir(path_vgt):
+		m = re.search("^vm(?P<id>[0-9]+)$", path)
+		if not m:
+			continue
+		id = int(m.group("id"))
+		if id in e["dom_info"]:
+			if id in s:
+				err_exit(e, "Instance(vm%d) already exists!!!" % id)
+			if e["vmid"] != -1 and e["vmid"] != id:
+				print "Skip instance vm%d" % id
+				continue
+			print "Find an instance (vm%d), collecting..." % id
+			s[id] = {}
+			get_vm_stat(s[id], id)
+		else:
+			err_exit(e, "Instance(vm%d) not listed before!!!" % id)
+
+	if e["vmid"] != -1 and not e["vmid"] in s:
+		err_exit(e, "Failed to find instance (vm%s)!!!" % e["vmid"])
+
+	return s
+
+def calculate_vm_stat_delta(e, s1, s2, t):
+	if s1.keys() != s2.keys():
+		err_exit(e, "Unmatched VM instances before/after sampling!" + s1.keys() + s2.keys())
+
+	s = {}
+	for key in s1.keys():
+		s[key] = {}
+		for attr in s1[key].keys():
+			s[key][attr] = (s2[key][attr] - s1[key][attr])/t
+	return s
+
+def percentage(v1, v2):
+	residue = (v1 * 100) / v2
+	remainder = (((v1 * 100) % v2) * 100) / v2
+	return "%ld.%02ld%%" % (residue, remainder)
+
+def avg(cnt, cycles):
+	if not cnt:
+		return 0
+	return cycles/cnt
+
+format1 = "%-32s %-16s %-16s %-16s %-8s %-8s"
+format2 = "%-32s %-16ld %-16ld %-16ld %-8s %-8s" 
+bias_vm=20000
+bias_dom0=1500
+
+def print_param2(e, vs, type, bias):
+	if not type in state_nodes:
+		print "Unknown stat (%s)\n" % type
+		return
+
+	count = state_nodes[type]["node"] + 'cnt'
+	cycles = state_nodes[type]["node"] + 'cycles'
+	print format2 % (type, vs[count], vs[cycles],
+			 avg(vs[count], vs[cycles]),
+			 percentage(vs[cycles], e["cpu_freq"]),
+			 percentage(bias * vs[count] + vs[cycles], e["cpu_freq"]))
+
+def print_gpu_cycles(e, vs, bias):
+	type = "Allocated GPU cycles"
+	if not type in state_nodes:
+		print "Unknown stat (%s)\n" % type
+		return
+
+	count = 0
+	cycles = state_nodes[type]["node"] + 'cycles'
+	print format2 % (type, count, vs[cycles],
+			 avg(count, vs[cycles]),
+			 '/', '/')
+
+def print_cmd_stats(e, vs, bias):
+	count = vs['total_cmds']
+	cycles = vs['vring_scan_cycles']
+	avg_cycles = avg(count, cycles)
+	avg_cmds = 0
+	if vs['vring_scan_cnt']:
+		avg_cmds = vs['total_cmds']/vs['vring_scan_cnt']
+	print "Scanned CMDs: %ld" % vs['total_cmds']
+	print "cycles per CMD scan: %ld" % avg_cycles
+	print "CMDs per scan: %ld" % avg_cmds
+	print "Skipped batch buffers: %ld" % vs['skip_bb_cnt'] 
+
+def show_result(e, s, r, time):
+	print "===================================="
+	print "Elapsed time: %ds (%ld cycles), CPU MHz(%s)" % (time, time * e["cpu_freq"], e["cpu_mhz"])
+
+	if e["sample_gpu"]:
+		print "----"
+		print "GPU: %dMHz" % r['gpu_freq']
+
+	if e["sample_vm"]:
+		print "----"
+		for id in e["dom_info"]:
+			print "[xentop]vm%s: %2.2f%%" % (id, r[id])
+
+	if e["sample_top"]:
+		print "----"
+		print "[top]%16s : %2.2f%%" % ("Total CPU%", (100.0 - r['dom0_idle']) * e["cpu_num"])
+		count = 6
+		for cmd, val in sorted(r['procs'].iteritems(), key=lambda(k, v): (v, k), reverse=True):
+			if not count or val == 0:
+				break
+			count -= 1
+			print "[top]%16s : %s%%" % (cmd, val)
+
+	if not e["sample_mmio"]:
+		return
+
+	print "----"
+	gen = s['general']
+	print "Context switches: %ld (%ld cycles, %s)" % (gen['context_switch_num'], gen['context_switch_cycles'], percentage(gen['context_switch_cycles'], time * e["cpu_freq"]))
+	print "Avg context switch overhead: %ld" % avg(gen['context_switch_num'], gen['context_switch_cycles'])
+	print "Avg ring wait idle overhead: %ld" % avg(gen['context_switch_num'], gen['ring_idle_wait'])
+	for id in e["dom_info"]:
+		if e["vmid"] != -1 and e["vmid"] != id:
+			continue
+
+		if id == 0:
+			bias = bias_dom0
+		else:
+			bias = bias_vm
+
+		vs = s[id]
+		print "----------------------------vm%d--------------------" % id
+		print format1 % ("Type", "Count", "Cycles", "Avg", "CPU%", "CPU%(bias)")
+		print format1 % ("----", "----", "----", "----", "----", "----")
+
+		print_gpu_cycles(e, vs, bias)
+		print_param2(e, vs, "MMIO reads", bias)
+		print_param2(e, vs, "MMIO writes", bias)
+		print_param2(e, vs, "GTT reads", bias)
+		print_param2(e, vs, "GTT writes", bias)
+		print_param2(e, vs, "PPGTT writes", bias)
+		#print_param2(e, vs, "PM accesses", bias)
+		#print_param2(e, vs, "IRQ accesses", bias)
+		#print_param2(vs, "Emulations", freq, bias)
+		if id == 0:
+			print_param2(e, vs, "GP faults", 0)
+
+		print "----"
+		print_param2(e, vs, "Ring tail writes", bias)
+		print_param2(e, vs, "CMD scans", bias)
+		print "----"
+		print_cmd_stats(e, vs, bias)
+
+def analyze_output(path, pattern, key):
+	items = []
+	#print pattern
+	for line in fileinput.input(path):
+		m = re.search(pattern, line)
+		if not m:
+			continue
+		items.append(m.group(key))
+		#print m.group(key)
+	return items
+
+def get_avg_util_int(s):
+	count = 0
+	tot = 0
+	for item in s:
+		if item == '0' or item == '0.0':
+			continue
+		tot += int(item)
+		count += 1
+
+	if count:
+		#print tot, count, tot/count
+		return tot / count
+	else:
+		return 0
+
+def get_avg_util_float(s):
+	count = 0
+	tot = 0.0
+	first = 0.0
+	for item in s:
+		if item == '0' or item == '0.0':
+			continue
+		tot += float(item)
+		count += 1
+		if count == 1:
+			first = float(item)
+
+	if count:
+		#print tot, count, tot/count
+		count -= 1
+		tot -= first
+		if count:
+			return tot / count
+		else:
+			return 0
+	else:
+		return 0
+
+def calculate_utilization(e):
+	r = {}
+
+	if e["sample_gpu"]:
+		gpu_freqs = analyze_output(e["log_gpu"], "^CAGF: (?P<freq>[0-9]*)MHz$", "freq")
+		r['gpu_freq'] = get_avg_util_int(gpu_freqs)
+
+	if e["sample_top"]:
+		idle_data = analyze_output(e["log_top"], "Cpu\(s\):.*, (?P<idle>[0-9]+\.[0-9]+)\%id", "idle")
+		r['dom0_idle'] = get_avg_util_float(idle_data)
+		
+		count = 0
+		iter = 0
+		cmds = {}
+		for line in fileinput.input(e["log_top"]):
+			if line.find("top -") >= 0:
+				count += 1
+				compare = False
+				continue
+		
+			if line.find("PID USER") >= 0:
+				compare = True
+				continue
+		
+			if not compare:
+				continue
+		
+			m = re.search(" *\w+ +\w+ +\w+ +[\w\-]+ +[\w\.]+ +[\w\.]+ +[\w\.]+ +\w+ +(?P<cpu>[0-9\.]+) +[0-9\.]+ +[0-9\:\.]+ +(?P<cmd>[^ ]+)", line)
+			if not m:
+				continue;
+		
+			if m.group("cmd") not in cmds:
+				cmds[m.group("cmd")] = []
+			cmds[m.group("cmd")].append(m.group("cpu"))
+		
+		r['procs'] = {}
+		for cmd in cmds.keys():
+			tot = 0
+			for var in cmds[cmd]:
+				tot += int(float(var))
+			r['procs'][cmd] = tot / count
+
+	if not e["sample_vm"]:
+		return r
+
+	for id in e["dom_info"]:
+		r[id] = {}
+	 	dom_utils = analyze_output(e["log_xentop"], "^[ ]+%s[ ]+[a-z\-]+[ ]+[0-9]+[ ]+(?P<cpu>[0-9]+\.[0-9]+)" % e["dom_info"][id], "cpu")
+
+		r[id] = get_avg_util_float(dom_utils)
+		#print key, r[key]
+
+	return r
+
+stat1 = {}
+stat2 = {}
+stat = {}
+count = 0
+#fi = open("/sys/kernel/debug/dri/0/i915_cur_delayinfo", "r")
+#freq += read_freq(fi)
+
+print "Wait for %d seconds..." % e["timeout"]
+
+if e["sample_top"]:
+	line = "/usr/bin/top -b -d 1"
+	args = shlex.split(line)
+	e["top_file"] = open(e["log_top"], "w")
+	e["p_top"] = subprocess.Popen(args, stdout = e["top_file"])
+
+if e["sample_vm"]:
+	line = "/usr/sbin/xentop -b -d 1"
+	args = shlex.split(line)
+	e["xentop_file"] = open(e["log_xentop"], "w")
+	e["p_xentop"] = subprocess.Popen(args, stdout = e["xentop_file"])
+
+if e["sample_gpu"]:
+	line = "/bin/sh /usr/bin/gpu_freq"
+	args = shlex.split(line)
+	e["gpu_file"] = open(e["log_gpu"], "w")
+	e["p_gpu"] = subprocess.Popen(args, stdout = e["gpu_file"])
+
+if e["sample_mmio"]:
+	stat1 = collect_vm_stat(e)
+
+t1 = time.time()
+try:
+	time.sleep(e["timeout"])
+except KeyboardInterrupt:
+	print "Interrupted"
+t2 = time.time()
+
+if e["sample_mmio"]:
+	stat2 = collect_vm_stat(e)
+
+if e["sample_top"]:
+	e["p_top"].terminate()
+	e["top_file"].close()
+if e["sample_vm"]:
+	e["p_xentop"].terminate()
+	e["xentop_file"].close()
+if e["sample_gpu"]:
+	e["p_gpu"].terminate()
+	e["gpu_file"].close()
+
+print "Analyze result...\n\n\n"
+elapsed = int(t2 - t1)
+if e["sample_mmio"]:
+	stat = calculate_vm_stat_delta(e, stat1, stat2, elapsed)
+
+stat_r = calculate_utilization(e)
+show_result(e, stat, stat_r, elapsed)
+
+sys.stdout = e["old_stdout"]
+e["logf"].close()
diff --git a/tools/vgt/vgt_report b/tools/vgt/vgt_report
new file mode 100644
index 0000000..e4d5c80
--- /dev/null
+++ b/tools/vgt/vgt_report
@@ -0,0 +1,376 @@
+#!/usr/bin/python
+
+#Copyright (c) 2011, Intel Corporation.
+#
+#This program is free software; you can redistribute it and/or modify it
+#under the terms and conditions of the GNU General Public License,
+#version 2, as published by the Free Software Foundation.
+#
+#This program is distributed in the hope it will be useful, but WITHOUT
+#ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+#FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+#more details.
+#
+#You should have received a copy of the GNU General Public License along with
+#this program; if not, write to the Free Software Foundation, Inc.,
+#51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+import os, sys, re
+from optparse import OptionParser
+from subprocess import *
+
+parser = OptionParser()
+parser.add_option("-i", "--info", dest="reginfo", action="store_true",
+		   help="show info/stat for accessed registers", default=True)
+parser.add_option("-r", "--regname", dest="regname", action="store_true",
+		   help="show the list of register names")
+parser.add_option("-f", "--file", dest="filename", metavar="FILE",
+		   default="drivers/gpu/drm/i915/vgt/reg.h",
+		   help="the location of vgt/reg.h (default drivers/gpu/drm/i915/vgt/reg.h)")
+parser.add_option("-u", "--untracked", dest="untrack", action="store_true",
+		   help="show the list of untracked registers")
+parser.add_option("-U", "--unused", dest="unuse", action="store_true",
+		   help="show the list of tracked but unused registers")
+parser.add_option("-d", "--vdiff", dest="vdiff", action="store_true",
+		   help="show the list of registers with different vreg values among VMs")
+parser.add_option("-s", "--sdiff", dest="sdiff", action="store_true",
+		   help="show the list of registers with different sreg values among VMs")
+parser.add_option("-p", "--preg", dest="preg", action="store_true",
+		   help="dump the physical HW registers with --vdiff and --sdiff")
+parser.add_option("-g", "--gpuaccess", dest="gpuaccess", action="store_true",
+		   help="show the list of gpu-accessed registers")
+parser.add_option("-c", "--ctxswitch", dest="ctxswitch", action="store_true",
+		   help="show the list of registers saved/restored at ctx switch time")
+parser.add_option("-v", "--verbose", dest="verbose",
+		   help="pring status message", action="store_true")
+
+(options, args) = parser.parse_args()
+if options.verbose:
+	print (options, args)
+
+if options.verbose:
+	print "Use %s as the reg file" % options.filename
+fi = open(options.filename, "r")
+s = fi.read()
+fi.close()
+
+path_reginfo = "/sys/kernel/debug/vgt/reginfo"
+path_preg = "/sys/kernel/debug/vgt/preg"
+path_debugfs = "/sys/kernel/debug/vgt/"
+path_vreg = "/virtual_mmio_space"
+path_sreg = "/shadow_mmio_space"
+
+# Collect reg name
+reginfo = {}
+for line in s.split("\n"):
+	m = re.search("^#define[ \t]+_REG_[A-Z_]+", line)
+	if not m:
+		continue
+	m = re.search("^#define[ \t]+_REG_(?P<name>[0-9A-Z_]+)[ \t]+(?P<offset>0x[0-9a-zA-Z]+)", line)
+
+	if not m or m.group("name") == "INVALID":
+		continue
+	offset = int(m.group("offset"), 16)
+	if not offset in reginfo:
+		reginfo[offset] = {}
+		reginfo[offset]["name"] = m.group("name")
+	elif reginfo[offset]["name"].find(m.group("name")) == -1:
+		reginfo[offset]["name"] += " | " + m.group("name")
+		if options.verbose:
+			print "find multiple names for regs", hex(offset), reginfo[offset]["name"]
+	#print hex(offset), reginfo[offset]["name"]
+
+# get info for each access reg
+fi = open(path_reginfo, "r")
+info = fi.read()
+fi.close()
+i = 0
+for line in info.split("\n"):
+	if len(line.split(":")) == 1 or line.find("Reg:") != -1:
+		continue
+	m = re.search("^[ \t]*(?P<reg>[0-9a-zA-Z]+):[ \t]*(?P<flags>[0-9a-zA-Z]+) ", line)
+	if not m:
+		continue
+	reg = int(m.group("reg"), 16)
+	if not reg in reginfo:
+		reginfo[reg] = {}
+		reginfo[reg]["name"] = ""
+	flags = int(m.group("flags"), 16)
+
+	if reg >= 0x140000 and reg < 0x180000:
+		continue
+
+	if reg >= 0x78000 and reg < 0x79000:
+		continue
+
+	reginfo[reg]["Valid"] = True
+	reginfo[reg]["Owner"] = "N/A"
+	i += 1
+	if (flags & 0xf == 0):
+		reginfo[reg]["Owner"] = "None"
+	elif (flags & 0xf == 4):
+		reginfo[reg]["Owner"] = "RDR"
+	elif (flags & 0xf == 5):
+		reginfo[reg]["Owner"] = "DPY"
+	elif (flags & 0xf == 6):
+		reginfo[reg]["Owner"] = "PM"
+	elif (flags & 0xf == 7):
+		reginfo[reg]["Owner"] = "MGMT"
+
+	reginfo[reg]["Type"] = "N/A"
+	if (flags & 0xf != 0):
+		reginfo[reg]["Type"] = "MPT"
+	elif (flags & (1 << 4)):
+		reginfo[reg]["Type"] = "PT"
+	elif (flags & (1 << 7)):
+		reginfo[reg]["Type"] = "Virt"
+
+	if (flags & (1 << 5)):
+		reginfo[reg]["AddressFix"] = True;
+	if (flags & (1 << 6)):
+		reginfo[reg]["HwStatus"] = True;
+	if (flags & (1 << 8)):
+		reginfo[reg]["ModeMask"] = True;
+	if (flags & (1 << 10) == 0):
+		reginfo[reg]["Untracked"] = True;
+	if (flags & (1 << 11)):
+		reginfo[reg]["Accessed"] = True;
+	if (flags & (1 << 12)):
+		reginfo[reg]["Saved"] = True;
+	if (flags & (1 << 14)):
+		reginfo[reg]["CmdAccess"] = True;
+	reginfo[reg]["Flags"] = flags;
+print "Total %d registers reported" % i
+
+def get_reg_attr(reg):
+	out = ""
+	if "AddressFix" in reg:
+		out += " AF"
+	if "HwStatus" in reg:
+		out += " HW"
+	if "ModeMask" in reg:
+		out += " MD"
+	return out
+
+def get_reg_state(reg):
+	out = ""
+	if "Untracked" in reg:
+		out += " u"
+	elif not "Accessed" in reg:
+		out += " U"
+	if "CmdAccess" in reg:
+		out += " G"
+	return out
+
+def show_reginfo():
+	print "===================================="
+	print "Owner Type:"
+	print "\tNone, RDR(Render), DPY(Display), PM, MGMT(Management)"
+	print "Type:"
+	print "\tVIRT - default virtualized"
+	print "\tMPT - Mediated Pass-Through based on owner type"
+	print "\tPT - Pass-through to any VM, for read-only access"
+	print "\tBOOT - pass-through to dom0 at boot time. Otherwise virtualized"
+	print "Attributes:"
+	print "\tAF - Address check required"
+	print "\tHW - Contain HW updated status bit"
+	print "\tMD - High 16bits as mask for change"
+	print "State:"
+	print "\tu - Untracked"
+	print "\tD - Different value among VMs"
+	print "\tG - Accessed by GPU CMDs"
+	print "\tU - Tracked but unused"
+
+	print "\n%10s: %5s|%5s|%12s|%8s|%-8s" % ("Reg", "Owner", "Type", "Attributes", "State", "Name")
+	print "------------------------------------"
+
+	i = 0
+	for reg in sorted(reginfo):
+		if not "Accessed" in reginfo[reg]:
+			continue
+		print "%10s: %5s|%5s|%12s|%8s|%s" % (hex(reg), reginfo[reg]["Owner"], reginfo[reg]["Type"], get_reg_attr(reginfo[reg]), get_reg_state(reginfo[reg]), reginfo[reg]["name"])
+		i += 1
+	print "Total %d registers" % i
+	print "===================================="
+
+def show_regname():
+	print "=============Reg Name==============="
+	print "\"A\": the reg is accessed"
+	print "------------------------------------"
+	i = 0
+	for reg in sorted(reginfo):
+		if "Accessed" in reginfo[reg]:
+			ac = "A"
+		else:
+			ac = " "
+		print "%10s(%s): %s" % (hex(reg), ac, reginfo[reg]["name"])
+		i += 1
+	print "Total %d registers" % i
+	print "===================================="
+
+def show_untracked():
+	print "===========Untracked Regs==========="
+	i = 0
+	for reg in sorted(reginfo):
+		if not "Valid" in reginfo[reg]:
+			continue;
+
+		if "Untracked" in reginfo[reg]:
+			if "CmdAccess" in reginfo[reg]:
+				print "[G]%10s: %s" %(hex(reg), reginfo[reg]["name"])
+			else:
+				print "%10s: %s" %(hex(reg), reginfo[reg]["name"])
+			i += 1
+	print "Total %d registers" % i
+	print "===================================="
+
+def show_unused():
+	print "===========Unused Regs==========="
+	i = 0
+	for reg in sorted(reginfo):
+		if not "Valid" in reginfo[reg]:
+			continue;
+
+		if not "Accessed" in reginfo[reg] and not "Untracked" in reginfo[reg]:
+			print "%10s: %s" %(hex(reg), reginfo[reg]["name"])
+			i += 1
+	print "Total %d registers" % i
+	print "===================================="
+
+def show_gpu_access():
+	print "===========GPU-accessed Regs==========="
+	i = 0
+	for reg in sorted(reginfo):
+		if not "Valid" in reginfo[reg]:
+			continue;
+
+		if "CmdAccess" in reginfo[reg]:
+			print "%10s: %20s" %(hex(reg), reginfo[reg]["name"])
+			i += 1
+	print "Total %d registers" % i
+	print "===================================="
+
+def get_reginfo(output):
+	info = {}
+	for line in output.stdout.read().split("\n"):
+		if not line:
+			continue
+
+		if line.find(":") < 0:
+			continue
+
+		reg_base = line.split(":")[0]
+		vars = line.split(":")[1].strip().split(" ")
+		if len(vars) != 16:
+			print "messed values:", vars, line
+			continue
+
+		for i in range(16):
+			reg = int(reg_base, 16) + i*4
+			info[reg] = vars[i]
+	return info
+
+def show_diff(path_reg):
+	print "=============Diff List=============="
+	print "Collecting VM vReg information..."
+	dirinfo = Popen(["ls", path_debugfs], stdout=PIPE)
+	#print dirinfo.stdout.read().split("\n")
+	vminfo = {}
+	num = 0
+	outputs = {}
+	preginfo = {}
+	for node in dirinfo.stdout.read().split("\n"):
+		m = re.search("^(?P<vmid>vm[0-9]+)$", node)
+		if not m:
+			continue
+		vmid = m.group("vmid")
+		if not vmid in vminfo:
+			print "Found %s..." % vmid
+			vminfo[vmid] = {}
+		else:
+			print "Found dupliacted vm instance: " + vmid
+		outputs[vmid] = Popen(["cat", path_debugfs + vmid + path_reg], stdout=PIPE)
+
+	if options.preg:
+		print "Get preg info..."
+		p_output = Popen(["cat", path_preg], stdout=PIPE)
+
+	num = len(vminfo)
+	for vmid in outputs.keys():
+		print "Analyze %s..." % vmid
+		vminfo[vmid] = get_reginfo(outputs[vmid])
+	if options.preg:
+		print "Analyze preg info..."
+		preginfo = get_reginfo(p_output)
+
+	print "Calculating difference among %d vm instances." % num
+	print "------------------------------------"
+	title = "%10s: |" % "Reg"
+	title += "%5s|" % "Type"
+	title += "%5s|" % "Saved"
+	for i in range(num):
+		title += "   VM%-4s |" % i
+	if options.preg:
+		title += "   Preg   |"
+	title += " %-8s" % "Name"
+	print title
+
+	vm0 = vminfo.keys()[0]
+	cnt = 0
+	for reg in sorted(vminfo[vm0]):
+		if not reg in reginfo or not "Accessed" in reginfo[reg]:
+			continue;
+
+		val = vminfo[vm0][reg]
+		found = False
+		for vm in vminfo.keys():
+			if val != vminfo[vm][reg]:
+				found = True
+		if found:
+			cnt += 1
+			line = "%10x: |" % reg
+			line += "%5s|" % reginfo[reg]["Type"]
+			if "Saved" in reginfo[reg]:
+				line += "%5s|" % "Y"
+			else:
+				line += "%5s|" % " "
+			for i in sorted(vminfo.keys()):
+				line += " %8s |" % vminfo[i][reg]
+			if options.preg:
+				line += " %8s |" % preginfo[reg]
+			if reg in reginfo:
+				line += " %-8s" % reginfo[reg]["name"]
+			print line
+	print "Total %d registers found." % cnt
+	print "===================================="
+
+def show_saved():
+	print "=========Saved/Restored Regs========"
+	i = 0
+	print "%10s: %5s: %s" % ("Reg", "Owner", "Name")
+	for reg in sorted(reginfo):
+		if not "Accessed" in reginfo[reg]:
+			continue;
+
+		if "Saved" in reginfo[reg]:
+			print "%10s: %5s: %s" %(hex(reg), reginfo[reg]["Owner"], reginfo[reg]["name"])
+			i += 1
+	print "Total %d registers" % i
+	print "===================================="
+
+if options.regname:
+	show_regname()
+elif options.untrack:
+	show_untracked()
+elif options.unuse:
+	show_unused()
+elif options.vdiff:
+	show_diff(path_vreg)
+elif options.sdiff:
+	show_diff(path_sreg)
+elif options.ctxswitch:
+	show_saved()
+elif options.gpuaccess:
+	show_gpu_access()
+else:
+	show_reginfo()
diff --git a/vgt_mgr b/vgt_mgr
new file mode 100644
index 0000000..afc26fe
--- /dev/null
+++ b/vgt_mgr
@@ -0,0 +1,158 @@
+#!/bin/bash
+
+get_port_name() {
+	type=`echo $1|cut -c7-`
+	port_name=`cat $i915_debugfs_dir/i915_display_info|sed -n '/Connector info/,$p'|grep $type|awk '{print $6}'|sed 's/,//g'`
+	if [ x"$port_name" != x"PORT_A" ] && [ x"$port_name" != x"PORT_B" ] &&\
+		[ x"$port_name" != x"PORT_C" ] && [ x"$port_name" != x"PORT_D" ] && [ x"$port_name" != x"PORT_E" ]; then
+		port_name=unknown
+	fi
+}
+
+get_drm_name () {
+	drm_name=card0-`cat $i915_debugfs_dir/i915_display_info|sed -n '/Connector info/,$p'|grep $1|grep $2|awk '{print $4}'|sed 's/,//g'`
+	if [ x"$drm_name" == x"card0-" ]; then
+		drm_name=unknown
+	fi
+}
+
+
+wait_for_edid () {
+	EDID=""
+	for try in 1 2 3 4 5 6 7 8 9 10
+	do
+		EDID=`cat $1`
+		if [ -z "$EDID" ]; then
+			sleep 1;
+		else
+			break;
+		fi
+	done
+
+	if [ -z "$EDID" ]; then
+		echo "Did not get EDID for a connected monitor from $1! Hotplug may not work properly!"
+	fi
+}
+
+vgt_dir=/sys/kernel/vgt
+i915_debugfs_dir=/sys/kernel/debug/dri/64
+
+if [ -e ${i915_debugfs_dir}/i915_display_info ]; then
+	:
+else
+	echo "vGT warning:(vgt_mgr $1 $2 $3) vGT: debugfs node i915_display_info does not exist!" > /dev/kmsg
+fi
+#######################################################################
+############################ --help ###################################
+#######################################################################
+if [ x"$1" == x"--help" ]; then
+	echo Usage: vgt_mgr --command param1 param2
+	exit 0
+fi
+
+
+#######################################################################
+############################ --version ################################
+#######################################################################
+if [ x"$1" == x"--version" ]; then
+	echo 1.0
+	exit 0
+fi
+
+
+#######################################################################
+############################ --get port name ##########################
+#######################################################################
+if [ x"$1" == x"--get-port-name" ]; then
+	get_port_name $2
+	echo $port_name
+	exit 0
+fi
+
+
+#######################################################################
+############################ --get drm name ###########################
+#######################################################################
+if [ x"$1" == x"--get-drm-name" ]; then
+	get_drm_name $2 $3
+	echo $drm_name
+	exit 0
+fi
+
+
+#######################################################################
+############################ --detect display #########################
+#######################################################################
+
+if [ x"$1" == x"--detect-display" ]; then
+	check_src2=0
+	type=0
+	connect=0
+	filelist=`ls $vgt_dir`
+	for guest in $filelist
+	do
+		if [ x"${guest:0:2}" != x"vm" ]; then
+			continue
+		fi
+
+		if [ x"$2" == x"PORT_A" ]; then
+			src1=/sys/class/drm/card0-eDP-1
+			target=$vgt_dir/$guest/PORT_A
+			type=1
+		elif [ x"$2" == x"PORT_B" ]; then
+			get_drm_name $2 DP
+			src1=/sys/class/drm/$drm_name
+			get_drm_name $2 HDMI
+			src2=/sys/class/drm/$drm_name
+			target=$vgt_dir/$guest/PORT_B
+			check_src2=1
+			type=2
+		elif [ x"$2" == x"PORT_C" ]; then
+			get_drm_name $2 DP
+			src1=/sys/class/drm/$drm_name
+			get_drm_name $2 HDMI
+			src2=/sys/class/drm/$drm_name
+			target=$vgt_dir/$guest/PORT_C
+			check_src2=1
+			type=3
+		elif [ x"$2" == x"PORT_D" ]; then
+			get_drm_name $2 DP
+			src1=/sys/class/drm/$drm_name
+			get_drm_name $2 HDMI
+			src2=/sys/class/drm/$drm_name
+			target=$vgt_dir/$guest/PORT_D
+			check_src2=1
+			type=4
+		elif [ x"$2" == x"PORT_E" ]; then
+			src1=/sys/class/drm/card0-VGA-1
+			target=$vgt_dir/$guest/PORT_E
+			type=0
+		fi
+
+		if [ ! -d $target ]; then
+			continue
+		fi
+
+		if [ -d $src1 ] && [ `cat $src1/status` == "connected" ]; then
+			wait_for_edid $src1/edid
+			dd if=$src1/edid of=$target/edid bs=128 count=1
+			echo $type > $target/type
+			connect=1
+		fi
+
+		if [ -d $src2 ] && [ $check_src2 -eq 1 ] && [ `cat $src2/status` == "connected" ]; then
+			wait_for_edid $src2/edid
+			dd if=$src2/edid of=$target/edid bs=128 count=1
+			type=`expr $type + 3`
+			echo $type > $target/type
+			connect=1
+		fi
+
+		if [ $connect -eq 1 ]; then
+			echo "connect" > $target/connection
+		else
+			echo "disconnect" > $target/connection
+		fi
+	done
+	exit 0
+fi
